title: 'SphNet: A Spherical Network for Semantic Pointcloud Segmentation'
abstract: 'Semantic segmentation for robotic systems can enable a wide range of applications,
  from self-driving cars and augmented reality systems to domestic robots. We argue
  that a spherical representation is a natural one for egocentric pointclouds. Thus,
  in this work, we present a novel framework exploiting such a representation of LiDAR
  pointclouds for the task of semantic segmentation. Our approach is based on a spherical
  convolutional neural network that can seamlessly handle observations from various
  sensor systems ( e.g., different LiDAR systems) and provides an accurate segmentation
  of the environment. We operate in two distinct stages: First, we encode the projected
  input pointclouds to spherical features. Second, we decode and back-project the
  spherical features to achieve an accurate semantic segmentation of the pointcloud.
  We evaluate our method with respect to state-of-the-art projection-based semantic
  segmentation approaches using well-known public datasets. We demonstrate that the
  spherical representation enables us to provide more accurate segmentation and to
  have a better generalization to sensors with different ﬁeld-of-view and number of
  beams than what was seen during training.'
abstract_is_verbatim: true
segmented_markdown: '# SphNet: A Spherical Network for Semantic Pointcloud Segmentation


  ## Abstract

  <block id="0">

  Semantic segmentation for robotic systems can enable a wide range of applications,
  from self-driving cars and augmented reality systems to domestic robots. We argue
  that a spherical representation is a natural one for egocentric pointclouds. Thus,
  in this work, we present a novel framework exploiting such a representation of LiDAR
  pointclouds for the task of semantic segmentation. Our approach is based on a spherical
  convolutional neural network that can seamlessly handle observations from various
  sensor systems ( e.g., different LiDAR systems) and provides an accurate segmentation
  of the environment. We operate in two distinct stages: First, we encode the projected
  input pointclouds to spherical features. Second, we decode and back-project the
  spherical features to achieve an accurate semantic segmentation of the pointcloud.
  We evaluate our method with respect to state-of-the-art projection-based semantic
  segmentation approaches using well-known public datasets. We demonstrate that the
  spherical representation enables us to provide more accurate segmentation and to
  have a better generalization to sensors with different ﬁeld-of-view and number of
  beams than what was seen during training.


  </block>

  ## I. INTRODUCTION

  <block id="1">

  Over the past years, there has been a growing demand in robotics and self-driving
  cars for reliable semantic segmentation of the environment, i.e., associating a
  class or label with each measurement sample for a given input modality. A semantic
  understanding of the surroundings is a critical aspect of robot autonomy. It has
  the potential to, e.g., enable a comprehensive description of the navigational risks
  or disambiguate challenging situations in planning or mapping. For many of the currently
  employed robotic systems, the long-term stability of maps is a pertaining issue
  due to the often limited metrical understanding of the environment for which high-level
  semantic information is a possible solution.


  With the advances in deep learning, vision-based semantic segmentation frameworks
  have become a very mature field. While there has also been significant progress
  on LiDAR-based semantic segmentation frameworks, it is still not as developed as
  their vision-based counterpart.


  Nevertheless, LiDAR-based approaches have certain crucial advantages over other
  modalities as they are unaffected by the illumination conditions of the environment.
  This is in contrast to cameras which provide crucial descriptive information but
  are heavily affected by poor lighting conditions. Consequently, LiDAR-based systems
  effectively provide a more resilient segmentation system for a variety of challenging
  scenarios, such as operating at night and dynamically changing lighting conditions.


  Many existing approaches operate using projection models, which typically transform
  the irregular pointcloud data into an ordered 2D domain, allowing them to utilize
  the extensive research available for images. The downside is that this requires
  a predefined configuration based on the number of beams, angular resolution, and
  vertical Field-of-View (FoV). LiDAR systems differ in these properties, which means
  that changing the sensory system after training might yield projections with geometrical
  and structural scarcity. Consequently, the resulting projection is often insufficient
  to express the complexity of arbitrary environments accurately. Accordingly, utilizing
  these approaches in generic environments with an arbitrary sensor system is often
  impossible without refining the initial network on the data from the new sensor
  environment.


  LiDAR sensors are known to yield accurate geometrical and structural cues. Thus,
  modern LiDAR sensors often provide large FoV measurements to precisely measure the
  robot’s surroundings. However, the projection onto the 2D domain of such large FoV
  scans introduces distortions that deform the physical dimensions the environment.
  Thus, dealing with different FoV, sensor frequencies, and scales remains an open
  research problem for which the input representation constitutes a major factor.
  In recent years, LiDAR sensors have become more affordable and available, becoming
  abundant in the context of robotics. However, many state-of-the-art segmentation
  methods are often limited to a particular sensor and cannot benefit from multiple
  LiDARs due to their representation of the input. For example, systems employing
  multiple LiDARs pointing in different directions are typically processed sequentially
  or using multiple instances of the same network, one for each sensor. However, more
  crucial insights and structural dependencies in overlapping areas can be considered
  by jointly predicting the segmentation using all available sensors.


  In this work, we propose a framework that takes LiDAR scans as input, projects them
  onto a sphere, and utilizes a spherical Convolutional Neural Network (CNN) for the
  task of semantic segmentation. The projection of the LiDAR scans onto the sphere
  does not introduce any distortions and is independent of the utilized LiDAR, thus,
  yielding an agnostic representation for various LiDAR systems with different vertical
  FoV. We adapt the structure of common 2D encoder and decoder networks and support
  simultaneous training on different datasets obtained with varying LiDAR sensors
  and parameters without having to adapt our configuration. Moreover, since our approach
  is invariant to rotations due to the spherical representation, we support arbitrarily
  rotated input pointclouds. In summary, the key contributions of this paper are as
  follows:

  - A spherical end-to-end pipeline for semantic segmentation supporting various input
  configurations.

  - A spherical encoder-decoder structure including a spectral pooling and unpooling
  operation for SO(3) signals.


  </block>

  ## II. RELATED WORK

  <block id="2">

  Methods using a LiDAR have to deal with the inherent sparsity and irregularity of
  the data in contrast to vision-based approaches. Moreover, LiDAR-based methods have
  various choices on how to represent the input data [1], including, directly using
  the pointcloud [2], [3], voxel-based [4]–[6] or projection-based [7]–[10] representations.
  The selection of the input representation, which yields the best performance for
  a specific task, however, still remains an open research question.


  Direct methods such as PointNet [2], [3] operate on the raw unordered pointcloud
  and extract local contextual features using point convolutions [11]. Voxel-based
  approaches [4], [5], [12] keep all the geometric understanding of the environment
  and can readily accumulate multiple scans either chronologically or from different
  sensors. SpSequenceNet [13] explicitly uses 4D pointclouds and considers the temporal
  information between consecutive scans. However, it is evident that the computational
  complexity of voxel-based approaches is high due to their high-dimensional convolutions,
  and their accuracy and performance are directly linked to the chosen voxel size,
  which resulted in works that organize the pointclouds into an Octree, Kdtree, etc.
  [14] for efficiency. Furthermore, instead of using a cartesian grid, PolarNet [15]
  discretizes the space using a polar grid and shows superior quality.


  A different direction of research is offered by graph-based approaches [16] which
  can seamlessly model the irregular structure of pointclouds though more experimental
  directions in terms of graph building, and network design are still to be addressed.


  Projection-based methods differ from other approaches by transforming the pointcloud
  into a specific domain, such as 2D images, which the majority of projection-based
  methods [7]–[10] rely on. Furthermore, projections to 2D images are appealing as
  it enables leveraging all the research in image-based deep learning but generally
  need to rely on the limited amount of labeled pointcloud data. Hence, the work of
  Wu et al. [17] tackles the deficiency in labeled pointcloud data by using domain-adaption
  between synthetic and real-world data.


  The downsides of the projection onto the 2D domain are: i) the lack of a detailed
  geometric understanding of the environment and ii) the large FoV of LiDARs, which
  produces significant distortions, decreasing the accuracy of these methods. Hence,
  recent approaches have explored using a combination of several representations [6],
  [18] and convolutions [19]. Recent works [20], [21] additionally learn and extract
  features from a Bird’s Eye View projection that would otherwise be difficult to
  retain with a 2D projection.


  In contrast to 2D image projections, projecting onto the sphere is a more suitable
  representation for such large FoV sensors. Recently, spherical CNNs [22]–[24] have
  shown great potential for, e.g., omnidirectional images [25], [26] and cortical
  surfaces [27], [28].


  Moreover, Lohit et al. [25] proposes an encoder-decoder spherical network design
  that is rotation-invariant by performing a global average pooling of the encoded
  feature map. However, their work discards the rotation information of the input
  signals and thus, needs a special loss that includes a spherical correlation to
  find the unknown rotation w.r.t. the ground truth labels.


  Considering the findings above, we propose a composition of spherical CNNs, based
  on the work of Cohen et al. [23], that semantically segments pointclouds from various
  LiDAR sensor configurations.


  </block>

  ## III. SPHERICAL SEMANTIC SEGMENTATION

  <block id="3">

  This section describes the core modules of our spherical semantic segmentation framework,
  which mainly operates in three stages: i) feature projection, ii) semantic segmentation,
  and iii) back-projection.


  Initially, we discuss the projection of LiDAR pointclouds onto the unit sphere and
  the feature representation that serves as input to the spherical CNN. Next, we describe
  the details of our network design and architecture used to learn a semantic segmentation
  of LiDAR scans.


  </block>

  ### A. Sensor Projection and Feature Representation

  <block id="4">

  Initially, the input to our spherical segmentation network is a signal defined on
  the sphere S2 = { p ∈ R3 | ∥p∥2 = 1 }, with the parametrization as proposed by Healy
  et al. [29], i.e. ω(φ,θ) = [cos φ sinθ, sinφ sinθ, cosθ]⊤, where ω ∈ S2, and φ ∈
  [0, 2π] and θ ∈ [0,π] are azimuthal and polar angle, respectively.


  We then operate in an end-to-end fashion by transforming the input modality (i.e.,
  the pointcloud scan) into a spherical representation in S2. Consequently, as an
  initial step, a pointcloud is projected onto the unit sphere, e.g. the jth point
  pj = [xj,yj,zj]⊤ is projected using

  φj = arctan(yj / xj), θj = arccos(zj / ||pj||2).


  From the LiDAR projection, we sample the range values using an equiangular grid
  complying with the sampling theorem by Driscoll and Healy [30]. It is important
  to note that we omit the sampling of the intensity/remission values of the LiDAR
  scans since their values significantly fluctuate between LiDAR sensors, particularly
  between different manufacturers. Thus, would require additional investigation and
  further calibration of the input data.


  Although in this work, we only utilize measurements from a LiDAR scanner, our approach
  is not limited to this sensor type. Other sensory systems, such as multi-camera
  systems, thermal cameras, and depth sensory systems, can seamlessly be integrated
  by performing the same spherical projection. Additionally, since all sensors would
  share the same feature representation, our approach readily facilitates various
  combinations of heterogeneous sensory types [26]. Moreover, despite utilizing only
  LiDAR sensors defined over 360◦, our sampling approach is agnostic to the resolution
  and field-of-view of the sensor and can be used with arbitrary viewpoint coverage.


  Finally, after the projection and sampling, the spherical network will receive the
  LiDAR scan as a feature vector ∈ R2×2BW×2BW, where BW corresponds to the spherical
  bandwidth used for the equiangular sampling [30]. The chosen bandwidth directly
  controls the employed spatial discretization and, consequently, the spectrum’s resolution
  in the frequency domain. In particular, the higher the bandwidth, the finer the
  spectral resolution, and more memory is required, which needs to be carefully considered
  when designing the spherical network, e.g., the first convolutional layer is designed
  to operate on the exact BW as used by the sampling but later layers will decrease
  the bandwidth. In the following, we discuss the design of the remaining layers in
  our network.


  </block>

  ### B. Network Design and Architecture

  <block id="5">

  In this section, we will discuss the design choices of our network that takes LiDAR
  scans as input and is able to learn a semantic segmentation of it. The input of
  the base network will be the sampled spherical features from the LiDAR pointcloud,
  i.e., the range values. We found that an initial spherical bandwidth between 50
  - 120 yields a good trade-off between accuracy and memory consumption.


  Overall, the network design is based on a spherical encoder-decoder structure. The
  spherical encoder network increases the number of features from layer to layer while
  the bandwidth is decreased instead. In a similar vein, the spherical decoder decreases
  the number of features while increasing the bandwidth from layer to layer back to
  the originally utilized bandwidth. Moreover, by decreasing and increasing the bandwidth
  during encoding and decoding, we will also increase and decrease the size of the
  kernels for consecutive convolutions, respectively.


  1) Feature Encoding: Our network is based on the work of Cohen et al. [23]. Thus
  we lift the features to SO(3) during the encoding and have to revert to S2 during
  the decoding. In other words, only the initial layer of the network performs a convolution
  over S2, whereas the remaining convolutional layers act on SO(3) to preserve the
  convolution’s equivariance property [31]. During the convolutions, we employ spatially
  localized kernels that are rotated around the sphere using operations in SO(3).
  Here, SO(3) denotes the three-dimensional rotation group consisting of α, β, and
  γ, corresponding to roll, pitch, and yaw (RPY).


  Each convolution in our network is efficiently implemented based on the convolution
  theorem by performing a Fourier transform in S2 and SO(3) [32], respectively, i.e.
  the convolution between two signals f and g is given by f ∗ g = F−1{F{f} · F{g}},
  where F is either a S2 or SO(3) Fourier transform and F−1 its inverse. Additionally,
  we apply a PReLu [33] activation function followed by a three-dimensional batch
  normalization after each convolution. The last convolutional block during the encoding
  also applies a dropout for additional regularization.


  In our approach, the S2 convolution increases the number of features but is not
  done in place, i.e., it does not preserve the input bandwidth. Rather, it directly
  decreases the output bandwidth as part of the S2 Fourier transform by having a smaller
  output bandwidth for the inverse S2 Fourier transform in Eq. (3). The SO(3) convolution
  increases the number of input features but preserves the utilized spherical bandwidth.
  And, the SO(3) blocks decrease the bandwidth by applying a pooling operation in
  SO(3).


  Pooling and unpooling are done in the spectral domain of SO(3), which has the advantage
  of retaining the equivariance [22], [34] as opposed to spatial pooling. In practice,
  the SO(3) pooling is implemented by transforming the signal to the spectral domain
  using an SO(3) Fourier transform [29], [32] and subsequent low-pass filtering of
  the resulting spectrum. The inverse SO(3) Fourier transform then yields the pooled
  SO(3) signal. Between the forward and backward passes, we temporarily store the
  input bandwidth and inverse the operation by zero-padding the spectrum to the original
  size.


  2) Feature Decoding: All the convolutions in the spherical decoding component are
  done in SO(3). Moreover, we unpool the input signals to increase the bandwidth to
  match the input bandwidth again. In particular, we apply the SO(3) Fourier transform
  to the input signal and zero-pad the resulting spectrum to a larger size. Finally,
  the zero-padded signal is inverse SO(3) Fourier transformed and passed to the next
  convolutional layer. Similar to the encoding part, we perform the inverse by applying
  an idealized low-pass filter for the backward pass.


  The last convolution during decoding is different from the preceding operations
  as it zero-pads the convolved signal in the spectral domain to achieve the initial
  input bandwidth (sampling bandwidth). Thus, the last layer does not rely on any
  unpooling operation. Although the output has the correct size, it still needs to
  be mapped back to its original space, i.e., S2. To transform the SO(3) signal back
  to S2, a max pooling operation or an integration of the SO(3) signal over the last
  entry γ (i.e., the yaw angle) can be used, resulting in R2BW×2BW×2BW ↦→ R2BW×2BW.
  We have selected the latter approach for its efficient computation and simplicity.
  During the inference, the spherical semantic segmentation is then achieved by applying
  a final softmax layer to the result of the integration.


  3) Loss: Finally, our proposed spherical network uses a common loss definition for
  semantic segmentation [8], [20], i.e. for prediction ˆy and ground truth y labels

  LXC(y, ˆy) = − ∑i wi P(yi) log P(ˆyi)

  LLZ(y, ˆy) = 1/|C| ∑c∈C J(e(c))

  L(y, ˆy) = LXC(y, ˆy) + LLZ(y, ˆy),

  where LXC is a weighted cross-entropy loss where wi are the class weights and P(·)
  the corresponding probabilities. The latter term LLZ is a lovasz-softmax [41] loss
  where J is the lovasz intersection over union, e(c) the errors for class c and C
  the set of all classes. Notably, the loss operates on the equiangular samples in
  S2, and since we do not discard the rotational information of the signals on the
  sphere, we have a direct mapping between the input and the output signals.


  4) Back-Projection: The final pointcloud segmentation in R3 is achieved by back-projecting
  the spherical projection from S2 to R3 using the sampled range values, i.e., inverting
  Eq. (2), such that for the jth projected point with φj and θj

  xj = rj · cos(φj) sin(θj)

  yj = rj · sin(φj) sin(θj)

  zj = rj · cos(θj),

  where rj = ||pj||2.


  </block>

  ## IV. EXPERIMENTS

  <block id="6">

  This section presents the experimental validation of our spherical segmentation
  framework, where we show that our pipeline gives an accurate semantic segmentation
  of the environment and that it generalizes well to different sensory setups. We
  first validate the segmentation quality of our spherical network and compare it
  to current state-of-the-art projection-based segmentation frameworks. Next, we demonstrate
  the flexibility of our representation by increasing the input field-of-view drastically.
  Finally, we evaluate the computational cost to show the method’s applicability to
  real-world scenarios. We use the structure depicted in Figure 3 for all experiments.


  </block>

  ### A. Experiment and Training Setup

  <block id="7">

  We validate our proposed approach by comparing the performance of our spherical
  network to state-of-the-art projection-based segmentation frameworks, RangeNet++
  [7], SqueezeSeg [9], [17], 3D-MiniNet [10], and SalsaNext [8]. For the comparison,
  we utilize several datasets with various different LiDARs to show the ease of use
  of our proposed approach given sensors with different vertical FoV (vFoV) and number
  of beams. All networks use only the range values as feature and are trained on the
  nuScenes [35], SemanticKITTI [36], SemanticPOSS [37], Waymo [38], A2D2 [39]. We
  trained all networks for 50 epochs and used the mean Intersection over Union (mIoU)
  and accuracy metric to assess the quality of the similarity to the ground truth.


  Moreover, since all these datasets provide significantly different classes, e.g.
  nuScenes groups all objects into man-made objects. Similar to Sanchez et al. [42],
  we abstracted the semantic classes to a total of five classes in order to provide
  a shared set of classes between the datasets: vehicles, persons, ground, man-made,
  and vegetation.


  In addition, for each dataset that provides a split between training and validation,
  we randomly selected 4000 pointclouds from the former for training the networks
  and used the latter for inference. For the datasets without a split, we created
  a split and followed the above procedure for training and inference.


  The input data to the projection-based methods was projected into the largest FoV
  of the LiDARs known during training. It is important to note that our approach,
  in contrast to the other ones used, does not require any such considerations and
  projects all pointclouds directly onto the sphere.


  </block>

  ### B. Segmentation Quality Comparison and Validation

  <block id="8">

  We first evaluate the performance within the training domain, i.e., the training
  and test are in the same urban setting. One can see that most of the other baseline
  methods suffer from the large variations in the data and consequently also fluctuate
  in their performance significantly. Notably, due to the difference in vFoV the projections
  distort the physical dimensions, resulting in less accuracy where vFoV is smaller
  (e.g., SemanticPOSS [37] and Waymo [38]).


  In contrast, our proposed approach achieves a good segmentation of the input pointcloud
  and, most importantly, achieves consistent scores across the various datasets. Although
  our approach does not achieve the segmentation quality for the nuScenes [35] dataset
  in our tests, its primary benefit is the generalization capabilities between sensory
  systems. Thanks to the spherical representation of pointcloud data employed by our
  method, we can correctly perform the projection required by the different opening
  angles of the LiDARs. Consequently, our approach is less affected by the changing
  angular resolution between the datasets in comparison to the image-based projection
  approaches.


  We additionally evaluate the networks on an unseen sensory system, PC-Urban [40]
  to assess our approach’s generalization capabilities within the same domain. The
  difference in vFoV (12.5◦ down and −7.5◦ up) results in warped physical proportions
  for the 2D projection-based methods, which greatly decreases their segmentation
  performance. Consequently, the segmentation quality of high objects such as man-made
  buildings and vegetation significantly degrades. Our approach is less affected by
  the change in the vFoV and hence, also attains the highest mIoU on this dataset.


  </block>

  ### C. Segmentation Quality on a Different Domain

  <block id="9">

  In this experiment, we test all approaches on the SemanticUSL [43] (45◦ vFoV and
  64 beams) dataset, which offers a different sensory system and environment (off-road
  and campus scenes) than what was seen by the networks during training. The change
  in the vFoV is the same as for the PC-Urban dataset in the previous experiment.
  Table II shows a comparison between the class-wise and mean IoU.


  Due to the vastly different environment and sensor intrinsics, all but our proposed
  method fail entirely to segment the vegetation class as the learned representation
  no longer matches. Our approach is not affected by such warping of the physical
  dimensions and, therefore, achieves a better segmentation quality.


  However, it is difficult for our representation to disambiguate persons that are
  far away from man-made objects and small vegetation when only range values are available.
  Providing additional information through multiple modalities is a possible solution
  and is left for future work. Nevertheless, our approach still maintains the highest
  mIoU compared to the other approaches. This highlights the main benefit of our method,
  which is its ability to generalize and consequently provide an improved overall
  segmentation.


  </block>

  ### D. Semantic Segmentation of Rotated Pointclouds

  <block id="10">

  Next, we show that our representation has the advantage of rotational invariance
  to the input data, allowing pointclouds to be arbitrarily rotated. This allows our
  method to support various input configurations such as different angular resolutions
  and tilted sensor mounts.


  In this experiment, we applied a predefined rotation from 0◦ to 180◦ around the
  RPY axes of the input pointcloud. The rotation of the pointclouds yields odd and
  ineffective representations of the scans, and thus, all other projection-based methods
  experience a large drop in their performance. Note that since the rotation of 180◦
  around RPY results in the original pointcloud, the initial mIoU is restored again.
  Generally, 2D projection methods implicitly require the pointclouds to be horizontally
  oriented for an efficient prediction. Hence, it is particularly difficult for these
  methods to utilize multiple LiDARs simultaneously if one of the sensors is tilted
  w.r.t. the other ones.


  The spherical projection is not only more efficient and natural, but the spherical
  Fourier transform is also invariant to rotations. Thus, our approach is completely
  unaffected by the rotated pointclouds and maintains the mIoU over all rotational
  shifts.


  </block>

  ### E. Runtime Evaluation

  <block id="11">

  Finally, we assess the runtime performance of our proposed approach to understand
  its potential for deploying it in real-world applications. In this experiment, we
  consider a sampling bandwidth of 50 for the LiDAR as depicted in Figure 3. The benchmark
  was performed on an Intel Xeon E5-2640v3 with an NVIDIA Titan RTX, and all scripts
  are written using PyTorch. It is evident that the discretization of the irregular
  pointcloud data on the sphere takes a considerable portion. Nevertheless, our approach
  is able to infer a semantic segmentation in approximately 60 ms.


  </block>

  ## V. CONCLUSION AND FUTURE WORK

  <block id="12">

  In this paper, we proposed a spherical representation of pointclouds that can be
  used to train a model using various LiDAR sensors with different parameters. We
  presented in this context an end-to-end approach for semantic segmentation based
  on a spherical encoder-decoder network and showed that the spherical representation
  is a much more favorable representation, especially for high FoV LiDAR systems.
  Most importantly, our findings also indicate that our approach is invariant to rotations
  and has a better generalization to unseen LiDAR systems after training a model.
  Furthermore, our proposed approach is not limited to depth sensors, and other sensor
  types, such as RGB and thermal cameras, can be readily incorporated [26].


  In future research, we intend to investigate two separate research directions. First,
  we explore the fusion of multiple camera images to refine the initial semantic segmentation
  and its ontology. Second, we intend to migrate our spherical network to be fully
  in S2 in order to decrease the memory requirements and improve its practical applicability.

  </block>'
