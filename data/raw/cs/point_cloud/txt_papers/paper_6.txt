This paper has been accepted for publication at 2023 IEEE International Conference on Robotics and
Automation (ICRA 2023)
Light-Weight Pointcloud Representation with Sparse Gaussian Process
Mahmoud Ali and Lantao Liu
Abstract— This paper presents a framework to represent
high-ﬁdelity pointcloud sensor observations for efﬁcient com-
munication and storage. The proposed approach exploits Sparse
Gaussian Process to encode pointcloud into a compact form.
Our approach represents both the free space and the occupied
space using only one model (one 2D Sparse Gaussian Process)
instead of the existing two-model framework (two 3D Gaussian
Mixture Models). We achieve this by proposing a variance-
based sampling technique that effectively discriminates between
the free and occupied space. The new representation requires
less memory footprint and can be transmitted across limited-
bandwidth communication channels. The framework is exten-
sively evaluated in simulation and it is also demonstrated using
a real mobile robot equipped with a 3D LiDAR. Our method
results in a 70∼100 times reduction in the communication rate
compared to sending the raw pointcloud.
I. I NTRODUCTION
With the rapid advancement of LiDAR technology, we
now can build maps with remarkably high resolution. For
example, each full scan of an only 16-channel 3D LiDAR
can give us 57600 points in the pointcloud that represents
the surrounding obstacles. However, a price for using the
high resolution LiDAR is the computation, storage, and com-
munication costs when mapping the environments. While
one might be able to upgrade the computation and storage
by using a high performance computer system, the com-
munication usually becomes a bottleneck due to the low
communication bandwidth available. In practice, the low
bandwidth communication is considered as a major challenge
for many robotics applications such as occupancy mapping
of underwater and subterranean environments (caves, tunnels,
mines, etc), search-and-rescue missions in disaster scenarios
with a degraded communication infrastructure, and planetary
exploration missions [1]. The low bandwidth can prevent
a robot from real-time sharing its sensor observations, and
this can signiﬁcantly degrade the system responsiveness if
the robot needs to follow or interact with external control
or supervision platforms. This work tackles the problem
of sharing high-ﬁdelity 3D pointcloud through a limited
bandwidth communication channel.
The system we consider consists of a robot (the scout)
equipped with a LiDAR and a communication apparatus, and
deployed in a low-bandwidth environment. The scout sends
1Mahmoud Ali and Lantao Liu are with the Luddy School of Informatics,
Computing, and Engineering, Indiana University, Bloomington, IN 47408
USA, {alimaa, lantao }@iu.edu
Occupancy Surface VSGP
variance-based sampling SGPOctoMap wiﬁ
Scout
Base
Fig. 1: System Overview.
(a)
 (b)
 (c)
Fig. 2: (a) Gazebo simulated mine tunnel; (b) Original pointcloud
generated by a VLP16 LiDAR in red, and reconstructed pointcloud
from the VSGP model in white; (d) Occupancy Map generated by
OctoMap from the reconstructed pointcloud.
the observations that it acquires to a base for building the
occupancy map of the environment, see Fig. 1. Our approach
exploits the Variational Sparse Gaussian Process (VSGP) [2]
as a generative model to represent the pointcloud in a
compact form. This lightweight representation is transmitted
through low-bandwidth communication to the base where the
original pointcloud is reconstructed. Extensive evaluations
reveal that our approach results in a 70∼100 times reduction
in the memory as well as the communication rate required to
transmit pointcloud data. For example, Fig. 2a shows a scene
of a simulated mine tunnel, where its raw pointcloud (shown
in red, Fig. 2b) requires around 750 KB of memory. Our
approach is able to represent the same observation using only
6 KB of memory and transmit through limited-bandwidth
communication. On the receiver side of the communication
channel, the compact representation is used to reconstruct
the original pointcloud (reconstructed pointcloud shown in
white, Fig. 2b). An occupancy map of the scene can be built
using the reconstructed pointcloud, see Fig. 2c.
II. R ELATED WORK
Pointcloud compression algorithms have been investigated
in recent years to cope with the demands to store and
communicate the high-precision 3D points [3]. For example,
the space partitioning trees approaches that exploit the 3D
correlation between pointcloud points are widely used to
arXiv:2301.11251v1  [cs.RO]  26 Jan 2023
compress the pointcloud data [4]–[9]. Recently, deep learning
based approaches were also proposed to leverage data and
learn or encode the pointcloud compression [10]–[12]. Dif-
ferent from these frameworks, the probabilistic approaches
exploit the compactness of the distributions to compress 3D
sensor observation. For instance, Gaussian Mixture Models
(GMM) [13]–[15] have been proposed as a generative model
to encode 3D occupancy map. The GMM approach encodes
the 3D data as a mixture of Gaussian densities to represent
the occupied and free spaces around the robot.
Gaussian Process (GP) has been proven to be an excellent
framework to model spatial phenomena or features in a
continuous domain [16]–[18]. Unfortunately, the standard
GP has a cubic time complexity and this results in very
limited scalability to large datasets. Methods for reducing the
computing burdens of GPs have been previously investigated.
For example, GP regressions can be done in a real-time
fashion where the problem can be estimated locally with
local data [19]. Sparse GPs (SGPs) [20]–[26] tackle the com-
putational complexity of the normal GP through leveraging
the Bayesian rule with a sequential construction of the most
relevant subset of the data.
We propose a new probabilistic pointcloud compression
approach which is based on the VSGP [2] and inspired by
the GMM approach. While the GMM shares the accumulated
sensory information as a set of accumulated Gaussian den-
sities which are sampled and used as an occupancy map of
the environment, in contrast, the proposed approach relies on
sharing of immediate sensor observation to be reconstructed
on the other side of the communication channel for further
processing based on the required task (e.g. 3D mapping,
object recognition, tracking, etc).
This proposed VSGP-based approach offers a few ad-
vantages over the recent GMM approach: while the GMM
approach uses two 3D GMMs to ﬁt the occupied and free
points [13]–[15], our approach uses only one 2D VSGP to
ﬁt all the occupancy surface, including both the occupied
and free points. The primary reason that our approach uses
one VSGP instead of two is that we are using the variance
calculated by the VSGP at each sampled point during the
reconstruction process to decide if it belongs to the occupied
or the free space. Therefore, the proposed approach results
in a more compact representation of the sensor observation,
which requires less memory than the GMM approach and,
as a consequence, leads to a lower communication rate.
III. B ACKGROUND
GP is a non-parametric model described by a mean
function m(x), and a co-variance function (kernel) k(x,x′),
where x is the GP input [27]:
f (x)∼ GP
(
m(x),k
(
x,x′))
. (1)
Considering a data set D ={(xi,yi)}N
i=1 with N training
inputs x and their corresponding scalar outputs (observations)
y. After training the GP using the data set D, the output y∗ for
any new query x∗ can be estimated using the GP prediction:
p(y∗|y) = N(y∗|my(x∗),ky(x∗,x∗) +σ 2), (2)
where my(x) and ky(x,x′) are the posterior mean and co-
variance functions [2]. The GP prediction equation depends
on the values of the hyperparameters (Θ, σ 2) where Θ is the
kernel parameters and σ 2 is the noise variance.
The computation complexity of a full GP is O(N3).
In order to reduce the computation complexity, different
approximation methods were proposed in the literature by
considering only M input points to represent the entire
training data [27]. These input points are called the inducing
points X m and their corresponding values of the underlying
function f (x) are called the inducing variables f m. Replacing
the entire data set with only the M-inducing inputs leads to
the SGP which has a computational complexity of O(NM 2).
Titsias [2] proposed a variational learning framework to
jointly estimate the kernel hyperparameters and the inducing
points. Titsias’ framework approximates the true exact poste-
rior of a GP p( f|y, Θ) by a variational posterior distribution
q( f , fm),
q( f , fm) = p( f| fm)φ ( fm), (3)
where φ ( fm) is the free variational Gaussian distribution. The
Kullback-Leibler (KL) divergence is used to describe the dis-
crepancy between the approximated and the true posteriors.
Minimizing the KL divergence between the approximated
and the true posteriors KL[q( f , fm)||p( f|y, Θ)] is equivalent
to maximizing the variational lower bound of the true log
marginal likelihood:
FV (Xm) = log
[
N
(
y| 0, σ 2I + Qnn
)]
− 1
2σ 2 Tr( ˜K),
Qnn = KnmK−1
mmKmn,
˜K = Cov(f| fm) = Knn− KnmK−1
mmKmn,
(4)
where FV (Xm) is the variational objective function, Tr( ˜K) is a
regularization trace term, Knn is the original n×n co-variance
matrix, Kmm is m× m co-variance matrix on the inducing
inputs, Knm is n×m cross-covariance matrix between training
and inducing points, and Knm = KT
mn. More details on VSGP
can be found in Titsias’s work [2].
IV. M ETHODOLOGY
The proposed approach exploits the VSGP as a generative
model to encode 3D pointcloud. The VSGP is selected
among different approximation approaches of GP due to
the following reasons: i) The variational approximation dis-
tinguishes between the inducing points M (as a variational
parameter) and the kernel hyperparameters (Θ, σ ). ii) The
regularization term Tr ( ˜K) in the variational objective func-
tion (Eq. (4)) regularizes the hyperparameters to avoid over-
ﬁtting of the data. iii) The variational approximation offers
a discrete optimization scheme for selecting the inducing
inputs Xm from the original data 1.
A. VSGP as a generative model for the occupancy surface
Inspired by [13], we project the occupied points ob-
served by a ranging sensor, e.g., LiDAR, onto a circular
surface around the sensor origin with a predeﬁned radius
1For more information about the inducing point selection, check [2]
roc. This surface is called occupancy surface , see Fig. 3.
In our approach, the sensor observation is deﬁned in the
spherical coordinate system, where any observed point x i
is described by the tuple (θi, αi,ri) which represents the
azimuth, elevation, and radius values, respectively. Also,
any pointcloud data can be converted from the cartesian
coordinates (xi,yi,zi) to the spherical coordinates (θi, αi,ri)
using the following equations:
ri =
√
x2
i + y2
i + z2
i , θi = tan−1(yi,xi), αi = cos−1(zi/ri).
(5)
All observed points that lie outside the circular occupancy
surface (with a radius ri > roc) or on the surface (with a
radius ri = roc) are neglected and considered as free space.
The rest of the points that are inside the circular surface (with
a radius ri < roc) are projected on the occupancy surface and
called the occupied points. Therefore, the occupancy surface
radius roc acts as the maximum range of the sensor. Each
occupied point xi on the surface is deﬁned by two attributes:
the azimuth and elevation angles xi = (θi, αi), and assigned
an occupancy value f (xi) that is a function of the point radius
ri. The probability of occupancy f (xi) at each point on the
occupancy surface is modeled by a VSGP:
f (x)∼ VSGP
(
m(x),k
(
x,x′))
. (6)
Considering noisy measurements, we add a white noise ε to
the occupancy function f (x), so the observed occupancy is
described as yi = f (xi) +ε where ε follows a Gaussian dis-
tribution N
(
0, σ 2
n
)
. The ﬁnal model of the occupancy surface
is a 2D VSGP where the input is the azimuth and elevation
angles, x∈{ (θ , α)}n
i=1, and the corresponding output is the
expected occupancy yi. The three main components of the
ﬁnal VSGP are:
1) Zero-Mean Function m (x): There are different for-
mulas to describe the relationship between the occupancy
of a point f (xi) on the occupancy surface and its radius
ri [13]. For example, one candidate is f (xi) = 1/ri where ri
is bounded by the minimum and the maximum range of the
sensor rmin < ri < rmax = roc, where rmin > 0. Our approach
relates the occupancy of a point f (xi) to its radius ri by the
following equation f (xi) = roc− ri. This mapping between
the occupancy and the radius of a point is compatible with
the previous assumption that the occupancy surface radius
roc represents the maximum range of the sensor. Moreover,
this mapping is encoded in our VSGP model as a zero-mean
function m(x) = 0 that sets the occupancy value of the non-
observed points to zero. This mapping behavior mimics the
mechanism of the LiDAR itself.
2) Rational Quadratic (RQ) Kernel: The RQ kernel is
selected because a GP prior with an RQ kernel is expected
to have functions that vary across different length scales.
This quality of the RQ kernel copes with the nature of the
occupancy surface, speciﬁcally in unstructured environments
where a range of diverse length scales is required, i.e.,
kRQ
(
x,x′)
= σ 2
(
1 + (x− x′)2
2αℓ2
)−α
, (7)
(a)
 (b)
 (c)
Fig. 3: (a) Gazebo scene of a robot in a tunnel (black); (b)
The occupancy surface generated from the original pointcloud,
where warmer colors reﬂect smaller f (xi) values (less occupancy);
(c) The inner surface represents the original occupancy surface
(same as in b), and the middle surface represents the reconstructed
occupancy surface using the VSGP model. The outer grey-coded
surface represents the variance associated with each point on the
reconstructed occupancy surface where brighter colors reﬂect high
uncertainty. Raw pointcloud is shown in red in (b) and (c).
where σ 2
f is the signal variance, l is the length-scale, and α
sets the relative weighting of large and small scale variations.
The RQ co-variance function is more expressive in terms of
modeling the occupancy surface than the most commonly
used Squared Exponential (SE) co-variance function. This
can be reasoned by the fact that the RQ kernel (when α
and l > 0) is equivalent to a scale mixture of SE kernels
with mixed characteristic length-scales [27]. In practice, we
take into account the resolution of LiDAR along both the
azimuth and elevation axes to initiate different length-scales
along each axis to reﬂect the LiDAR resolution.
3) Inducing Points Selection: The variational learning
framework proposed in [2] jointly optimizes the variational
parameters (inducing points) and the hyperparameters ( Θ, σ)
through a variational Expectation-Maximization (EM) algo-
rithm. In general, the original discrete optimization frame-
work [2] suggests having an incremental set of the inducing
points, so that during the Expectation step (E-step) a point
from the input data is added to the inducing points set to
maximize the variational objective function FV and minimize
the KL divergence between the true and approximated pos-
teriors KL[q( f )||p( f|y, Θ)]. Then the hyperparameters are
updated during the Maximization step (M-step).
Since LiDAR’s ﬁeld of view is limited within a certain
range, the projection of the observed points on the circular
surface leads to a limited input domain for the VSGP. In
our case, the azimuth and the elevation axes are limited
to (−π to π) and (−15◦ to 15◦), respectively. The limited
input domain is used to initiate a ﬁxed number of inducing
points at evenly distributed locations on the occupied part of
the occupancy surface. In this way, a different combination
of the points is selected at each E-step to maximize the
variational objective function FV and minimize the KL
divergence. Then the hyperparameters are updated during the
M-step. The number of the inducing points M is chosen to
compromise the computational and memory complexity on
one side and the accuracy of the reconstructed pointcloud
on the other side. More inducing points result in higher
computations complexity O(NM 2), larger memory to store
the encoded observation, and higher bandwidth to transfer it.
However, more inducing points increase the accuracy of the
reconstructed pointcloud. We chose M=500 inducing points
to keep the average deviation between the reconstructed
pointcloud and the original pointcloud under 15 cm, see
Section V-A.2 and Fig. 5. After the training phase on the
scout side is completed, the selected inducing points are
combined together with the hyperparameters values of the
VSGP and are transmitted from the scout to the base.
B. Variance-based sampling
On the base side, the inducing points and the values of
the hyperparameters, which are received from the scout,
are used to reconstruct the original occupancy surface. The
reconstruction is done through a GP conﬁgured with the
same kernel (RQ) and likelihood (Gaussian) as the VSGP
on the scout side. The base GP is trained on the inducing
points and has a computation complexity of O(M3) where
M is the number of the inducing points, so we refer it as
a sparse GP (SGP) and refer the reconstructed occupancy
surface as the SGP occupancy surface. A grid of query points
x∗ ={(θ , α)}K
i=1 with the same resolution of the LiDAR
along the azimuth and the elevation axes is generated to
reconstruct the original pointcloud from the SGP occupancy
surface – we refer the reconstructed pointcloud as the SGP
pointcloud. If up-sampling of the pointcloud is required for
any reason, a query grid with higher resolution can be used
for the reconstruction process. The SGP occupancy surface
is used to predict the occupancy f (xi) of each point xi of
the query grid x∗. The occupancy is converted back to the
spherical radius ri = roc− f (xi) to restore the 3D spherical
coordinates of each point.
One advantage of the GP and its variants over other
modeling techniques is the uncertainty (variance) associated
with the predicted value at any query point. Considering
the VSGP model of the occupancy surface on the scout
side, the variance associated with the occupied points is low
compared to the variance related to the free points. Selecting
the inducing points as a set from the original occupied
points maintains low-variance values for the occupied part of
the reconstructed SGP occupancy surface on the base side.
Therefore, the variance value associated with any point on
the reconstructed SGP occupancy surface is used to predict
if that point belongs to the occupied or the free part of the
occupancy surface, see Fig. 4. We use a variance threshold
Vth as a judging criterion. In fact, the variance related to
the occupancy surface is different from one observation to
another, and it is affected by both the number of observed
(occupied) points and their distribution over the occupancy
surface. Therefore, we chose the variance threshold Vth as
a variable that changes with the distribution of the variance
over the occupied and free parts of the occupancy surface.
Vth is deﬁned as a linear combination of the variance mean
vm and standard deviation vstd over the surface, i.e., Vth =
Km∗ vm + Kstd∗ vstd where Km and Kstd are constants. These
two constants are tuned by ﬁrst setting Vth = vm (Km = 1 ,
Kstd = 0), then we increase Kstd and decrease Km gradually
till we get the values that give the highest accuracy for the
reconstructed SGP pointcloud (considering a ﬁxed number of
(a)
 (b)
 (c)
Fig. 4: Variance-based sampling. (a) Gazebo scene shows the
entrance of the tunnel; (b) shows the original (inner), reconstructed
(middle), and variance (outer) surfaces. It also shows the re-
constructed pointcloud (in white) through reconstructing from all
points (free and occupied) of the occupancy surface. (c) shows
reconstructed SGP pointcloud after removing all points that most
likely belong to the free part of the occupancy surface. Raw
pointcloud is shown in red in (b) and (c).
inducing points). Our sampling-based approach is capable
of discriminating between the free points that most likely
belong to the free part of the SGP occupancy surface and
the occupied points that belong to the the occupied part of
the SGP occupancy surface. After removing the free part
of the SGP occupancy surface, the Cartesian coordinates of
the occupied points are calculated using the inverse form of
Eq. (5) to restore the original point cloud, see Fig. 4c.
V. E XPERIMENTAL DESIGN AND RESULTS
The proposed approach is implemented in Python3 on
top of GPﬂow-v2 [28] and TensorFlow-v2.4 [29] under
ROS framework [30]. Both real-time simulation and real-
time demonstration were considered to evaluate the proposed
approach. In both the simulation and the hardware experi-
ments, a VLP-16 LiDAR was used with a maximum range
of 10 m, a frequency of 4 Hz, and a resolution of (0.1◦,2◦)
along the azimuth and the elevation axis, respectively. This
conﬁguration results in a maximum pointcloud size of 57600
points. The query grid, which is used to sample the SGP
occupancy surface on the base side, has the same resolution
as the VLP-16 LiDAR. A 3D occupancy grid map with a
resolution of 5 cm is generated from the reconstructed SGP
pointcloud through Octomap [31].
We investigate the performance of our framework and
compare it with the GMM approach [13]–[15]. While the
GMM approach tackles the occupancy mapping problem as
a whole, our approach focuses on compressing sensor obser-
vations through limited-bandwidth communication channels.
To be able to compare the two approaches, we implemented
the GMM approach in such a way that it is used to encode
one sensor observation at a time instead of generating an
entire occupancy map. We compared our approach with two
versions of the GMM approach: i) A CPU-based implemen-
tation of GMM that follows the same guidelines of [13].
ii) An upgraded GPU-based implementation of GMM. We
implemented the GPU-GMM to have a fair computation
comparison with our VSGP approach which runs on GPU.
A. Simulation Experiments
1) Simulation Setup:: The simulation setup consists of
two machines that communicate to each other over WiFi:
The ﬁrst machine, where the scout and the environment are
simulated, is an Intel® Core™ i7 NUC11 PC equipped with
64 GB RAM and 6 GB Geforce RT X2060 GPU. The second
machine, which acts as the base, is an Intel® Core™ i7
Alienware Laptop equipped with 32 GB RAM and 8 GB
Geforce RT X2080 GPU. Both are connected using a 2.4 GHz
WiFi router. The network ﬂow is monitored using the ifstat
tool to evaluate the communication performance. The mine
tunnel of the cpr inspection world, which is developed by
ClearPath robotics, is used as our simulation environment.
This environment is selected because it represents one of
the targeted low-bandwidth subterranean environments. The
mine tunnel part of the cpr inspection world ﬁts in a rectan-
gular area with an approximated area of 30×65m2, the tunnel
length is around 135 m. The ground elevation and the height
of the tunnel are different from one place to another. The
ClearPath Jackal robot is used as the scout. The proposed
approach was evaluated through 20 real-time simulation
trials. In each trial, the robot starts at the beginning of the
cave and follows a predeﬁned path along the mine using
way-point based navigation.
2) Simulation Results: We evaluate the performance of
our approach based on the reduction in the memory and the
communication rate required to transmit the sensor observa-
tions between the scout and the base. The VSGP representa-
tion requires only 1514 ﬂoating points (FP) to represent the
entire pointcloud (3 FP for each inducing point (3x500) + 6
FP for robot pose + 6 FP for the hyperparameters). This value
is less than the memory needed by the GMM approach which
requires∼ 2000 FP (10 FP for each component (10x200)
distributed as 6 FP for covariance + 3 FP for mean + 1
FP for weight) [13]. We send the robot pose to the base
because our approach encodes the observation relative to the
robot body frame, while the GMM approach ﬁrst transforms
the observation from the robot body frame to a global frame
using the robot current pose and then sends the encoded
Gaussians densties with respect to the global frame.
To quantify the accuracy of the reconstructed SGP point-
cloud, we use the Root Mean Square Deviation (RMSD)
between the radius predicted by our approach and the actual
radius of each point on the occupancy surface.
RMSD =
√
∑N
i=1 (ri− ˆri)2
N , (8)
where N is the size of the pointcloud, ri is the actual radius at
(θi, αi), and ˆri is the estimated radius value at the same point
(θi, αi). Fig. 5a shows the mean and the standard deviation
of the RMSD for each predicted point over 110 observations
(each observation has around 10 K to 50 K points). Also,
Fig. 5a implicitly reﬂects the memory required by VSGP and
GMM to store one observation, as described before that the
memory required to store one observation can be calculated
by multiplying the number of inducing points (bottom x-axis)
by 3 and multiplying the number of components (top x-axis)
by 10. We match pairs of the VSGP and GMM models (in
terms of the number of inducing points and components)
based on the memory requirement and the accuracy of the
(a)
 (b)
(c)
 (d)
Fig. 5: Performance comparisons. (a) shows the RMSD between
the reconstructed and the original pointcloud for VSGP(vs #induc-
ing points) and GMM(vs #components); (b) illustrates the training
time against the pointcloud size (considering 500-inducing points
VSGP, and equivalently, 200-components GMM); (c) represents
the training time versus the #VSGP-inducing points and #GMM-
components; (d) shows the prediction time versus the #VSGP-
inducing points and #GMM-components.
reconstructed pointcloud (reﬂected by the RMSD) for each
pair, see table I. For example, 500-inducing points VSGP
results in an average RMSD value for each point of 9 cm
with a standard deviation of 10 cm. This corresponds to an
average RMSD of 11 cm with a standard deviation of 25 cm
for a 200-components GMM.
TABLE I: VSGP vs GMM(ind: inducing, cps: components)
VSGP GMM
# Memory RMSD # Memory RMSD
ind ∼FPs ∼cm cps ∼FPs ∼cm
200 600 20±22 50 500 27±50
300 900 14±15 100 1000 16±35
400 1200 12±14 150 1500 13±31
500 1500 9±10 200 2000 11±29
600 1800 9±10 250 2500 11±30
Now we analyze the results in Fig. 5. Fig. 5a shows the
RMSD values associated with VSGP have a smaller standard
deviation than the GMM’s. It also shows that increasing the
number of the VSGP-inducing points (bottom x-axis) or the
number of the GMM-components (top x-axis) will result in
smaller RMSD (higher accuracy).
An intensive evaluation of the training and the prediction
phases is presented in Figs. 5b-5d. The reduction in the
training time versus the reduction in the size of the raw
pointcloud is presented in Fig. 5b, where 0% removal percent
means a pointcloud size of 57.6K points. Fig. 5c shows
the increase in training time versus the number of inducing
points and the number of components. We compare the
training time of the VSGP, the GMM-CPU (considering the
(a)
 (b)
 (c)
(d)
Fig. 6: (a) shows the simulated mine environment in Gazebo;
(b) shows the Octomap of the mine generated from the original
pointcloud; (c) shows the Octomap generated from the recon-
structed SGP pointcloud; (d) shows the communication rate and the
accumulated data sent from the scout to the base in case of sending
raw pointcloud PCL(1750KB/S, 840MB), GMM data(25.8KB/S,
12.4MB), and VSGP data(18.2KB/S, 8.7MB). The y-axis is plotted
in log-scale.
default conﬁguration of the GMM approach used in [13]),
and the GMM-GPU implementation. The results show that
our approach outperforms both the CPU and GPU imple-
mentation of the GMM approach in terms of training time.
Fig. 5d presents the variation of the prediction time of the
VSGP versus the number of the inducing points, where the
values shown in the ﬁgure represent the time required to
predict the occupancy value associated with all the points of
the grid query x (57600 points).
Fig. 5d indicates that for a matching pair of GMM and
VSGP (Table I), GMM has a less sampling time than
the paired VSGP. However, the pointcloud reconstruction
process of the VSGP is more convenient than the GMM
approach because a fundamental difference between sam-
pling the VSGP and the GMM is that: when we sample
from a GMM, we get a sample (from a distribution) with
random values (θs, αs,rs), so we do not have control over
the location of the sample on the occupancy surface (θs, αs).
In contrast, for the VSGP approach, we predict the radius
value rs for a certain point on the occupancy surface deﬁned
by (θs, αs). So, we have control over the point location on
the occupancy surface. While constructing the 3D octomap
of the tunnel environment using the scout-base scheme, the
average communication rate was 1750 KB/S, 25.8 KB/S,
and 18.2 KB/S for sending raw point clouds, GMM encoded
data, and VSGP encoded data respectively, see Fig. 6d. The
accumulated data sent through the network is reduced from
840 MB for sending raw pointcloud to 12.4 MB in case
of GMM and 8 .7 MB in case of VSGP. This indicates a
(a)
 (b)
(c)
Fig. 7: Indoor demonstration. (a) shows octomap of the laboratory
building generated from the original pointcloud. (b) shows octomap
generated from the reconstructed SGP pointcloud. (c) shows the
reduction in the communication rate and the accumulated data sent
from the scout to the base, where log-scale is used for y-axis. PCL
represents the raw pointcloud.
compression ratio of ∼ 96 (840 /8.7∼ 1750/18.2).
B. Hardware Experiment
A Jackal mobile robot, equipped with a VLP-16 LiDAR
and NUC11 PC, was used as the scout, while the Alien-
ware laptop was used as the base. The demonstration was
conducted in an indoor environment, where the VSGP-
encoded pointcloud data was sent from the scout to the
base to generate a 3D Octomap [31] of the building from
the SGP reconstructed pointcloud in real-time, see Fig. 7.
Fig. 7c shows the reduction in the communication rate for
the hardware experiment. The communication rate dropped
from around 560 KB for transmitting raw pointcloud to
around 8 KB for transmitting the encoded VSGP (this ratio
is equivalent to 70 times smaller rate). The communication
rate of the hardware experiment is low compared to the
simulation experiment because the LiDAR resolution was
halved during the hardware experiment. The total amount
of data transmitted at the end of each trial was around 100
MB for sending raw pointcloud and only around 1.4 MB for
sending the VSGP encoded observation.
VI. C ONCLUSION
In this paper, we introduce a lightweight representation
for the 3D pointcloud using the VSGP. This representation
allows high-ﬁdelity observations to be efﬁciently stored
and transmitted through limited-bandwidth communication
channels. Based on the results of the simulation and hardware
experiments, our approach results in around 70-100 times
smaller size representation of the sensor observation. This
compact representation can facilitate many of the robotics
applications which are limited by the communication band-
width such as subterranean and underwater exploration,
search and rescue missions, and planetary exploration. In
addition, our approach can also be beneﬁcial in the context
of multi-robot collaboration where a number of robots are
required to share high-volume information (3D pointcloud)
through low-bandwidth channels.
ACKNOWLEDGEMENT
This work was supported by National Science Foundation
with grant numbers 2006886 and 2047169.
REFERENCES
[1] V . H. Cid et al. Keeping communications ﬂowing during large-scale
disasters: leveraging amateur radio innovations for disaster medicine.
Disaster medicine and public health preparedness , 12(2):257–264,
2018.
[2] Michalis Titsias. Variational learning of inducing variables in sparse
gaussian processes. In Artiﬁcial intelligence and statistics, pages 567–
574. PMLR, 2009.
[3] Chao Cao, Marius Preda, and Titus Zaharia. 3d point cloud com-
pression: A survey. In The 24th International Conference on 3D Web
Technology, pages 1–9, 2019.
[4] Yu Feng, Shaoshan Liu, and Yuhao Zhu. Real-time spatio-temporal
lidar point cloud compression. In 2020 IEEE/RSJ international
conference on intelligent robots and systems (IROS) , pages 10766–
10773. IEEE, 2020.
[5] Tim Golla and Reinhard Klein. Real-time point cloud compression.
In 2015 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS), pages 5087–5092. IEEE, 2015.
[6] S ´ebastien Lasserre, David Flynn, and Shouxing Qu. Using neighbour-
ing nodes for the compression of octrees representing the geometry
of point clouds. In Proceedings of the 10th ACM Multimedia Systems
Conference, pages 145–153, 2019.
[7] Dorina Thanou, Philip A Chou, and Pascal Frossard. Graph-based
compression of dynamic 3d point cloud sequences. IEEE Transactions
on Image Processing , 25(4):1765–1778, 2016.
[8] Yan Huang, Jingliang Peng, C-C Jay Kuo, and M Gopi. Octree-based
progressive geometry coding of point clouds. In PBG@ SIGGRAPH,
pages 103–110, 2006.
[9] Lila Huang, Shenlong Wang, Kelvin Wong, Jerry Liu, and Raquel
Urtasun. Octsqueeze: Octree-structured entropy model for lidar com-
pression. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pages 1313–1323, 2020.
[10] Maurice Quach, Jiahao Pang, Dong Tian, Giuseppe Valenzise, and
Fr´ed´eric Dufaux. Survey on deep learning-based point cloud com-
pression. Frontiers in Signal Processing , 2022.
[11] Louis Wiesmann, Andres Milioto, Xieyuanli Chen, Cyrill Stachniss,
and Jens Behley. Deep compression for dense point cloud maps. IEEE
Robotics and Automation Letters , 6(2):2060–2067, 2021.
[12] Wei Yan, Shan Liu, Thomas H Li, Zhu Li, Ge Li, et al. Deep
autoencoder-based lossy geometry compression for point clouds. arXiv
preprint arXiv:1905.03691, 2019.
[13] W. Tabib et al. Real-time information-theoretic exploration with
gaussian mixture model maps. In Robotics: Science and Systems ,
2019.
[14] C. O’Meadhra et al. Variable resolution occupancy mapping using
gaussian mixture models. IEEE Robotics and Automation Letters ,
4(2):2015–2022, 2018.
[15] Task-Speciﬁc Manipulator Design. Communication-efﬁcient planning
and mapping for multi-robot exploration in large environments. Jour-
nal Article, 15(2):e1971, 2019.
[16] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian
Processes for Machine Learning . The MIT Press, 2005.
[17] A. Singh, A. Krause, C. Guestrin, W. Kaiser, and M. Batalin. Efﬁcient
planning of informative paths for multiple robots. In the 20th
International Joint Conference on Artiﬁcal Intelligence , IJCAI’07,
pages 2204–2211, 2007.
[18] Ruofei Ouyang, Kian Hsiang Low, Jie Chen, and Patrick Jaillet.
Multi-robot active sensing of non-stationary gaussian process-based
environmental phenomena. In Proceedings of the 2014 International
Conference on Autonomous Agents and Multi-agent Systems , pages
573–580, 2014.
[19] Duy Nguyen-tuong and Jan Peters. Local gaussian process regression
for real time online model learning and control. In In Advances in
Neural Information Processing Systems 22 (NIPS , 2008.
[20] Lehel Csat ´o and Manfred Opper. Sparse on-line gaussian processes.
Neural computation, 14(3):641–668, 2002.
[21] A. J. Smola and P. L. Bartlett. Sparse greedy gaussian process
regression. In Advances in neural information processing systems ,
pages 619–625, 2001.
[22] Ch. Williams and M. Seeger. Using the nystr ¨om method to speed
up kernel machines. In Proceedings of the 14th annual conference
on neural information processing systems, number CONF, pages 682–
688, 2001.
[23] N. Lawrence, M. Seeger, and R. Herbrich. Fast sparse gaussian process
methods: The informative vector machine. In 16th annual conference
on neural information processing systems, number CONF, pages 609–
616, 2003.
[24] E. Snelson and Z. Ghahramani. Sparse gaussian processes using
pseudo-inputs. Advances in neural information processing systems ,
18:1257, 2006.
[25] Matthias Seeger. Bayesian gaussian process models: Pac-bayesian
generalisation error bounds and sparse approximations. Technical
report, University of Edinburgh, 2003.
[26] Rishit Sheth, Yuyang Wang, and Roni Khardon. Sparse variational
inference for generalized gp models. In International Conference on
Machine Learning, pages 1302–1311. PMLR, 2015.
[27] Christopher K Williams and Carl Edward Rasmussen. Gaussian
processes for machine learning , volume 2. MIT press Cambridge,
MA, 2006.
[28] A. Matthews et al. Gpﬂow: A gaussian process library using tensor-
ﬂow. J. Mach. Learn. Res. , 18(40):1–6, 2017.
[29] M. Abadi et al. Tensorﬂow: A system for large-scale machine
learning. In 12th{USENIX} symposium on operating systems design
and implementation ({OSDI} 16), pages 265–283, 2016.
[30] Morgan Quigley, Ken Conley, Brian Gerkey, Josh Faust, Tully Foote,
Jeremy Leibs, Rob Wheeler, Andrew Y Ng, et al. Ros: an open-source
robot operating system. In ICRA workshop on open source software ,
volume 3, page 5. Kobe, Japan, 2009.
[31] A. Hornung et al. Octomap: An efﬁcient probabilistic 3d mapping
framework based on octrees. Autonomous robots , 34(3):189–206,
2013.