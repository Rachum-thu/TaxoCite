title: 'Seeing Through the Grass: Semantic Pointcloud Filter for Support Surface Learning'
blocks:
- block_id: 0
  content: Abstract— Mobile ground robots require perceiving and understanding their surrounding support surface to move around
    autonomously and safely. The support surface is commonly estimated based on exteroceptive depth measurements, e.g., from
    LiDARs. However, the measured depth fails to align with the true support surface in the presence of high grass or other
    penetrable vegetation. In this work, we present the Semantic Pointcloud Filter (SPF), a Convolutional Neural Network (CNN)
    that learns to adjust LiDAR measurements to align with the underlying support surface. The SPF is trained in a semi-self-supervised
    manner and takes as an input a LiDAR pointcloud and RGB image. The network predicts a binary segmentation mask that identifies
    the specific points requiring adjustment, along with estimating their corresponding depth values. To train the segmentation
    task, 300 distinct images are manually labeled into rigid and non-rigid terrain. The depth estimation task is trained
    in a self-supervised manner by utilizing the future footholds of the robot to estimate the support surface based on a
    Gaussian process. Our method can correctly adjust the support surface prior to interacting with the terrain and is extensively
    tested on the quadruped robot ANYmal. We show the qualitative benefits of SPF in natural environments for elevation mapping
    and traversability estimation compared to using raw sensor measurements and existing smoothing methods. Quantitative analysis
    is performed in various natural environments, and an improvement by 48% RMSE is achieved within a meadow terrain.
  citations: []
- block_id: 1
  content: 'A comprehensive understanding of the robot’s surroundings is a key element in achieving autonomous robot navigation
    in outdoor environments. Typically, ground robots are equipped with exteroceptive sensors such as LiDARs and depth cameras
    to directly capture the structure of the 3D environment. The captured geometrical information can be accumulated in a
    2D occupancy map [1], [2], 2.5D elevation map [3], [4], or 3D voxel map [5] representation. These maps provide information
    about the support surface (the rigid surface that can provide support for the robot during traversing) and obstacles for
    downstream tasks such as motion planning [6], [7], traversability assessment [8], [9], and trajectory planning [10], [11].
    However, these conventional approaches are based on the assumption that the world is rigid, i.e., the robot will step
    on the terrain or collide with the obstacles rather than moving or deforming them. While the assumption holds in most
    structured environments, it is not the case for unstructured natural environments with vegetation or soft terrain. Most
    navigation planning and control pipelines often treat the penetrable vegetation as rigid, which leads to sub-optimal locomotion
    and navigation. For example, a robot may try to step over vegetation or avoid non-existing obstacles without the capability
    to identify the support surface. In contrast, humans easily traverse natural terrains by associating semantic information
    with the terrain property. For example, before walking on grass, humans can anticipate from visual information that the
    high grass is penetrable and correctly adjust their gait.


    In this work, we present a novel approach to enhance the environment perception capabilities of robots by learning the
    support surface from RGB images and LiDAR, thereby overcoming the limitations of prior geometric methods. Instead of directly
    acquiring a fused support surface representation, such as an elevation map or voxel map, we opt to refine raw sensor data
    for broader downstream application adaptability. Specifically, we propose a multi-modal model that filters pointcloud
    measurements by jointly performing semantic segmentation and depth estimation. The semantic segmentation module identifies
    the regions where the raw pointcloud does not align with the support surface, enabling the preservation of rigid obstacles
    during pointcloud filtering, which is essential for navigation tasks. Considering the inherent challenge associated with
    annotating depth data, the depth estimation component employs a self-supervised learning approach by leveraging supervision
    signals generated by foothold positions. Conversely, the semantic segmentation component is trained in a supervised manner
    since the task of labeling the regions within an image that require depth adjustment is comparatively less arduous.


    We deploy our method on the legged robot ANYmal and validate it in a range of outdoor environments, including Grassland,
    Forest, and Hillside, against existing methods. Additionally, we show the benefit of our method for multiple robotic downstream
    tasks, including elevation mapping and traversability estimation. Our results highlight our approach’s practicality and
    potential applicability in real-world scenarios.


    Our main contributions are the following:

    - 1) A novel Semantic Pointcloud Filter architecture that enables the filtering of raw LiDAR data to achieve alignment
    with the underlying support surface.

    - 2) A semi-self-supervised learning framework that jointly performs support surface segmentation and depth estimation.
    Our framework utilizes manually labeled segmentation masks and support surface depths annotated by foothold positions.

    - 3) Real-world experiments using the legged robot ANYmal, showcasing the benefits of the SPF for elevation mapping and
    traversability estimation.'
  citations:
  - marker: '[1]'
    intent_label: Prior Methods
    topic_label: Projection-based representations
  - marker: '[2]'
    intent_label: Prior Methods
    topic_label: Projection-based representations
  - marker: '[3]'
    intent_label: Prior Methods
    topic_label: Projection-based representations
  - marker: '[4]'
    intent_label: Prior Methods
    topic_label: Projection-based representations
  - marker: '[5]'
    intent_label: Prior Methods
    topic_label: Volumetric voxelization
  - marker: '[6]'
    intent_label: Prospective Application
    topic_label: Projection-based representations
  - marker: '[7]'
    intent_label: Prospective Application
    topic_label: Projection-based representations
  - marker: '[8]'
    intent_label: Prospective Application
    topic_label: Projection-based representations
  - marker: '[9]'
    intent_label: Prospective Application
    topic_label: Projection-based representations
  - marker: '[10]'
    intent_label: Prospective Application
    topic_label: Projection-based representations
  - marker: '[11]'
    intent_label: Prospective Application
    topic_label: Projection-based representations
- block_id: 2
  content: We provide an overview of existing support surface estimation methods and the building blocks of our work, namely,
    depth estimation and semantic segmentation.
  citations: []
- block_id: 3
  content: 'Support surface estimation focuses on determining stable surfaces for robot mobility. In the situation where e.g.
    sparse vegetation occludes the support surface, the vegetation can be filtered out by regarding them outliers. This problem
    can be tackled by methods using Kalman Filters [4], [12], [13] and other heuristics [7], [14].


    However, these methods assume that the underlying surface is mostly flat. One can get a more realistic and robust estimation
    of the support surface by explicitly modeling the vegetation and the support surface. Various probabilistic models capturing
    both vegetation and ground characteristics [15], [16] have been proposed. They use proprioceptive and exteroceptive information
    to estimate support surfaces. Rather than using heuristic modeling, ˇSalansk´y et al. [17] introduced a learning-based
    method to filter a 2.5D map, while Wellington et al. [18] employ a learning approach to filter a voxel map. In our research,
    instead of using a fused representation such as an elevation map or voxel map, we refine the raw sensor measurements directly.
    This allows our approach to generalize more effectively for a wider range of downstream applications.'
  citations:
  - marker: '[4]'
    intent_label: Prior Methods
    topic_label: Raw point-based representations
  - marker: '[7]'
    intent_label: Prior Methods
    topic_label: Raw point-based representations
  - marker: '[12]'
    intent_label: Prior Methods
    topic_label: Raw point-based representations
  - marker: '[13]'
    intent_label: Prior Methods
    topic_label: Raw point-based representations
  - marker: '[14]'
    intent_label: Prior Methods
    topic_label: Raw point-based representations
  - marker: '[15]'
    intent_label: Prior Methods
    topic_label: Hybrid multi-modal representations
  - marker: '[16]'
    intent_label: Prior Methods
    topic_label: Hybrid multi-modal representations
  - marker: '[17]'
    intent_label: Prior Methods
    topic_label: Projection-based representations
  - marker: '[18]'
    intent_label: Prior Methods
    topic_label: Volumetric voxelization
- block_id: 4
  content: 'Supervised depth estimation has a significant body of work [19], [20], but requires pixel-wise depth labeling.
    Self-supervised learning has been applied to monocular depth estimation, achieving better performance than supervised
    methods [21], [22]. Further research incorporates semantic information [23], [24], but monocular depth estimation remains
    challenging due to its ill-posed nature.


    Depth completion methods use LiDAR pointcloud as additional input to provide explicit geometric information [25], [26].
    The trained networks often employ an encoder-decoder structure [27], which we also adapt in our work to estimate the depth
    of the support surface. A similar problem of filtering a point cloud in the domain of city-scale mapping is tackled by
    Stucker et al. [28] where they leverage semantics to refine a large pointcloud scan.'
  citations:
  - marker: '[19]'
    intent_label: Prior Methods
    topic_label: Projection-based representations
  - marker: '[20]'
    intent_label: Prior Methods
    topic_label: Projection-based representations
  - marker: '[21]'
    intent_label: Prior Methods
    topic_label: Projection-based representations
  - marker: '[22]'
    intent_label: Prior Methods
    topic_label: Projection-based representations
  - marker: '[23]'
    intent_label: Prior Methods
    topic_label: Projection-based representations
  - marker: '[24]'
    intent_label: Prior Methods
    topic_label: Projection-based representations
  - marker: '[25]'
    intent_label: Prior Methods
    topic_label: Hybrid multi-modal representations
  - marker: '[26]'
    intent_label: Prior Methods
    topic_label: Hybrid multi-modal representations
  - marker: '[27]'
    intent_label: Model/Architecture Adoption
    topic_label: Hybrid multi-modal representations
  - marker: '[28]'
    intent_label: Prior Methods
    topic_label: Hybrid fusion methods
- block_id: 5
  content: Image-based semantic segmentation has been successfully applied to various robotic applications, including terrain
    classification [29], [30]. To further improve the precision, various studies have explored the fusion of RGB and depth
    data for multi-modal semantic segmentation [31], [32]. Typically, these methods employ convolutional neural networks (CNNs)
    to process input data and fuse features at different stages of the network. In our task, we employ semantic segmentation
    as a means of aiding the estimation of the depth of the support surface.
  citations:
  - marker: '[29]'
    intent_label: Prospective Application
    topic_label: Projection-based segmentation
  - marker: '[30]'
    intent_label: Prospective Application
    topic_label: Projection-based segmentation
  - marker: '[31]'
    intent_label: Prospective Application
    topic_label: Hybrid fusion methods
  - marker: '[32]'
    intent_label: Prospective Application
    topic_label: Hybrid fusion methods
- block_id: 6
  content: 'The overview of our approach is given in Fig. 2. To better distinguish the supervisions for two tasks, the self-supervised
    label for depth estimation is named the support surface depth estimation (SSDE) label, and the label obtained manually
    for semantic segmentation is named support surface segmentation (SSSeg) label. The whole process comprises three stages:
    SSDE labels for depth estimation are generated from robot trajectories in a self-supervised manner without the need for
    human annotation. Subsequently, the SSSeg labels for semantic segmentation are manually labeled. Then the SPF is trained
    to do joint depth estimation and semantic segmentation. Finally, the pointcloud predicted by the SPF can be used for downstream
    applications.'
  citations: []
- block_id: 7
  content: As shown in Fig. 1, given a camera image xrgb∈ Rm×n×3 and raw pointcloud Pi∈ R3,i∈{ 1 . . .N}, our objective is
    to derive the filtered pointcloud, ˆP, using our SPF, ˆP = fSPF(P,xrgb). Each point in the filtered pointcloud, ˆPi∈ R3,i∈{
    1 . . .N}, aligns with the support surfaces or the impenetrable obstacles.
  citations: []
- block_id: 8
  content: 'We extract the foothold positions of the robot and approximate the support surface using a Gaussian process (GP)
    similar to [16]. Then we generate pixel-wise depth labels by projecting the reconstructed support surface into the image.


    1) Foothold extraction: Foothold positions serve as ground truth samples for the support surface. From the dataset, foot
    position trajectories are obtained using forward kinematics, comprising discrete height points over time. Footholds are
    extracted, assuming foot-ground contact occurs at local height minima in the world frame.


    Specifically, to extract footholds, a large temporal window WB with duration tB is constructed, and a short temporal window
    WS with duration tS is built around a point within WB (Fig. 3a). Mean foot heights within WS and WB are calculated, yielding
    meanS and meanB, and the maximum height change within WS is denoted as rS. A point is identified as a foothold if meanS≤
    meanB and rS≤ rt, where rt is a threshold. This is applied iteratively for each point in WB. After evaluating all points,
    WB is moved forward, and the process is repeated. Footholds are denoted as Pi∈ R3,i∈ 1 . . .M, with M as the number of
    extracted footholds.


    2) Support surface reconstruction: We extrapolate the positions of the extracted footholds in world frame to generate
    the support surface as done by Homberger et al. [16]. The foothold extrapolation method is valid under the assumption
    that the support surface is continuous and smooth.


    To achieve this, a grid map is employed, and the sparsely distributed footholds are used to determine the height of the
    corresponding cells in the grid map (Fig. 3b). This sparse grid map is then utilized to extrapolate the height values
    to adjacent grid cells via the implementation of a GP.


    Given a group of footholds Pi = (xi,zi), we stack their x-y positions into X = (x1, ...,xM) and heights Z = (z1, ...,zM).
    We model them as a GP with a data distribution of

    P (Z| X)∼ N (µ,K + σ 2

    n∗ I) (1)

    with µ∈ Rn the mean of Z, σn the noise and K the covariance matrix, Ki, j = k(xi,x j), where k is the kernel function
    [33]. Then the distribution of the prediction at position x∗ can be written as z∗∼ N (µ∗,v∗), where

    µ∗ = kT (

    K + σ 2

    n I

    )−1

    Z

    v∗ = k∗ + σ 2

    n− kT (

    K + σ 2

    n I

    )−1

    k

    (2)

    with k∈ Rn, k j = k(x∗,x j). Our experimental results indicate that employing solely the Radial Basis Function (RBF) kernel
    is sufficient to achieve a high level of accuracy and smoothness in the reconstructed support surface:

    k (xi,x j) = exp

    (

    −

    xi− x j

    2

    2l2

    )

    (3)

    To reduce the computational complexity, we fit multiple GPs to small overlapping tiles for each local grid map. Subsequently,
    the individual GPs are integrated into a composite grid map that provides a variance and height estimate to represent
    the support surface. The height value is determined by calculating the mean GP prediction for the overlapping regions,
    while the variance is set to the maximum variance.


    3) Support Surface Projection: To generate training labels for SPF depth estimation, we reproject the generated support
    surface (Sec. III-B.2) into all captured camera images along the trajectory. By raycasting the generated height grid map
    (Fig. 3c), we obtain correspondences between the pixels on the image plane and grid map cells. Given the correspondences,
    we can generate a support surface depth image capturing the distance from the camera center to the support surface (Fig.
    3e), as well as project the variance of the Gaussian process σ into the image plane (Fig. 3f). The variance of the Gaussian
    Process does not capture the uncertainty with respect to the distance from which the support surface is observed. We found
    experimentally that using a more sophisticated formulation incorporating the observation distance during training did
    not increase the performance. The variance is used to mask the region of valid depth information using a threshold parameter
    τmax, resulting in the support surface label yde used as a target during training.'
  citations:
  - marker: '[16]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Projection-based representations
  - marker: '[33]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Projection-based representations
- block_id: 9
  content: 'To leverage image-based computer vision techniques, P is projected to the camera image plane and obtain a sparse
    depth image xd∈ Rm×n. We use an image processing neural network to predict the filtered depth, ˆy = h(xrgb,xd). The filtered
    depth image is re-projected to the space as the filtered pointcloud ˆP.


    1) Network Structure: The neural network model follows an encoder-decoder structure with skip connections, as proposed
    in [27]. The encoder employs a pretrained EfficientNet-B5 [34] as the backbone, where the first convolution layer is adapted
    to accommodate 4 input channels. The input to the network is a stacked tensor of the sparse depth image xd and the camera
    image xrgb.


    The decoder concurrently generates the support surface depth ˆyde, and binary semantic segmentation mask ˆys with two
    classes: rigid obstacles and support surfce. The final output ˆy combines the support surface depth estimation ˆyde with
    the raw pointcloud xd

    ˆy = xd[ˆys == rigid] + ˆyde[ˆys == support ] (4)


    2) Training: The network is trained to minimize the total training loss Ltotal, given by the weighted sum of the support
    surface depth estimation loss Lde and semantic segmentation loss Lseg:

    Ltotal = Lseg + wLde (5)

    where w is used to balance the loss contributed by depth estimation and semantic segmentation. Lde of prediction ˆyde
    is computed as the mean squared error (MSE) with respect to SSDE labels, and is weighted by the pixel-wise variance σi
    of the support surface.

    .Lss = ∑

    i∈Css

    σ−2

    i

    (ˆyde,i− yde,i

    )2 (6)

    Css is set of pixels where yde has valid values. While Lseg is computed by employing the cross-entropy loss with respect
    to SSSeg labels, which is commonly used for semantic segmentation tasks.'
  citations:
  - marker: '[27]'
    intent_label: Model/Architecture Adoption
    topic_label: Projection-based segmentation
  - marker: '[34]'
    intent_label: Model/Architecture Adoption
    topic_label: Projection-based representations
- block_id: 10
  content: ''
  citations: []
- block_id: 11
  content: When generating the SSDE labels, we set the length of WB to 1 .5 s, length of SB to 0 .07 s and rt to 0 .015 m.
    In RBF kernel, the l is set to 1 e−5. For the depth image of the reconstructed terrain, we only use the region with a
    variance lower than τmax = 0.03 as our SSDE labels.
  citations: []
- block_id: 12
  content: The neural network is trained for 9000 steps ( ∼1 hour) using a batch size of 6 on a single NVIDIA GeForce RTX3090,
    with an EfficientNet-B5 [34] encoder pre-trained on ImageNet [35]. The resulting weights of the additional depth input
    channel and the decoder are randomly initialized following [36]. To train the network, we use the AdamW optimizer [37]
    and schedule the learning rate following [38]. In the loss function, the w is set to 0.02. During training for data augmentations,
    we first flip xrgb and xd horizontally with a probability of 0.5. Secondly, the input images are randomly cropped from
    a shape of 540 × 720 to 352 × 704.
  citations:
  - marker: '[34]'
    intent_label: Model/Architecture Adoption
    topic_label: Projection-based segmentation
  - marker: '[35]'
    intent_label: Resource Utilization
    topic_label: Hybrid multi-modal representations
  - marker: '[36]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Hybrid multi-modal representations
  - marker: '[37]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Projection-based segmentation
  - marker: '[38]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Projection-based segmentation
- block_id: 13
  content: We first describe the experiment dataset collection and training data generation (Sec. V-A). Then we test the trained
    SPF depth prediction performance and perform an ablation study (Sec. V-C). Finally, we showcase that the SPF filtered
    pointcloud can generate more accurate results in two downstream applications, namely elevation mapping (Sec. V-D) and
    traversability estimation (Sec. V-E).
  citations: []
- block_id: 14
  content: 'The data is collected using the legged robot ANYmal, controlled by a human operator, in various outdoor environments
    in Perugia, Italy and H önggerberg, Switzerland. ANYmal is equipped with a Robosense RS-Bpearl 32 beam Dome-LiDAR and
    an Alphasense Core 0 .4 MPix RGB camera unit. The camera images and LiDAR data are collected at a rate of 10 Hz, and downsampled
    during post-processing to 2 Hz, with image resolution at 540 × 720. We recorded six 10-20 minutes trajectories from Perugia,
    classified into three environments: Grassland, Hillside, and Forest. Each environment contains two trajectories. The training
    set and testing set each contain three trajectories, one from each of these three environments. For each trajectory, 100
    images were sampled at a 3.5-second interval to avoid repetition, and each image was manually annotated into two classes:
    support surface and rigid obstacles for semantic segmentation. Additionally, a trajectory was collected from H önggerberg,
    Switzerland, only for evaluating downstream applications.'
  citations: []
- block_id: 15
  content: The result presented in Fig. 5 shows the distribution of the absolute error of support surface depth estimation
    in relation to distance compared to the raw pointcloud. The absolute error is computed with respect to the SSDE labels
    from the camera and is binned by the distance. The result demonstrates that the performance of the raw pointcloud worsens
    in Grassland, Hillside, and Forest environments. The filtered pointcloud also exhibits this trend, indicating that the
    quality of the raw pointcloud directly affects the performance of our filtered pointcloud. Notably, in all environments,
    within any distance bins, the filtered pointcloud outperforms the raw pointcloud. The error increases with respect to
    the distance, which is common in monocular depth estimation tasks. Additionally, the reconstructed support surface, while
    being locally consistent in proximity to the robot, becomes less accurate at longer ranges due to localization drift.
  citations: []
- block_id: 16
  content: 'The first ablation assesses the impact of providing varying input modalities followed by investigating the use
    of hand-labeled SSSeg labels in the training of the semantic segmentation task. For rigors analysis, we train all models
    10 times with varying random seeds and report the standard deviation. We evaluate depth estimation and semantic segmentation
    separately for each model. To measure the performance of depth estimation, we use the following metrics:

    1) Average Relative Error (REL): 1

    n ∑n

    p

    |yp− ˆyp|

    y

    2) Root Mean Squared Error (RMSE):

    √

    1

    n ∑n

    p (yp− ˆyp)2

    )

    where all metrics are commonly used to evaluate monocular depth estimation performance [38]. The metrics are evaluated
    with respect to all the validation pixels of the SSDE label within 5m. The result is shown in Table I. It is important
    to note that the error in depth estimation is assessed within the image space, and therefore, does not directly correspond
    to the error in the estimation of the height of the support surface.


    We present an ablation comparison for support surface depth estimation in Table I. All our models, as well as the Only
    RGB and Only LiDAR input models, outperform the raw pointcloud. In average, our model outperforms single-modality models
    ( Only RGB , Only LiDAR ) by 14% in terms of RMSE. While single-modal models may yield better results in specific environments,
    our model remains competitive and close (less than 0 .02 m) to the best ablations in all environments. Conversely, ablation
    models can exhibit an RMSE increase of more than 20% in comparison to our model in less favorable environments. These
    findings indicate that the multi-modal design significantly improves the model’s robustness in the face of varying environmental
    conditions.


    We evaluate the semantic segmentation of ours and the ablated models with the mean intersection of union (mIoU) metric,
    which is a widely adopted measurement in the field of semantic segmentation tasks [39]. We show the results for two exemplary
    inputs in Fig. 6, and provide numerical mIoU values in Table II. Surprisingly, our results in Table. II indicate that
    our model, as well as two ablations, demonstrate comparable performance in mIoU. This can be attributed to two factors.
    First, as illustrated in Fig. 6, the majority of the image is relatively straightforward to segment. The key challenge
    lies in accurately determining the boundaries of the support surface, which represents only a minor portion in comparison
    to the entire label. This causes the mIoU to be less sensitive to the segmentation of the area of interest. Secondly,
    while RGB images provide more explicit semantic information than pointcloud, they can suffer from poor lighting conditions
    (as demonstrated in Fig. 6), making it difficult to generalize on our small dataset. Consequently, incorporating geometric
    information into our model can potentially enhance its robustness. Considering all three environments, our ablation shows
    that multi-modal and single-modal perform on par for the task of semantic segmentation.


    We also investigate the necessity of manually labeled segmentation labels by comparing it to solely employing labels generated
    based on the valid regions of the SSDE label Css and considering other regions as rigid obstacles. This label does not
    reflect the scene well given that all untraversed regions are regarded as obstacles/ rigid. As shown in Table II, the
    models with hand-labeled data exhibit significant improvement to the model trained without hand-labels.'
  citations:
  - marker: '[38]'
    intent_label: Metrics Utilization
    topic_label: Projection-based representations
  - marker: '[39]'
    intent_label: Metrics Utilization
    topic_label: 'Semantic Segmentation: Approaches'
- block_id: 17
  content: 'To validate SPF’s practical benefits, we input the filtered pointcloud into an elevation mapping algorithm [3],
    which fuses the pointcloud with the robot pose to generate an 8 m×8 m grid map at 0 .04 m resolution. We compare our method
    with three baselines: Raw generates maps using the same elevation mapping algorithm but inputs the raw pointcloud. Smooth
    involves applying a smooth operation [3] on the elevation map generated from Raw. For these two baselines, we only feed
    the points within the camera’s field of view (FoV) into the algorithm. Moreover, Foothold refers to the nearest-neighbor
    completion of the footholds, where the robot traversed. This baseline exemplifies methods relying solely on proprioceptive
    observation and a flat-ground assumption.


    The result in Fig. 7 shows three elevation mapping samples together with two baseline methods. In each image, we illustrate
    the robot as non-transparent (past) and transparent (future). These robots allow us to validate the elevation mapping
    accuracy by examining the foothold alignment with the estimated terrain surface. Sample (a) features high grass. The elevation
    map constructed with our SPF matches the robot’s foot height, while the maps from the two baselines are higher than the
    robot’s leg. Smoothing the raw elevation map reduces its root-mean-square error (RMSE) by 12.6% while our method reduces
    it by 56.0%. Sample (b), set in a forest with minimal grass, also demonstrates our method’s superior performance in representing
    support surfaces and recognizing obstacles. Sample (c), taken from H önggerberg Zurich, highlights a domain shift from
    nature to an urban setting. Similar to (a), our method reduces the RMSE by 42.0% compared to the raw elevation map and
    accurately captures the fence’s structure. These comparisons demonstrate that our SPF effectively and robustly distinguishes
    vegetation from rigid obstacles, correctly lowering the perceived vegetation height while preserving the structure of
    rigid obstacles.


    For quantitative comparison over the testing trajectory in Perugia Grassland, we compare the constructed elevation map
    with the SSDE labels described in Sec. III-B. Specifically, elevation maps are generated at a rate of 10 Hz as the trajectory
    is replayed. On each update of the maps, an error is computed between the new map and the SSDE labels. Then we transform
    these error maps into the robot’s frame, accumulate them, and compute the RMSE, and error variance, as depicted in Fig.
    8. The trajectory is from the test set featuring a hillside. In the trajectory, the robot was mostly walking forward with
    some turning motions (The forward direction is shown in the bottom right of Fig. 8). Moreover, the elevation perceived
    in the front goes towards the back of the robot’s frame as the robot moves forward. Therefore, the upper parts (the part
    in FoV) of each subfigure correspond to the ”prediction” of elevation height, while the lower parts correspond to the
    mapping result fused over several frames.


    The elevation map constructed with our filtered pointcloud has an RMSE of 0 .133 m, which is about 48% less than the 0.183
    m of the raw baseline. Smoothing results in an RMSE of 0 .170 m, which is only a 7% improvement. Our method has a lower
    error at both the front and back of the robot. The high error value on the top left is exceptional and is caused by overexposure
    when the robot leaves tree shades. As the footholds are exact samples of the support surface, foothold baseline achieves
    almost zero RMSE in the areas under the robot and behind the robot. However, it predicts the height of the untraversed
    area in the front much worse than the methods with exteroceptive observations. The standard deviations of the errors are
    shown in the second row. The error from the smooth baseline is more stable, resulting in a lower standard deviation. Both
    our method and the raw pointcloud have comparable standard deviations for their errors. Please note that thanks to the
    modularity design of our proposed SPF. It’s possible to leverage the benefits of both pointcloud filtering and smoothing
    and obtain prediction that is more accurate and stable.'
  citations:
  - marker: '[3]'
    intent_label: Setting/Protocal Adoption
    topic_label: Projection-based representations
- block_id: 18
  content: Filtering out noisy high grass and revealing true support terrain is beneficial for an accurate traversability
    estimation. We estimate the traversability using the method described in [40]. Elevation maps constructed in Sec. V-D
    are used as the input to this algorithm. Two samples of traversability estimation in the natural environment are shown
    in Fig. 9. The grid maps on the right rows are colored with traversability, where a high traversability corresponds to
    blue, while low to red. When the robot is in the vegetation, our method recognizes vegetation and estimates the underlying
    support surface leading to the correct traversability estimate. On the other hand, directly using the pointcloud or the
    smoothed version lead to incorrect traversability rendering motion planning and navigation impossible. When the robot
    is in the forest, our method and baselines all yield reasonable traversability maps.
  citations:
  - marker: '[40]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Projection-based representations
- block_id: 19
  content: 'In this work, we present our semantic pointcloud filter (SPF) trained in a semi-self-supervised manner, which
    can accurately adjust the pointcloud to the support surface on a vegetation-occluded terrain, leveraging semantics from
    the camera images and pointcloud. Using pointcloud filtered by SPF, we improved elevation mapping and traversability estimation
    performance compared to existing baseline methods, potentially increasing autonomy for a variety of robotic systems within
    natural environments.


    While multiple real-world experiments demonstrated the efficacy of the proposed method, limitations exist, such as errors
    in filtered depth estimation under adverse lighting conditions and the possibility of inaccuracy due to domain shifts
    in vegetation species and seasons. Future research will explore the incorporation of anomaly detection into label generation
    for semantic segmentation, eliminating the need for manual labeling and scaling up data collection to enhance robustness
    further. Moreover, integrating the pointcloud filter with downstream tasks, including locomotion, path planning, and exploration,
    holds significant promise.'
  citations: []
