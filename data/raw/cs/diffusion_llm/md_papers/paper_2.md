# An Integration Model Based on Graph Convolutional Network for Text Classification

## Abstract
Graph Convolutional Network (GCN) is extensively used in text classiﬁcation tasks and performs well in the process of the non-euclidean structure data. Usually, GCN is implemented with the spatial-based method, such as Graph Attention Network (GAT). However, the current GCN-based methods still lack a more reasonable mechanism to account for the problems of contextual dependency and lexical polysemy. Therefore, an improved GCN (IGCN) is proposed to address the above problems, which introduces the Bidirectional Long Short-Term Memory (BiLSTM) Network, the Part-of-Speech (POS) information, and the dependency relationship. From a theoretical point of view, the innovation of IGCN is generalizable and straightforward: use the short-range contextual dependency and the long-range contextual dependency captured by the dependency relationship together to address the problem of contextual dependency and use a more comprehensive semantic information provided by the BiLSTM and the POS information to address the problem of lexical polysemy. What is worth mentioning, the dependency relationship is daringly transplanted from relation extraction tasks to text classiﬁcation tasks to provide the graph required by IGCN. Experiments on three benchmarking datasets show that IGCN achieves competitive results compared with the other seven baseline models.

## Index Terms
Bidirectional long short-term memory network, dependency relationship, graph convolutional network, part-of-speech information, text classiﬁcation.

## Introduction
Text classiﬁcation is always a hot topic of Natural Language Processing (NLP), which is widely applied for text recognition and opinion extraction [1]–[4]. Currently, a large amount of the non-euclidean structure data, which can be quantiﬁed and analyzed, is generated by the social media every day, such as social network reviews, interview records, product reviews, email records, etc. Over the past few decades, the non-euclidean structure data has been studied mainly based on the traditional classiﬁcation methods (e.g. Support Vector Machine (SVM) [5]), the typical neural network methods (e.g. Convolutional Neural Network (CNN) [6], Recurrent Neural Network (RNN) [7], Capsule Networks [8]), and their excellent variants (e.g. Bidirectional Recurrent Neural Network (BRNN) [9], Long Short-Term Memory (LSTM) [10] Network, Gated Recurrent Unit (GRU) [11], Recurrent Convolutional Neural Network (RCNN) [12], Convolutional Recurrent Neural Network (CRNN) [13]). However, these methods have been greatly challenged on the graph-structure data. For instance, the graph-structure data cannot be directly processed by CNN, for the reason that CNN cannot maintain the translation invariant. Besides, the ﬁxed size of the convolution kernel limits the range of the dependency. Therefore, the methods based on Graph Convolutional Network (GCN) [14] receive a growing attention from researchers and engineers. With regarding the graph as a spectral graph, GCN can realize the end-to-end learning of node feature and structure feature. Moreover, GCN is applicable to the graph of arbitrary topology. Although GCN is gradually becoming a good choice for text classiﬁcation based on the graph, there are still certain defects not to be neglected in current study.

The original GCN cannot capture the short-range contextual dependency and the long-range contextual dependency together. Look at this example "the movie, which is the product of an unknown French director, is wonderful." Due to the mechanism that GCN only aggregates the information of the direct neighbor nodes, GCN can only capture the short-range contextual dependency information. This question can only be solved by increasing the number of GCN layers to capture the long-range contextual dependency, such as the dependency between the words "movie" and "wonderful". However, the current researches reveal that the multi-layer GCN for text classiﬁcation tasks will give rise to a high spatial complexity [15]. Meanwhile, the over-smoothing of the node feature will also be caused by increasing the number of network layers, which will make local features converge to a similar value.

In addition to the problem of contextual dependency, the problem of lexical polysemy also exists in GCN. The problem of lexical polysemy can be described that the same word may express different semantics in same or different positions. In the sentences "I bought an apple." and "I bought an apple X.", the meaning of the word "apple" is different due to the difference of the context. Meanwhile, in the sentences "Our object is to further cement trade relations." and "Their voters do not object much.", the meaning of the word "object" is also different owing to the difference of the part-of-speech. Although the relevant researches have claimed that the problem of lexical polysemy can be solved without relying on the syntactic information and the semantic information, their effects are not up to expectation unfortunately [16], [17].

To overcome the problems of contextual dependency and lexical polysemy, an improved GCN (IGCN) is proposed for text classiﬁcation in this paper. Based on the original GCN, IGCN introduces the Bidirectional Long Short-Term Memory (BiLSTM) [18], [19] Network, the Part-of-Speech (POS) information, and the dependency relationship reasonably. In this paper, the text feature and the POS feature sequentially obtained through BiLSTM will be applied to solve the problem of lexical polysemy. Through constructing the dependency relationship, IGCN can take good advantage of the short-range contextual dependency and the long-range contextual dependency together. Meanwhile, the adjacency matrix based on the dependency relationship will also be generated to provide syntactic constraints. Subsequently, the different attentions of the neighbor nodes to the central node can be learned during the aggregation process. Namely, the weights of the features will be calculated by the attention mechanism in the propagation process. What is worth mentioning, to provide the graph required by IGCN, the dependency relationship is daringly transplanted from relation extraction tasks to text classiﬁcation tasks. The difference of capturing the dependency between the original GCN and IGCN is shown in Fig. 1.

Experiments on three benchmarking datasets demonstrate that the problems in the current GCN-based method can be effectively addressed by IGCN which has some advantages than the other researches. The main contributions of this paper are as follows:
- The text information and the POS information can be effectively applied to generate the initial features by BiLSTM. Simultaneously, the features can not only make up for the deﬁciency of the content-level context effectively, but also provide a new idea to address the problem of lexical polysemy.
- The dependency relationship and the attention mechanism are fully integrated into the IGCN. They can effectively deal with the problem of contextual dependency, and reduce the number of GCN layers partly. Namely, they can solve the high space complexity and over-smoothing caused by the multi-layer GCN indirectly. Such a cross-task study is useful to meet the challenges in NLP, and further demonstrates the signiﬁcance of the dependency relationship.
- A large number of experimental results not only prove the reasonability of integrating the BiLSTM, the POS information, and the dependency relationship, but also prove the efﬁciency of IGCN for text classiﬁcation. IGCN will contribute to the continuous development of the research ﬁeld of the non-euclidean structure data.

## Related Work
Unlike the traditional classiﬁcation methods based on extracting text features manually, the current methods based on the deep learning can directly output the category of the text by training the neural network. For example, Tang et al. [20] adopted two LSTMs for text classiﬁcation which effectively integrate sentiment words and contextual information. Zhang et al. [21] classiﬁed texts through introducing the sentiment word information into BiLSTM. Yang et al. [22] integrated the common sense into the deep neural networks based on BiLSTM to enhance the accuracy of text classiﬁcation. Xue and Li [23] utilized CNN and the gate mechanism inversely to achieve higher accuracy which broke away from the network structure on the base of RNN and attention mechanism. Huang and Carley [24] achieved amazing results through designing the parameterized filters and the gate mechanism on CNN to capture text features. Li et al. [25] proposed a feature transformation component and a context retention mechanism to learn contextual information and combined contextual features with their transformed contextual features to obtain local salient features. Dong et al. [26] proposed a CNN with multiple non-linear transformations which has pursued a good result. Akhter et al. [27] achieved a great performance by introducing the single-layer CNN with multiple filters into document-level text classiﬁcation.

At the same time, the attention-based model had also been proposed, which was used to capture the weight of each word within a sentence. Wang et al. [28] introduced the attention mechanism into LSTM, which provided a new idea for text classiﬁcation. Chen et al. [29] introduced the product and user information of different semantic levels to classify texts through the attention mechanism. Liu and Zhang [30] proposed a method which introduces three attention mechanisms to determine the contribution of each word in the context in text classiﬁcation. Gu et al. [31] proposed a position-aware bidirectional attention network based on the Bidirectional Gated Recurrent Unit(BiGRU) for text classiﬁcation. Especially, Vaswani et al. [32] proposed the self-attention mechanism which is good at capturing the internal relevance between features and is less dependence on external information. The self-attention mechanism not only overcame the shortcoming of being unable to calculate parallelly on RNN, but also solved the problem of capturing the long-range dependency information difficultly on CNN. Dong et al. [33] obtained text representations containing more comprehensive semantics for text classiﬁcation by introducing the Bidirectional Encoder Representation from Transformers(BERT) [34] and the self-interaction attention mechanism.

When processing text classiﬁcation through the attention-based model, many breakthroughs in the ﬁeld of node classiﬁcation and edge prediction have been made with GCN in recent years. Hamilton et al. [35] proposed a variety of aggregation functions to learn the feature representation of each node to enhance the effect of GCN. Chen et al. [36] proposed a random training method. Their method can reduce the time complexity greatly with selecting two neighbor nodes for the convolution operation randomly. In the case of different sampling sizes, features will converge to a local optimum. Li et al. [37] proposed a method that can adaptively construct new Laplacian matrices based on tasks and generate different task-driven convolution kernels. This method is superior to GCN in processing multitask datasets. Velickovic et al. [38] used the attention mechanism to calculate the correlation between nodes dynamically and achieved good results in many public datasets. Yao et al. [39] introduced GCN for text classiﬁcation and modeled the whole corpus into a heterogeneous network, which can learn word embeddings and document embeddings simultaneously. Cavallari et al. [40] introduced a new setting for graph embedding, which considers embedding communities instead of individual nodes.

Although GCN performs well in text classiﬁcation, it still fails to solve the problems of contextual dependency and lexical polysemy in text classiﬁcation tasks. With addressing the two problems as a starting point, an improved model based on GCN is proposed accordingly.

## IGCN
Through an in-depth research on text classiﬁcation based on the neural network, IGCN is proposed in this paper, which builds three components of BiLSTM, the POS information, and the dependency relationship on GCN. The whole process of IGCN can be divided into steps of extracting the text feature, concatenating the POS feature, constructing the adjacency matrix based on the dependency relationship, training the neural network, and making a ﬁnal prediction. Firstly, the text features and their corresponding POS features will be successively obtained by BiLSTM, which can utilize their respective contextual information effectively. Then, two kinds of features will be concatenated together to form the required feature of IGCN. Meanwhile, the dependency relationship will be generated to confront the problem of contextual dependency, and to construct the adjacency matrix required by IGCN. After that, the feature and the adjacency matrix will be input to train the neural network. With the hidden state vectors of the last layer obtained, the weights between features and hidden state vectors can be calculated to determine the contribution of each word by the attention mechanism. Subsequently, the final features will be generated to predict the category of the given sentence.

### The basics of IGCN: GCN
As known, the original GCN takes the pre-processed words as nodes and takes the relationship between words as edges. The original GCN can be divided into the input layer, the hidden layer, and the output layer.

#### The input layer of the original GCN
The input layer of the original GCN consists of the input feature matrix and the adjacency matrix of the graph.

The adjacency matrix is provided to express the reference relationship between the nodes. Let X ∈ R^{N×D} be the input feature matrix where N is the size of V, and D represents the dictionary set size of V. When one word in the i-th node is at the m-th position of the dictionary set, it can be expressed as X_{im}={0_1,...,0_{m−1},1_m,0_{m+1},...,0_D}. Let A ∈ R^{N×N} be the self-loop adjacency matrix of the graph G. The adjacency matrix of an undirected graph can be expressed as follows.

A_{ij}= A_{ji}=
{
0, i /→ j
1, i→ j
1, i= j
}
(1)

where A_{ij} represents that whether there is a connection between the i-th node and the j-th node.

#### The hidden layer of the original GCN
The hidden layer of the original GCN can aggregate the node information of the current layer through the propagation rules and transmit the features to the next layer. The features become more abstract as the propagation through successive hidden layers. The layer-wise propagation rules of the i-th node can be expressed as follows.

h_i^l = σ( Σ_{j=1}^N \bar{A}_{ij} · W^l · h_j^{l−1} + b^l )
\bar{A} = D^{−1/2} · A · D^{−1/2}
D_{ii} = Σ_{j=1}^N A_{ij}
(2)

where W^l is a trainable linear transformation weight which can be obtained by minimizing the cross-entropy loss function on all labeled samples. b^l is a bias term. \bar{A} ∈ R^{N×N} is the normalization adjacency matrix of the graph G. D ∈ R^{N×N} is the degree matrix of the graph G. σ denotes a nonlinear activation function (e.g. ReLU). h_i^l is the i-th node feature of the l-th hidden layer. Initially, h_i^0 = X.

#### The output layer of the original GCN
After obtaining the final features of the hidden layer, the output layer of the original GCN can generate the probability value of each category through the softmax function and classify the category of the text according to the maximum value of the probability.

### The input layer of IGCN
The quality of the initial feature directly affects the performance of the text classiﬁcation model. In terms of feature extraction, the one-hot encoding features are generated with building a global text dictionary in the original GCN, which belong to the shallow feature. Therefore, to obtain more advanced and abstract features, BiLSTM is introduced to implement a deeper extraction of text features. On one hand, the text is a kind of the non-euclidean structure data. BiLSTM can retain the position information of text and capture the serialized features of text. On the other hand, the general social media text belongs to the typical short text. The gate mechanism of BiLSTM can effectively solve the problems of less contextual information and ambiguous semantics in such a short text. Besides, the bi-directional mechanism of BiLSTM ensures that each word can obtain more semantic information with full consideration of the context. Such a bi-directional model can provide a deeper text feature representation to the neural network.

Given a k-words sentence S = {W1, W2,..., Wk}, the model embeds the initial input text through the pre-trained embedding matrix. Then the text feature representation matrix M ∈ R^{k×d} can be obtained, where k is the vocabulary size of the sentence S and d represents the dimension of word embedding. Meanwhile, the text feature representations F = {f1, f2,..., fk} of the sentence S with context information can also be got by BiLSTM, where f_i ∈ R^{k×d_f} represents the text feature of the i-th word. d_f is the dimension of the hidden state vector of BiLSTM.

In view of that the extraction of feature mentioned above is not sufficient to deal with the problem of lexical polysemy, the POS information of text is introduced to further eliminate the problem. Unlike the other attributes, the POS information is a basic grammatical attribute of the word which does not vary much with the difference of the ﬁeld. As everyone knows, the short text lacks a certain degree of grammar and does not contain the whole rigorous structure information. Hence, more acceptable information will be provided by POS feature for short text classiﬁcation.

Accordingly, the POS information will be input to BiLSTM to generate the POS feature representations P = {p1, p2,..., pk}, where p_i ∈ R^{k×d_f} is the POS feature of the i-th word.

Through the above feature extraction, the text feature representations F and the POS feature representations P have been obtained by BiLSTM. On this basis, the concatenation operation is chosen in this paper to effectively utilize the POS information to solve the problem of lexical polysemy. The text feature representations F and their corresponding POS feature representations P are concatenated by

fp_i = Concat(f_i, p_i)
(3)

where fp_i ∈ R^{k×2d_f} represents the feature of the i-th word.

Given the loose syntactic constraint and the vague dependency, the dependency relationship is introduced to reveal a clear syntactic structure for text classiﬁcation. The so-called dependency relationship is a more ﬁne-grained attribute which can recognize the grammatical components in a sentence. To a certain extent, this paper also pays some attention to non-notional words (e.g. prepositions) in structure analysis. The dependency relationship graph is applied to construct the adjacency matrix A ∈ R^{k×k} of a given sentence.

### The hidden layer of IGCN
Compared with the adjacency matrix constructed by the dependencies of each node to its two neighbor nodes, the adjacency matrix based on the dependency relationship can provide the short-range contextual dependency and the long-range contextual dependency together, which is more effective to solve the problem of contextual dependency. Furthermore, an example is also used to explain how to construct the adjacency matrix with dependency relations.

In the original GCN, the weights of edges are uniformly set to 1 so that the inﬂuences of each node to its neighbor nodes are same. However, not all edges have the same weight in reality and some of them must be important to text classiﬁcation. For example, in the sentence "I like this movie so much.", the edge between the nodes "like" and "much" should contribute much more to text classiﬁcation than the edge between the nodes "this" and "movie". Therefore, it is necessary to capture which neighbor nodes contribute more to the central node. To learn the contributions of neighbor nodes to the central node, the attention mechanism based on the dependency relationship is also introduced. The attention mechanism acts between the central node and neighbor nodes, which builds up the performance of the model with quantifying the contributions between nodes.

In this paper, the similarity between nodes is selected to measure the dependency relationship. Based on the principle of not destroying the integrity of network, the topological information of the graph will be quantitatively analyzed from the perspective of local attributes and global attributes.

\tilde{A}_{ij} = A_{ij} · α_{ij}
α_{ij} = softmax(e_{ij}) = exp(e_{ij}) / Σ_{i=1}^k exp(e_{ij})
e_{ij} = f_{pi} · f_{pj}
(4)

where (4) is used to calculate the inﬂuences between the i-th node and its neighbor nodes with the dependency relationship. The softmax function ensures that the sum of each row of the matrix is 1. In other words, the sum of the inﬂuences of all nodes in the sentence to the i-th node is 1 during the feature aggregation process. α_{ij} is the inﬂuence of the j-th node to the i-th node. e_{ij} is the similarity of the j-th node to the i-th node. A dot product operation is used to calculate the similarity between nodes. The larger the value of e_{ij} is, the more similar the i-th node and the j-th node are.

After the above data processing, the feature representations FP required by IGCN and the adjacency matrix \tilde{A} based on the attention mechanism are both obtained. Subsequently, they will be fed into a single-layer GCN for training, and the hidden state vectors H = {h1, h2,..., hk} can be obtained.

h_i = σ( \tilde{A}_i · FP · W_h + b_h )
(5)

where h_i ∈ R^{k×2d_f} represents the hidden state vector of the i-th word, W_h is the trainable weight matrix, and b_h is a bias term.

### The output layer of IGCN
With the feature representations FP and the hidden state vectors H obtained, the attention mechanism is introduced once more to ﬁnd the important relevant semantical features. Unlike the general attention network, the contribution μ = {μ1, μ2,..., μk} of each word to text classiﬁcation is obtained with linking the feature representations FP with the hidden state vectors H in the output layer. Meanwhile, the final feature representation y of the sentence is also obtained as follows.

y = μ · FP
μ_i = softmax(λ_i) = exp(λ_i) / Σ_{i=1}^k exp(λ_i)
λ_i = Σ_{j=1}^k h_i · fp_j^T
(6)

where μ_i is the contribution of the i-th node to text classiﬁcation and λ_i is the sum of the similarity between the i-th node and all nodes in the sentence.

Subsequently, the final feature representation y is input into the fully-connected layer and the category of text will be predicted by the softmax function.

Y = softmax(y · W + b)
(7)

where W is a trainable weight matrix and b is a bias term.

## Experiments
In this section, this paper first describes how to set up the experiment and analyzes the experimental results of different models. Subsequently, the ablation study is implemented to further prove the performance brought by each component.

### Datasets and parameter settings
The following three datasets are adopted in the experiment.

- IMDB (Internet Movie DataBase) dataset, which belongs to the binary-classiﬁcation English dataset, contains 10,662 pieces of the internet movie review data provided by the data competition platform Kaggle. The training set contains 6,000 pieces of data and the test set contains 4,662 pieces of data, where the positive comments and the negative comments in the training set and the test set account for 50% respectively.

- AAR (Amazon Alexa Reviews) dataset, which belongs to the multiclass-classiﬁcation English dataset, contains 3,150 pieces of the Amazon Alexa product review data collected in 2018. The training set contains 1,575 pieces of data, and the test set contains 1,575 pieces of data.

- TUA (Twitter US Airline) dataset, which belongs to the multiclass-classiﬁcation English dataset, contains 14,640 pieces of Twitter user comments on American Airlines collected in 2015. The training set contains 7,320 pieces of data and the test set contains 7,320 pieces of data. The negative comments, the neutral comments, and the positive comments in the training set and the test set account for 63%, 21%, and 16% respectively.

Two pre-trained vector matrices of Yelp and Sogou News are used in this paper to achieve the initial word embedding. The initialization of weight matrix randomly adopts three methods: uniformly distributed initialization, normal distribution initialization, and orthogonal initialization. All the signiﬁcant parameters in the experiment are recorded.

In the training process, the dropout is used to introduce the randomness and prevent overﬁtting in this paper, and the nonlinear function LeakyReLU is selected as the activation function of GCN to overcome the problem of vanishing gradient and to speed up the training speed. The cross-entropy loss function is used to train the model.

LeakyReLU(x) = { 0.2x, x < 0; x, x ≥ 0 } (8)

Besides, the L2 regularization is also used to reduce the overﬁtting. The adaptive moment estimation optimizer is selected as the model optimizer. The stopping condition of the training process is that the loss function has not decreased for 10 consecutive iteration cycles.

To comprehensively evaluate the model, the accuracy is selected as the evaluation metric and IGCN is compared with the following baseline models.

- SVM is a binary-classiﬁcation model of the supervised learning. It is commonly applied for text classiﬁcation, pattern recognition, and regression analysis. Moreover, SVM is mainly oriented to small samples, nonlinear data, and high-dimensional data.
- TextCNN is a speciﬁc form of CNN. It obtains the shallow feature representation of the sentence effectively with the one-dimensional convolution operation. Moreover, it performs well in the ﬁeld of short text classiﬁcation, such as the search ﬁeld and the dialogue ﬁeld.
- LSTM is a speciﬁc form of RNN, which achieves good results in both the short text classiﬁcation and the long text classiﬁcation.
- BiLSTM is a speciﬁc form of LSTM, which further captures bidirectional semantic dependencies.
- GCN is a classiﬁcation model based on the non-euclidean structure data, which is gradually emerging in the ﬁeld of text classiﬁcation.
- GAT is a classiﬁcation model which borrows the attention mechanism into GCN.
- TextGCN is a text classiﬁcation model with utilizing the document-word relation and the word-word relation, which gains a variety of improvements over many state-of-the-art models.

### Main experimental results
The experimental results on three benchmarking datasets conﬁrm that the performance of IGCN is better than other baseline models, which further prove the effectiveness and robustness of IGCN in text classiﬁcation.

The accuracy of SVM on the IMDB dataset reaches 77.75%, which is higher than the accuracies of the six models based on neural networks (TextCNN, LSTM, BiLSTM, GCN, GAT, and TextGCN). On the AAR dataset and the TUA dataset, the accuracies of SVM are lower than the accuracies of the other four models based on the neural network (TextCNN, LSTM, BiLSTM, and GAT), which only reach 64.66% and 69.4%. Accidentally, the accuracy of SVM is higher than the accuracy of GCN. It shows that SVM is weak to tackle the large-scale training samples and the multi-classiﬁcation problems. Besides, the accuracies of TextCNN on the three benchmarking datasets are surprisingly higher than the accuracies of LSTM, BiLSTM, GCN, GAT, and TextGCN. It means that the text classiﬁcation model may pay more attention to the short-range dependency information on the three benchmarking datasets.

The loss value of IGCN on the IMDB dataset decreases faster than the loss value of GCN and the accuracy of IGCN on the IMDB dataset increases faster than the accuracy of GCN. The results show that IGCN can improve the effectiveness and achieve better results in less iteration.

Compared with the other models based on neural network, the accuracies of the graph-based GCN are lowest on the three benchmarking datasets, which reach 73.9%, 63.58%, and 68.36% respectively. The reason is that GCN cannot make full use of contextual dependency information in the short text classiﬁcation due to the sparse adjacency matrix constructed by GCN. At the same time, compared with GCN, IGCN boosts the performance on the IMDB, AAR, and TUA datasets. IGCN increases the accuracies by 6.95%, 4.49%, and 5.46% respectively, which fully proves the effectiveness of IGCN. There are two reasons for this result.

- The multiple effects of the pre-trained word embedding, the BiLSTM, and the POS information are fully utilized in this paper to extract more advanced and abstract feature representations to further solve the problem of lexical polysemy. At the same time, the dependency relationship introduced in this paper can make reasonable use of the short-range contextual dependency and the long-range contextual dependency together.
- The attention mechanism introduced in this paper captures not only the importances of different neighbor nodes, but also the importances of different types of nodes. It can reduce the weight of noise nodes to a certain extent.

### Additional experimental results
Whether using the same BiLSTM to generate the different kinds of feature representations is studied on the IMDB dataset. To analyze its impact on the performance of IGCN, there are three cases listed as follows.

- One case is that the same BiLSTM is used to train the text features F and the POS feature P successively, which is used in IGCN.
- Another case is that the same BiLSTM is used to train the POS feature P and text features F successively, which is named as MPF.
- The last case is that two BiLSTMs are used to train the feature, which is named as M2Bi.

The experimental results on the three datasets illustrate that the accuracies of IGCN have increased by 0.9%, 1.02%, and 1.33% respectively compared with M2Bi. Using the same BiLSTM to generate the text feature and the POS feature can improve the overall performance of IGCN.

Moreover, the effect of the number of layers on the performance of IGCN is also studied on the IMDB dataset. The results show that IGCN achieves the best performance when the number of layer is 1. Meanwhile, it is found that the accuracy decreases when the number of layer increases.

### Ablation study
To further examine the beneﬁt brought by each component of IGCN, an ablation study is implemented in this paper. The results of IGCN on the three datasets are presented as the baseline. The experimental results of IGCN and the three variant models prove that the three components applied in this paper can promote the performance of IGCN.

After the dependency relationship is removed, the model named as MRD is only constructed by learning the dependency information between each node and its two neighbor nodes. The accuracies of MRD on the IMDB, AAR, and TUA datasets decrease by 18.17%, 15.63%, and 17.65% respectively compared with the accuracies of IGCN. The average decrease rate of its accuracy reaches 17.15%. It shows that the dependency relationship is signiﬁcantly relevant to text classiﬁcation based on the graph. The weights of neighbor nodes to the central node can be deeply learned to further boost the performance of model by the attention mechanism on the dependency relationship.

Subsequently, after the POS feature is removed, the performance of the model named as MRP also drops. The accuracies of MRP on the three datasets are lower than the accuracies of IGCN, which decrease by 5.3%, 6.29%, and 4.8% respectively. The experimental results demonstrate that the introduction of the POS information can solve the problem of lexical polysemy. With the average decrease rate of the accuracy reaching 5.46%, it means that the POS information works in text classiﬁcation tasks.

Finally, after BiLSTM is removed, the performance of the model named as MRBi also drops. Compared with the accuracies of IGCN, the accuracies of MRBi on the IMDB, AAR, and TUA datasets decrease by 16.13%, 13.92%, and 16.17% respectively. The average decrease rate of the accuracy reaches 15.41%. Its decrease of the accuracy proves that BiLSTM can enhance the performance of IGCN with the contextual dependency information.

To sum up, the decreases of the accuracies of MRD, MRP, and MRBi prove that each component of IGCN proposed in this paper is beneﬁcial to address the problems in text classiﬁcation.

## Conclusion and Future Work
Based on a review of the existing challenges faced in text classiﬁcation, the applicability of the GCN-based model is discussed. In this paper, an improved integration model based on GCN is formulated, analyzed, and performed to handle the problems of contextual dependency and lexical polysemy. Experiments on three datasets show that IGCN performs better than other models, especially than GCN. Thus, the success of IGCN can be attributed to the integration of three components. On the one hand, the BiLSTM and the POS information can generate deeper feature representations for text classiﬁcation. On the other hand, the dependency relationship provides a required graph for GCN which can be used to capture the short-range contextual dependency and the long-range contextual dependency together. Due to the good performance with the reasonable combination of dependency relationship and GCN, it will certainly lead to a further exploration of the dependency relationship in the future. What is more, the transplantation of the dependency relationship from relation extraction tasks to text classiﬁcation tasks can also give a new idea to process the non-euclidean structure data.

There is also a domanial problem in text classiﬁcation. Some text in different ﬁelds has the same literal meaning. However, its importance is different for text classiﬁcation. For example, the word "cancer" is a neutral tendency in the medical ﬁeld but a negative tendency in other ﬁelds. The difference of the tendency may cause a wrong prediction. Therefore, an in-depth study will be carried out in the domain knowledge transfer.