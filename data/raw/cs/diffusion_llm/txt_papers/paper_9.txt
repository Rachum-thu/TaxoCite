3124 IEEE JOURNAL OF BIOMEDICAL AND HEAL TH INFORMA TICS, VOL. 29, NO. 5, MA Y 2025
Multimodal Distillation Pre-T raining Model for
Ultrasound Dynamic Images Annotation
Xiaojun Chen ,J i aK e , Y aning Zhang , Jianping Gou , Senior Member, IEEE , Anna Shen ,
and Shaohua Wan , Senior Member, IEEE
Abstract—With the development of medical technol-
ogy, ultrasonography has become an important diagnostic
method in doctors’ clinical work. However, compared with
the static medical image processing work such as CT, MRI,
etc., which has more research bases, ultrasonography is
a dynamic medical image similar to video, which is cap-
tured and generated by a real-time moving probe, so how
to deal with the video data in the medical ﬁeld and cross
modal extraction of the textual semantics in the medical
video is a difﬁcult problem that needs to be researched.
For this reason, this paper proposes a pre-training model
of multimodal distillation and fusion coding for processing
the semantic relationship between ultrasound dynamic Im-
ages and text. Firstly, by designing the fusion encoder, the
visual geometric features of tissues and organs in ultra-
sound dynamic images, the overall visual appearance de-
scriptive features and the named entity linguistic features
are fused to form a uniﬁed visual-linguistic feature, so that
the model obtains richer visual, linguistic cues aggregation
and alignment ability. Then, the pre-training model is aug-
mented by multimodal knowledge distillation to improve the
learning ability of the model. The ﬁnal experimental results
on multiple datasets show that the multimodal distillation
pre-training model generally improves the fusion ability of
Received 4 January 2024; revised 27 June 2024; accepted 30 July
2024. Date of publication 5 August 2024; date of current version 7 May
2025. This work was supported in part by the Major Project of Philos-
ophy and Social Science Research in Higher Education Institutions in
Jiangsu Province under Grant 2023SJZD026, in part by the National
Natural Science Foundation of China under Grant 61976107, in part
by Zhenjiang Policy Guidance Plan under Grant RK2023011, in part
by Research Project on Reﬁned Management and Evaluation of Public
Hospitals in National Health Commission under Grant NIHA23JXH031,
in part by “1 + 12” Public Hospital Quality Development Innovation
and Enhancement Project of Zhenjiang Medical Group under Grant
GZL202406, and in part by Jiangsu Province Hospital Association Hos-
pital Management Innovation Research Project under Grant JSYGY -2-
2024-444. (Corresponding authors: Jia Ke; Jianping Gou.)
Xiaojun Chen is with the Afﬁliated Hospital of Jiangsu University ,
Zhenjiang 212013, China, and with the School of Computer Science
and Communication Engineering, Jiangsu University , Zhenjiang 212013,
China, and also with the School of Management, Jiangsu University ,
Zhenjiang 212013, China (e-mail: cxj@ujs.edu.cn).
Jia Ke and Y aning Zhang are with the School of Management,
Jiangsu University , Zhenjiang 212013, China (e-mail: kejia@ujs.edu.cn;
2814301746@qq.com).
Jianping Gou is with the College of Computer and Information Sci-
ence, College of Software, Southwest University , Chongqing 400715,
China (e-mail: pjgzy61@swu.edu.cn).
Anna Shen is with the Afﬁliated Hospital of Jiangsu University , Zhen-
jiang 212013, China (e-mail: annashen@ujs.edu.cn).
Shaohua Wan is with the Shenzhen Institute for Advanced Study,
University of Electronic Science and T echnology of China, Shenzhen
518100, China (e-mail: shaohua.wan@uestc.edu.cn).
Digital Object Identiﬁer 10.1109/JBHI.2024.3438254
various types of features in ultrasound dynamic images,
and realizes the automated and accurate annotation of
ultrasound dynamic images.
Index Terms —Annotation, knowledge distillation,
multimodal distillation, pre-training model, transformer,
ultrasound dynamic image.
I. I NTRODUCTION
I
N THE process of medical ultrasound examination, the
doctor generates objective and standardized description of
the distribution, morphology, size, density and edge of internal
human tissues and organs, through the real-time ultrasound
images. Furthermore, descriptive diagnosis is manually sum-
marized and concluded. At last, a complete ultrasound report
is generated; meanwhile, the artiﬁcial annotation process from
ultrasound image to textual description is completed. Due to
certain differences in theoretical knowledge, reading method,
reading sequence, and work experience of different doctors,
different language descriptions may be interpreted for the same
medical image, thus affecting patients’ diagnosis and treatment
plan.
With the advent of the artiﬁcial intelligence era, the ﬁelds of
Computer Vision (CV) [1], which focuses on image data such as
images and videos, and Natural Language Processing (NLP)[2],
which focuses on textual data, have both achieved great suc-
cess and gained a wide range of applications. Speciﬁcally in
medicine, the ﬁeld of Computer Vision mainly researches how
to realize the techniques of target detection, image classiﬁ-
cation, image segmentation, and image alignment in medical
images [3], [4], [5], [6]. Thereinto, medical image classiﬁca-
tion technology has already made considerable achievements,
and some related technologies have already realized industrial
landing, which are used to assist doctors in the diagnostic work
of medical images. The ﬁeld of Natural Language Processing
mainly studies how to implement techniques such as struc-
tured extraction, text classiﬁcation, and semantic comparison
of electronic medical record texts [7], [8], [9]. The current
research related to the structuring of electronic medical record
text has also made good development, and has been practically
applied to the named entity resolution of a large amount of
unstructured text in hospital data centers for further use in
search engines and knowledge graph presentation [10], [11].
Li By combining gated convolutional neural networks (gated
CNNs) and recurrent neural networks (RNNs) designed to learn
2168-2194 © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:48:16 UTC from IEEE Xplore.  Restrictions apply.
CHEN et al.: MUL TIMODAL DISTILLA TION PRE-TRAINING MODEL FOR UL TRASOUND DYNAMIC IMAGES ANNOT A TION 3125
spatio-temporal embedded representations in gene sequences to
predict translation initiation sites (TIS) [12]. Guo presented an
innovative deep spatial-temporal neural network model designed
to predict Poly(A) signals in genomic sequences [13].
However, most of the aforementioned ﬁelds of Computer
Vision and Natural Language Processing focus on a single
modality or deal with one-dimensional sequential data. Ultra-
sonography, on the other hand, needs to implement a cross
modal conversion process from image to textual descriptions,
ﬁnd semantic associations that exist between multiple modalities
of data, and realize textual annotation of image data. Although
multimodal data can provide richer feature information, the
heterogeneity of multimodality have brought new problems and
challenges to the task of ultrasound image annotation, and have
become the focus and hot issues for medical multimodal data
processing and fusion researchers in recent years.
Image Annotation is a comprehensive research direction com-
bining Computer Vision and Natural Language Processing. The
goal of image annotation is to realize the fusion of image
semantic content and text semantics by computer analysis of
image information and automatic generation of corresponding
description words. The technique of Image Captioning has
been further developed to describe an image through a text. Y .
Mori in the ﬁrst International Workshop on Intelligent Storage
and Retrieval Management for Multimedia in 1999 proposed
a method to establish a relationship between images and text,
which achieved a certain degree of effectiveness by dividing
each image uniformly into a process of sub-images contain-
ing keywords and vector quantizing the sub-images [14].T o
further enhance the efﬁciency of image annotation, Vinyals O
introduced deep learning techniques and proposed the Seq2Seq
framework, which was originally applied in translation systems,
to solve the image annotation problem by switching the original
encoder for sentences from RNN to CNN and thus encoding
the images, which has achieved better results on several public
datasets [15]. Swinburne N C created a software module to parse
medical images in DICOM format, generate annotations auto-
matically, and apply it in a variety of PACS systems [16]. Y ang
provides new ideas for solving the problem of image quality
enhancement in low-light environments by combining the global
feature capture capability of Transformer with the generative
advantages of Generative Adversarial Networks (GANs) [17].
With the continuous development of deep learning tech-
niques, pre-training techniques have been introduced to visual
language models. Xia proposed task modeling structure for
textual continuous masks masking out a continuous sequence
of words to obtain semantic and visual information [18].M u l -
timodal pre-training requires precise alignment and fusion of
multimodal information to identify the correspondence between
various types of modalities [19]. To enhance the cross modal
alignment and zero-sample learning capability, the pre-training
model can also adopt a contrast learning approach, where the
distances of image-text pairs are computed after the image and
text are feature extracted and encoded separately[20]. Fusion en-
coders also play an important role in visual language pre-training
models [21]. Zhou proposed the unsupervised visual-linguistic
pre-training model UVLP to learn strong joint visual and
linguistic representations of unaligned text and image sources
through retrieval-based multi-granularity alignment [22].T o
further enhance the effect of multimodal pre-training model,
multimodal distillation technique is introduced to help the stu-
dent model to achieve higher efﬁciency through the teacher
model [23]. To improve the training capability, knowledge graph
is introduced to assist the visual language model to achieve better
training effect [24].
Considering the above background, this paper seeks to in-
troduce deep learning techniques in the ﬁeld of ultrasound
medicine, design pre-training models, and study new methods of
cross modal data processing and fusion of Computer Vision and
Natural Language Processing. The signiﬁcance of the study of
automated annotation of ultrasound dynamic images is reﬂected
in the improvement of the abstraction of image features and in
the supplementation of existing techniques for intelligent pro-
cessing of medical images. Speciﬁcally, the main contribution
points of this paper are reﬂected in the following aspects:
1) We propose the model of dynamic image language
bidirectional encoder representations from transformers
(DILBERT) to extend traditional medical image process-
ing to dynamic image processing. This model is used
to deal with the semantic relationships between ultra-
sound dynamic images and texts and carry out automatic
and accurate annotation of ultrasound dynamic images
precisely.
2) Through Self-Attention Mechanism and Modality Fusion
(MF), we design a fusion encoder to fuse the overall
visual appearance of descriptive features and the named
entity features, to form a uniﬁed visual-language fea-
ture. We propose a multimodal distillation framework
that simultaneously migrates both response-based knowl-
edge and relationship-based knowledge. The pre-training
teacher model is used to constrain the corresponding
network, reducing the training parameters and preventing
the knowledge between the two networks from differing
too much thus causing a negative optimization of the
model performance. The DILBERT model can obtain
richer aggregation and alignment capabilities of visual
and verbal cues.
3) We design pre-training and ﬁne-tuning tasks of the DIL-
BERT model and introduce the knowledge graph and hu-
man tissue and organ features as named entities and rela-
tions as prior knowledge for masking. The pre-training of
the model is performed using four training tasks: Masked
Language Modeling (MLM), Masked Description Clas-
siﬁcation (MDC), Masked Organ Classiﬁcation (MOC),
and Cross-Modal Matching (CMM). The pre-training
range is reduced and the applicability of the actual scene
of the ultrasound examination is strengthened.
II. R ELA TEDWORKS
A. Visual Language Pre-T raining Model
Pre-training models are widely used in various ﬁelds of arti-
ﬁcial intelligence. A single modality such as vision, language,
and speech has a variety of pre-training models for practical
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:48:16 UTC from IEEE Xplore.  Restrictions apply.
3126 IEEE JOURNAL OF BIOMEDICAL AND HEAL TH INFORMA TICS, VOL. 29, NO. 5, MA Y 2025
applications. The visual-linguistic pre-training model [25] is a
special form of multimodal pre-training model for fusing vision
and language for pre-training to learn joint visual and linguistic
representations for speciﬁc tasks such as visual annotation [26],
visual question and answer [27], graphic and text matching [28],
and visual interactions [29], and thus can be used to learn
dynamic images and diagnosis in ultrasonography from the
multimodal perspective of the joint characterization of text, and
further to achieve automatic annotation of ultrasound dynamic
images.
The core of the visual language pre-training model is Modality
Fusion (MF), which models internal of multimodal fusion to
produce contextually joint representations of images and lan-
guage, integrating encoded disparate modal information into
a stable multimodal feature. MF usually employs dot product
operations to fuse image and text feature vectors, which are
mainly categorized into two types: Dual Stream Models (DSM)
and Single Stream Models (SSM) [30], which are described
below.
1) Dual Stream Models: The dual stream model aims to
map vision and language into the same semantic space. It is
a pioneering approach to modal fusion. Instead of combining
visual and verbal coded inputs, two independent encoders are
used to learn high-level representations of vision and language,
respectively, and the parameters are not shared between the two
encoders; instead, cross modal interactions are realized through
Co-Attention. The dual stream design allows the network depth
and architecture to be adapted to each mode, and in addition to
modal fusion within each modality, some studies have explicitly
designed intermodal interactions between the two encoders to
enable modal fusion at different encoding stages. Lu proposed
ViLBERT based on dual stream structure. That is, image and text
are encoded by separate encoders and then modal information is
fused by cross modal encoders. Language and vision are fused
through the attention mechanism module [31]. Sun proposed the
CBT model to study the advantages and disadvantages of fusion
structures in dual encoder structures[32]. Li proposed the HERO
model, which uses the double dual stream structure as ViLBERT,
and the cross modal encoders are stacked in multiple layers to
compute cross-attention for video-to-language and language-
to-video, respectively [33]. In addition, Urooj proposed a three
streams model based on the dual stream model to further enhance
the input multimodal data [34].
2) Single Stream Models: The single stream model aims to
learn a joint representation that fuses multimodal inputs by
Merged Attention. The images and text markers are linked and
fed into the encoder. Since both modalities use the same set of
parameters, the parameters are reduced by half relative to the
dual stream model. Moreover, single stream modeling performs
implicit internal of multimodal fusion, and is not constrained
by the architectural design of the fusion phase in dual stream
models. So most visual language pre-training models use this
modality fusion scheme. Chen from Google Research proposed
VideoBERT based on single stream structure. i.e., image and
text are encoded by the same encoder. Language and vision
are fused through BERT module [25]. Su from Microsoft Re-
search Asia proposed VL-BERT based on single stream model
and introduced a new pre-trainable generic representation for
visual-linguistic tasks [35]. Zhu from Baidu Research proposed
ActBERT based on single stream model and leveraged global
action information to catalyze mutual interactions between lin-
guistic texts and local regional objects. It uncovers global and
local visual clues from paired video sequences and text de-
scriptions for detailed visual and text relation modeling. Due
to the need to input visual information of “image regions” and
“image descriptions” in ultrasound dynamic images, as well as
language information of “named entity”, ActBERT’s TaNgled
Transformer (TNT) fusion encoder is introduced in this article.
To further improve training efﬁciency, a knowledge distillation
training model was constructed based on this architecture, which
will be introduced in the next section [36].
B. Knowledge Distillation
Knowledge distillation is a Teacher-Student training structure
in which a trained teacher model usually provides knowledge
and a student model is trained to acquire the teacher’s knowledge
through distillation. It is commonly used to migrate and com-
press knowledge from a complex teacher model into a simple
student model at the cost of a slight performance loss [37].
Typically, knowledge distillation uses a larger T-value during
training because of its small soft target variance, and the model
is trained to pay more attention to smaller logical units so that
the student model learns information about the relationship
between these negative and positive samples. The relational
information in the teacher model is called “Dark Knowledge”,
and knowledge distillation model transfers “Dark Knowledge”
from the teacher model to the student model by adjusting T
to generate appropriate soft targets during the training process,
thus improving the prediction performance of the student model.
Knowledge distillation is set at T =1 in the testing phase, which
creates a good level of differentiation due to the large differences
in the soft targets for different logical unit values.
Knowledge distillation can not only be used for model com-
pression, it can also improve the performance of a complex
model through optimization strategies such as mutual learning
and self-learning. Meanwhile, its utilization of features such
as unlabeled and cross modal data also has a signiﬁcant im-
provement on model enhancement. Based on this, two technical
directions, model compression based on knowledge distillation
which was proposed by the researchers of our team [38] and
model enhancement [39], are delineated on the basis of the
application scenario, i.e., whether obtained network model is
intended to be applied to resource-constrained devices. Since
knowledge distillation has a model enhancement function, it is
suitable for the scenarios of different modal data interaction and
fusion, as described below.
1) Cross Modal Distillation: In many practical applications,
data usually exists in multiple modalities, and some data in
different modalities are all describing the same thing or event,
and cross modal distillation can be achieved using synchronized
modal information. Often, many labeled large-scale datasets are
available on the Web, and information from that dataset can be
migrated to student models of different datasets through cross
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:48:16 UTC from IEEE Xplore.  Restrictions apply.
CHEN et al.: MUL TIMODAL DISTILLA TION PRE-TRAINING MODEL FOR UL TRASOUND DYNAMIC IMAGES ANNOT A TION 3127
Fig. 1. A framework for automatic annotation of ultrasound dynamic images.
modal knowledge distillation. The modal information synchro-
nized and aligned by the teacher can be used to compensate for
the ﬂow of information not otherwise available to the student
network and continue to enhance the performance of the student
network through training in knowledge distillation. Girdhar
collects and labels large and clean still image datasets as a
rich knowledge representation for teacher training and guided
unlabeled video learning[40]. Y uan promote knowledge transfer
from image semantic understanding to the task of text-to-image
synthesis [41]. Tavakolian distills spatial-temporal information
from each frame of a video into the image through frame adaptive
weighting [42]. Duan proposed a novel network architecture
aimed at efﬁcient cross-modal information fusion through the
Transformer model to improve the accuracy and generalization
of semantic segmentation tasks [43]. Wang proposed a novel
image fusion method that aims to optimize the fusion of in-
frared and visible images by semi-supervised transfer learning
technique [44].
2) Multimodal Distillation: Machine learning, like the way
humans know the world, improves the performance of process-
ing tasks by utilizing multimodal information, and the distil-
lation of knowledge applied to multimodal learning is called
multimodal distillation. Multimodal distillation extracts hetero-
geneous features from different modal data domains that can
provide complementary information on the same topic, and as-
sociates them in the learning of knowledge distillation to provide
diverse knowledge for the target task. Cioppa proposed to train
a single network using multimodal information from different
viewpoints of the same scene [45]. Wu extracted integrated
features from videos with different modal data relationships to
improve the performance of violence detection[46]. Li proposes
an innovative strategy for the task of sentiment recognition,
namely detached multimodal distillation (DMD), which aims
to enhance the recognition performance by improving the mul-
timodal data processing mechanism [47]. Radevski explores
a multimodal knowledge distillation approach for ﬁrst-person
perspective (egocentric) action recognition tasks [48].
III. M ETHODS
In this paper, Transformer Dynamic Image Language Bidi-
rectional Encoder Representations from Transformers (DIL-
BERT) is designed based on the visual language pre-training
model, the overall framework is shown in Fig. 1,t h em a i n
parameters are described in Table I. Firstly, a large amount
of dynamic images are collected from online medical image
datasets and ultrasound equipment in hospitals to generate an
image database; secondly, data preprocessing is performed on
the images, removing noise and normalization, and annotation of
named entities is performed on the images ready to be trained;
then, 3D-CNN module is used to extract “image description”
visual features; ﬁnally, multimodal distillation and fusion coding
of “image description” visual features, “image region” visual
features and “named entity” linguistic features are used for
feature learning. The pre-training of the model is performed
using four training tasks: Masked Language Modeling (MLM),
Masked Description Classiﬁcation (MDC), Masked Organ Clas-
siﬁcation (MOC), Cross Modal Matching (CMM), and using
the named entities in the knowledge graph that have a location
relationship with the named entity or describe the event as the a
priori knowledge for masking. The ﬁne-tuning of the model uses
the image classiﬁcation results to match the associated “named
entities”, optimizes the deep learning parameter computation,
enhances the performance of the model, and further identiﬁes
the actual health conditions of the tissues and organs, as well
as the semantic associations between the tissues and organs and
the descriptive diagnostic text, so as to realize the automatic
annotation of ultrasound dynamic images. The related research
work of each module is described in detail below Fig. 1.
A. Fusion Encoder
At the heart of the DILBERT model is Modality Fusion (MF),
which models internal of multimodal fusion to produce contex-
tual joint representations of images and text. Before multimodal
fusion, the ﬁrst step is to resolve the modal differences between
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:48:16 UTC from IEEE Xplore.  Restrictions apply.
3128 IEEE JOURNAL OF BIOMEDICAL AND HEAL TH INFORMA TICS, VOL. 29, NO. 5, MA Y 2025
T ABLE I
DESCRIPTION OF MAIN PARAMETERS
ultrasound dynamic images and ultrasound text at different lev-
els regarding dimension and structure using modal embedding,
i.e., extracting features from each modality independently and
then mapping the features into a shared feature space. Text needs
to be tokenized before it can be embedded. Visual embedding
aims to convert images into tokens whose multiple feature levels
are textual with reference to textual embedding. Textual markers
are discrete and arranged in a single dimension, but images have
interrelated pixel values and exist in a high-dimensional space,
so image tokenization is usually more complex than textual
tokenization. After completing the task of labeling multiple
modalities, this paper designs the fusion encoder to obtain the
multimodal input information itself and its interdependence by
reprogramming the workings of the self-attention mechanism
in the single stream model of visual language pre-training. The
working principle of the self-attention mechanism in the fusion
encoder is as follows:
1) Self-Attention Mechanism: The self-attention mechanism
computes the semantic representation of entities and inter-entity
dependencies in a sequence by associating different positions in
the sequence of input information. The self-attention mechanism
computes the data on the input and target side separately to
capture the dependencies between the entities and then adds the
self-attention value computed on the input side to the attention
value computed on the target side to capture the dependencies be-
tween the entities on the input side and the target side. Therefore,
the self-attention mechanism can not only obtain the dependency
relationship between the entities of the input and the target, but
also effectively obtain the dependency relationship between the
entities of the input and the target itself.
The workﬂow of the self-attention mechanism is shown
in Fig. 2(a), where the bottom inputs x1,x 2,x 3,...x t de-
note the input sequence data. Using image features as an
example, x1 can represent the feature corresponding to the
ﬁrst target in the image. Firstly, embedding operation is con-
ducted to initial data through embedding layer to get the
vectors α1,α2,α3 ...αT ; then, qi,ki,vi,iϵ (1, 2, 3...T ) are
obtained by multiplying with the WQ,WK,WV matrices re-
spectively.q1 withk1,k 2,k 3 ...k T respectively are used to com-
pute the vector dot product to obtain α1,1,α1,2,α1,3,...α 1,T .
Further, α1,1,α1,2,α1,3,...α 1,T are inputted into the Soft-
max function to obtain the attention between 0-1 weight val-
ues: c1,1,c 1,2,c 1,3,...c 1,T ; the output b1 corresponding to
the input x1 is obtained by multiplying c1,1,c 1,2,c 1,3,...c 1,T
Fig. 2. Working principle of self-attention mechanism.
with v1,v 2,v 3,...v T at the corresponding positions and then
summing.
As shown in Fig. 2(b), the self-attention mechanism per-
forms parallel computation on x1,x 2,x 3,...x t and obtains the
Q,K,W matrices by multiplying the inputs I by the matrices
WQ,WK,WV , respectively, which are followed by the subse-
quent computation to obtain the attention matrices, which in turn
gives the outputs O.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:48:16 UTC from IEEE Xplore.  Restrictions apply.
CHEN et al.: MUL TIMODAL DISTILLA TION PRE-TRAINING MODEL FOR UL TRASOUND DYNAMIC IMAGES ANNOT A TION 3129
Fig. 3. Process of modal fusion.
Multi-head Attention mechanism is to have multiple sets of
weight matrices for the input vector matrices, which extends the
ability of the model to focus on different target vectors in the
picture, and forms a multiplicity of subspaces at the attention
layer, i.e., it has multiple sets of weight matrices, which allows
mapping the input target vectors to different feature subspaces.
As shown in Fig. 2(c), multiple sets of qi,ki,vi are ob-
tained for the computation of multiple heads of attention. Tak-
ing the input α1 in the ﬁgure as an example, three outputs
are b1
head,b 2
head,b 3
head obtained through the multi-head (head =3)
attention mechanism, and in order to obtain the output b1
corresponding to α1, b1
head,b 2
head,b 3
head is spliced using vector
head-to-tail concatenation, and then b1 is obtained by linear
transformation.
2) Fusion Coding: As shown in Fig. 3, the input of the single
stream model of the visual language is respectively from three
types of basic data: the visual information of “image area”,
the visual information of “image description” and the language
information of “named entity”. In this paper, a fusion encoder
is designed by introducing the self-attention mechanism, which
is used to input visual information into the language encoder
and input the language information into the visual encoder to
strengthen the interaction between visual and linguistic features,
as the ActBERT[36]. The difference is that image regions, image
descriptions and named entity are used in DILBERT instead
of the global actions, local regional objects and linguistic de-
scriptions in ActBERT. The fusion encoder performs the fusion
operation by means of a multi-head self-attention mechanism.
The formula is as follows (1) and (2):
Cw = Multihead
(
W 1
q hl
a,W w
k hl
w,W w
νhl
w
)
(1)
Cr = Multihead
(
W 2
q hl
a,W r
khl
r,W r
νhl
r
)
(2)
Where Cw is the fused linguistic representation and Cr is the
fused regional feature.Cw uses a linear layer to obtain new key-
value pairs and stacks them with the original a-transformer and
r-transformer key values. In this way visual and verbal features
are fused together.
Fig. 4. Diagram of multimodal knowledge distillation framework.
B. Multimodal Distillation
With the above design of fusion encoder, the multimodal
information of visual language can be pre-trained after uniﬁed
encoding. Due to the need to deal with data from multiple
modalities, multimodal pre-training models, as opposed to sin-
gle modal pre-training models, not only face the problem of
pairing data from different modalities, but also generate more
parameters after the training is completed. The DILBERT model
designed in this paper improves the performance of student
networks by using information from different modal data to pro-
vide complementary cues for the target task through knowledge
distillation. Integrating information from multiple modal data
improves model generalization.
As shown in Fig. 4, the teacher model and the student model
have the same network architecture, the pre-training of the
teacher model is completed ﬁrst, and then the student model is
guided to pre-training through soft labeling, after the pre-training
phase is completed, the two networks begin to learn from each
other, and the migrated knowledge is of two kinds, one is the
response-based knowledge, and the other is the relationship-
based knowledge. At the same time, the pre-training teacher
model is used to constrain the corresponding network, prevent-
ing the knowledge between the two networks from differing too
much and thus causing a negative optimization of the model
performance.
As shown in Fig. 5 for the speciﬁc structure of the multi-
DILBERT model, the relevant computational variables and
functions are deﬁned as follows:
Let X =
{
x1,x 2,...,x n
}
be n samples from m cat-
egories, each corresponding to a true label labeled y ={
y1,y 2,...,y m}
. Label the logits output by the kth network
Nk as zk(x)= {z1
k,z 2
k,...,z m
k }. Its output after the Softmax
function is σi(zk(x),t ).t is the temperature parameter.
The loss function is deﬁned based on the MTKD-SSR [38]
proposed by the researchers of our team which is to migrate the
response-based knowledge of network k′ to network k(k ̸= k′)
when different networks learn from each other is deﬁned as
follows (3):
LKL (pk,pk′ )=
∑
x∈X
m∑
i=1
σi (zk′ (x), 1) logσi (zk′ (x), 1)
σi (zk(x), 1) (3)
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:48:16 UTC from IEEE Xplore.  Restrictions apply.
3130 IEEE JOURNAL OF BIOMEDICAL AND HEAL TH INFORMA TICS, VOL. 29, NO. 5, MA Y 2025
Fig. 5. Diagram of DMILBERT model structure.
Due to the extensive relationship-based knowledge between the
multimodal entities processed by DILBERT, when two net-
works learn from each other, different from MTKD-SSR [38],
DILBERT additionally transfers relationship-based knowledge
in addition to the response-based knowledge described above.
Transfer knowledge of the relationship between two types
of samples, a distance relationship between different samples
and an angle relationship between different samples. Let sk
j =
φk(xj ) be the output of network Nk for sample xj at any
layer. Whereφk(·) is a feature mapping function of the network
Nk. χT denotes the T -tuple of different samples. Therefore,
the set of binary groups between samples is denoted as χ2 =
{(xu,xν)|u ̸= ν}, and the set of ternary groups is denoted as
χ3 = {(xu,xν,xw)|u ̸= ν̸= w}.
For the network, the function that captures the similarity of
the distance between two samples in a binary group is (4):
f
(
sk
u,sk
ν
)
= 1
π
sk
u −sk
ν

2 (4)
Whereπ= 1
|χ2|
∑
(xu,xv )∈χ2 ∥sk
u −sk
v∥2 is normalization con-
stant. Therefore, the distance-based loss function used to convert
sample relationships between network Nk and network Nk′ is
(5):
LDD (xu,xν)=
∑
(xu,xν)∈χ2
R
(
f
(
sk
u,sk
ν
)
,f
(
sk′
u ,sk′
ν
))
(5)
Where is the Huber loss reﬂecting the sample relationship,
deﬁned as follows (6):
R(a,b )=
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
1
2 (a−b)2, if|a−b|≤ 1
|a−b|− 1
2, otherwise
(6)
For the network Nk, the function that captures the similarity of
angles between the three samples in a ternary group is (7):
f
(
sk
u,sk
ν,sk
w
)
=c o s∠sk
usk
νsk
w = ⟨euν,ewν⟩ (7)
Where euν = sk
u−sk
ν
∥sku−skν∥2
, ewν = sk
w−sk
ν
∥skw−skν∥2
. Therefore, the angle-
based loss function used to migrate the sample relationship
between network Nk and network Nk′ is:
LAD (xu,xν,xw )=
∑
(xu,xν,xw )
∈χ3
R(f (sk
u,sk
ν,sk
w),f (sk′
u ,sk′
ν,sk′
w ))
(8)
Therefore, the loss function for migrating the relation-based
knowledge of k′ to network k(k ̸= k′) when different networks
learn from each other is deﬁned as:
LRD = LDD (xu,xν)+ β1LAD (xu,xν,xw) (9)
Where β1 is the tuning parameter that controls the balance be-
tween the loss terms. The ﬁnal loss function for mutual learning
between the two networks is:
Lk
MD = LRD +β2LKL (pk,pk′ ) (10)
Where β2 is the tuning parameter that controls the balance be-
tween the loss terms. When the network undergoes self-learning,
a pre-training teacher model is used to constrain the loss function
of the corresponding network to be:
Lk
SD
(
pt
k,p−t
k
)
=
∑
x∈X
m∑
i=1
σt
i (zk,t )l o gσt
i (zk,t )
σt
i (zk,t ) (11)
For the classiﬁcation task, the cross entropy between the output
of network and the true label is:
LCE (y,pk)= −
∑
x∈X
m∑
i=1
yi log(σi (zk(x), 1)) (12)
In summary, the objective loss function for training networkNk
is:
Lk
KD = αLk
CE +βLk
MD +γLk
SD (13)
Whereα,β,γare the tuning parameters that control the balance
between the loss terms.
C. A Pre-T raining Model for Multimodal Distillation
In this paper, a DMILBERT pre-training model for multi-
modal distillation is designed. The input data are the “image
description” visual feature of the ultrasound dynamic image,
the “image area” visual feature of the ultrasound dynamic im-
age, and the “named entity” linguistic feature of the electronic
medical record. The DMILBERT pre-training model performs
multimodal distillation and fusion coding of “image description”
visual features, “image region” visual features and “named
entity” linguistic features.
The pre-training of the multimodal distillation model was
performed using four training tasks, MLM, MDC, MOC, and
CMM. The ﬁne-tuning of the model employs the image classiﬁ-
cation results to match the associated named entities, to identify
the actual health conditions of the tissues and organs, as well
as the semantic associations between the tissues and organs
and the descriptive diagnostic texts, to realize the automatic
annotation of ultrasound dynamic images. The speciﬁc process
is as follows:
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:48:16 UTC from IEEE Xplore.  Restrictions apply.
CHEN et al.: MUL TIMODAL DISTILLA TION PRE-TRAINING MODEL FOR UL TRASOUND DYNAMIC IMAGES ANNOT A TION 3131
1) Model Pre-T raining Phase: Masked modeling is a com-
mon task in pre-training models, which is essentially an in-
ference task because masking and replying for keywords in
descriptive text, such as quantiﬁers, adjectives, nouns, and ac-
tions are essentially inference tasks from different perspectives.
The setup of MLM is consistent with other pre-training models:
randomly masking or replacing a portion of tokens and learning
a multilayer encoder to predict the original token through a
representation of the model output. Bringing Masked Image
Modeling (MIM) to the ﬁeld of computer vision in the same
way that MLM was done in natural language processing.
Masked Language Modeling Task: The masked language
modeling task in the DMILBERT model is similar to the task in
natural language processing, by randomly masking textual infor-
mation in a portion of the language so that unmasked information
can be inferred from masked textual information. In addition to
the randomized way of masking the named entities, the named
entities with positional relationships and describing events in the
relationship graph of the knowledge graph are further randomly
selected for masking, which enhances the efﬁciency of the
reasoning task in the pre-training model. The masked language
modeling task helps the multimodal pre-training model to learn
the ﬁne-grained correlations between languages.
Masked Description Classiﬁcation Task: the masked named
entities are further learned to predict the semantics represented
by each masked named entity by the mask description classi-
ﬁcation task, which predicts the masked named entities based
on the linguistic features of the organizing organ. The task of
explicitly predicting cues for long-time action sequences that
primarily tap into the language and discriminating the temporal
order of named entities enhances the ability to categorize named
entities in the pre-training model, which is further generalized
to the ﬁne-tuning task.
Masked Organ Classiﬁcation Task: The masked organ classi-
ﬁcation task works in a similar mode to the masked description
classiﬁcation task by randomly masking image regions in ul-
trasound dynamic images to learn the high-level semantics of
tissues and organs in the masked regions of ultrasound dynamic
images. The classiﬁcation ability is further trained by predicting
the classiﬁcation of tissues and organs in the masked region by
the absence of information about the tissues and organs in the
masked region.
Cross Modal Matching Task: Visual features in image regions
and image descriptions naming entities are utilized to derive
relationships between visual and linguistic entities. This task
trains the model to learn from contextual descriptions while
extracting relevant visual features to assist in text prediction,
which provides the ability to align multimodal pre-training
models on a coarse-grained level.
2) Fine-T uning of Mandates: The ﬁne-tuning task outputs
natural language descriptions of tissues and organs in the ul-
trasound dynamic image region, using the image classiﬁcation
results to match the associated “named entity” descriptions.
The performance of the model is enhanced by optimizing the
computation of deep learning parameters to further identify the
actual health conditions of tissues and organs, as well as the se-
mantic associations between tissues and organs and descriptive
Fig. 6. Schematic diagram of ultrasound dynamic images experimen-
tal data from BUSI [49] and AHJUUI.
diagnostic texts, to achieve automatic annotation of ultrasound
dynamic images.
IV . EXPERIMENT ALDET AILS
A. Experimental Basics
1) Experimental Data: The experimental data sources were
publicly available medical datasets on the Internet: the BUSI
(Breast Ultrasound Images) dataset, which provided 127 breast
ultrasound reports [49], the CLUST (Challenge on Liver Ul-
trasound Tracking) dataset, which provided 64 liver ultrasound
reports [50], the USSS (Ultrasound simulation & segmentation)
dataset, which provided a total of 617 ultrasound reports of gall-
bladder, pancreas, spleen, kidney [51]; and internal ultrasound
images obtained from the Afﬁliated Hospital of Jiangsu Uni-
versity (AHJU), the author’s institution. University Ultrasound
Imaging, AHJUUI) dataset, which provided “Liver”, “Gall-
bladder”, “Pancreas”, “Spleen”, “Kidney”, “Thyroid”, and 500
ultrasound images and reports, and generated 3000 serialized
images.
A ss h o w ni nF i g .6,F i g .6(a), (b), (c), (d) shows four consecu-
tive ultrasound dynamic images of medical ultrasound examina-
tion of breast nodules in the BUSI[49] dataset, where the nodules
in the red boxes of image Fig. 6(a) and (c) h a v eah i g hd e g r e eo f
malignancy, and those in the red boxes of image Fig.6(b) and (d)
have a fair degree of malignancy. The sonographer can conclude
that the breast nodule is malignant by synthesizing the ultra-
sound dynamic images. Fig. 6(e), (f), (g), (h) shows four con-
secutive ultrasound dynamic images from medical ultrasound
examinations of thyroid nodules in the AHJUUI dataset, with
the green box labeling the thyroid gland, the red box labeling
the thyroid nodule, and the blue box labeling the echogenicity
status of the thyroid nodule. Fig. 6(e) shows the thyroid gland
with one hypoechoic nodule with one strong echo-calciﬁcation;
Fig. 6(f) shows the thyroid gland with two hypoechoic nodules
with three strong echo-calciﬁcation points in the left hypoechoic
nodule; Fig. 6(g) shows the thyroid gland with two hypoechoic
nodules with three strong echo-calciﬁcation points in the left
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:48:16 UTC from IEEE Xplore.  Restrictions apply.
3132 IEEE JOURNAL OF BIOMEDICAL AND HEAL TH INFORMA TICS, VOL. 29, NO. 5, MA Y 2025
T ABLE II
UL TRASOUNDREPORT OF ORGANS AND ST A TISTICALTABLES OF
SERIALIZED IMAGES
hypoechoic nodule and one strong echo-calciﬁcation point in
the right hypoechoic nodule; Fig. 6(h) shows the thyroid gland
with two hypoechoic nodules, the hypoechoic nodule on the left
has one strongly echogenic calciﬁed point and the hypoechoic
nodule on the right has one strongly echogenic calciﬁed point.
The ultrasound physician can conclude that the thyroid nodules
are benign by synthesizing the ultrasound dynamic images.
The experimental data speciﬁcally includes two parts: ultra-
sound dynamic images and ultrasound reports. The format of
the ultrasound dynamic images data is stored in the form of
serialized images, and the format of the ultrasound report data is
mainly stored in the form of unstructured text. It mainly contains
6 organs: mammary glands, liver, gallbladder, pancreas, spleen,
and kidney. TableII shows the proportion of reports and images
of each organ.
2) Baseline Methods: To evaluate the effectiveness, we com-
pare the performance of our model with ﬁve representatives of
visual language pre-training models both in dual stream and
single stream.
VideoBERT [25] extends BERT for joint video-language
learning with masked prediction, aligning data sans annotations,
and enhancing cross-modal understanding and generation.
CBT [32] builds on the Bidirectional Transformer with
contrastive learning, using self-attention for sequence de-
pendency and contrastive strategies for discriminative frame
representation.
HERO [33] was a hierarchical pre-training model with Cross-
modal and Temporal Transformers for local and broad video
context, plus tasks like Video-Subtitle Matching for alignment
and comprehensive video usage.
MMFT-BERT [34] features separate BERTs for modalities
fused via a transformer, excelling on TVQA with multimodal
fusion at the feature level.
ActBERT [36] employs self-supervision for reﬁned visual-
linguistic interaction, using TNT to encode global actions,
local objects, and language, deepening visual-textual relation
modeling.
The above ﬁve methods leverage self-supervised learning and
transformers for joint video-text representation learning, en-
hancing multimodal understanding without explicit supervision.
3) Experimental Environments: The experimental environ-
ment and the description of the use of the toolkit are shown in
Table III:
4) Evaluation of Indicators: Sensitivity, Precision, and F1-
Score were used as evaluation indicators. Data were calculated
T ABLE III
EXPERIMENT ALENVIRONMENT AND TOOLKIT USAGE INSTRUCTION TABLE
as True Positive (TP); True Negative (TN); False Positive (FP);
and False Negative (FN) as (14), (15), (16):
Sensitivity = TP
TP +FN (14)
Precision = TP
TP +FP (15)
F 1−Score = 2×Sensitivity ×Precision
Sensitivity +Precision (16)
B. Experimentation and Analysis of Automatic
Annotation of Medical Images
1) Setting: We use 13 as the initial task layer mapping, so
that the student model and the teacher have 13layer multimodal
encoder for visual-textual. The parameter temperature t is ﬁxed
to 1, α= 0.2, β= 1,γ= 0.6. The training strategy adopts
a stochastic gradient descent (SGD) optimizer that drives the
quantity and weight decay for model optimization, where the
momentum coefﬁcient is initially set to 0.9 and the weight decay
coefﬁcient is set to 0.0001. Set the initial learning rate to 0.24 and
use a learning rate adjustment mechanism based on the function
proposed in this article. The pre-trained model trains 800 epochs
by default.
2) Pre-T raining of Multimodal Distillation Models: Fig. 7
shows the multimodal distillation model in relation to the data
changes during pre-training and validation. Fig. 7(a) shows
the data dimensionality reduction of linguistic features in pre-
training and validation, and it can be seen that the text data di-
mensionality reduction interval in the training process is within
the range of −0.88 to 0.82, and then the validation has better
results. Fig. 6(b) shows the data dimensionality reduction of
image features in pre-training and validation. It can be seen that
the dimensionality reduction interval of the image data during
training is in the range of −0.51 to 0.52, and then the validation
has better results.
Fig. 8 shows the variation of weights and bias parameters
of the multimodal distillation model during pre-training and
validation. It can be seen that during training the weights and
biases data are in the range of 10000 to 3000, while during
validation the weights and biases data remain in the range of
43400 to 43480.
Fig. 9 shows a schematic diagram of the image-to-text cor-
respondence of the visual language model after pre-training
is completed. The top and bottom panels show the associated
image vectors after different named entities are selected, respec-
tively. The numerical values labeled on each node in the ﬁgure
represent the sequence number of the image vectors.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:48:16 UTC from IEEE Xplore.  Restrictions apply.
CHEN et al.: MUL TIMODAL DISTILLA TION PRE-TRAINING MODEL FOR UL TRASOUND DYNAMIC IMAGES ANNOT A TION 3133
Fig. 7. Process of vision language pre-training data dimensionality reduction.
Fig. 8. Process of weight and offset values during vision language pre-training and validation.
As shown in Fig. 10, after the pre-training of the DMILBERT
model, the matching relationship graph between the ultrasound
image and the text of the ultrasound report is generated, which
shows the recognition of the named entity and the image region
under the conditions that the Transformer is set to 3-layer 8-head
attention, 6-layer 6-head attention, and 9-layer 9-head attention.
The thickness of the line reﬂects the degree of association
between the named entity and the image region, for example, the
named entity “Thyroid” corresponds to region 1 in the image,
and the line connecting the named entity “Thyroid” and region
1 is thicker due to the high scores of the two matches. It can
be found that the number of multi-heads attention improves
the model efﬁciency better than the number of layers, and the
DMILBERT pre-training model has the best recognition efﬁ-
ciency when the Transformer is conﬁgured with 9-layer 9-head
attention.
3) Knowledge Distillation Performance Comparison: In or-
der to validate the effectiveness of the multimodal knowledge
distillation pre-training model proposed in this paper, the bench-
mark method uses a simple BERT pre-training model. The batch
size of the teacher model in knowledge distillation is set to 32,
the maximum number of ultrasound serialized images is set to
128, and the maximum number of training rounds is 4. The batch
size of the student model is set to 32, the maximum number of
ultrasound serialized images is set to 128, the maximum number
of training rounds is 4, the distillation temperature T is set to 5,
10, 20, and the equalization coefﬁcients between the different
losses in the distillation are set to 0.2, 0.5, 0.8.
TableIV documents the performance of the DILBERT teacher
model, DILBERT student model and the basic BERT model
proposed in this paper on the BUSI(Breast Ultrasound Images),
CLUST&USSS and AHJUUI datasets. It can be found that the
BERT model achieves relatively close accuracy in the BUSI,
CLUST&USSS and AHJUUI datasets, and with the increase of
the number of Transformer layers, the accuracy of the BERT
model grows slightly, which indicates that the BERT model has
a better stability, and in the case of limited computing resources,
the BERT model with fewer layers can be chosen to fulﬁll the
basic tasks. The DILBERT teacher model achieves better results
than BERT in all three models, and as the number of Transformer
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:48:16 UTC from IEEE Xplore.  Restrictions apply.
3134 IEEE JOURNAL OF BIOMEDICAL AND HEAL TH INFORMA TICS, VOL. 29, NO. 5, MA Y 2025
Fig. 9. Diagram of relationship between dynamic image and text after
visual language pre-training.
T ABLE IV
EFFICIENCY OF KNOWLEDGE DISTILLA TION
layers increases, the DILBERT teacher model can achieve higher
accuracy than BERT, suggesting that more Transformer layers
can enhance the effect of pre-training. The DILBERT student
model achieves the best accuracy in all 3 datasets, indicating
that the multimodal knowledge distillation proposed in this
paper allows the pre-training student model to obtain richer
visual, verbal cues aggregation and alignment capabilities, and
Fig. 10. Diagram of Visual language pre-training.
T ABLE V
EFFICIENCY OF KNOWLEDGE DISTILLA TION
the multimodal knowledge distillation embodies a good model
enhancement capability.
4) Pre-T raining Model Performance Comparison: Table V
records the recall performance of the DILBERT model proposed
in this paper with VideoBERT [25], CBT [32],H E R O [33],
MMFT-BERT[34], ActBERT [36] for the pre-training task with
K=10 on BUSI, CLUST&USSS and AHJUUI datasets. It can
be found that the R-values of the MLM, MDC, MOC, and CMM
tasks on BUSI(Breast Ultrasound Images) for all types of models
are lower relative to the CLUST&USSS and AHJUUI datasets,
indicating that all types of models are affected by the data noise
and reduce their efﬁciency. The R-values of the various models
in the MLM task are relatively high compared to the other tasks,
indicating that the language modeling task can well help the
models to learn the ﬁne-grained relevance of textual information.
The DILBERT model has the highest R-value for the MLM
task for all 3 types of datasets, indicating that the positional
relationships or descriptive events of the Knowledge Graph
can effectively help the model to enhance the textual semantic
comprehension. Except for the DILBERT model, the R-values
of various other types of models in the MDC task are relatively
low compared to other tasks, indicating that the classiﬁcation of
named entities in the text of ultrasound reports is challenging.
The DILBERT model has the highest R-value for the MDC
task of the CLUST&USSS and AHJUUI datasets, indicating
that named entities can effectively enhance the model for the
task of classifying descriptive text, but is also affected by noisy
data. The DILBERT model has the highest R-value for the MOC
task for the 3 types of datasets, indicating that the task is more
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:48:16 UTC from IEEE Xplore.  Restrictions apply.
CHEN et al.: MUL TIMODAL DISTILLA TION PRE-TRAINING MODEL FOR UL TRASOUND DYNAMIC IMAGES ANNOT A TION 3135
T ABLE VI
EFFICIENCY OF KNOWLEDGE DISTILLA TION
T ABLE VII
EFFICIENCY OF KNOWLEDGE DISTILLA TION
reliant on the object detector and the region being recognized.
The DILBERT model has the highest R-value in the CMM task
for 3 types of datasets, indicating that the model’s learning ability
is enhanced by introducing multimodal knowledge distillation,
which strengthens the multimodal interactions and makes it
more capable of visual-linguistic alignment, and helps the model
to better learn the semantic associations between images and
texts.
5) Performance of Pre-T raining Models on Automatic Anno-
tation T asks: Table VI records the performance of the automat-
ically labeled downstream task of the pre-training model for
ﬁne-tuning on the BUSI, CLUST&USSS and AHJUUI datasets.
It can be found that the performance of the DILBERT model
proposed in this paper is higher than that of the other models on
all three datasets, indicating that adequate pre-training provides
strong support for the ﬁne-tuning task, and that the general-
ization ability of the DILBERT model on all types of datasets
optimizes the efﬁciency of parameter computation, enhances the
performance of classiﬁcation, strengthens the ability of recog-
nizing the actual health status of tissues and organs in ultrasound
images, and strengthens multimodal interaction between tissues,
organs and descriptive diagnostic texts which is for the task of
automatic annotation of ultrasound dynamic images.
6) Analysis of Time Expenditure: In order to explore the time
overhead scenario of the proposed method, Table VII records
the time consumed for training the proposed model in this paper
with the comparison model on the BUSI, CLUST&USSS and
AHJUUI datasets. All the experimental environments, such as
hardware and software, remain the same, as do the deep learning
tools and parameter settings. From the data in Table VII, it can
be found that the training elapsed time of the model designed
in this paper in each dataset is higher than that of the CBT,
HERO, and MMFT-BERT models, close to the ActBERT model,
and lower than that of the VideoBERT model, and the elapsed
time is increased in parallel because the size of the AHJUUI
dataset is larger than that of the BUSI(Breast Ultrasound Im-
ages) and CLUST&USSS datasets. The time-consuming part of
the model mainly involves two parts: the computation of the
spatio-temporal hybrid attention mechanism and the knowledge
distillation. Considering its improvement in the robustness of
ultrasound dynamic image feature extraction and the efﬁciency
of multi-modal model pre-training, the increased time overhead
is still within an acceptable range, and in general, the model has
the conditions to be put into practical application scenarios.
V. C ONCLUSION
In this paper, in order to investigate the relationship between a
series of dynamic image representations captured after real-time
probe movement in ultrasound medical examinations and the
actual health conditions of tissues and organs, and the semantic
correlation between the dynamic image sequences and descrip-
tive diagnostic language, a pre-training model of multimodal
distillation and fusion coding is proposed for automated anno-
tation of ultrasound dynamic images. A pre-training model of
multimodal distillation and fusion coding is used for multimodal
feature enhancement learning by knowledge distillation of “im-
age area” visual features, “image description” visual features
and “named entity” linguistic features. Experimental results on
multiple datasets show that the method proposed in this study
can effectively improve the effectiveness of feature fusion and
can signiﬁcantly enhance the accuracy of automatic annotation
of ultrasound dynamic images. The main future work focuses
on the following three areas:
1) Research on the knowledge representation of dynamic
images. Aiming at the regular sequence composition of
ultrasound dynamic images and the structured topological
information of its tissues and organs, we consider adding
formal semantic analysis to the existing model, in order to
realize the knowledge representation of dynamic images
and improve the interpretability of the model.
2) The multimodal distillation pre-training model currently
used plays a good role in automatic annotation of ultra-
sound dynamic images. However, how the model can be
applied to automatic annotation of other medical dynamic
images such as gastroscopy, enteroscopy, laryngoscopy,
bronchoscopy, etc. is a direction that needs to be studied
in the future.
3) Improvement of multimodal distillation and fusion cod-
ing pre-training using deep neural networks, e.g., im-
provement of our model training using a large multimodal
model, enhancement of multimodal distillation and fu-
sion coding, further improvement of feature fusion, and
improvement of the accuracy of automatic annotation of
ultrasound dynamic images.
REFERENCES
[1] E. R. Davies, Computer Vision: Principles, Algorithms, Applications,
Learning. San Francisco, CA, USA: Academic, 2017.
[2] C. Manning and H. Schutze, F oundations of Statistical Natural Language
Processing. Cambridge, MA, USA: MIT Press, 1999.
[3] S. Rani, B. K. Singh, D. Koundal, and V . A. Athavale, “Localization of
stroke lesion in MRI images using object detection techniques: A compre-
hensive review,”Neurosci. Inform., vol. 2, no. 3, 2022, Art. no. 100070.
[4] Z. Senousy, M. M. Gaber, and M. M. Abdelsamea, “Auqanto: Action-
able uncertainty quantiﬁcation optimization in deep learning architectures
for medical image classiﬁcation,” Appl. Soft Comput. , vol. 146, 2023,
Art. no. 110666.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:48:16 UTC from IEEE Xplore.  Restrictions apply.
3136 IEEE JOURNAL OF BIOMEDICAL AND HEAL TH INFORMA TICS, VOL. 29, NO. 5, MA Y 2025
[5] G. C. Ates, P . Mohan, and E. Celik, “Dual cross-attention for medical image
segmentation,” Eng. Appl. Artif. Intell. , vol. 126, 2023, Art. no. 107139.
[6] H. G. Khor, G. Ning, Y . Sun, X. Lu, X. Zhang, and H. Liao, “Anatomically
constrained and attention-guided deep feature fusion for joint segmen-
tation and deformable medical image registration,” Med. Image Anal. ,
vol. 88, 2023, Art. no. 102811.
[7] D. Fraile Navarro et al., “Clinical named entity recognition and relation
extraction using natural language processing of medical free text: A
systematic review,” Int. J. Med. Inform. , vol. 177, pp. 1–21, Sep. 2023.
[8] L. Santoro and J. Melinek, “Forensic implications of classiﬁcation of
accident-related deaths: A case report and review of the medical and legal
literature,” F orensic Sci. Int.: Rep., vol. 7, 2023, Art. no. 100307.
[9] D. Gao et al., “Simulating doctors’ thinking logic for chest X-ray report
generation via transformer-based semantic query learning,” Med. Image
Anal., vol. 91, 2024, Art. no. 102982.
[10] M. Y . Landolsi, L. B. Romdhane, and L. Hlaoua, “Medical named entity
recognition using surrounding sequences matching,” in Procedia Comput.
Sci., 2022, vol. 207, pp. 674–683.
[11] Y . Xiong et al., “Leveraging multi-source knowledge for Chinese clinical
named entity recognition via relational graph convolutional network,” J.
Biomed. Inform., vol. 128, 2022, Art. no. 104035.
[12] W. Li, Y . Guo, B. Wang, and B. Y ang, “Learning spatiotemporal embed-
ding with gated convolutional recurrent networks for translation initia-
tion site prediction,” Pattern Recognit., vol. 136, 2023, Art. no. 109234.
[Online]. Available: https://www.sciencedirect.com/science/article/pii/
S0031320322007130
[13] Y . Guo, D. Zhou, P . Li, C. Li, and J. Cao, “Context-aware poly(a) signal
prediction model via deep spatial–temporal neural networks,”IEEE Trans.
Neural Netw. Learn. Syst., vol. 35, no. 6, pp. 8241–8253, Jun. 2024.
[14] Y . Mori, H. Takahashi, and R. Oka, “Image-to-word transformation based
on dividing and vector quantizing images with words,” in Proc. 1st Int.
Workshop Multimedia Intell. Storage Retrieval Manage., 1999.
[15] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A
neural image caption generator,” inProc. IEEE Conf. Comput. Vis. Pattern
Recognit., 2015, pp. 3156–3164.
[16] N. C. Swinburne, D. Mendelson, and D. L. Rubin, “Advancing semantic
interoperability of image annotations: Automated conversion of non-
standard image annotations in a commercial PACs to the annotation and
image markup,” J. Digit. Imag., vol. 33, pp. 49–53, 2020.
[17] S. Y ang, D. Zhou, J. Cao, and Y . Guo, “Rethinking low-light en-
hancement via transformer-GAN,” IEEE Signal Process. Lett. , vol. 29,
pp. 1082–1086, 2022.
[18] Q. Xia et al., “XGPT: Cross-modal generative pre-training for image
captioning,” in Proc. Conf. Natural Lang. Process. Chin. Comput. , 2020,
pp. 786–797.
[19] X. Li et al., “OSCAR: Object-semantics aligned pre-training for vision-
language tasks,” in Proc. 16th Eur . Conf. Comput. Vis. , Glasgow, U.K.,
Aug. 23–28, 2020, pp. 121–137.
[20] A. Radford et al., “Learning transferable visual models from natural lan-
guage supervision,” inProc. Int. Conf. Mach. Learn., 2021, pp. 8748–8763.
[21] H. Bao et al., “VLMO: Uniﬁed vision-language pre-training with mixture-
of-modality-experts,” in Proc. Int. Conf. Adv. Neural Inf. Process. Syst. ,
vol. 35, pp. 32897–32912, 2022.
[22] M. Zhou, L. Y u, A. Singh, M. Wang, Z. Y u, and N. Zhang, “Unsupervised
vision-and-language pre-training via retrieval-based multi-granular align-
ment,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2022,
pp. 16485–16494.
[23] L. Zhang et al., “VLDeFormer: Vision–language decomposed trans-
former for fast cross-modal retrieval,”Knowl.-Based Syst., vol. 252, 2022,
Art. no. 109316.
[24] T. Shen, Y . Mao, P . He, G. Long, A. Trischler, and W. Chen, “Exploiting
structured knowledge in text via graph-guided representation learning,”
2020, arXiv:2004.14224.
[25] C. Sun, A. Myers, C. V ondrick, K. Murphy, and C. Schmid, “VideoBERT:
A joint model for video and language representation learning,” in Proc.
IEEE/CVF Int. Conf. Comput. Vis. , 2019, pp. 7464–7473.
[26] D. J. Pangal, G. Kugener, S. Shahrestani, F. Attenello, G. Zada, and D.
A. Donoho, “A guide to annotation of neurosurgical intraoperative video
for machine learning analysis and computer vision,” World Neurosurgery,
vol. 150, pp. 26–30, 2021.
[27] J. Xie, J. Chen, W. Fang, Y . Cai, and Q. Li, “Visual question generation
for explicit questioning purposes based on target objects,” Neural Netw.,
vol. 167, pp. 638–647, 2023.
[28] M. Nabati and A. Behrad, “Multimodal video-text matching using a deep
bifurcation network and joint embedding of visual and textual features,”
Expert Syst. Appl., vol. 184, 2021, Art. no. 115541.
[29] L. Zhou, H. Palangi, L. Zhang, H. Hu, J. Corso, and J. Gao, “Uniﬁed
vision-language pre-training for image captioning and VQA,” in Proc.
AAAI Conf. Artif. Intell. , 2020, vol. 34, pp. 13041–13049.
[30] F.-L. Chen et al., “VLP: A survey on vision-language pre-training,” Mach.
Intell. Res., vol. 20, no. 1, pp. 38–56, 2023.
[31] J. Lu, D. Batra, D. Parikh, and S. Lee, “ViLBERT: Pretraining task-agnostic
visiolinguistic representations for vision-and-language tasks,” inProc. Int.
Conf. Adv. Neural Inf. Process. Syst., 2019, vol. 32, pp. 13–23.
[32] C. Sun, F. Baradel, K. Murphy, and C. Schmid, “Learning video
representations using contrastive bidirectional transformer,” 2019,
arXiv:1906.05743.
[33] L. Li, Y .-C. Chen, Y . Cheng, Z. Gan, L. Y u, and J. Liu, “HERO: Hierarchi-
cal encoder for video+language omni-representation pre-training,” 2020,
arXiv:2005.00200.
[34] A. U. Khan, A. Mazaheri, N. D. V . Lobo, and M. Shah, “MMFT-BERT:
Multimodal fusion transformer with BERT encodings for visual question
answering,” 2020, arXiv:2010.14095.
[35] W. Su et al., “VL-BERT: Pre-training of generic visual-linguistic repre-
sentations,” 2019, arXiv:1908.08530.
[36] L. Zhu and Y . Y ang, “ACTBERT: Learning global-local video-text rep-
resentations,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. ,
2020, pp. 8746–8755.
[37] J. Gou, B. Y u, S. J. Maybank, and D. Tao, “Knowledge distillation: A
survey,” Int. J. Comput. Vis., vol. 129, pp. 1789–1819, 2021.
[38] J. Gou, X. Xiong, B. Y u, L. Du, Y . Zhan, and D. Tao, “Multi-target
knowledge distillation via student self-reﬂection,” Int. J. Comput. Vis. ,
vol. 131, no. 7, pp. 1857–1874, 2023.
[39] L. Li, K. Sun, and J. Zhu, “A novel multi-knowledge distillation approach,”
IEICE Trans. Inf. Syst., vol. 104, no. 1, pp. 216–219, 2021.
[40] R. Girdhar, D. Tran, L. Torresani, and D. Ramanan, “DistInit: Learning
video representations without a single labeled video,” in Proc. IEEE/CVF
Int. Conf. Comput. Vis., 2019, pp. 852–861.
[41] M. Y uan and Y . Peng, “CKD: Cross-task knowledge distillation for text-to-
image synthesis,” IEEE Trans. Multimedia, vol. 22, no. 8, pp. 1955–1968,
Aug. 2020.
[42] M. Tavakolian, H. R. Tavakoli, and A. Hadid, “AWSD: Adaptive weighted
spatiotemporal distillation for video representation,” in Proc. IEEE/CVF
Int. Conf. Comput. Vis., 2019, pp. 8020–8029.
[43] Z. Duan, X. Huang, and J. Ma, “Transformer-based cross-modal infor-
mation fusion network for semantic segmentation,” Neural Process. Lett.,
vol. 55, no. 5, pp. 6361–6375, 2023.
[44] X. Wang, Z. Guan, W. Qian, J. Cao, C. Wang, and R. Ma, “STFuse:
Infrared and visible image fusion via semisupervised transfer learning,”
IEEE Trans. Neural Netw. Learn. Syst., 2023, early access, Nov. 08, 2023,
doi: 10.1109/TNNLS.2023.3328060.
[45] A. Cioppa, A. Deliege, N. U. Huda, R. Gade, M. V an Droogenbroeck,
and T. B. Moeslund, “Multimodal and multiview distillation for real-time
player detection on a football ﬁeld,” in Proc. IEEE/CVF Conf. Comput.
Vis. Pattern Recognit. Workshops, 2020, pp. 880–881.
[46] P . Wu et al., “Not only look, but also listen: Learning multimodal violence
detection under weak supervision,” in Proc. 16th Eur . Conf. Comput. Vis.,
Glasgow, U.K., Aug. 23–28, 2020, pp. 322–339.
[47] Y . Li, Y . Wang, and Z. Cui, “Decoupled multimodal distilling for emotion
recognition,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. ,
2023, pp. 6631–6640.
[48] G. Radevski, D. Grujicic, M. Blaschko, M.-F. Moens, and T. Tuyte-
laars, “Multimodal distillation for egocentric action recognition,” in Proc.
IEEE/CVF Int. Conf. Comput. Vis. , 2023, pp. 5213–5224.
[49] W. Al-Dhabyani, M. Gomaa, H. Khaled, and A. Fahmy, “Dataset of breast
ultrasound images,” Data Brief, vol. 28, 2020, Art. no. 104863.
[50] V . De Luca et al., “Evaluation of 2D and 3D ultrasound tracking algorithms
and impact on ultrasound-guided liver radiotherapy margins,”Med. Phys.,
vol. 45, no. 11, pp. 4986–5003, 2018.
[51] S. Vitale, J. I. Orlando, E. Iarussi, and I. Larrabide, “Improving realism in
patient-speciﬁc abdominal ultrasound simulation using cycleGANs,” Int.
J. Comput. Assist. Radiol. Surg. , vol. 15, no. 2, pp. 183–192, 2020.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:48:16 UTC from IEEE Xplore.  Restrictions apply.