IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 36, NO. 9, SEPTEMBER 2024 4781
MatchXML: An Efﬁcient Text-Label Matching
Framework for Extreme Multi-Label
Text Classiﬁcation
Hui Y e , Rajshekhar Sunderraman , and Shihao Ji , Senior Member , IEEE
Abstract—The eXtreme Multi-label text Classiﬁcation (XMC)
refers to training a classiﬁer that assigns a text sample with rel-
evant labels from an extremely large-scale label set (e.g., millions
of labels). We propose MatchXML, an efﬁcient text-label match-
ing framework for XMC. We observe that the label embeddings
generated from the sparse Term Frequency-Inverse Document
Frequency (TF–IDF) features have several limitations. We thus
propose label2vec to effectively train the semantic dense label
embeddings by the Skip-gram model. The dense label embeddings
are then used to build a Hierarchical Label Tree by clustering. In
ﬁne-tuning the pre-trained encoder Transformer, we formulate the
multi-label text classiﬁcation as a text-label matching problem in
a bipartite graph. We then extract the dense text representations
from the ﬁne-tuned Transformer . Besides the ﬁne-tuned dense text
embeddings, we also extract the static dense sentence embeddings
from a pre-trained Sentence Transformer . Finally, a linear ranker
is trained by utilizing the sparse TF–IDF features, the ﬁne-tuned
dense text representations, and static dense sentence features.
Experimental results demonstrate that MatchXML achieves the
state-of-the-art accuracies on ﬁve out of six datasets. As for the
training speed, MatchXML outperforms the competing methods
on all the six datasets.
Index Terms—Extreme multi-label classiﬁcation, label2vec, text-
label matching, bipartite graph, contrastive learning.
I. I NTRODUCTION
T
HE eXtreme Multi-label text Classiﬁcation (XMC) refers
to learning a classiﬁer that can annotate an input text with
the most relevant labels from an extremely large-scale label set
(e.g., millions of labels). This problem has many real world
applications, such as labeling a Wikipedia page with relevant
tags [1], providing a customer query with related products
in product search [2], and recommending relevant items to a
customer in recommendation systems [3].
To address the issue of the extremely large output space in
XMC, the Hierarchical Label Tree (HLT) [2] has been proposed
Manuscript received 24 August 2023; revised 24 January 2024; accepted 27
February 2024. Date of publication 12 March 2024; date of current version 7
August 2024. This work was supported in part by Presidential Fellowship in the
Transcultural Conﬂict and Violence Initiative (TCV) at Georgia State Univer-
sity, in part by National Science Foundation Major Research Instrumentation
(MRI) under Grant CNS-1920024. Recommended for acceptance by J.-G. Lee.
(Corresponding author: Hui Ye.)
The authors are with the Department of Computer Science, Georgia State
University, Atlanta, GA 30302 USA (e-mail: hye2@student.gsu.edu; rsunder-
raman@gsu.edu; sji@gsu.edu).
Our source code is publicly available at https://github.com/huiyegit/
MatchXML.
Digital Object Identiﬁer 10.1109/TKDE.2024.3374750
to effectively reduce the computational complexity from O(L)
to O(logL), where L is the number of labels. Taking label
embeddings as input, an HLT can be constructed by partition
algorithms [2], [4] based on the K-means clustering. Prior
works [2], [4], [5], [6], [7] have applied the Positive Instance Fea-
ture Aggregation (PIFA) to compute label embeddings, where
one label embedding is the summation of the TF–IDF features
of the text samples when the label is positive. However, the label
embeddings generated from PIFA have several limitations. First,
current machine learning algorithms are more efﬁcient to pro-
cess the data of small dense vectors than the large sparse vectors.
Second, the TF–IDF features of text data, which are required
by PIFA to generate the label embeddings, may not be always
available and thus limits the applications of PIFA. Inspired by
the word2vec [8], [9] in training word embeddings, we propose
label2vec to learn the semantic dense label embeddings. We
consider a set of labels assigned to a text sample as an unordered
sequence, where each label can be treated as one word/token, and
the Skip-gram model [8], [9] is applied to train the embedding
for each label. The label2vec approach has better generalization
than PIFA as it does not require the TF–IDF features. Besides,
the dense label embeddings have smaller storage size that are
more efﬁcient to process by the downstream machine learning
algorithms. Our experiments demonstrate that the dense label
embeddings can capture the semantic label relationships and
generate improved HLTs compared to the sparse label em-
beddings, leading to improved performance in the downstream
XMC tasks.
Most of the early works in XMC [2], [10], [11], [12], [13],
[14], [15], [16], [17], [18], [19], [20], [21], [22] leverage the
statistical Bag-Of-Words (BOW) or Term Frequency-Inverse
Document Frequency (TF–IDF) features as the text represen-
tations to train a text classiﬁer. This type of text features is
simple, but it can not capture the semantic meaning of text
corpora due to the ignorance of word order. Recent works [5],
[6], [23], [24], [25] explore deep learning approaches to learn
the dense vectors as the text representations. These methods
leverage the contextual information of words in text corpora to
extract the dense text representations, leading to improved clas-
siﬁcation accuracies. On the other hand, the recently proposed
XR-Transformer [7] and CascadeXML [26] have showed that
sparse TF–IDF features and dense text features are not mutually
exclusive to each other, but rather can be leveraged together as
the text representations to boost the performance. Inspired by
1041-4347 © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:47:05 UTC from IEEE Xplore.  Restrictions apply.
4782 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 36, NO. 9, SEPTEMBER 2024
this strategy, we generate the ﬁnal text representations by taking
advantage of both sparse TF–IDF features and dense vector
features, and we propose a novel method to improve the quality
of dense vector features for XMC. Speciﬁcally, in the ﬁne-tuning
stage of pre-trained encoder Transformer, we formulate the
multi-label text classiﬁcation as a text-label matching problem
in a bipartite graph. Through text-label alignment and label-text
alignment in a bipartite graph, the ﬁne-tuned Transformer can
yield robust and effective dense text representations. Besides the
dense text representations ﬁne-tuned from the above-mentioned
method, we also utilize the static dense sentence embeddings
extracted from pre-trained Sentence Transformers, which are
widely used in NLP for the tasks, such as text classiﬁcation,
clustering, retrieval, and paraphrase detection, etc. Compared
with the sparse TF-IDF representations, the static dense sentence
embeddings can capture the semantic meaning and facilitate
the downstream applications. In particular, we extract the static
sentence embeddings from Sentence-T5 [27] and integrate them
into our MatchXML. We have found that this approach is very
effective as shown in our ablation study.
The remainder of the paper is organized as follows. In Section
II, we review the related works from the perspectives of extreme
classiﬁcation, cross-modal learning and contrastive learning.
The proposed method MatchXML is presented in Section III,
where its main components: label2vec, hierarchical label tree,
text-label matching, and linear ranker are introduced. Exper-
imental results on six benchmark datasets are presented in
Section IV, with comparisons to other algorithms currently in
the literature. Conclusions and future work are discussed in
Section V.
II. R ELA TEDWORKS
Extreme Classiﬁcation: A great number of works have been
proposed to address the extreme classiﬁcation problem [19],
[21], [22], [28], [29], [30], [31], [32], [33], [34], [35], [36],
[37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47],
which can be categorized to One-vs-All approaches, tree-based
approaches, embedding-based approaches, and deep learning
approaches. The One-vs-All approaches, such as PDSparse[13],
train a binary classiﬁer for each label independently. To speed
up the computation, these approaches leverage the negative
sampling and parallel computing to distribute the training over
multiple cores or servers. The tree-based approaches, such as
FastXML [10], train a hierarchical tree structure to divide the
label set into small groups. These approaches usually have the
advantage of fast training and inference. The embedding-based
approaches, such as SLEEC [11], seek to lower the compu-
tational cost by projecting the high-dimensional label space
into a low-dimensional one. However, information loss during
the compression process often undermines the classiﬁcation
accuracy.
Deep learning approaches leverage the raw text to learn
semantic dense text representations instead of the statisti-
cal TF-IDF features. Recent works (e.g., X-Transformer [5],
APLC-XLNet [25], LightXML [6]) ﬁne-tune the pre-trained
encoder Transformers, such as BERT [48], RoBERTa [49] and
XLNet [50], to extract the dense text features. Further, a cluster-
ing structure or a shallow hierarchical tree structure is designed
to deal with the large output label space rather than the traditional
linear classiﬁer layer. For example, XR-Transformer [7] pro-
poses a shallow balanced label tree to ﬁne-tune the pre-trained
encoder Transformer in multiple stages. The dense vectors
extracted from the last ﬁne-tuning stage and sparse TF-IDF
features are leveraged to train the ﬁnal classiﬁer. Compared with
XR-Transformer, we generate the Hierarchical Label Tree by the
label embeddings learned fromlabel2vec rather than the TF-IDF
features. Besides, we formulate the XMC task as a text-label
matching problem to ﬁne-tune the dense text representations. In
addition, we extract the static dense sentence embeddings from
a pre-trained Sentence Transformer for the classiﬁcation task.
Cross-Modal Learning: In the setting of text-label matching,
we consider the input texts (i.e., sentences) as the text modality,
while the class labels (i.e., 1, 2, 3) as another label modality .
Therefore, the line of research in cross-modal learning is relevant
to our text-label matching problem. The cross-modal learning
involves processing data across different modalities, such as text,
image, audio, and video. Some typical Image-Text Matching
tasks have been well studied in recent years, including Image-
Text Retrieval [51], [52], Visual Question Answering [53], [54]
and Text-to-Image Generation [55], [56], [57], [58], [59].T h e
general framework is to design one image encoder and one
text encoder to extract the visual representations and textual
representations, respectively, and then fuse the cross-modal in-
formation to capture the relationships between them. In contrast
to the framework of Image-Text Matching, we develop one text
encoder for the text data and one embedding layer to extract
the dense label representations. Furthermore, the relationship
between image and text in Image-Text Matching usually belongs
to an one-to-one mapping, while the relationship between text
and label in the context of XMC is a many-to-many mapping.
Contrastive Learning: Another line of research in contrastive
learning is also related to our proposed method. Recently, self-
supervised contrastive learning [60], [61], [62] has attracted
great attention due to its remarkable performance in visual
representation learning. Typically, a positive pair of images
is constructed from two views of the same image, while a
negative pair of images is formed from the views of different
images. Then a contrastive loss is designed to push together
the representations of positive pairs and push apart the ones of
negative pairs. Following the framework of self-supervised con-
trastive learning, supervised contrastive learning[63] constructs
additional positive pairs by utilizing the label information. The
application of the supervised contrastive loss can be found in
recent works [64], [65], [66], [67] to deal with text classiﬁcation.
In this paper, we leverage the supervised constrastive loss as
the training objective for text-label matching, and we develop a
novel approach to construct the positive and negative text-label
pairs for XMC. MACLR [67] is a recent work that applies the
contrastive learning for the Extreme Zero-Shot Learning, and
thus is related to our MatchXML. However, there are two main
differences between these two works. First, the contrastive learn-
ing paradigm in MACLR belongs to self-supervised contrastive
learning, while MatchXML is a supervised contrastive learning
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:47:05 UTC from IEEE Xplore.  Restrictions apply.
YE et al.: MatchXML: AN EFFICIENT TEXT-LABEL MA TCHING FRAMEWORK FOR EXTREME MULTI-LABEL TEXT CLASSIFICA TION 4783
method. Speciﬁcally, MACLR constructs the positive text-text
pair, where the latter text is a sentence randomly sampled from
a long input sentence, while MatchXML constructs the positive
text-label pair, where the label is one of the class labels of the
input text. Second, MACLR utilizes the Inverse Cloze Task
which is a frequently used pre-training task for the sentence
encoder, while MatchXML is derived from the Cross-Modal
learning task.
III. M ETHOD
A. Preliminaries
Given a training dataset withN samples{(xi,yi)}N
i=1, where
xi denotes text sample i, and yi is the ground truth that can
be expressed as a label vector with binary values of 0 or 1.
Let yil,f o rl ∈{1,...,L }, denote the lth element of yi, where
L is the cardinality of the label set. When yil =1 , label l is
relevant to text i, and otherwise not. In a typical XMC task,
number of instances N and number of labels L can be at the
order of millions or even larger. The objective of XMC is to learn
a classiﬁer f (x,l ) from the training dataset, where the value of
f indicates the relevance score of text x and labell, with a hope
that f can generalize well on test dataset {(xj,yj )}Nt
j=1 with a
high accuracy.
The training of MatchXML consists of four steps. In the
ﬁrst step, we train the dense label vectors by our proposed
label2vec. In the second step, a preliminary Hierarchical Label
Tree (HLT) is constructed using a Balanced K-means Clustering
algorithm [4]. In the third step, a pre-trained Transformer model
is ﬁne-tuned recursively from the top layer to bottom layer
through the HLT. Finally, we train a linear classiﬁer by utilizing
all three text representations: (1) sparse TF-IDF text features,
(2) the dense text representations extracted from the ﬁne-tuned
Transformer, and (3) the static dense sentence features extracted
from a pre-trained Sentence Transformer. As for the inference,
the computational cost contains the feature extraction of input
text from the ﬁne-tuned Transformer and the beam search guided
by the trained linear classiﬁer through the reﬁned HLT. Thus,
the computational complexity of MatchXML inference can be
expressed as
O(T1 +kbd log(L)), (1)
whereT1 denotes the cost of extracting the dense text represen-
tation from the text encoder, kb is the size of beam search, d is
the dimension of the concatenated text representation, and L is
the number of labels. The details of MatchXML are elaborated
as follows.
B. label2vec
The Hierarchical Label Tree (HLT) plays a fundamental
role in reducing the computational cost of XMC, while the
high-quality label embeddings is critical to construct an HLT
that can cluster the semantically similar labels together. In this
section, we introduce label2vec to train the semantic dense label
embeddings for the HLT. Note that the training label set{yi}N
i=1
contains a large amount of semantic information among labels.
We therefore treat the positive labels in yi as a label sequence, 1
similar to the words/tokens in one sentence in word2vec. We then
adopt the Skip-gram model to train the label embeddings, which
can effectively learn high-quality semantic word embeddings
from large text corpora. The basic mechanism of the Skip-gram
model is to predict context words from a target word. The
training objective is to minimize the following loss function:
−logσ(wT
t wc)−
k∑
i=1
Ezi∼ZT
[
logσ(−wT
t wzi )
]
, (2)
wherewt andwc denote the target word embedding and context
word embedding, respectively, and zi is one of the k negative
samples. To have the Skip-gram model adapt to the label2vec
task, we simply make several necessary modiﬁcations as fol-
lows. First, in word2vec the 2nk training target-context word
pairs can be generated by setting a context window of size
nk, consisting of nk context words before and after the target
word. A small window size (i.e., nk = 2) tends to have the
target word focusing more on the nearby context words, while
a large window size (i.e., nk = 10) can capture the semantic
relationship between target word and broad context words. The
Skip-gram model adopts the strategy of dynamic window size
to train word2vec. However, in label2vec there is no distance
constraint between target label and its context labels since they
are semantically similar if both labels co-occur in one training
sample. Therefore, we set the window size nk to the maxi-
mum number of labels among all training samples. Second, the
subsampling technique is leveraged to mitigate the imbalance
issue between the frequent and rare words in word2vec since the
frequent/stop words (e.g., “in”, “the”, and “a”) do not provide
much semantic information to train word representations. In
contrast, the frequent labels are usually as important as rare
labels in XMC to capture the semantic relationships among
labels in label2vec. Therefore, we do not apply the subsampling
to the frequent labels in label2vec.
C. Hierarchical Label Tree
Once the dense label vectors W = {wi}L
i=1 are extracted
from {yi}N
i=1 with label2vec, we build a Hierarchical Label
Tree (HLT) of depthD from the label vectors W by a Balanced
K-means Clustering algorithm [4]. In the construction of HLT,
the link relationships of nodes between two adjacent layers are
organized as 2D matrices C = {Ct}D
t=1 based on the clustering
assignments. Then the ground truth label assignment oftth layer
Y (t) can be generated by the (t +1 )th layer Y (t+1) as follows:
Y (t) = binarize(Y (t+1)C(t+1)). (3)
The original ground truth yi corresponds to Y (D) in the bottom
layer, and thus the ground truth label assignment Y (t) can be
inferred from the bottom layer to the top layer according to
1The label order doesn’t matter in label2vec. Therefore, the label sequence
here is actually a label set. However, for easy understanding of label2vec,w e
adopt the same terminology of word2vec and treat the positive labels of yi as a
label sequence.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:47:05 UTC from IEEE Xplore.  Restrictions apply.
4784 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 36, NO. 9, SEPTEMBER 2024
Fig. 1. Architecture of text-label matching in a bipartite graph. When ﬁne-tuning a pre-trained encoder Transformer for the tth layer of HLT, we consi der the
input text set U (i.e., training samples) as the text modality, while the label set V (t) (i.e., training labels Y (t)) as the label modality.
(3). Subsequently, we ﬁne-tune the pre-trained Transformer in
multiple stages from the top layer to the bottom layer through
the HLT.
D. Text-Label Matching
In this section, we present our text-label matching framework
for XMC. In the ﬁne-tuning stage, we consider the multi-label
classiﬁcation as a text-label matching problem. We model this
matching problem in a bipartite graph G(U,V (t),E ), where U
andV (t) denote a set of text samples and the labels in thetth layer
of HLT, respectively, andE is a set of edges connecting U and
V (t).I ft e x ti has a positive label j, edge eij is created between
them. A text node in U can have multiple edges connecting it
to multiple label nodes in V (t). Vice versa, a label node in V (t)
can have multiple edges connecting it to multiple text nodes in
U . We ﬁne-tune a pre-trained encoder Transformer and the HLT
from the top layer to the bottom layer in multiple stages. Fig. 1
illustrates the framework of our approach for ﬁne-tuning the
encoder Transformer and one layer of the HLT. During training,
we sample a mini-batch of training data, from which the text
samples are fed to a text encoder to extract the text representa-
tions, and the corresponding labels are fed to an embedding layer
to extract the label representations. We consider the text-label
matching problem from two aspects: text-label alignment and
label-text alignment.
Text-Label Alignment: In the text-label matching setting, one
text sample aligns with multiple positive labels and contrasts
with negative labels in a mini-batch. We construct the set with
multiple positive text-label pairs{(zi,ep)}, wherep is a positive
label of text i. Following the previous work [7], we also mine
the hard negative labels (e.g., negative labels with high output
scores) to boost the performance. We then generate the set with a
number of negative text-label pairs{(zi,en)}, wheren is one of
hard negative labels of texti. We utilize the dot product(zi,ej ) as
the quantitative metric to measure the alignment of the text-label
pair. To align the text with labels, we train our model to maximize
the alignment scores of positive text-label pairs and minimize the
ones of negative text-label pairs. The loss function of text-label
alignment is deﬁned as
Ltl = 1
Nb
Nb∑
i=1
1
|P1(i)|
∑
p∈P1(i)
−log exp((zi,ep)/τ)∑
a∈A1(i) exp((zi,ea)/τ),
(4)
where Nb denotes the batch size, P1(i) is the set of indices of
positive labels related to texti,|P1(i)| is its cardinality,A1(i) is
the set of indices of positive and negative labels corresponding
to text i, and τ∈R+ is a scalar temperature parameter.
Label-Text Alignment: We also consider the label-text align-
ment in a reverse way for the text-label matching problem. In
the above-mentioned text-label alignment, we mine a number of
hard negative labels for each text to facilitate the training process.
On the contrary, if we form the label set by combining all the
positive labels and hard negative labels within a mini-batch, the
computational cost is likely to increase notably due to the large
cardinality of the label set. To reduce the computational cost,
we construct the label set only from all the positive labels within
a mini-batch. Similar to the previous text-label alignment, one
label sample corresponds to several text samples and contrasts
with the remaining text samples in the mini-batch. We generate
the set with several positive label-text pairs {(ei,zp)}, where
i is a positive label for text p. Otherwise, they form the set
with a number of negative label-text pairs {(ei,zn)}, where i
is a negative label for text n. To align the label with texts, we
train our model to maximize the alignment scores of positive
label-text pairs and minimize the ones of negative label-text
pairs. Similarly, the loss function of label-text alignment is
deﬁned as
Llt = 1
M
M∑
i=1
1
|P2(i)|
∑
p∈P2 (i)
−log exp((ei,zp)/τ)∑
a∈A2 (i) exp((ei,za)/τ),
(5)
whereM is the number of positive labels in the mini-batch,P2(i)
is the set of indices of positive text samples related to label i,
|P2(i)| is its cardinality, and A2(i) is the set of indices of text
samples within the mini-batch.
Loss Function: The overall loss function of our text-label
matching task is a linear combination of the two loss functions
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:47:05 UTC from IEEE Xplore.  Restrictions apply.
YE et al.: MatchXML: AN EFFICIENT TEXT-LABEL MA TCHING FRAMEWORK FOR EXTREME MULTI-LABEL TEXT CLASSIFICA TION 4785
Algorithm 1: MatchXML Training.
Input: Training dataset {X,Y } = {(xi,yi)}N
i=1,T F - I D F
features{ ¯X} = {(¯xi)}N
i=1, static dense sentence
embeddings { ˇX} = {(ˇxi)}N
i=1, Skip-gram model
h, text encoder g, the depth of HLT D
Output: Optimized text encoder g and the hierarchical
linear ranker {R(t)}D
t=1
1: Generate label pairs {(lk
i ,lk
j )}K
k=1 from{Y} = {yi}N
i=1
2: for {1,... , # of training epochs} do
3: for {1, ... , # of training steps} do
4: Sample a mini-batch of label pairs {(li,lj )}
5: Update Skip-gram model h to minimize (2)
6: end for
7: end for
8: Obtain dense label vectors W = {wi}L
i=1 ← h
9: {C(t)}D
t=1 ← Balanced K-means Clustering(W )
10: Get hierarchical ground truth label assignment
{Y (t)}D
t=1 by (3)
11: for {1,... ,D }d o
12: Initialize label embedding layer E(t) by the Bootstrap
13: for {1,... , # of training steps} do
14: Sample a mini-batch of training samples {(xi,y (t)
i }
15: Construct text-label pairs {(zi,ej}
16: Construct label-text pairs {(eˆi,z ˆj )}
17: Update Encoder g and Embedding E(t) to
minimize (6)
18: end for
19: end for
20: Obtain dense text features ˆX = {ˆxi}N
i=1 ←g(X)
21: Obtain ﬁnal text features ˜X = {˜xi}N
i=1 ←
Concat( ˆX, ¯X, ˇX)
22: for {1,... ,D }d o
23: Train the linear ranker R(t) of the tth layer by (7)
24: end for
deﬁned above
L = λLtl +( 1−λ)Llt, (6)
with λ ∈[0, 1]. Experiments show that the setting of hy-
perparameter λ has a notable impact on the performance of
MatchXML, and we thus tune it for different datasets.
E. Linear Ranker
Once the multi-stage ﬁne-tuning with (6) is completed, we
extract the dense text representations from the text encoder.
The extracted dense representations are then concatenated with
the static dense sentence embeddings from the Sentence Trans-
former and the sparse TF-IDF features as the ﬁnal text rep-
resentations {˜xi}N
i=1, which are used to train a linear ranking
model based on XR-LINEAR [4]. Speciﬁcally, let W (t) denote
the learnable parameter matrix of the ranker corresponding to
the tth layer of HLT, ˆM (t) denote the matrix of sampled labels
by the combination of the Teacher-Forcing Negatives (TFN)
and Matcher-Aware Negatives (MAN), Y (t) denote the label
assignment at the tth layer of HLT. The linear ranker at the tth
layer can be optimized as
arg min
W (t)
∑
ℓ: ˆM (t)
i,ℓ ̸=0
L(Y (t)
iℓ ,W (t)⊤
ℓ ˜xi)) +α∥W (t)∥2, (7)
where αis the hyperparameter that balances the classiﬁcation
loss and the L2 regularization on the parameter matrix W (t).
In summary, the training procedure of MatchXML is provided
in Algorithm 1.
IV . EXPERIMENTS
We conduct experiments to evaluate the performance of
MatchXML on six public datasets[68],2 including EURLex-4K,
Wiki10-31K, AmazonCat-13K, Wiki-500K, Amazon-670K,
and Amazon-3M, which are the same datasets used by XR-
Transformer [7]. The statistics of these datasets can be found
in Table I. It is well-known that the label distribution of the
XMC datasets follows the power (Zipf’s) law, where most of
the probability mass is covered by a small fraction of the label
set. As for the text distribution, each document is categorized
by a different number of labels, and this distribution doesn’t
follow a particular standard form. This can be observed from
Fig. 2, where the label and text distributions of Amazon-670K
and Amazon-3M are provided.
We consider EURLex-4K, Wiki10-31K, and AmazonCat-
13K as medium-scale datasets, while Wiki-500K, Amazon-
670K, and Amazon-3M as large-scale datasets. We are more
interested in the performance on large-scale datasets since they
are more challenging XMC tasks.
A. Evaluation Metrics
The widely used evaluation metrics for XMC are the precision
at k (P@k) and ranking quality at k (nDCG@k), which are
deﬁned as
P@k = 1
k
∑
l∈rankk (ˆy)
yl, (8)
DCG@k = 1
k
∑
l∈rankk (ˆy)
yl
log(l + 1), (9)
nDCG@k = DCG@k
∑min(k,||y0||)
l=1
1
log(l+1)
, (10)
where y ∈{0, 1}L is the ground truth label, ˆy is the predicted
score vector, and rankk(ˆy) returns the k largest indices of ˆy,
sorted in descending order.
For datasets that contain a large percentage of head (popular)
labels, high P@k or nDCG@k may be achieved by simply
predicting well on head labels. For performance evaluation on
tail (infrequent) labels, the XCM methods are recommended
to evaluate with respect to the propensity-scored counterparts
of the precision P@k and nDCG metrics (PSP and PSnDCG),
2https://ia802308.us.archive.org/21/items/pecos-dataset/xmc-base/
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:47:05 UTC from IEEE Xplore.  Restrictions apply.
4786 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 36, NO. 9, SEPTEMBER 2024
TABLE I
STA TISTICS OFDA TASETS
Fig. 2. Label distributions of Amazon-670K and Amazon-3M follow the power (Zipf’s) Law, as shown in (a) and (b). Text distributions of Amazon-670K and
Amazon-3M don’t follow a particular standard form, as shown in (c) and (d).
TABLE II
SETTINGS OF HYPERPARAMETERS TO TRAIN LABEL2VEC ON SIX DA TASETS
which are deﬁned as
PSP@k = 1
k
∑
l∈rankk (ˆy)
yl
pl
, (11)
PSDCG@k = 1
k
∑
l∈rankk (ˆy)
yl
pl log(l +1 ), (12)
PSnDCG@k = PSDCG@k∑k
l=1
1
log(l+1)
, (13)
where pl is the propensity score of label l that is used to
make metrics unbiased with respect to missing labels [68].F o r
consistency, we use the same setting as XR-Transformer [7] for
all datasets.
Following the prior works, we also record the Wall-clock time
of our program for speed comparison.
B. Experimental Settings
We train the dense label embeddings by using the Skip-gram
model of the Gensim library, which contains an efﬁcient im-
plementation of word2vec as described in the original paper [8].
We take the label sequences{yi}N
i=1 of training data as the input
corpora, and set the dimension of label vector to 100 and number
of negative label samples to 20. In word2vec, some rare words
would be ignored if the frequency is less than a certain threshold.
We keep all the labels in the label vocabulary regardless of
the frequency. The settings of the Skip-gram model for the six
datasets are listed in Table II.
Following the prior works, we utilize BERT [69] as the major
text encoder in our experiments. Instead of using the same
learning rate for the whole model, we leverage the discrimi-
native learning rate [25], [70] to ﬁne-tune our model, which
assigns different learning rates for the text encoder and the label
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:47:05 UTC from IEEE Xplore.  Restrictions apply.
YE et al.: MatchXML: AN EFFICIENT TEXT-LABEL MA TCHING FRAMEWORK FOR EXTREME MULTI-LABEL TEXT CLASSIFICA TION 4787
TABLE III
SETTING OF LEARNING RA TES AND TRAINING STEPS ON SIX DA TASETS
TABLE IV
SETTING OF HYPERPARAMETERS FOR FINE-TUNING ON SIX DA TASETS
embedding layer. Following XR-Transformer, we use different
optimizers AdamW [71] and SparseAdam for the text encoder
and the label embedding layer, respectively. Since the size of
parameters in the label embedding layer can be extremely large
for large datasets, the SparseAdam optimizer is utilized to reduce
the GPU memory consumption and improve the training speed.
Further, prior Transformer-based approaches [5], [6], [25] have
shown that the longer input text usually improves classiﬁcation
accuracy, but leads to more expensive computation. However,
we ﬁnd that the classiﬁcation accuracy of MatchXML is less
sensitive to the length of input text since MatchXML utilizes
both dense feature vectors extracted from Transformer and the
TF-IDF features for classiﬁcation. We therefore truncate the
input text to a reasonable length to balance the accuracy and
speed. In the multi-stage ﬁne-tuning process, we only apply the
proposed text-label matching learning in the last stage, while
we keep the original multi-label classiﬁcation learning for the
other ﬁne-tuning stages. As shown in Table III, we set different
learning rates for the text encoder and the label embedding
layer in each ﬁne-tuning stage. There is a three-stage pro-
cess for ﬁne-tuning the Transformer on ﬁve datasets, including
Eurlex-4K, Wiki10-31K, AmazonCat-13K, Amazon-670K, and
Amazon-3M, and a four-stage process on Wiki-500K. Table IV
provides the further details of the hyperparameters. We extract
the static sentence embeddings from the pre-trained Sentence-T5
model [27].
We compare our MatchXML with 12 state-of-the-art
(SOTA) XMC methods: AnnexML [16],D i S M E C[15],P f a s -
treXML [12], Parabel[2], eXtremeText[18], Bonsai [20],X M L -
CNN [23], XR-Linear [4], AttentionXML [24], LightXML [6],
APLC-XLNet [25], and XR-Transformer [7]. For deep learning
approaches (XML-CNN, AttentionXML, LightXML, APLC-
XLNet, XR-Transformer, MatchXML), we list the results of
the single model for a fair comparison. We also provide the
results of ensemble model. The results of the baseline meth-
ods are cited from the XR-Transformer paper. For parts of
the results that are not available in XR-Transformer, we re-
produce the results using the source code provided by the
original papers. The original paper of APLC-XLNet has re-
ported the results of another version of datasets, which are
different from the ones in XR-Transformer. We therefore re-
produce the results of APLC-XLNet by running the source
code on the same datasets as XR-Transformer. Our experi-
ments were conducted on a GPU server with 8 Tesla V100
GPUs and 64 CPUs, which has the same number of Tesla
V100 GPUs and CPUs as the AWS p3.16xlarge utilized by
XR-Transformer.
C. Experimental Results
Classiﬁcation Accuracy: Table V shows the classiﬁcation
accuracies of our MatchXML and the baseline methods over the
six datasets. Overall, MatchXML has achieved state-of-the-art
results on ﬁve out of six datasets. Especially, on three large-scale
datasets: Wiki-500K, Amazon-670K, and Amazon-3M, and the
gains are about 1.70%, 1.73% and 1.62% in terms of P@1,
respectively, over the second best results. Compared with the
baseline XR-Transformer, MatchXML has a better performance
in terms of precision on all the six datasets. For AmazonCat-13K,
our approach has achieved the second best result, with the
performance gap of 0.05% compared with LightXML. Note
that the number of labels for this dataset is not large (about
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:47:05 UTC from IEEE Xplore.  Restrictions apply.
4788 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 36, NO. 9, SEPTEMBER 2024
TABLE V
COMPARISON OF OUR APPROACH WITH RECENT XMC METHODS ON SIX PUBLIC DA TASETS
TABLE VI
COMPARISON OF OUR APPROACH AND BASELINES IN TERMS OF ENSEMBLE MODEL
13K), indicating that it can be handled reasonably well by the
linear classiﬁer in LightXML, while our hierarchical structure is
superior when dealing with datasets with extremely large label
outputs.
Results of Ensemble Models: We have the similar ensemble
strategy as XR-Transformer. That is, three pre-trained text en-
coders (BERT, RoBERTa, XLNet) are utilized together as the
ensemble model for three small datasets, including Eurlex-4K,
Wiki10-31K, and AmazonCat-13K; and one text encoder with
three different Hierarchical Label Trees are formed the ensemble
model for three large datasets, including Wiki-500K, Amazon-
670K, and Amazon-3M. As shown in Table VI, our MatchXML
again achieves state-of-the-art results on four datasets: Wiki10-
31K, Wiki-500K, Amazon-670K, and Amazon-3M in terms of
three metrics P@1, P@3 and P@5, which is consistent with
the results of the single model setting. For dataset Eurlex-4K,
P@1 and P@3 of our approach are the best, similar to the single
model results, while the P@5 is the second best with a slight
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:47:05 UTC from IEEE Xplore.  Restrictions apply.
YE et al.: MatchXML: AN EFFICIENT TEXT-LABEL MA TCHING FRAMEWORK FOR EXTREME MULTI-LABEL TEXT CLASSIFICA TION 4789
TABLE VII
COMPARISON OF OUR APPROACH WITH RECENT XMC METHODS ON SIX PUBLIC DA TASETS
TABLE VIII
COMPARISON OF OUR APPROACH AND BASELINES ON THREE LARGE DA TASETS W.R.T. PSP @k
performance gap of 0.15, compared with the best result. For
AmazonCat-13K, P@1 of our approach achieves the best result,
while P@3 and P@5 are the second best, similar to the ones in
single model.
Table VII shows the performances in terms of the ranking
metric nDCG@k of our MatchXML and the baselines over the
six datasets. Similarly, our MatchXML achieves state-of-the-
art results on all the six datasets. For the three medium-scale
datasets, Eurlex-4K, Wiki10-31K, AmazonCat-13K, the perfor-
mance gains of nDCG@1 are about 0.65%, 1.0% and 0.12% over
the second best results, respectively. For the three large-scale
datasets: Wiki-500K, Amazon-670K and Amazon-3M, the gains
are about 1.23%, 1.62% and 1.66% over the second best results,
respectively.
Results of Propensity Scored Precision: We compute the
propensity scored precision (PSP@k) to measure the perfor-
mance of MatchXML on tail labels. The results of the baselines:
PfastreXML and Parabel are cited from the ofﬁcial website. 3
The results reported in the XR-Transformer paper are computed
using a different version of source code. We reproduce the
results of XR-Transformer and compute the PSP@k using the
ofﬁcial source code. 4 As shown in Table VIII, our MatchXML
3http://manikvarma.org/downloads/XC/XMLRepository.html
4https://github.com/kunaldahiya/pyxclib
again achieves state-of-the-art results on two out of three large
datasets Wiki-500K and Amazon-670K in terms of three metrics
PSP@1, PSP@3 and PSP@5. For Amazon-3M, our approach
has achieved the second best performance. Note that Parabel
has developed speciﬁc techniques to boost the performance on
tail labels, and thus has the best performance on tail labels of
Amazon-3M. However, as shown in Table V, the performance
of Parabel on all the labels is about 6% lower than our approach.
Computation Cost: Table IX reports the training costs of
our MatchXML and other deep learning based approaches. The
baseline results of training time are cited from XR-Transformer.
For the unavailable training time of a single model, we calculate
it by dividing the training time of ensemble model by the
number of models in the ensemble. In XR-Transformer, 13.2
hour is the reported training time of the ensemble of three
models for AmazonCat-13K. We have checked the sequence
length (which is 256) and the number of training steps (which
is 45,000). We believe this cost should be the training time
of single model. Overall, our approach has shown the fastest
training speed on all the six datasets. We ﬁne-tune the text
encoder in three stages from the top layer to the bottom layer
through the HLT. Furthermore, we leverage several training
techniques, such as discriminative learning rate, small batch size
and less training steps, to improve the convergence rate of our
approach. Our MatchXML has the same strategy for inference
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:47:05 UTC from IEEE Xplore.  Restrictions apply.
4790 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 36, NO. 9, SEPTEMBER 2024
TABLE IX
TRAINING TIME (IN HOURS) OF SINGLE MODEL OF OUR APPROACH AND RECENT DEEP LEARNING METHODS ON SIX PUBLIC DA TASETS
TABLE X
TRAINING TIME (IN HOURS) OF LABEL2VEC
TABLE XI
COMPARISON OF THE EMBEDDING SIZES (IN MB)B ETWEEN TF-IDF FEA TURES AND DENSE VECTORS ON SIX DA TASETS
as XR-Transformer. The inference time on six datasets can be
found in Appendix A.4.2 of XR-Transformer [7].
D. Ablation Study
The framework of our MatchXML follows the training proce-
dure as the baseline XR-Transformer, including the construction
of HLT, ﬁne-tuning the encoder Transformer from the top to
bottom layers through the HLT, and training a linear classiﬁer.
Besides, we have proposed three novel techniques to boost
the performance, namely label2vec to learn the dense label
embeddings for the HLT construction, text-label matching for
ﬁne-tuning the encoder Transformer, and extraction of static
dense text embeddings from pre-trained Sentence Transformer.
In the ablation study, we set up our technical contribution
one by one and report the experimental results to show the
effectiveness of each component. The performance of our base
model is comparable to or slightly better than the baseline
XR-Transformer, since we have leveraged some techniques to
speed up the training.
Performance of label2vec: Table XII reports the performance
comparison of label2vec (number 2) and TF-IDF (number 1) in
terms of precision for the downstream XMC tasks. On the small
datasets, e.g., Eurlex-4K, Wiki10-31K and AmazonCat-13K, the
performances of label embeddings from label2vec are compa-
rable to the ones from TF-IDF features. However, on the large
datasets, e.g., Wiki-500K, Amazon-670K and Amazon-3M, la-
bel2vec outperforms TF-IDF, indicating that a large training
corpus is essential to learn high-quality dense label embeddings.
Our experimental results show that label2vec is more effective
than TF-IDF to utilize the large-scale datasets.
Table X reports the training time of label2vec on the six
datasets. The training of label2vec is highly efﬁcient on ﬁve
of them, including Eurlex-4K, Wiki10-31K, AmazonCat-13K,
Wiki-500K and Amazon-670K, as the cost is less than 0.3 hours.
The training time on Amazon-3M is about 3.6 hours, which is
the result of large amount of training label pairs. As shown
in Table I, the number of instances Ntrain and the average
number of positive labels per instance L are the two factors that
determine the size of training corpus. Note that we do not add
the training time of label2vec into the classiﬁcation task since
we consider the label2vec task as the preprocessing step for the
downstream tasks.
Table XI compares the sizes of label embedding from la-
bel2vec and TF-IDF. The dense label vectors have much smaller
size than that of the sparse TF-IDF label representations. Es-
pecially, on the large dataset, such as Wiki-500K, the size of
label embeddings can be reduced by 35× (from 7,109.2MB
to 200.4MB), which beneﬁts the construction of HLT
signiﬁcantly.
Performance of Text-Label Matching: Table XII reports the
performance of our text-label matching (number 3) on the
six datasets. The baseline objective is the weighted squared
hinge loss [7] (number 2). Our text-label matching approach
outperforms the baseline method on ﬁve out of six datasets,
including Eurlex-4K, Amazoncat-13K, Wiki-500K, Amazon-
670K and Amazon-3M. For Wiki10-31K, the metric P@1
is still better than the baseline, while P@3 and P@5 are
slightly worse. On the three large-scale datasets, the text-
label matching has achieved the largest gain of about 0.91%
on Amazon-670K, while the small gain of about 0.16% on
Amazon-3M.
Performance of Static Sentence Embedding: Table XII also
reports the performance of static dense sentence embedding
(number 4) on the six datasets. The technique has achieved
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:47:05 UTC from IEEE Xplore.  Restrictions apply.
YE et al.: MatchXML: AN EFFICIENT TEXT-LABEL MA TCHING FRAMEWORK FOR EXTREME MULTI-LABEL TEXT CLASSIFICA TION 4791
TABLE XII
ABLA TIONSTUDY OF OUR MAT C HXML. “L2V”R EFERS TO THE LABEL 2VEC METHOD,“TLM”R EFERS TO THE TEXT LABEL MAT C H I N GMETHOD AND “SEN”
DENOTES THE STATIC SENTENCE EMBEDDINGS
performance gains in 16 out of 18 metrics over the six datasets,
with two performance drops of P@3 and P@5 on Eurlex-4K.
On the three large-scale datasets: Wiki-500K, Amazon-670K
and Amazon-3M, the performance gains in P@1 are 0.66%,
0.39% and 0.95%, respectively. There are three types of text
features in our proposed MatchXML: sparse TF-IDF features,
dense text features ﬁne-tuned from pre-trained Transformer, and
the static dense sentence embeddings extracted from Sentence-
T5. The sparse TF-IDF features contains the global statistical
information of input text, but it does not capture the semantic
information. The dense text features ﬁne-tuned from pre-trained
Transformer are likely to lose parts of textual information due
to the truncation operation (i.e., context window size of 512
tokens), while the static dense sentence embeddings can support
much longer text sequence than the ﬁne-tuned text embeddings
from the encoder Transformer. Therefore, the static dense sen-
tence embeddings can be considered as an effective comple-
ment to the sparse TF-IDF features and dense text features
ﬁne-tuned from pre-trained Transformer. As shown in TableXII
(number 4), including the static dense sentence embeddings
boosts the performance of MatchXML consistently over the
sparse TF-IDF baselines and the ﬁne-tuned dense text feature
baselines.
V. C ONCLUSION
This paper proposes MatchXML, a novel text-label matching
framework, for the task of XMC. We introduce label2vec to
train the dense label embeddings to construct the Hierarchical
Label Tree, where the dense label vectors have shown superior
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:47:05 UTC from IEEE Xplore.  Restrictions apply.
4792 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 36, NO. 9, SEPTEMBER 2024
performance over the sparse TF-IDF label representations. In the
ﬁne-tuning stage of MatchXML, we formulate the multi-label
text classiﬁcation as the text-label matching problem within a
mini-batch, leading to robust and effective dense text represen-
tations for XMC. In addition, we extract the static sentence
embeddings from the pre-trained Sentence Transformer and
incorporate them into our MatchXML to boost the performance
further. Empirical study has demonstrated the superior perfor-
mance of MatchXML in terms of classiﬁcation accuracy and
training speed over six benchmark datasets. It is worthy men-
tioning that although we propose MatchXML in the context of
text classiﬁcation, our framework is general and can be extended
readily to other modalities for XMC, including image, audio, and
video, etc. as long as a modality-speciﬁc encoder is available.
The training of MatchXML consists of four stages: training of
label2vec, construction of HLT, ﬁne-tuning the text encoder, and
training a linear classiﬁer. As of future work, we plan to explore
an end-to-end training approach to improve the performance of
XMC further.
REFERENCES
[1] O. Dekel and O. Shamir, “Multiclass-multilabel classiﬁcation with more
classes than examples,” in Proc. 13th Int. Conf. Artif. Intell. Statist., 2010,
pp. 137–144.
[2] Y . Prabhu, A. Kag, S. Harsola, R. Agrawal, and M. V arma, “Parabel: Par-
titioned label trees for extreme classiﬁcation with application to dynamic
search advertising,” in Proc. World Wide Web Conf., 2018, pp. 993–1002.
[3] P . Covington, J. Adams, and E. Sargin, “Deep neural networks for Y ouTube
recommendations,” in Proc. 10th ACM Conf. Recommender Syst. , 2016,
pp. 191–198.
[4] H.-F. Y u, K. Zhong, J. Zhang, W.-C. Chang, and I. S. Dhillon, “PECOS:
Prediction for enormous and correlated output spaces,” J. Mach. Learn.
Res., vol. 23, no. 98, pp. 1–32, 2022.
[5] W.-C. Chang, H.-F. Y u, K. Zhong, Y . Y ang, and I. S. Dhillon, “Taming
pretrained transformers for extreme multi-label text classiﬁcation,” in
Proc. 26th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining , 2020,
pp. 3163–3171.
[6] T. Jiang, D. Wang, L. Sun, H. Y ang, Z. Zhao, and F. Zhuang, “LightXML:
Transformer with dynamic negative sampling for high-performance ex-
treme multi-label text classiﬁcation,” in Proc. AAAI Conf. Artif. Intell. ,
2021, pp. 7987–7994.
[7] J. Zhang, W.-C. Chang, H.-F. Y u, and I. Dhillon, “Fast multi-resolution
transformer ﬁne-tuning for extreme multi-label text classiﬁcation,” inProc.
Adv. Neural Inf. Process. Syst., 2021, pp. 7267–7280.
[8] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed
representations of words and phrases and their compositionality,” in Proc.
Adv. Neural Inf. Process. Syst., 2013, pp. 3111–3119.
[9] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of
word representations in vector space,” 2013, arXiv:1301.3781.
[10] Y . Prabhu and M. V arma, “FastXML: A fast, accurate and stable tree-
classiﬁer for extreme multi-label learning,” in Proc. 20th ACM SIGKDD
Int. Conf. Knowl. Discov. Data Mining , 2014, pp. 263–272.
[11] K. Bhatia, H. Jain, P . Kar, M. V arma, and P . Jain, “Sparse local embeddings
for extreme multi-label classiﬁcation,” in Proc. 28th Int. Conf. Neural Inf.
Process. Syst., 2015, pp. 730–738.
[12] H. Jain, Y . Prabhu, and M. V arma, “Extreme multi-label loss functions for
recommendation, tagging, ranking & other missing label applications,” in
Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining , 2016,
pp. 935–944.
[13] I. E. Y en, X. Huang, K. Zhong, P . Ravikumar, and I. S. Dhillon,
“PD-Sparse: A primal and dual sparse approach to extreme multiclass
and multilabel classiﬁcation,” in Proc. Int. Conf. Mach. Learn. , 2016,
pp. 3069–3077.
[14] I. E. Y en, X. Huang, W. Dai, P . Ravikumar, I. Dhillon, and E. Xing,
“PPDsparse: A parallel primal-dual sparse method for extreme classiﬁca-
tion,” inProc. 23rd ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining,
2017, pp. 545–553.
[15] R. Babbar and B. Schölkopf, “DiSMEC: Distributed sparse machines for
extreme multi-label classiﬁcation,” in Proc. 10th ACM Int. Conf. Web
Search Data Mining, 2017, pp. 721–729.
[16] Y . Tagami, “AnnexML: Approximate nearest neighbor search for extreme
multi-label classiﬁcation,” in Proc. 23rd ACM SIGKDD Int. Conf. Knowl.
Discov. Data Mining, 2017, pp. 455–464.
[17] W. Siblini, P . Kuntz, and F. Meyer, “CRAFTML, An efﬁcient clustering-
based random forest for extreme multi-label learning,” in Proc. 35th Int.
Conf. Mach. Learn., 2018, pp. 4664–4673.
[18] M. Wydmuch, K. Jasinska, M. Kuznetsov, R. Busa-Fekete, and K. Dem-
bczynski, “A no-regret generalization of hierarchical softmax to extreme
multi-label classiﬁcation,” in Proc. 32nd Int. Conf. Neural Inf. Process.
Syst., 2018, pp. 6358–6368.
[19] H. Jain, V . Balasubramanian, B. Chunduri, and M. V arma, “SLICE:
Scalable linear extreme classiﬁers trained on 100 million labels for related
searches,” in Proc. 12th ACM Int. Conf. Web Search Data Mining , 2019,
pp. 528–536.
[20] S. Khandagale, H. Xiao, and R. Babbar, “BONSAI-diverse and shallow
trees for extreme multi-label classiﬁcation,”Mach. Learn., vol. 109, no. 11,
pp. 2099–2119, 2020.
[21] K. Dahiya et al., “SiameseXML: Siamese networks meet extreme classi-
ﬁers with 100m labels,” in Proc. Int. Conf. Mach. Learn. , PMLR, 2021,
pp. 2330–2340.
[22] K. Dahiya et al., “DeepXML: A deep extreme multi-label learning frame-
work applied to short text documents,” in Proc. 14th ACM Int. Conf. Web
Search Data Mining, 2021, pp. 31–39.
[23] J. Liu, W.-C. Chang, Y . Wu, and Y . Y ang, “Deep learning for extreme
multi-label text classiﬁcation,” in Proc. 40th Int. ACM SIGIR Conf. Res.
Develop. Inf. Retrieval, 2017, pp. 115–124.
[24] R. Y ou, Z. Zhang, Z. Wang, S. Dai, H. Mamitsuka, and S. Zhu, “At-
tentionXML: Label tree-based attention-aware deep model for high-
performance extreme multi-label text classiﬁcation,” in Proc. Adv. Neural
Inf. Process. Syst., 2019, pp. 5812–5822.
[25] H. Y e, Z. Chen, D.-H. Wang, and B. Davison, “Pretrained generalized
autoregressive model with adaptive probabilistic label clusters for extreme
multi-label text classiﬁcation,” in Proc. Int. Conf. Mach. Learn. ,P M L R ,
2020, pp. 10809–10819.
[26] S. Kharbanda, A. Banerjee, E. Schultheis, and R. Babbar, “CascadeXML:
Rethinking transformers for end-to-end multi-resolution training in ex-
treme multi-label classiﬁcation,” in Proc. Adv. Neural Inf. Process. Syst. ,
2022, pp. 2074–2087.
[27] J. Ni et al., “Sentence-T5: Scalable sentence encoders from pre-trained
text-to-text models,” in Proc. Findings Assoc. Comput. Linguistics, 2022,
pp. 1864–1874.
[28] I. Evron, E. Moroshko, and K. Crammer, “Efﬁcient loss-based decoding
on graphs for extreme classiﬁcation,” in Proc. Adv. Neural Inf. Process.
Syst., 2018, pp. 7233–7244.
[29] A. Jalan and P . Kar, “Accelerating extreme classiﬁcation via adaptive
feature agglomeration,” in Proc. 28th Int. Joint Conf. Artif. Intell. , 2019,
pp. 2600–2606.
[30] I. Chalkidis, E. Fergadiotis, P . Malakasiotis, and I. Androutsopoulos,
“Large-scale multi-label text classiﬁcation on EU legislation,” in Proc.
57th Annu. Meeting Assoc. Comput. Linguistics , 2019, pp. 6314–6322.
[31] T. Medini, Q. Huang, Y . Wang, V . Mohan, and A. Shrivastava, “Extreme
classiﬁcation in log memory using count-min sketch: A case study of
Amazon search with 50m products,” in Proc. 33rd Int. Conf. Neural Inf.
Process. Syst., 2019, pp. 13265–13275.
[32] Y . Prabhu et al., “Extreme multi-label learning with label features for
warm-start tagging, ranking & recommendation,” in Proc. 11th ACM Int.
Conf. Web Search Data Mining, 2018, pp. 441–449.
[33] A. Mittal et al., “DECAF: Deep extreme classiﬁcation with label features,”
in Proc. 14th ACM Int. Conf. Web Search Data Mining , 2021, pp. 49–57.
[34] A. Mittal, N. Sachdeva, S. Agrawal, S. Agarwal, P . Kar, and M. V arma,
“ECLARE: Extreme classiﬁcation with label graph correlations,” in Proc.
ACM Int. World Wide Web Conf., 2021, pp. 3721–3732.
[35] D. Saini et al., “GalaXC: Graph neural networks with labelwise attention
for extreme classiﬁcation,” in Proc. Web Conf., 2021, pp. 3733–3744.
[36] N. Gupta, S. Bohra, Y . Prabhu, S. Purohit, and M. V arma, “Generalized
zero-shot extreme multi-label learning,” inProc. 27th ACM SIGKDD Conf.
Knowl. Discov. Data Mining, 2021, pp. 527–535.
[37] A. Mittal et al., “Multi-modal extreme classiﬁcation,” in Proc. IEEE/CVF
Conf. Comput. Vis. Pattern Recognit., 2022, pp. 12393–12402.
[38] K. Dahiya et al., “NGAME: Negative mining-aware mini-batching for
extreme classiﬁcation,” in Proc. 16th ACM Int. Conf. Web Search Data
Mining, 2023, pp. 258–266.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:47:05 UTC from IEEE Xplore.  Restrictions apply.
YE et al.: MatchXML: AN EFFICIENT TEXT-LABEL MA TCHING FRAMEWORK FOR EXTREME MULTI-LABEL TEXT CLASSIFICA TION 4793
[39] R. Babbar and B. Schölkopf, “Data scarcity, robustness and extreme multi-
label classiﬁcation,” Mach. Learn., vol. 108, pp. 1329–1351, 2019.
[40] M. Wydmuch, K. Jasinska-Kobus, R. Babbar, and K. Dembczynski,
“Propensity-scored probabilistic label trees,” inProc. 44th Int. ACM SIGIR
Conf. Res. Develop. Inf. Retrieval , 2021, pp. 2252–2256.
[41] M. Qaraei, E. Schultheis, P . Gupta, and R. Babbar, “Convex surrogates for
unbiased loss functions in extreme classiﬁcation with missing labels,” in
Proc. Web Conf., 2021, pp. 3711–3720.
[42] E. Schultheis and R. Babbar, “Speeding-up one-versus-all training for
extreme classiﬁcation via mean-separating initialization,” Mach. Learn.,
vol. 111, pp. 3953–3976, 2022.
[43] E. Schultheis, M. Wydmuch, R. Babbar, and K. Dembczynski, “On missing
labels, long-tails and propensities in extreme multi-label classiﬁcation,”
in Proc. 28th ACM SIGKDD Conf. Knowl. Discov. Data Mining , 2022,
pp. 1547–1557.
[44] J.-Y . Jiang, W.-C. Chang, J. Zhang, C.-J. Hsieh, and H.-F. Y u, “Relevance
under the iceberg: Reasonable prediction for extreme multi-label classiﬁ-
cation,” in Proc. 45th Int. ACM SIGIR Conf. Res. Develop. Inf. Retrieval ,
2022, pp. 1870–1874.
[45] T. Z. Baharav, D. L. Jiang, K. Kolluri, S. Sanghavi, and I. S. Dhillon,
“Enabling efﬁciency-precision trade-offs for label trees in extreme clas-
siﬁcation,” in Proc. 30th ACM Int. Conf. Inf. Knowl. Manage. , 2021,
pp. 3717–3726.
[46] X. Liu, W.-C. Chang, H.-F. Y u, C.-J. Hsieh, and I. Dhillon, “Label
disentanglement in partition-based extreme multilabel classiﬁcation,” in
Proc. Adv. Neural Inf. Process. Syst., 2021, pp. 15359–15369.
[47] D. Zong and S. Sun, “BGNN-XML: Bilateral graph neural networks for
extreme multi-label text classiﬁcation,” IEEE Trans. Knowl. Data Eng. ,
vol. 35, no. 7, pp. 6698–6709, Jul. 2023.
[48] J. D. M.-W. C. Kenton and L. K. Toutanova, “BERT: Pre-training of
deep bidirectional transformers for language understanding,” in Proc.
Annu. Conf. North Amer . Chapter Assoc. Comput. Linguistics: Hum. Lang.
Technol., 2019, pp. 4171–4186.
[49] Y . Liu et al., “RoBERTa: A robustly optimized BERT pretraining ap-
proach,” 2019, arXiv: 1907.11692.
[50] Z. Y ang, Z. Dai, Y . Y ang, J. Carbonell, R. Salakhutdinov, and Q. V . Le,
“XLNet: Generalized autoregressive pretraining for language understand-
ing,” in Proc. 33rd Int. Conf. Neural Inf. Process. Syst., 2019, Art. no. 517.
[51] L. Wang, Y . Li, and S. Lazebnik, “Learning deep structure-preserving
image-text embeddings,” inProc. IEEE Conf. Comput. Vis. Pattern Recog-
nit., 2016, pp. 5005–5013.
[52] K.-H. Lee, X. Chen, G. Hua, H. Hu, and X. He, “Stacked cross attention for
image-text matching,” inProc. Eur . Conf. Comput. Vis., 2018, pp. 201–216.
[53] Y .-C. Chen et al., “UNITER: Universal image-text representation learn-
ing,” in Proc. Eur . Conf. Comput. Vis., 2020, pp. 104–120.
[54] H. Tan and M. Bansal, “LXMERT: Learning cross-modality encoder
representations from transformers,” in Proc. Conf. Empir . Methods Nat-
ural Lang. Process. 9th Int. Joint Conf. Natural Lang. Process. , 2019,
pp. 5100–5111.
[55] T. Xu et al., “AttnGAN: Fine-grained text to image generation with
attentional generative adversarial networks,” inProc. IEEE Conf. Comput.
Vis. Pattern Recognit., 2018, pp. 1316–1324.
[56] M. Zhu, P . Pan, W. Chen, and Y . Y ang, “DM-GAN: Dynamic memory
generative adversarial networks for text-to-image synthesis,” in Proc.
IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 5802–5810.
[57] G. Yin, B. Liu, L. Sheng, N. Y u, X. Wang, and J. Shao, “Semantics
disentangling for text-to-image generation,” in Proc. IEEE/CVF Conf.
Comput. Vis. Pattern Recognit., 2019, pp. 2327–2336.
[58] H. Zhang, J. Y . Koh, J. Baldridge, H. Lee, and Y . Y ang, “Cross-modal
contrastive learning for text-to-image generation,” in Proc. IEEE/CVF
Conf. Comput. Vis. Pattern Recognit., 2021, pp. 833–842.
[59] H. Y e, X. Y ang, M. Takac, R. Sunderraman, and S. Ji, “Improving text-
to-image synthesis using contrastive learning,” in Proc. 32nd Brit. Mach.
Vis. Conf., Paper 154, 2021.
[60] K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick, “Momentum contrast for
unsupervised visual representation learning,” in Proc. IEEE/CVF Conf.
Comput. Vis. Pattern Recognit., 2020, pp. 9729–9738.
[61] X. Chen, H. Fan, R. Girshick, and K. He, “Improved baselines with
momentum contrastive learning,” 2020, arXiv: 2003.04297.
[62] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework
for contrastive learning of visual representations,” inProc. Int. Conf. Mach.
Learn., PMLR, 2020, pp. 1597–1607.
[63] P . Khosla et al., “Supervised contrastive learning,” in Proc. Adv. Neural
Inf. Process. Syst., 2020, pp. 18661–18673.
[64] Q. Chen, R. Zhang, Y . Zheng, and Y . Mao, “Dual contrastive
learning: Text classiﬁcation via label-aware data augmentation,”
2022, arXiv:2201.08702.
[65] B. Gunel, J. Du, A. Conneau, and V . Stoyanov, “Supervised contrastive
learning for pre-trained language model ﬁne-tuning,” in Proc. Int. Conf.
Learn. Representations, 2020.
[66] H. Sedghamiz, S. Raval, E. Santus, T. Alhanai, and M. Ghassemi, “SupCL-
seq: Supervised contrastive learning for downstream optimized sequence
representations,” in Proc. Findings Assoc. Comput. Linguistics , 2021,
pp. 3398–3403.
[67] Y . Xiong, W.-C. Chang, C.-J. Hsieh, H.-F. Y u, and I. Dhillon, “Extreme
zero-shot learning for extreme text classiﬁcation,” in Proc. Conf. North
Amer . Chapter Assoc. Comput. Linguistics, 2022, pp. 5455–5468.
[68] K. Bhatia et al., “The extreme classiﬁcation repository: Multi-label
datasets and code,” 2016. [Online]. Available: http://manikvarma.org/
downloads/XC/XMLRepository.html
[69] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
training of deep bidirectional transformers for language understanding,”
in Proc. Conf. North Amer . Chapter Assoc. Comput. Linguistics , 2019,
pp. 4171–4186.
[70] J. Howard and S. Ruder, “Universal language model ﬁne-tuning for text
classiﬁcation,” in Proc. 56th Annu. Meeting Assoc. Comput. Linguistics ,
2018, pp. 328–339.
[71] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,” in
Proc. Int. Conf. Learn. Representations , 2019.
Hui Y ereceived the bachelor’s and master’s degrees
in computer science from the Huazhong University
of Science and Technology, Wuhan, China and Leigh
University, Bethlehem. He is currently working to-
ward the PhD degree in computer science with Geor-
gia State University, Atlanta. His research interests
include large-scale text classiﬁcation, text to image
generation, multi-agent learning and 3D computer
vision.
Rajshekhar Sunderraman received the PhD degree
in computer science from Iowa State University, in
1988. He is professor and associate chair with the De-
partment of Computer Science, Georgia State Univer-
sity. His research expertise is in the areas of databases,
data mining, Big Data, knowledge representation,
Semantic Web, and reasoning with incomplete and
inconsistent information. He has published more than
150 research articles in these areas in leading com-
puter science journals and conference proceedings
and is the author of a popular textbook “Oracle 10g
Programming: A Primer”, published by Pearson, in 2007.
Shihao Ji (Senior Member, IEEE) received the PhD
degree in electrical and computer engineering from
Duke University, in 2006. He is an associate professor
with the Department of Computer Science, Georgia
State University. His principal research interests lie
in the area of deep learning and its applications to
computer vision, natural language processing, and
robotics, with an emphasis on high-performance com-
puting. He is interested in developing efﬁcient algo-
rithms that can learn from a variety of data sources
(e.g., image, audio, text, and point clouds) on large
scale and automate decision-making processes in dynamic environments. He
has published more than 50 papers in top-ranked journals and prestigious
conferences with high impact factors. His research has been supported by federal
agencies, including NSF, NIH, DoD, ARO, as well as industry companies, such
as VMware, Cisco, Nvidia, and Bill & Melinda Gates Foundation. Before joining
GSU, he had been in industry research labs for about ten years.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on February 02,2026 at 22:47:05 UTC from IEEE Xplore.  Restrictions apply.