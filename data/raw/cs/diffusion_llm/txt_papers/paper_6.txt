2538 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 28, 2020
Knowledge Guided Capsule Attention Network for
Aspect-Based Sentiment Analysis
Bowen Zhang ,X u t a oL i , Xiaofei Xu, Ka-Cheong Leung , Senior Member , IEEE,
Zhiyao Chen, and Y unming Y e
Abstract—Aspect-based (aspect-level) sentiment analysis is an
important task in ﬁne-grained sentiment analysis, which aims to
automatically infer the sentiment towards an aspect in its context.
Previous studies have shown that utilizing the attention-based
method can effectively improve the accuracy of the aspect-based
sentiment analysis. Despite the outstanding progress, aspect-based
sentiment analysis in the real-world remains several challenges.
(1) The current attention-based method may cause a given aspect
to incorrectly focus on syntactically unrelated words. (2) Con-
ventional methods fail to identify the sentiment with the special
sentence structure, such as double negatives. (3) Most of the studies
leverage only one vector to represent context and target. However,
utilizing one vector to represent the sentence is limited, as the
natural languages are delicate and complex. In this paper, we
propose a knowledge guided capsule network (KGCapsAN), which
can address the above deﬁciencies. Our method is composed of
two parts, a Bi-LSTM network and a capsule attention network.
The capsule attention network implements the routing method by
attention mechanism. Moreover, we utilize two prior knowledge
to guide the capsule attention process, which are syntactical and
n-gram structures. Extensive experiments are conducted on six
datasets, and the results show that the proposed method yields the
state-of-the-art.
Index Terms —Aspect-based sentiment analysis, attention
mechanism, capsule attention network.
I. I NTRODUCTION
A
SPECT-BASED sentiment analysis (ABSA) 1 is a
ﬁne-grained task in sentiment analysis. It aims to
identify the sentiment polarity of the opinion targets in a
sentence or document (e.g., negative, neutral, or positive) [1].
Most sentences or documents come from online posts, such
Manuscript received March 18, 2020; revised July 9, 2020; accepted August 3,
2020. Date of publication August 17, 2020; date of current version September
7, 2020. This work was supported in part by the National Key Research and
Development Program of China under Grant 2018YFB1402500, and in part by
the National Science Foundation of China under Grants 61772155, 61802089,
61832004, 61832014, U1836107, 61972111, 61572158, and 61602132. The
associate editor coordinating the review of this manuscript and approving it for
publication was Dr. Taro Watanabe.(Corresponding authors: Xiaofei Xu; Xutao
Li.)
Bowen Zhang and Xiaofei Xu are with the College of Computer Science and
Technology, Harbin Institute of Technology, Harbin 150000, China (e-mail:
zhang_bo_wen@foxmail.com; xiaofei@hit.edu.cn).
Xutao Li, Ka-Cheong Leung, Zhiyao Chen, and Y unming Y e are with the
School of Computer Science and Technology, Harbin Institute of Technology,
Shenzhen 518055, China, and also with Shenzhen Key Laboratory of Internet In-
formation Collaboration, Shenzhen 518055, China (e-mail: lixutao@hit.edu.cn;
kcleung@ieee.org; dyleaf@foxmail.com; yeyunming@hit.edu.cn).
Digital Object Identiﬁer 10.1109/TASLP .2020.3017093
1also called aspect-level sentiment analysis.
as Amazon reviews or Twitter. ABSA has gained increasing
popularity in recent years since it has a wide range of
applications in the real world [2]. For example, it can help raise
perspicacity on consumer needs or their product experience,
guiding producers to improve their products.
Aspect-based sentiment analysis can be classiﬁed into two
subtasks, namely, aspect-category sentiment analysis (ACSA)
and aspect-term sentiment analysis (A TSA). ACSA aims to
identify the sentiment polarity to a given aspect target, which
is one of a few predeﬁned categories, while the goal of A TSA is
to predict the sentiment polarity of the aspect term that appears
in the text, which can be the word or phrase (multi-word). For
example, the sentence “The food price is reasonable although
the service is poor” expresses the positive sentiment for the
“food price” aspect, but it also conveys the negative sentiment
for the “service” aspect. The number of distinct words used as
the aspect terms could be more than a thousand, which poses
more challenges. Here, we focus on A TSA in this paper.
Existing A TSA methods can be divided into two categories.
Traditional approaches mainly leverage the statistical methods
to classify the sentiment of the aspect through designing a set
of hand-crafted features to train a classiﬁer such as SVM [3].
However, the preparation of massive number of hand-crafted
features is labor-intensive and cost expensive. Inspired by the
recent performance breakthroughs of employing deep learn-
ing in natural language processing, the deep neural networks
(say convolutional neural network (CNN) and recurrent neu-
ral network (RNN)) have become dominant in the literature
[4], [5]. This is because such methods can automatically generate
useful low-dimensional representations from both aspects and
contexts so as to achieve remarkable results without careful
feature engineering [6]. Recently, several studies attempt to deal
with A TSA problem based on deep learning methods [7], [8].
Ever since Tang et al. [9] raised the challenge of modeling
semantic relationships between aspect and context, researchers
resorted to RNN models with attention mechanisms for A TSA,
which can effectively identify more informative and relevant
words to a given aspect in a sentence [10], [11].
Despite the effectiveness of previous studies, it is challenging
to apply them in real-life applications: (1) The current atten-
tion mechanism may cause a given aspect to incorrectly focus
on syntactically unrelated words. Take sentence “The food is
delicious and the price is acceptable” as an example. Due to
the lack of ﬁne-grained attention mechanism, previous methods
tend to take “acceptable” to incorrectly infer the sentiment of the
2329-9290 © 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on January 31,2026 at 21:17:00 UTC from IEEE Xplore.  Restrictions apply.
ZHANG et al.: KNOWLEDGE GUIDED CAPSULE A TTENTION NETWORK FOR ASPECT-BASED SENTIMENT ANALYSIS 2539
Fig. 1. An example for the illustration of the syntactical dependency.
aspect “food,” which indeed is related to the “price” aspect. (2)
Conventional methods fail to tackle the sentences with special
structures nicely. For example, “the food is not very good”. Here,
“not very good” should be considered as the whole phrase which
implies a negative sentiment, but existing attention mechanisms
fail to model such syntactic structures. (3) Existing models rely
heavily on the quality of instance representation. Conventional
methods mainly represent the context and aspect with a vector.
For example, some studies leverage the aspect term as a query
to employ the attention method with context to acquire the
representation vector [10], [11]. However, utilizing one vector
to represent the instance is limited, as the natural languages are
delicate and complex.
In this paper, we propose a knowledge guided capsule atten-
tion network (KGCapsAN) for A TSA. Our model is motivated
by the fact that prior knowledge, such as syntactic knowledge,
can help identify the syntactically related words to the aspect
and understand the special sentence structure. In Figure 1, we
observe that syntactical knowledge can help identify the senti-
ment related words towards the aspect. In KGCapsAN, we ﬁrst
propose a Bi-LSTM network to model the text. Moreover, we de-
velop a capsule attention network (CAN) to enhance the sentence
and aspect representation. Capsule Network was ﬁrst proposed
by [12] and good at modeling the part-whole relationships
between the lower-level capsules and higher-level capsules. Cap-
sNet transfers the information by utilizing the dynamic routing
mechanism, which updates the coupling coefﬁcients between
capsules in lower and upper layers through iterations. It aims
to extend the multi-hop attention mechanism [5] by controlling
the number of self-loops to achieve multi-step attention in the
single attention layer.
CAN draws on the idea of dynamic routing and treats output
(matrix) of the hidden layer obtained by Bi-LSTM as lower-level
capsules. The attention output of CAN is regarded as a higher-
layer capsule obtained through dynamic interactions with these
lower-layer capsules. CAN extend the conventional multi-hop
attention mechanisms by incorporating high-level information
(such as syntactic and sentence structures) as the attention query
to guide the attention process and improve the performance of
A TSA. Speciﬁcally, theﬁrst query aims to utilize the syntactic
knowledge. We ﬁrst feed each sentence to the syntactical de-
pendency tree to acquire the syntactic relationships. Then, we
build a small graph speciﬁc to the sentence, where each word is a
node and an edge between each node pair indicates the syntactic
relationship (such as 0/1 for indicating the existence of the
syntactic relationship). Afterwards, a graph-based convolutional
network (GCN) [13] is employed to learn the graph represen-
tation, which is an effective graph-based neural network that
captures the high order neighborhood information to achieve the
graph representation so as to capture the syntactically relevant
words. The second query is designed to capture the special
sentiment carrying phrases (also known as n-grams in natural
language processing), say, “not bad” or “should be”. To this
end, we develop a CNN-based local n-gram layer, which can
utilize the informative words (1-gram) or phrases (n-gram) as the
second query to guide the attention mechanism. Finally, we pro-
pose a CapsAttention network, which simulates the information
transfer of dynamic routing by designing multiple knowledge
guided attention mechanisms.
The main contributions of this work can be summarized as
follows:
r We propose KGCapsAN, 2 a novel framework for A TSA,
which simulates the capsule network by utilizing the atten-
tion mechanism. KGCapsAN makes use of multi-queries to
guide the attention process, and provides the output capsule
more information which effectively improves the sentiment
classiﬁcation.
r We propose a multi-knowledge guided capsule attention
network, which can dynamically adjust the information
transfer of different prior knowledge.
r We collect a unique dataset (SpA TSA) for special sentence
structures, such as the conditional statement and subjunc-
tive A TSA and release it at http://dwz1.cc/5AWF8Mg.
r To evaluate the effectiveness of our approach, we conduct
extensive experiments on ﬁve widely used datasets. The
experimental results show that our proposed CAN model
can make better use of syntactic information to enhance
text representation. This hence allows the model to adapt
to a more complex sentence structure for A TSA. The results
also demonstrate that our model achieves the state-of-the-
art results.
II. R ELA TEDWORK
A. Aspect-Level Sentiment Analysis
Previous studies [14]–[16] on sentiment classiﬁcation have
achieved remarkable results at the sentence or document level.
2[Online]. Available: http://dwz1.cc/f1zndpU.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on January 31,2026 at 21:17:00 UTC from IEEE Xplore.  Restrictions apply.
2540 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 28, 2020
However, the methods only produce the sentiment classiﬁcation
on the whole text, which is aspect independent.
Recently, ABSA draws more attention and many methods are
developed, which can be classiﬁed into conventional machine
learning methods and the neural network based methods [17].
The traditional machine learning methods focus on extracting
a set of handcraft features like sentiment lexicons to train a
sentiment statistic-based classiﬁer [3]. However, such methods
rely heavily on hand-crafted features that are labor-intensive and
costly.
Driven by the remarkable progress of attention-based deep
neural networks, many studies sentiment classiﬁers of the type
have been developed. For example, Tang et al. [9] developed
a memory network to learn the weights of the context words
by utilizing the multi-hop attention mechanism and use the
weighted sums to compute the aspect-speciﬁc textual repre-
sentations. Tang et al. [6] proposed TD-LSTM to extend the
standard structure by using two separate LSTMs to model both
left context and right contexts of the target word, respectively. Li
et al. [18] utilized the hierarchical attention network to identify
the informative sentiment words towards the target to guide the
classiﬁer. Ma et al. [11] proposed IAN to learn the represen-
tations of the target and context with two attention networks
interactively.
B. Attention-Based Capsule Networks
Capsule network (CapsNet) was ﬁrst proposed by Hinton
et al. [19], which has introduced the concept of “capsules”
with transformation matrices to let networks learn part-whole
relationships automatically. Subsequently, Sabour et al. [12]
proposed a routing-based method for the capsule network. Each
capsule is an aggregation of neurons that represent various at-
tributes of a particular feature. These attributes represent differ-
ent instantiation parameters, such as relative position. Thus, the
capsule network has much stronger text representation capability
than conventional deep neural networks. Further studies [20],
[21] have extended the routing-based capsule network for the
natural language processing applications.
The dynamic routing method is similar to the multi-hop atten-
tion method since the lower-level capsules are aggregated to the
upper-level through self-iterative coupling coefﬁcient updating.
To enhance the operation speed and parallelism capability, some
studies extend the dynamic routing based capsule method by
using the attention mechanism. Zhou et al. [22] introduced a
capsule-based attention method for visual question answering
task and has achieved remarkable results. Capsule attention uti-
lizes the multi-hop attention mechanism and denotes the atten-
tion weight as the coupling coefﬁcient. Wanget al. [15] proposed
an RNN-based capsule network for sentence-level sentiment
analysis. Given a hidden vector encoded by a standard RNN as
the attention query, the capsule representation can be acquired
by the typical attention mechanism. In [23], an aspect-target
level capsule model was devised, which integrates the target
information into the single capsule cell and achieves signiﬁcant
progress. Y ang et al. [24] developed a query-guided capsule
network in order to integrate the capsule routing mechanism into
the multi-head attention structure for a signiﬁcant performance
improvement in terms of sentiment identiﬁcation accuracy.
C. Graph Neural Networks
Graph neural networks have received growing interest in NLP
tasks recently [25].
With the development of deep learning method, many re-
search studies have extended the deep neural network structure
that can be used for the arbitrarily structured graph. Among
them, Kipf and Welling [26] proposed a graph convolutional
network (GCN). This yields the remarkable results on many
benchmark datasets. Subsequently, many other studies extended
GCN to various tasks such as machine translation and text
classiﬁcation. Recent studies have explored graph neural net-
works for textual classiﬁcation. For example, a graph-CNN
method was proposed in [27] to convert text to graph structure
which can capture non-consecutive and long-distance semantics.
In [28], a heterogeneous graph was constructed by representing
documents and words as nodes and then uses the GCN for
classiﬁcation. This approach does not require inter-document
relationships, but it can achieve state-of-the-art results for textual
classiﬁcation.
D. Incorporating External Knowledge
Recently, many studies incorporated external resources (such
as logic reasoning rules, grammar knowledge and sentiment
lexicons, etc.) into deep learning framework to address A TSA
[29]–[32].
Among them, the method of fusing syntactical knowledge into
deep learning based methods has received extensive attention.
For example, Zhang et al. [33] proposed C-GCN that ﬁrst intro-
duced dependency trees knowledge into the neural network. The
main structure of C-GCN is a multi-layer GCN, where the input
is the embedding vectors of words in a sentence. For GCN layers,
the adjacency matrix is constructed based on the syntactical tree
structure. Upon the hidden representation from the GCN layers,
a softmax layer is appended to deliver the classiﬁcation result.
Subsequently, Zhanget al. [34] improve the C-GCN structure by
adding an LSTM layer in between the input embedding layer and
the ﬁrst GCN layer. As a result, the word sequential information
in the sentence can be exploited. Zuo et al. [35] focused on
optimizing the syntactic tree structure with heuristic rules, and
then employed the GCNs to solve the A TSA.
Another typical method is to incorporate the locality knowl-
edge. In [36], it has been proved that such knowledge is very
effective for ASTA. For example, Wei et al. [37] proposed to
extract the local structures, e.g., k-hop sub-tree, for sentimental
analysis. Hu et al. [38] proposed to integrate the locality infor-
mation with ﬁrst-order logic. With a carefully-designed deep
learning network, the logic rules can be effectively combined.
Zeng et al. [36] developed a locality weighting scheme to
leverage the word context around the aspects.
However, the above methods focus either on incorporating
syntactical knowledge or exploiting the locality knowledge.
None of the studies both simultaneously. In this paper, we pro-
pose a CAN method, which can ﬂexibly and effectively integrate
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on January 31,2026 at 21:17:00 UTC from IEEE Xplore.  Restrictions apply.
ZHANG et al.: KNOWLEDGE GUIDED CAPSULE A TTENTION NETWORK FOR ASPECT-BASED SENTIMENT ANALYSIS 2541
Fig. 2. The overall framework of knowledge guided capsule attention network (KGCapsAN) for aspect-level sentiment classiﬁcation.
TABLE I
NOT A TIONSUSED IN THIS PAPER
different types of prior knowledge with a carefully-designed
multi-knowledge guided capsule attention mechanism. Specif-
ically, we leverage the mechanism to exploit the syntactical,
locality and lexicon knowledge simultaneously.
III. KGC APSAN MODEL
KGCapsAN aims to address the deﬁciencies of the conven-
tional attention-based approach in A TSA. Among them, the
Capsule Attention Network (CAN) is a core component of KG-
CapsAN, which implements the dynamic routing process of the
CapsNet structure with a capsule-based attention mechanism.
Speciﬁcally, CAN uses syntactic knowledge and n-gram infor-
mation as the query to guide the attention, and then integrates
such knowledge with the representation vector to enhance the
capabilities of the representation.
KGCapsAN, depicted in Figure 2, consists of two compo-
nents, namely, a Bi-LSTM Network and a Capsule Attention
Network, to improve the performance of A TSA. We are going
to give the task deﬁnition and the overview of our model in
Section III.A and Section III.B, respectively. Then, we describe
the details of the Bi-LSTM network and CAN in Section III.C
and III.D, respectively. Finally, the training process is discussed
in Section III.E.
A. Problem Deﬁnition
The A TSA task can be formulated as follows. Given a sentence
x = {wc
1,...,w a
τ,...,w a
τ+m,...,w c
n} contains a correspond-
ing aspect-term wordswa
τ,...,w a
τ+m, wherew denotes the each
word in the sentence and m denotes the aspect term length.
Each sentence has a sentiment label y. A TSA aims to predict a
sentiment label for the input sentencex towards the given aspect
term. In this paper, we use superscripts “c,” “a” to indicate a
context word and aspect-term word, respectively. The notations
used in this paper are summarized in Table 1 for clarity.
B. Framework Overview
As shown in Figure 2, KGCapsAN consists of two main
components: Text Representation Layer and Capsule Attention
Network (CAN). The text representation layer employs a V anilla
Bi-LSTM structure trained with textual features. It contains an
embedding layer and a Bi-LSTM layer for capturing sequential
features of the text. CAN contains four layers. The ﬁrst layer is
the syntactic layer, which uses the syntactic graph constructed
through the use of the syntactical dependency trees to acquire
the syntactic query. The second layer is the local n-gram layer,
which uses CNN to capture informative n-gram features. The
third layer is the aspect query layer , which utilizes the as-
pect term to learn the aspect-speciﬁc information of the whole
sentence. In CAN, all three layers are denoted as the attention
query of the CapsAttention layer, which can effectively guide
the attention.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on January 31,2026 at 21:17:00 UTC from IEEE Xplore.  Restrictions apply.
2542 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 28, 2020
C. V anilla Bi-LSTM Network
Generally, V anilla Bi-LSTM networks are employed to en-
code the input sentence x. Bi-LSTM can capture the left and
right contexts of each word in the input. In particular, for thet-th
wordwt in the input sequence of the target, we ﬁrst convert the
t-th word into the word embedding layer E to acquire the word
embedding representation et. Here, the sentence representation
can be denoted as E(x). Then, we feed E(x) into Bi-LSTM to
compute its forward hidden state−→hp
t and backward hidden state←−hp
t :
[−→h 1,−→c 1]= −−−−→LSTM (e1,−→hP,−→c P )
[←−h 2,←−c 2]= ←−−−−LSTM (en,←−hP,←−c P ) (1)
where we concatenate both the forward and backward hidden
states to form the ﬁnal hidden state ht =[ −→ht ⊕←−ht] for the
word wt at the t-th position of the input target. h ∈Rd and
c ∈Rd denote the hidden state and state of the cell in LSTM,
respectively. The symbols −→ and ←− represent the forward or
backwards path directions. Thus, the ﬁnal representation of the
Bi-LSTM layer can be denoted as H = {h1,h 2,...,h n}.
Here, LSTM cell has three gates: an input gate it, a forget
gate ft, an output gate ot, and a memory cell ct. Formally, the
current hidden state ht in the LSTM networks are computed as
follows:
iT
t = σ(WT
i et +UT
i ht−1 +VT
i ct−1)
fT
t = σ(WT
f et +UT
f ht−1 +VT
f ct−1)
oT
t = σ(WT
o et +UT
o ht−1 +VT
o ct−1)
˜cT
t = tanh(WT
c pt +UT
c ht−1)
cT
t = fT
t ⊙cT
t−1 +iT
t ⊙ ˜cT
t
hT
t = oT
t ⊙tanh(cT
t ) (2)
whereWT
{i,f,o,c},UT
{i,f,o,c}, and VT
{i,f,o,c} are the set of all train-
able parameters to be learned, σdenotes the sigmiod function,
and⊙ represents the element-wise multiplication.
D. Capsule Attention Network
The traditional capsule network was proposed to capture the
part-whole relationships in the iterative routing procedure. The
capsules in the lower layer are transferred to the higher layer
by aggregating their transformations with iteratively updated
coupling coefﬁcients. Each capsule is an aggregation of neurons,
where each neuron indicates multiple attributes of the special
feature present in the text. These attributes can be kinds of
instantiation parameters, such as the syntactical relationship
between a word and its position in a sentence.
Nevertheless, there are two drawbacks of directly employing
such capsule network in A TSA. First, the capsule network cannot
focus on the aspect-speciﬁc words while inferring the sentiment.
Second, the original dynamic routing mechanism is independent
of the back propagation stage, which makes it time-consuming
and cannot be parallelized.
To alleviate the aforementioned issue, we propose CAN,
which makes use of the attention mechanism for realizing the
capsule structure. It is reasonable to utilize that capsule-based
structure to represent the sentence, since it can obtain more
information instead of using only one vector as for the con-
ventional attention-based methods. CAN is developed based on
two features: 1) The use of syntactic information can effectively
address the problem of incorrectly focusing on syntactically
unrelated words in a short or long range. 2) The enhancement
of the learning ability of n-gram can help the model to accu-
rately understand complex structures, such as “not bad” can be
considered as a whole.
Next, we will introduce each component respectively.
1) Syntactic Layer: The syntactic layer learns syntactically
relevant words towards the target aspect through the dependency
tree,3 which is wildly used in the NLP task and can effectively
identify the relationships between words. Given a sentence x,
we ﬁrst build the syntactic graph (S-Graph) to model the syntac-
tical relationships about this sentence. The S-graph utilizes the
words as nodes. It constructs the weighted edges based on the
syntactical relationships. We denote A as the adjacency matrix
of S-Graph.
After obtaining the hidden state of the sentence H ∈Rn×d,
we feed them into a two-layer GCN. The graph representation
ˆSl ∈Rn×k can be calculated as:
ˆSl = σ(Aσ(AHW0))Wl
1) (3)
where σrepresents a non-linear function, and W0 ∈Rd∗v and
Wl
1 ∈Rd∗k are trainable parameters. Note that, similar to CNN,
we usel different weightsWl
1 to capture multiple features. Here,
ˆSl denotesl-th graph representation.
Aspect-Speciﬁc Zero Masking: GCN draws syntactically
related words into an aspect term to achieve the syntactical
aspect-speciﬁc representation. In this layer, we introduce to use
the aspect-speciﬁc zero masking mechanism, which aims to
mask out the graph representation vectors of non-aspect term
words and keep merely high-level aspect-speciﬁc features.
Formally, given the l-th graph representation ˆSl =
{sl
1,...,s l
τ,...,s l
τ+m,...,s l
n}, the output of a mask can be
denoted as: ˆSl
mask = {0,...,sl
τ,...,sl
τ+m,..., 0}. After acquiring
all ˆSmask, we compute the weighted sum of all graph represen-
tations to achieve the output of the syntactic layer Smask.N o t e
that the dimension of Smask is the same as H.A ss h o w ni n
Figure 3, white blocks are employed to indicate the zero masked
hidden states.
2) Local n-Gram Layer: For A TSA, it is important for the
network to learn the sentiment-carry n-gram features, such as
“not bad”. Thus, we develop an n-gram layer to enhance the
learning ability of the n-gram features. The n-gram layer consists
of two convolutional layers to extract n-gram features of the input
sequence through the convolutional operations.
As the two convolutional layers share a similar structure, we
only give the details for one convolutional layer. LetW ∈Rk×d
be the convolutions ﬁlters, where k is the ﬁlter width. A ﬁlter
3We use spaCy toolkit: [Online]. Available: https://spacy.io/.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on January 31,2026 at 21:17:00 UTC from IEEE Xplore.  Restrictions apply.
ZHANG et al.: KNOWLEDGE GUIDED CAPSULE A TTENTION NETWORK FOR ASPECT-BASED SENTIMENT ANALYSIS 2543
Fig. 3. The structure of the syntactic layer.
of width k allows the convolutional layer to slide over the input
sequence and acquire new features. We denote zi as the new
feature obtained from a local window of the word sequence
ei:i+k−1, which can be computed as:
zi = σ(W ⊙ei:i+k−1 +b) (4)
where ⊙ denotes a convolutional operator, σis a non-linear
function and b is the trainable parameter. This convolution ﬁlter
is applied to every possible window of words in the input se-
quence{e1:k,e 2:k+1,...,e m−k+1:m} so as to produce a feature
map z ∈Rn−k+1 as exhibited below:
γ=[ z1,...,z m−k+1] (5)
Here, the ﬁlter weights and bias terms of each ﬁlter are shared
between all positions in the input, thereby preserving spatial
locality. Finally, we send γto the second convolutional layer
and we can obtain the convolutional representation Z.
We observe that, for A TSA, the sentiment-carry words to-
wards the aspect has the locality properties, where such words
appear in a small range around the aspect term. To obtain the
important local n-gram ﬁlters, we utilize the aspect-speciﬁc
zero masking to select the k-range words. This is because
the two-layer convolutional operations can express the n-gram
information over an area of size 2k into the representation vector
of the target aspect. The computational details are illustrated
in Figure 4. Finally, the masked hidden vectors are denoted as
Zmask.
3) Aspect Query Layer: This layer aims to learn the aspect-
speciﬁc queries for capsule attention. To better embed the
aspect-speciﬁc queries into the CapsAttention layer, we also
utilize the aspect-speciﬁc zero masking to ﬁt the dimension
size. Formally, we send H into the masking layer, and the
Fig. 4. The Structure of the local n-gram layer.
Fig. 5. Structure of the CapsAttention layer.
output of the aspect query layer can be represented as Hmask =
{0,...,h τ,...,h τ+m,..., 0}.
4) CapsAttention Layer: To achieve the dynamic routing
with the attention mechanism, we propose an iterative attention
algorithm, known as the CapsAttention layer. Figure 5 gives an
example. CapsAttention layer treats each vector in the hidden
state of text representation layer as input layer capsules, and the
attention output is the output capsule that contains information
related to the prediction. Here, the output capsule has of three
vectors which can be represented in the matrix form.
In the CapsAttention layer, we utilize three queries to guide
the attention in the iterative manner. Speciﬁcally, in the ﬁrst
iteration, we initialize the three queries V 1
s , V 1
z , V 1
h as Smask,
Zmask and,Hmask, respectively. Given the input capsulesH ∈
Rn×d, the coupling coefﬁcient matrix c can be computed as:
c1
s = V 1
s (H1
s )T
c1
z = V 1
z (H1
z )T
c1
h = V 1
h (H1
h)T (6)
where c{s,z,h} ∈Rn×n. In the ﬁrst iteration, input capsules are
the same for the three attention queries, where H = H1
s =
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on January 31,2026 at 21:17:00 UTC from IEEE Xplore.  Restrictions apply.
2544 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 28, 2020
Algorithm 1: Knowledge Guided Capsule Attention.
Input: H,Smask, Zmask, Hmask
Output: output capsule M
1: Initialize V{s,z,h}
2: fort in T iterations do
3: Obtain coupling coefﬁcients: ct−1
{s,z,h}
4: Update queries: Vt
{s,z,h}
5: Update input capsules: Ht
{s,z,h}
6: end for
7: Obtain the weighted sum feature: qt
{s,z,h}
8: Obtain the output capsules: M
9: returnM
H1
z = H1
h. The next layer V 2
{s,z,h} can be updated as:
V 2
s = c1
sH1
s
V 2
z = c1
zH1
z
V 2
h = c1
hH1
h, (7)
where the dimension of the new query V 2
{s,z,h} is the same as
that of the initial query V 1
{s,z,h}. The input capsules of the next
iteration can be updated by:
H2
s = λ LayerNorm (σ(V 2
s )) +H1
s
H2
z = λ LayerNorm (σ(V 2
z )) +H1
s
H2
h = λ LayerNorm (σ(V 2
h )) +H1
s, (8)
where LayerNorm performs the standard layer normaliza-
tion [39]. Note that, in CapsNet the “squash” activation function
is utilized [12]. In this paper, we adopt the widely used non-linear
sigmoid function, σ, instead of “squash”.
Aftert iterations, the output capsule M can be found as:
qs = Ht
s(softmax
∑
i
ct
s,i),
qz = Ht
z (softmax
∑
i
ct
z,i ),
qh = Ht
h(softmax
∑
i
ct
h,i),
M = {qs ⊕qz ⊕qh} (9)
wheresoftmax (fγ)= efγ
∑
δefδ ,⊕is the concatenation operator,
ci denotes the i-th vector of the coupling coefﬁcient matrix c,
andM ∈R1×3d.
Compared to the forward propagation algorithm of CapsNet,
we replace the dynamic routing by the attention-based mech-
anism. The updating strategy is kept such that the lower-level
capsules are transferred to the higher-level capsules by updating
the coupling coefﬁcients. The gradients of CAN can be com-
puted by the standard back-propagation algorithm. The detailed
process is presented in Algorithm 1.
E. Sentiment Classiﬁcation
After acquiring the representation M , it is fed into the feed-
forward layer and then the softmax layer to obtain the sentiment
probability distribution:
P = softmax (WM +b) (10)
whereW andb are trainable parameters.
Finally, the model parameters are trained to minimize the
following loss function:
Loss = −
N∑
logJβ+α||θ|| (11)
whereβis the label,N is the training size,Jβis theβ-th element
ofJ,θdenotes the trainable parameter matrix and αrepresents
the coefﬁcient of L2-regularization.
F . Two V ariants
It is worth pointing out that the proposed KGCapsAN is a very
general framework, which supports ﬂexible variants. For exam-
ple, in the text representation layer, we can replace the BiLSTM
with a pre-trained BERT. In this case, the model takes “[CLS]
+ sentence + [SEP] + aspect + [SEP]” as input. As a result,
KGCapsAN is able to exploit the extra knowledge delivered by
BERT. We refer to the variant as KGCapsAN-BERT. Also, the
CAN allows incorporating more prior knowledge. For exam-
ple, word emotional features from lexicon can be exploited as
the fourth query. There are many ways to obtain and encode the
emotional feature. Here we use the SenticNet [40] to obtain the
emotional-related terms for each word, and calculate the average
on their embeddings as query input. We name the variants as
KGCapsAN-LI.
IV . EXPERIMENTS
A. Datasets
To compare and evaluate the effectiveness of our proposed
method and the existing approaches, we have conducted exten-
sive experiments on ﬁve datasets.
r Twitter corpus . Twitter dataset is originally built via
Twitter4 [41]. Each sentence contains several aspect terms.
Each aspect term is assigned with a sentiment label drawn
from “positive,” “neutral,” or “negative”. Twitter corpus
includes 1561 positives, 3127 neutrals, and 1560 negatives
for training. The test data contains 692 tweets.
r Lap14 and Rest14. Lap14 and Rest14 datasets are taken
from SemEval-14 Task 4 in [42], respectively. Lap14 com-
prises of the laptop reviews, and it is composed of 2328
training samples of 3 sentiment classes (where there are
994 positives, 464 neutrals, and 870 negatives). Its test set
contains 638 samples. Rest14 composes of the restaurant
reviews, where there are 2164 positives, 637 neutrals, and
807 negatives. Its test set has 1120 samples.
4[Online]. Available: https://twitter.com/
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on January 31,2026 at 21:17:00 UTC from IEEE Xplore.  Restrictions apply.
ZHANG et al.: KNOWLEDGE GUIDED CAPSULE A TTENTION NETWORK FOR ASPECT-BASED SENTIMENT ANALYSIS 2545
TABLE II
STA TISTICS OF THE DA TASETS
r Rest15. Rest15 is collected from SemEval 2015 task 12
in [43], it contains in total 1204 training samples with three
sentiment classes and 542 samples for test.
r Rest16. Rest16 is taken from SemEval 2016 task 5 in [44].
The data set includes 1748 training samples of three
classes, 1240 positives, 69 neutrals, and 439 negatives. Its
test set has 616 samples.
r SpATSA. Many previous studies have revealed that ex-
isting methods fail to adequately predict the sentences
with special structures. To examine the performance in
such case, we construct an additional dataset by selecting
the sentence-aspect pairs from the above four datasets.
Here, each sentence contains the special sentence structure,
e.g., conditional statement and subjunctive A TSA, etc. For
example, “the staff should be a bit more friendly.” All the
sentences in the dataset contain special sentence structures,
and we refer to the data set SpA TSA. In SpA TSA, there are
4726 training samples with 1792 positive, 1559 neutral
and 1375 negative instances. The test set includes 1182
samples.
The statistical information of the ﬁve data sets is summarized
in Table II. As in [1], [9], [11], [34], [45], we discard the “conﬂict
sentences,” each of which has one aspect term labeled with
multiple sentiments.5
B. Ablation Study
C. Baselines and Experimental Setting
As for a comparison, we adopt thirteen sentiment classiﬁca-
tion methods as baselines, which can be categorized into three
groups: attention-based methods, capsule-based methods and
BERT-based methods.6
a) Attention-based methods:
r SVM [46]: SVM is an effective traditional mechine learn-
ing based method for sentiment analysis. Kiritchenko
et al. [46] devised SVM to solve SemEval 2014 Task 4.
r LSTM [6]: This method utilizes the standard LSTM to
model the sentiment representation. The last hidden state
is forwarded into the softmax layer to obtain the sentiment
5[Online]. Available: https://github.com/ganeshjawahar/mem_absa.
6[Online]. Available: https://github.com/songyouwei/ABSA-PyTorch.
probability. The method uses Adam Optimizer with a learn-
ing rate of 0.001. The dimension of embeddings is 300.
r IAN [11]: This method learns the representations of the
target and context with two LSTMs, and then utilizes the
interactive attention to model the relationship with respect
to each other. In the experiment, the hidden size is 300 and
learning rate is 0.001.
r MemNet [9]: MemNet utilizes the memory network,
which is widely used in aspect-level sentiment classiﬁca-
tion. This beneﬁts from the multi-hop attention mechanism
for sentiment inference. The hidden states dimension is
300, the hop for memory is 3, and the learning rate is 0.001.
r AOA [1]: The attention-over-attention (AOA) method
models the aspects and sentences jointly, and it explic-
itly captures the interaction between aspects and contexts.
Here, the hyper-parameters setting in our experiment are
the same with IAN.
r TNet-LF6 [45]: TNet-LF formulates a transformer-based
method, known as context preserving transformation, to
increase the informative element of contexts. For TNet-LF
the hop of the attention block is set to 2, the number of
convolutional ﬁlters are set to 50, and the learning rate is
0.001.
r ASGCN7 [34]: ASGCN utilizes the external dependency
tree to model long-range word dependencies, by making
use of GCN to model the dependency tree graph. ASGCN-
DT uses the directed graphs while ASGCN-DG employs
the unidirectional ones. In the expertment, we utilize spaCy
to access the dependency tree, learning rate is 0.001, di-
mension of hidden size is 300 and the number of GCN is
set to 2.
b) Capsule-based methods:
r TransCap8 [47]: TransCap makes use of a dynamic routing
based capsule network for aspect-based sentiment analysis.
The results on Rest14 and Lap14 are retrieved from [47].
For the reminder data sets, the parameter settings are as
follows. The routing iteration in our experiment is set to 3,
the learning rate is set to 0.01, and the slack parameter for
loss is set to 0.8.
r RNN-Cap9 [15]: RNN-Cap proposes the attention-based
capsule structure for aspect-based sentiment analysis,
where the hidden states of RNN denote as the lower cap-
sules. Note that the original RNN-Cap was developed for
sentence-level sentiment analysis, followed by [23], we
replace the capsule query by the embedding of aspect term.
The capsule block is set to 2, and learning rate is 0.01, the
hidden dimension is 512 with the weight decay rate as
0.0001.
c) BERT-based methods:
r BERT [48]: This method ﬁne-tunes from a pre-trained
BERT model to perform A TSA. Following [49], we convert
the given context and target to “[CLS] + sentence + [SEP]
7[Online]. Available: https://github.com/GeneZC/ASGCN.
8[Online]. Available: https://github.com/NLPWM-WHU/TransCap
9[Online]. Available: http://www.wangyequan.com/publications/
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on January 31,2026 at 21:17:00 UTC from IEEE Xplore.  Restrictions apply.
2546 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 28, 2020
TABLE III
EVA L U AT I O NRESULTS ON ALL DA TASETS.T HE BEST RESULT ON EACH DATA S E TISI N BOLD. ∗MARKS RESULTS REPORTED IN THE ORIGINAL PAPERS, † MARKS
RESULTS PRODUCED BY USING THE OPEN IMPLEMENTA TION AND THE NUMBER IN PARENTHESIS REPRESENTS THE VARIANCE
+ aspect+ [SEP]” structure. The learning rate is 2e-5, the
dropout rate is 0.1 and the dimension of BERT is 768.
r BERT-PT [50]: The BERT-PT method proposes a novel
post-training strategy for the basic BERT model, which can
effectively increase the performance for A TSA task. The
input structure is the same with “[CLS] + sentence + [SEP]
+ aspect+ [SEP],” the learning rate is 3e-5, and max length
of the post-training is set to 320, the dimension of BERT
is 768.
r LCF-BERT6 [36]: LCF-BERT is a method based on BERT
ﬁne-tuning. It contains a local context focus (LCF) mech-
anism method to learn local features from contexts. The
hyper-parameter setting is the same as BERT. Here, for
LCF, the input structure is “[CLS] + sentence + [SEP]”.
r AEN-BERT6 [49]: This method incorporates the atten-
tional encoder network into the BERT framework, which
can help the BERT-based method to learn target-speciﬁc
words. The hyper-parameter setting is same as BERT.
In all the experiments, the parameters initialized with Xavier
uniform [51], we utilize 300-dimensional pre-trained GloV e
vectors to initialize the word embeddings and the the optimizer
we use is Adam. Other weight parameters are initialized by
randomly sampling the values from the uniform distribution
U(−0.01, 0.01). The dimensions of the hidden state of Bi-LSTM
is 300, and the kernel size for the Local n-gram layer is 3. The
scale weight λ is set with 0.1, 0.01 and 0.001,αis set to 0.00001.
The model is optimized with the Adam optimization algorithm
with the batch size of 32 and the learning rate is 0.001.
As in [34], we use accuracy and Macro-Averaged F1 as the
evaluation metrics. We compute the metrics independently for
each class and then take the average (hence treating all classes
equally), as the ﬁnal performance.
D. Experimental Results
In the experiments, we apply sentiment classiﬁcation accuracy
and F1 score as evaluation metrics to evaluate our method. To
evaluate the stability of the model, following [34], we run the
method three times and reported the mean accuracy and standard
deviation in Table III. We also utilize the Friedman test to verify
TABLE IV
FRIEDMAN’S ANOV A TABLE
the signiﬁcance of differences between and other approaches
with p-value of 0.05.
From the results, we can observe that LSTM has the worst
performance because LSTM does not exploit the aspect infor-
mation for sentiment prediction. The neural networks with the
attention mechanism (e.g., MemNet, AOA, IAN, and TNET-LF)
signiﬁcantly perform better than the standard LSTM, since such
methods explicitly encode the target information. For example,
with the use of the Rest14 dataset, these methods improve 2.17%,
2.95%, 2.62%, and 3.56% in F1, respectively. This demonstrates
that the attention mechanism can help the model to capture the
aspect-speciﬁc information. ASGCN, which is the graph-based
neural network, outperforms all other methods in three out of
ﬁve datasets. For example, compared with the best competitor,
ASGCN improves 2.42% on Rest15 and 0.91% on Lap14 for
F1 score, respectively. This is because ASGCN utilizes the
syntactical dependency tree to enhance the learning ability of
the long-range word dependencies.
Our KGCapsAN achieves the best results among all ﬁve
widely used datasets. For instance, our proposed method outper-
forms the best existing method under study by 3.71% on Rest15,
1.97% on Rest14, and 1.81% on Lap14 in terms of the F1 score.
As we all know, it is difﬁcult to improve 1% of the F1 score on the
A TSA task [52]. This thus demonstrates the effectiveness of the
proposed model. Also, we have carried out the KGCapsAN-LI
model, which utilizes the emotion-related lexicon information
to construct an additional query (as introduced in section III F).
We observe that KGCapsAN-LI indeed improves KGCapsAN,
which validates the effectiveness and ﬂexibility of the proposed
method to incorporate other prior knowledge.
It is also interesting to compare our model with the existing
methods understudy that utilize the capsule network as the
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on January 31,2026 at 21:17:00 UTC from IEEE Xplore.  Restrictions apply.
ZHANG et al.: KNOWLEDGE GUIDED CAPSULE A TTENTION NETWORK FOR ASPECT-BASED SENTIMENT ANALYSIS 2547
TABLE V
EVA L U AT I O NRESULTS FOR BERT-BASED METHODS.T HE BEST RESULTS ARE IN BOLD,† MARKS RESULTS PRODUCED BY USING THE OPEN IMPLEMENTA TION
base sentiment classiﬁer, such as TransCap and RNN-CAP .
Compared with most methods under study, TransCap can still
yield comparable results. This is because the dynamic routing
method can capture the part-whole relationship and the output
capsules contain the supplementary information than only one
vector (non-capsule based networks utilized). Our model consis-
tently and substantially outperforms RNN-CAP and TransCap
in all tasks. This performance improvement can be explained
with the following two reasons. First, our method utilizes prior
knowledge as the queries to guide the capsule attention network.
Second, the output capsules contain supplementary information
that contributes to infer the sentiment.
As introduced in Section III F, the proposed method is
also able to incorporate the external knowledge conveyed in
BERT. Hence, in Table V we report and compare the results
of our method and relevant BERT models. We can see that
the proposed KGCapsAN-BERT in general outperforms other
models. The results demonstrate the effectiveness of our variant
model to incorporate BERT knowledge, and also validate our
KGCapsAN framework can effectively combine the syntactic
and local-n-gram prior knowledge for ASTA, due to its unique
multi-knowledge guided capsule attention mechanism.
We have also performed a Friedman’s test on ﬁve widely used
datasets for testing the statistical signiﬁcance of the performance
superiority of the proposed method IV. Test results show that the
proposed method is signiﬁcantly better than the baselines at p
value< 0.05.
d) Experimental Analysis on SpATSA: To evaluate the per-
formance of the proposed method for solving the sentence
with the special sentence structures (where the conventional
methods fail to predict), we select several strong baselines,
say, attention-based methods (MemNet, AOA) and syntactic
enhance method (ASGCN), and we run several experiments on
SpA TSA. The results are summarized in Table III. From the
results, our method achieves the state-of-the-art performance
on the SpA TSA dataset. Speciﬁcally, our method signiﬁcantly
outperforms other methods with attention-based structure. For
example, our method improves 8.86% and 8.8% on MemNet for
accuracy and F1 score, respectively. ASCGCN also performs
better than that of MemNet. This is because the conventional
method may mistakenly take some words into an aspect, but the
use of syntactic knowledge can effectively help understand the
sentence, especially for special sentence structure.
Compared with ASGCN, our method still yields an improve-
ment. For instance, our method improves 2.62% (accuracy) and
2.72% (F1) for SpA TSA. This shows that our methods can better
utilize the syntactic information (both syntactic and locality
knowledge). Besides, our Capsule Attention Method can provide
additional knowledge for text representation which helps for
signiﬁcant performance improvement.
E. Ablation Study
In order to study the inﬂuence of each component of our
model, we implement the ablation test of KGCapsAN in terms of
removing the Capsule Attention network (denoted as w/o Caps),
the structure can be regarded as the conventional attention-based
method, Syntactic layer (denoted as w/o Syntactic), the local
n-gram layer (denoted as w/o Local), the aspect query layer
(denoted as w/o Aspect), and the aspect-speciﬁc zero masking
layer (denoted as w/o Mask).
We construct the network of w/o Caps by utilizing the Bi-
LSTM and the standard attention mechanism [53], in which
the aspect term denotes the attention query. The model without
CapsAttention layer is similar to KGCapsAN with just one
iteration by only utilizing the aspect query layer. The results
are summarized in Table VI. From the results, we can see that
all the proposed parts provide a noticeable improvement to KG-
CapsAN. In particular, we can ﬁnd that the CAN has the largest
impact on the performance of KGCapsAN. The classiﬁcation
accuracy drops sharply when discarding CAN. This is within
our expectation since the capsule attention network is able to
capture the part-whole relationship and the output capsule can
utilize additional information.
Under the CAN structure, we can observe that all three query
layers contribute a lot to the performance. This is because
such layers provide the KGCapsAN with multiple high-level
knowledge. The syntactic layer can help capture the syntacti-
cal relationships between words, especially for the long-range
words. The local n-gram layer enhances the learning ability of
the local n-gram by using the locality of the aspect. The aspect
query layer establishes the correlation between the aspect and
sentiment-carry words. The mask strategy is a unique mecha-
nism in our method, which deployed in three query layers. We
observe from Table VI that the mask mechanism has a signiﬁcant
impact on the performance. To illustrate why the mechanism is
effective, we depict in Figure 6 the attention weights learned by
our model on an example sentence with/without the mechanism.
We can see that without the mask the model tends to pay mistaken
attentions to the non-local sentimental words, e.g., “fresh juice
concoctions”.
F . Case Study
To better understand how our method works, we perform
the case study with four examples. Speciﬁcally, we visualize
the attention weights offered by the strong baseline methods
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on January 31,2026 at 21:17:00 UTC from IEEE Xplore.  Restrictions apply.
2548 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 28, 2020
TABLE VI
EVA L U AT I O NRESULTS OF ABLA TIONSTUDY ON ALL DA TASETS
TABLE VII
CASE STUDY FOR ATTENTION SCORES OF MEMNET, IAN, ASGCN AND KGCAPSAN. THE
√INDICA TES A CORRECT PREDICTION WHILE ×
INDICA TES AN INCORRECT PREDICTION
Fig. 6. A comparison example of attention weights learned with/without mask
mechanism. The target of the sentence is “food,” and the sentiment is neutral.
(MemNet, IAN, ASGCN) and our model KGCapsAN in Ta-
ble VII. We also give prediction and the ground truth labels for
such examples. The ﬁrst sample “but, the ﬁlet mignon was not
very good at all cocktail hour includes free appetizers -LRB- nice
non-sushi selection -RRB-.,” with the aspect term “ﬁlet mignon”.
This sentence contains the special structure “not very good”.
It is difﬁcult to model such a structure using the conventional
methods. For example, MemNet fails to focus on all informative
words for the whole sentiment and it was not able to learn
aspect-acrry words. IAN tends to focus on “very good,” which
may lead to incorrect prediction. ASGCN also fails to make such
prediction, as it has a more attention to “good at”. However, the
KGCapsAN method can effectively predict this sentence. This
may contribute by the local n-gram layer, which enables the
model to consider “not very good” for the whole phase.
The second one “The Sashimi portion are big enough to
appease most people, but I didn’t like the fact they used arti-
ﬁcial lobster meat.,” has multiple aspect terms with different
sentiment labels inside the sentence during testing (e.g., sashimi
portion and artiﬁcial lobster meat). Such the case may lead
the conventional attention-based models to align the aspects
with their relevant descriptive words incorrectly. For example,
given the aspect “sashimi portion,” the existing methods tend
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on January 31,2026 at 21:17:00 UTC from IEEE Xplore.  Restrictions apply.
ZHANG et al.: KNOWLEDGE GUIDED CAPSULE A TTENTION NETWORK FOR ASPECT-BASED SENTIMENT ANALYSIS 2549
Fig. 7. Results of varying iteration numbers.
to focus on the words which in fact describe “artiﬁcial lobster
meat ”. KGCapsAN successfully predicts the instance, because
the aspect-speciﬁc zero masking mechanism can help focus on
locality words (multi-words) towards the aspect.
For the third one: “We recently spent New Y ear’s Eve at the
restaurant, and had a great experience, from the wine to the
dessert menu.,” there is a long-range words distance between
the sentiment-carry words and the aspect, which makes the
model hard to detect implicit semantics. For example, MemNet
and ASGCN focus on the un-related parts towards the aspect
“dessert menu”. IAN can correctly make the prediction for this
example. This may be due to the enhanced relationship between
words and aspect term by the interactive attention mechanism.
KGCapsAN can handle such a sample, as the uses of our capsule
attention network and multiple information as queries can guide
the attention process and retain more information in the output
layer, making classiﬁcation more accurate.
Finally, we give a neutral example: “the power plug has to
be connected to the power adaptor to charge the battery but
won’t stay connected.” with the target “power adaptor”. Such
a sentence pattern contains a sentiment-carry clause, but does
not directly describe the target. MemNet and IAN focus on the
long-range sentiment carry words “but,” “won’t “ towards the
aspect “price tag,” which results in wrong predictions. ASGCN
also fails to make the correct prediction. This may due to the
syntactical dependency tree can not handle complex sentence
structure nicely, such as “but”-clause. KGCapsAN avoids the
excessive focus on emotional words and gives a successful
prediction. This beneﬁts from our CAN structure, which allows
the model to understand the sentence from different perspectives,
instead of the current attention mechanism utilized in existing
methods.
G. Number of Routing Iterations
The number of routing iterations is an important hyper-
parameter of capsule-based structure, since it helps to model the
part-whole representation. Previous studies show that multiple
iterations can lead to better results. Here we would like to
investigate its impact on the proposed KGCapsAN. Speciﬁcally,
we report its performance on all the data sets by increasing the
number of routing iterations from one to ten. Here we would
like to study its impact on the proposed KGCapsAN. Speciﬁ-
cally, we give the performance for all datasets by running the
experiments with the number of routing interactions from one to
ten. Classiﬁcation accuracy and F1 scores are the average value
over 3 runs with random initialization. Figure 7 shows the results.
We observe that KGCapsAN can obtain the best performance
with the number of iterations within three iterations. After six
iterations, the performance tends to decline steadily. The reason
may be because our method utilizes the prior knowledge to
guide the attentions. As a result, the proposed method can focus
accurately on distinct features rapidly, and thus it needs very few
iterations to deliver the best performance.
H. Error Analysis
To investigate the limitation of the proposed methods, we
further analyze the prediction errors by our method. Speciﬁcally,
we analyze the error samples when the proposed method runs
over the Rest14 dataset. There are several reasons for explaining
the ﬁndings. First, our method fails to understand some sentences
that the sentiment hides in the latent opinions. For example, the
correct label for the sentence “you can eat gourmet food at a fast
food price.” should be “positive”. Our method tends to classify
the sentence as “neutral,” since the model was unable to explore
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on January 31,2026 at 21:17:00 UTC from IEEE Xplore.  Restrictions apply.
2550 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 28, 2020
the internal relationship between “fast food price” and “gourmet
food. Second, our method fails to classify some sentences that
require a deep comprehension such as “unfortunately, unless
you live in the neighborhood, it’s not in a convenient location
but is more like a hidden treasure.” The attention weights tend
to assign to the negative words such as “unfortunately,” “not
convenient”. This is because of the proposed method unable to
understand the special sentence structure, e.g., “but”-clause.
V. C ONCLUSION
In this paper, we propose a knowledge guided capsule network
(KGCapsAN) for aspect-level sentiment analysis. Our method
consists of two parts, a text representation layer and a capsule
attention network (CAN). CAN implements the routing method
by attention mechanism, in which multi-prior knowledge is
utilized to guide the capsule attention process. Among CAN,
we ﬁrst propose a GCN-based syntactic layer to integrate the
syntactic knowledge acquiring from the syntactic dependency
tree. Additionally, we propose a local n-gram layer to enhance
the ability of the model which can effectively focus on the
informative n-gram features. The experimental results demon-
strated that the KGCapsAN model signiﬁcantly outperformed
the state-of-the-art methods for aspect-based sentiment analysis.
In addition, ablation study, qualitative analysis, and case visu-
alization are provided to further demonstrate the effectiveness
of the proposed model. In the future, we prepare to combine
the sentiment resources (e.g., sentiment lexicon, emotional tags)
into the KGCapsAN, which can supply further comprehensive
knowledge for sentiment classiﬁcation.
REFERENCES
[1] B. Huang, Y . Ou, and K. M. Carley, “Aspect level sentiment classiﬁca-
tion with attention-over-attention neural networks,” in Proc. Int. Conf.
Soc. Comput., Behavioral-Cultural Model. Predict. Behavior . Represent.
Model. Simul., 2018, pp. 197–206.
[2] B. Pang et al. , “Opinion mining and sentiment analysis,” F ound. Trends
Inf. Retr ., vol. 2, no. 1/2, pp. 1–135, 2008.
[3] L. Jiang, M. Y u, M. Zhou, X. Liu, and T. Zhao, “Target-dependent
twitter sentiment classiﬁcation,” inProc. 49th Annu. Meet. Assoc. Comput.
Linguist. Human Language Technol.-V olume 1. 2011, pp. 151–160.
[4] S. Poria, E. Cambria, D. Hazarika, and P . Vij, “A deeper look into sarcastic
tweets using deep convolutional neural networks,” in Proc. 26th Int. Conf.
Comput. Linguist.: Tech. Papers, 2016, pp. 1601–1612.
[5] P . Chen, Z. Sun, L. Bing, and W. Y ang, “Recurrent attention network on
memory for aspect sentiment analysis,” in Proc. Conf. Empirical Methods
Natural Language Process., 2017, pp. 452–461.
[6] D. Tang, B. Qin, X. Feng, and T. Liu, “Effective LSTMs for target-
dependent sentiment classiﬁcation,” in Proc. 26th Int. Conf. Comput.
Linguist. Tech. Papers, 2016, pp. 3298–3307.
[7] Y . Wang et al. , “Attention-based LSTM for aspect-level sentiment clas-
siﬁcation,” in Proc. Conf. Empirical Methods in Natural Lang. Process. ,
2016, pp. 606–615.
[8] T. H. Nguyen and K. Shirai, “PhraseRNN: Phrase recursive neural network
for aspect-based sentiment analysis,” in Proc. Conf. Empirical Methods
Natural Lang. Process., 2015, pp. 2509–2514.
[9] D. Tang, B. Qin, and T. Liu, “Aspect level sentiment classiﬁcation with
deep memory network,” in Proc. Conf. Empirical Methods Natural Lang.
Process., 2016, pp. 214–224.
[10] J. Liu and Y . Zhang, “Attention modeling for targeted sentiment,” in Proc.
15th Conf. Eur . Chapter Assoc. for Comput. Linguist. V olume 2, Short
Papers, 2017, pp. 572–577.
[11] D. Ma, S. Li, X. Zhang, and H. Wang, “Interactive attention networks for
aspect-level sentiment classiﬁcation,” in Proc. 26th Int. Joint Conf. Artif.
Intell., 2017, pp. 4068–4074.
[12] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between cap-
sules,” in Proc. Adv. Neural Inform. Process. Syst., 2017, pp. 3859–3869.
[13] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph
convolutional networks,” 2016, arXiv:1609.02907.
[14] J. Wang, L. Y u, K. R. Lai, and X. Zhang, “Tree-structured regional CNN-
LSTM model for dimensional sentiment analysis,” IEEE ACM Trans.
Audio, Speech, Lang. Process., vol. 28, pp. 581–591, 2020.
[15] Y . Wang, A. Sun, J. Han, Y . Liu, and X. Zhu, “Sentiment analysis
by capsules,” in Proc. World Wide Web Conf. World Wide Web .I n -
ternational World Wide Web Conferences Steering Committee, 2018,
pp. 1165–1174.
[16] H. Xu, B. Liu, L. Shu, and S. Y . Philip, “Double embeddings and CNN-
based sequence labeling for aspect extraction,” in Proc. 56th Annu. Meet.
Assoc. Comput. Linguist. (V olume 2: Short Papers), 2018, pp. 592–598.
[17] Y . Song, J. Wang, T. Jiang, Z. Liu, and Y . Rao, “Attentional encoder
network for targeted sentiment classiﬁcation,” 2019, arXiv:1902.09314.
[18] L. Li, Y . Liu, and A. Zhou, “Hierarchical attention based position-aware
network for aspect-level sentiment analysis,” inProc. 22nd Conf. Comput.
Natural Lang. Learn., 2018, pp. 181–189.
[19] G. E. Hinton, A. Krizhevsky, and S. D. Wang, “Transforming auto-
encoders,” in Proc. Int. Conf. Artif. Neural Netw., 2011, pp. 44–51.
[20] M. Y ang, W. Zhao, L. Chen, Q. Qu, Z. Zhao, and Y . Shen, “Investigating the
transferring capability of capsule networks for text classiﬁcation,” Neural
Netw., vol. 118, pp. 247–261, 2019.
[21] B. Zhang, X. Xu, M. Y ang, X. Chen, and Y . Y e, “Cross-domain sentiment
classiﬁcation by capsule network with semantic rules,”IEEE Access,v o l .6 ,
pp. 58 284–58 294, 2018.
[22] Y . Zhou, R. Ji, J. Su, X. Sun, and W. Chen, “Dynamic capsule attention
for visual question answering,” 2019.
[23] Y . Wang, A. Sun, M. Huang, and X. Zhu, “Aspect-level sentiment analysis
using AS-capsules,” inProc. World Wide Web Conf., 2019, pp. 2033–2044.
[24] Z. Y ang, J. Zhang, F. Meng, S. Gu, Y . Feng, and J. Zhou, “En-
hancing context modeling with a query-guided capsule network for
document-level translation,” in Proc. Conf. Empirical Methods Natu-
ral Lang. Process. 9th Int. Joint Conf. Natural Lang. Process. , 2019,
pp. 1527–1537.
[25] H. Cai, V . W. Zheng, and K. C. Chang, “A comprehensive survey of graph
embedding: Problems, techniques, and applications,” IEEE Trans. Knowl.
Data Eng., vol. 30, no. 9, pp. 1616–1637, Sep. 2018.
[26] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph
convolutional networks,” in Proc. 5th Int. Conf. Learn. Representations ,
Toulon, France, Apr. 24-26, 2017.
[27] H. Peng, J. Li, Y . He, Y . Liu, M. Bao, L. Wang, Y . Song, and Q. Y ang,
“Large-scale hierarchical text classiﬁcation with recursively regularized
deep graph-CNN,” in Proc. World Wide Web Conf. International World
Wide Web Conferences Steering Committee, 2018, pp. 1063–1072.
[28] L. Y ao, C. Mao, and Y . Luo, “Graph convolutional networks for text
classiﬁcation,” in Proc. 33rd AAAI Conf. Artif. Intell., 31st Innovative
Appl. Artif. Intell. Conf., 9th AAAI Symp. Educational Advances Artiﬁcial
Intell., Honolulu, Hawaii, USA, Jan. 27–Feb. 1, 2019, pp. 7370–7377.
[29] B. Zhang, X. Xu, M. Y ang, X. Chen, and Y . Y e, “Cross-domain sentiment
classiﬁcation by capsule network with semantic rules,”IEEE Access,v o l .6 ,
pp. 58 284–58 294, 2018.
[30] M. Dragoni and G. Petrucci, “A fuzzy-based strategy for multi-
domain sentiment analysis,” Int. J. Approx. Reason. , vol. 93, pp. 59–73,
2018.
[31] J. Zhang, P . Lertvittayakumjorn, and Y . Guo, “Integrating semantic knowl-
edge to tackle zero-shot text classiﬁcation,” in Proc. Conf. North Amer .
Chapter Assoc. Comput. Linguist. Human Lang. Technol., V olume 1 (Long
and Short Papers), 2019, pp. 1031–1040.
[32] Z. Hu, X. Ma, Z. Liu, E. Hovy, and E. Xing, “Harnessing deep neural
networks with logic rules,” in Proc. 54th Annu. Meet. Assoc. Comput.
Linguist. (V olume 1: Long Papers), 2016, pp. 2410–2420.
[33] Y . Zhang, P . Qi, and C. D. Manning, “Graph convolution over pruned
dependency trees improves relation extraction,” in Proc. Conf. Empir .
Methods Nat. Lang. Process., 2018, pp. 2205–2215.
[34] C. Zhang, Q. Li, and D. Song, “Aspect-based sentiment classiﬁcation with
aspect-speciﬁc graph convolutional networks,” in Proc. Conf. Empirical
Methods Natural Lang. Process. 9th Int. Joint Conf. Natural Lang. Pro-
cess., 2019, pp. 4560–4570.
[35] E. Zuo, H. Zhao, B. Chen, and Q. Chen, “Context-speciﬁc heterogeneous
graph convolutional network for implicit sentiment analysis,” IEEE Ac-
cess, vol. 8, pp. 37 967–37 975, 2020.
[36] B. Zeng, H. Y ang, R. Xu, W. Zhou, and X. Han, “LCF: A local context
focus mechanism for aspect-based sentiment classiﬁcation,” Appl. Sci. ,
vol. 9, p. 3389, 08 2019.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on January 31,2026 at 21:17:00 UTC from IEEE Xplore.  Restrictions apply.
ZHANG et al.: KNOWLEDGE GUIDED CAPSULE A TTENTION NETWORK FOR ASPECT-BASED SENTIMENT ANALYSIS 2551
[37] W. Wei and J. A. Gulla, “Enhancing the HL-SOT approach to sentiment
analysis via a localized feature selection framework,” inProc. 5th Int. Joint
Conf. Natural Lang. Process., 2011, pp. 327–335.
[38] Z. Hu, Z. Y ang, R. Salakhutdinov, and E. Xing, “Deep neural networks with
massive learned knowledge,” in Proc. Conf. Empirical Methods Natural
Lang. Process., 2016, pp. 1670–1679.
[39] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” Stat,
vol. 1050, p. 21, 2016.
[40] E. Cambria, S. Poria, D. Hazarika, and K. Kwok, “Senticnet 5: Discov-
ering conceptual primitives for sentiment analysis by means of context
embeddings,” in Proc. 32nd AAAI Conf. Artif. Intell. , 2018.
[41] L. Dong, F. Wei, C. Tan, D. Tang, M. Zhou, and K. Xu, “Adaptive recursive
neural network for target-dependent Twitter sentiment classiﬁcation,” in
Proc. 52nd Ann. Meeting Assoc. Comput. Linguist. (volume 2: Short
papers), 2014, pp. 49–54.
[42] M. Pontiki, D. Galanis, J. Pavlopoulos, H. Papageorgiou, I. Androut-
sopoulos, and S. Manandhar, “SemEval-2014 task 4: Aspect based sen-
timent analysis,” in Proc. 8th Int. Workshop Semantic Evaluation (Se-
mEval 2014). Dublin, Ireland, Aug. 2014, pp. 27–35. [Online]. Available:
https://www.aclweb.org/anthology/S14-2004
[43] M. Pontiki, D. Galanis, H. Papageorgiou, S. Manandhar, and I.
Androutsopoulos, “Semeval-2015 task 12: Aspect based sentiment anal-
ysis,” in Proc. 9th Int. Workshop Semant. Eval. (SemEval 2015) , 2015,
pp. 486–495.
[44] M. Pontiki et al. , “Semeval-2016 task 5: Aspect based sentiment analy-
sis,” in Proc. 10th Int. Workshop Semantic Eval. (SemEval-2016) , 2016,
pp. 19–30.
[45] X. Li, L. Bing, W. Lam, and B. Shi, “Transformation networks for
target-oriented sentiment classiﬁcation,” in Proc. 56th Annu. Meet. Assoc.
Comput. Linguist. (V olume 1: Long Papers), 2018, pp. 946–956.
[46] S. Kiritchenko, X. Zhu, C. Cherry, and S. Mohammad, “Nrc-canada-2014:
Detecting aspects and sentiment in customer reviews,” in Proc. 8th Int.
Workshop Semantic Evaluation (SemEval 2014) , 2014, pp. 437–442.
[47] Z. Chen and T. Qian, “Transfer capsule network for aspect level sentiment
classiﬁcation,” in Proc. 57th Annu. Meet. Assoc. Comput. Linguist. , 2019,
pp. 547–556.
[48] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
of deep bidirectional transformers for language understanding,” in Proc.
Conf. North Amer . Chapter Assoc. Comput. Linguist. Human Lang. Tech-
nol., NAACL-HLT 2019, Minneapolis, MN, USA, Jun. 2-7, 2019, V olume 1
(Long and Short Papers) , J. Burstein, C. Doran, and T. Solorio, Eds.
Association for Computational Linguistics, 2019, pp. 4171–4186.
[49] Y . Song, J. Wang, T. Jiang, Z. Liu, and Y . Rao, “Attentional encoder
network for targeted sentiment classiﬁcation,”CoRR, vol. abs/1902.09314,
2019. [Online]. Available: http://arxiv.org/abs/1902.09314
[50] H. Xu, B. Liu, L. Shu, and P . S. Y u, “BERT post-training for review reading
comprehension and aspect-based sentiment analysis,” inProc. Conf. North
Amer . Chapter Assoc. Comput. Linguist. Human Lang. Technol., NAACL-
HLT 2019, Minneapolis, MN, USA, Jun. 2-7, 2019, V olume 1 (Long and
Short Papers), J. Burstein, C. Doran, and T. Solorio, Eds. Association for
Computational Linguistics, 2019, pp. 2324–2335.
[51] X. Glorot and Y . Bengio, “Understanding the difﬁculty of training deep
feedforward neural networks,” in Proc. Thirteenth Int. Conf. Artif. Intell.
Statist., AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-
15, 2010 , ser. JMLR Proceedings, Y . W. Teh and D. M. Tittering-
ton, Eds., vol. 9. JMLR.org, 2010, pp. 249–256. [Online]. Available:
http://proceedings.mlr.press/v9/glorot10a.html
[52] M. Y ang, Q. Jiang, Y . Shen, Q. Wu, Z. Zhao, and W. Zhou, “Hierarchical
human-like strategy for aspect-level sentiment classiﬁcation with senti-
ment linguistic knowledge and reinforcement learning,” Neural Netw. ,
vol. 117, pp. 240–248, 2019.
[53] D. Tang, B. Qin, X. Feng, and T. Liu, “Target-dependent sentiment
classiﬁcation with long short term memory,” CoRR, vol. abs/1512.01100,
2015. [Online]. Available: http://arxiv.org/abs/1512.01100
Bowen Zhang received the B.Sc. and M.Sc. degrees
in computer science and application from the Macau
University of Science and Technology, Taipa, China.
He is currently working toward the doctorate degree
with the the Harbin Institute of Technology, Harbin,
China. His research interests are in natural language
processing, big data analysis, and data mining.
Xutao Li received the bachelor’s degree from the
Lanzhou University of Technology, Lanzhou, China,
in 2007, and the master’s and Ph.D. degrees in com-
puter science from the Harbin Institute of Technology,
Harbin, China, in 2009 and 2013, respectively. He is
currently an Associate Professor with the Shenzhen
Graduate School, Harbin Institute of Technology. His
research interests include data mining, machine learn-
ing, graph mining and social network analysis, espe-
cially tensor based learning and mining algorithms.
Xiaofei Xu received the Ph.D. degree in computer
science from Harbin Institute of Technology (HIT),
Harbin, China, in 1988. He is currently a Professor of
computer science with the HIT, and Vice President of
HIT, and President of HIT, Weihai. His research inter-
ests include service computing and service engineer-
ing, cloud services and big services, enterprise com-
puting and enterprise interoperability, supply chain
management, software engineering, databases and
data mining, business intelligence, smart city ser-
vices, smart healthcare and elder-care, etc.
Ka-Cheong Leung (Senior Member, IEEE) received
the B.Eng. degree in computer science from the Hong
Kong University of Science and Technology, Hong
Kong, in 1994, the M.Sc. degree in electrical en-
gineering (Computer Networks) and the Ph.D. de-
gree in computer engineering from the University of
Southern California, Los Angeles, CA, USA, in 1997
and 2000, respectively. He was with Nokia Research
Center, Nokia, Inc., Irving, Texas, USA from 2001 to
2002, Texas Tech University, Lubbock, Texas, USA,
from 2002 to 2005, and the University of Hong Kong,
Hong Kong, from 2005 to 2019. He is currently an Associate Professor with the
School of Computer Science and Technology, Harbin Institute of Technology,
Shenzhen, China. His research interests include smart grid, vehicle-to-grid
(V2G), machine learning, future Internet, and wireless communications. He is an
Associate Editor for the IEEE S YSTEMS JOURNAL and ACM/Springer Wireless
Networks. He has also co-guest edited a special issue of the IEEE TRANSACTIONS
ON NETWORK SCIENCE AND ENGINEERING. Furthermore, he is a Subarea-Chair
of the IEEE SIG on Intelligent Internet Edge.
Zhiyao Chen received the B.Sc. degree in computer
science and application from the Hefei University of
Technology, Hefei, China. He is currently working
toward the master’s degree with the Harbin Institue
of Technology, Harbin, China. His research interests
are in natural language processing, big data analysis,
and data mining.
Yunming Y e received the Ph.D. degree in com-
puter science from Shanghai Jiao Tong University,
Shanghai, China. He is currently a Professor with
the Shenzhen Graduate School, Harbin Institute of
Technology, Harbin, China. His research interests in-
clude data mining, text mining, and ensemble learning
algorithms.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on January 31,2026 at 21:17:00 UTC from IEEE Xplore.  Restrictions apply.