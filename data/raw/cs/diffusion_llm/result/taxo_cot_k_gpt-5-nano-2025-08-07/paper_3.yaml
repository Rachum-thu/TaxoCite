"citations":
- "title": "Online large-margin training of dependency parsers"
  "unique_context_marker": "[1]"
  "block_ids":
  - 0
  - 2
  - 3
  - 12
  "intent_labels":
  - - "Domain Overview"
    - "Domain Overview"
    - "Domain Overview"
    - "Domain Overview"
    - "Domain Overview"
  - - "Prior Methods"
    - "Research Gap"
    - "Model/Architecture Adoption"
    - "Domain Overview"
    - "Prior Methods"
  - - "Prior Methods"
    - "Background"
    - "Prior Methods"
    - "Other Intent"
    - "Prior Methods"
  - []
  "topic_labels":
  - - "Graph neural methods for text"
    - "Other Topics"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
  - - "Graph neural methods for text"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
  - - "Graph neural methods for text"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
  - - "Graph neural methods for text"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
- "title": "Pseudo-projective dependency parsing"
  "unique_context_marker": "[2]"
  "block_ids": []
  "intent_labels": []
  "topic_labels": []
- "title": "CoNLL-X shared task on multilingual dependency parsing"
  "unique_context_marker": "[3]"
  "block_ids":
  - 12
  "intent_labels":
  - - "Domain Overview"
    - "Prior Methods"
    - "Domain Overview"
    - "Other Intent"
    - "Prior Methods"
  "topic_labels":
  - - "Other Topics"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
- "title": "The CoNLL 2007 shared task on dependency parsing"
  "unique_context_marker": "[4]"
  "block_ids": []
  "intent_labels": []
  "topic_labels": []
- "title": "Dependency parsing"
  "unique_context_marker": "[5]"
  "block_ids":
  - 12
  "intent_labels":
  - - "Domain Overview"
    - "Prior Methods"
    - "Prior Methods"
    - "Other Intent"
    - "Prior Methods"
  "topic_labels":
  - - "Other Topics"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
- "title": "Transition-based dependency parsing with stack long short-term memory"
  "unique_context_marker": "[6]"
  "block_ids":
  - 12
  "intent_labels":
  - []
  "topic_labels":
  - - "Other Topics"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Recurrent and hybrid networks"
- "title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations"
  "unique_context_marker": "[7]"
  "block_ids":
  - 12
  "intent_labels":
  - []
  "topic_labels":
  - - "Graph neural methods for text"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
- "title": "Graph-based dependency parsing with bidirectional LSTM"
  "unique_context_marker": "[8]"
  "block_ids": []
  "intent_labels": []
  "topic_labels": []
- "title": "Deep biaffine attention for neural dependency parsing"
  "unique_context_marker": "[9]"
  "block_ids":
  - 0
  - 1
  - 3
  - 4
  - 7
  - 12
  "intent_labels":
  - - "Model/Architecture Adoption"
    - "Prior Methods"
    - "Model/Architecture Adoption"
    - "Model/Architecture Adoption"
    - "Model/Architecture Adoption"
  - []
  - []
  - []
  - []
  - []
  "topic_labels":
  - []
  - []
  - - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
  - []
  - []
  - - "Graph neural methods for text"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
- "title": "Stack-pointer networks for dependency parsing"
  "unique_context_marker": "[10]"
  "block_ids":
  - 12
  "intent_labels":
  - - "Domain Overview"
    - "Prior Methods"
    - "Model/Architecture Adoption"
    - "Other Intent"
    - "Prior Methods"
  "topic_labels":
  - - "Other Topics"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
- "title": "Self-attentive biaffine dependency parsing"
  "unique_context_marker": "[11]"
  "block_ids":
  - 0
  - 2
  - 3
  - 12
  "intent_labels":
  - - "Model/Architecture Adoption"
    - "Prior Methods"
    - "Prior Methods"
    - "Prior Methods"
    - "Prior Methods"
  - - "Prior Methods"
    - "Model/Architecture Adoption"
    - "Prior Methods"
    - "Prior Methods"
    - "Prior Methods"
  - - "Research Gap"
    - "Prior Methods"
    - "Research Gap"
    - "Domain Overview"
    - "Prior Methods"
  - []
  "topic_labels":
  - - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
  - - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
  - []
  - - "Graph neural methods for text"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Pre-trained language models"
- "title": "A graph-based model for joint chinese word segmentation and dependency parsing"
  "unique_context_marker": "[12]"
  "block_ids":
  - 0
  - 9
  - 10
  - 11
  - 12
  "intent_labels":
  - - "Model/Architecture Adoption"
    - "Prior Methods"
    - "Prior Methods"
    - "Prior Methods"
    - "Prior Methods"
  - - "Background"
    - "Prior Methods"
    - "Prior Methods"
    - "Prior Methods"
    - "Prior Methods"
  - []
  - []
  - []
  "topic_labels":
  - - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Long-context and memory extensions"
    - "Graph neural methods for text"
  - []
  - - "Recurrent and hybrid networks"
    - "Graph neural methods for text"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Recurrent and hybrid networks"
  - []
  - - "Graph neural methods for text"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
- "title": "Character-level chinese dependency parsing"
  "unique_context_marker": "[13]"
  "block_ids":
  - 0
  - 7
  - 10
  - 12
  "intent_labels":
  - []
  - - "Setting/Protocal Adoption"
    - "Setting/Protocal Adoption"
    - "Setting/Protocal Adoption"
    - "Benchmark Utilization"
    - "Benchmark Utilization"
  - []
  - []
  "topic_labels":
  - - "Other Topics"
    - "Recurrent and hybrid networks"
    - "Recurrent and hybrid networks"
    - "Recurrent and hybrid networks"
    - "Recurrent and hybrid networks"
  - []
  - - "Recurrent and hybrid networks"
    - "Graph neural methods for text"
    - "Recurrent and hybrid networks"
    - "Graph neural methods for text"
    - "Recurrent and hybrid networks"
  - - "Other Topics"
    - "Encoder-only transformers"
    - "Recurrent and hybrid networks"
    - "Other Topics"
    - "Recurrent and hybrid networks"
- "title": "Character-level dependencies in chinese: Usefulness and learning"
  "unique_context_marker": "[14]"
  "block_ids": []
  "intent_labels": []
  "topic_labels": []
- "title": "Unified dependency parsing of chinese morphological and syntactic structures"
  "unique_context_marker": "[15]"
  "block_ids": []
  "intent_labels": []
  "topic_labels": []
- "title": "Neural character-level dependency parsing for chinese"
  "unique_context_marker": "[16]"
  "block_ids":
  - 0
  - 12
  "intent_labels":
  - []
  - []
  "topic_labels":
  - - "Other Topics"
    - "Other Topics"
    - "Recurrent and hybrid networks"
    - "Recurrent and hybrid networks"
    - "Recurrent and hybrid networks"
  - - "Other Topics"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
- "title": "Deep contextualized word representations"
  "unique_context_marker": "[17]"
  "block_ids":
  - 0
  - 2
  "intent_labels":
  - - "Domain Overview"
    - "Domain Overview"
    - "Prior Methods"
    - "Prior Methods"
    - "Prior Methods"
  - - "Prior Methods"
    - "Domain Overview"
    - "Prior Methods"
    - "Prior Methods"
    - "Prior Methods"
  "topic_labels":
  - - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Pre-trained language models"
    - "Encoder-only transformers"
  - - "Recurrent and hybrid networks"
    - "Pre-trained language models"
    - "Pre-trained language models"
    - "Pre-trained language models"
    - "Pre-trained language models"
- "title": "BERT: Pre-training of deep bidirectional transformers for language understanding"
  "unique_context_marker": "[18]"
  "block_ids":
  - 0
  - 2
  - 7
  "intent_labels":
  - - "Domain Overview"
    - "Domain Overview"
    - "Prior Methods"
    - "Prior Methods"
    - "Prior Methods"
  - - "Setting/Protocal Adoption"
    - "Model/Architecture Adoption"
    - "Model/Architecture Adoption"
    - "Model/Architecture Adoption"
    - "Model/Architecture Adoption"
  - - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
    - "Hyperparameter Utilization"
    - "Model/Architecture Adoption"
  "topic_labels":
  - []
  - - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
  - - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
- "title": "Multitask learning"
  "unique_context_marker": "[19]"
  "block_ids":
  - 0
  - 1
  "intent_labels":
  - []
  - []
  "topic_labels":
  - - "Other Topics"
    - "Other Topics"
    - "PLM-based fine-tuning strategies"
    - "Deep learning foundations for text classification"
    - "Other Topics"
  - []
- "title": "Joint chinese word segmentation and part-of-speech tagging via two-way attentions of auto-analyzed knowledge"
  "unique_context_marker": "[20]"
  "block_ids":
  - 0
  "intent_labels":
  - - "Model/Architecture Adoption"
    - "Domain Overview"
    - "Prior Methods"
    - "Prior Methods"
    - "Prior Methods"
  "topic_labels":
  - - "Long-context and memory extensions"
    - "Encoder-only transformers"
    - "Long-context and memory extensions"
    - "Long-context and memory extensions"
    - "Long-context and memory extensions"
- "title": "BERT and PALs: Projected attention layers for efficient adaptation in multi-task learning"
  "unique_context_marker": "[21]"
  "block_ids":
  - 0
  - 2
  "intent_labels":
  - - "Model/Architecture Adoption"
    - "Model/Architecture Adoption"
    - "Prior Methods"
    - "Model/Architecture Adoption"
    - "Model/Architecture Adoption"
  - - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
  "topic_labels":
  - []
  - - "PLM-based fine-tuning strategies"
    - "PLM-based fine-tuning strategies"
    - "PLM-based fine-tuning strategies"
    - "PLM-based fine-tuning strategies"
    - "PLM-based fine-tuning strategies"
- "title": "Parameter-efficient transfer learning for NLP"
  "unique_context_marker": "[22]"
  "block_ids":
  - 0
  - 2
  "intent_labels":
  - - "Model/Architecture Adoption"
    - "Model/Architecture Adoption"
    - "Prior Methods"
    - "Model/Architecture Adoption"
    - "Model/Architecture Adoption"
  - - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
  "topic_labels":
  - []
  - - "PLM-based fine-tuning strategies"
    - "PLM-based fine-tuning strategies"
    - "PLM-based fine-tuning strategies"
    - "PLM-based fine-tuning strategies"
    - "PLM-based fine-tuning strategies"
- "title": "Attention is all you need"
  "unique_context_marker": "[23]"
  "block_ids":
  - 0
  - 3
  - 7
  "intent_labels":
  - []
  - []
  - - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
  "topic_labels":
  - []
  - []
  - - "Encoder–decoder PLMs"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Encoder–decoder PLMs"
- "title": "A simple and effective neural model for joint word segmentation and POS tagging"
  "unique_context_marker": "[24]"
  "block_ids":
  - 2
  "intent_labels":
  - - "Prior Methods"
    - "Prior Methods"
    - "Prior Methods"
    - "Algorithm/Principle Adoption"
    - "Prior Methods"
  "topic_labels":
  - - "Recurrent and hybrid networks"
    - "Recurrent and hybrid networks"
    - "Recurrent and hybrid networks"
    - "Deep learning foundations for text classification"
    - "Recurrent and hybrid networks"
- "title": "Character-based joint segmentation and pos tagging for chinese using bidirectional RNN-CRF"
  "unique_context_marker": "[25]"
  "block_ids":
  - 2
  "intent_labels":
  - - "Prior Methods"
    - "Prior Methods"
    - "Prior Methods"
    - "Algorithm/Principle Adoption"
    - "Prior Methods"
  "topic_labels":
  - - "Recurrent and hybrid networks"
    - "Recurrent and hybrid networks"
    - "Recurrent and hybrid networks"
    - "Deep learning foundations for text classification"
    - "Recurrent and hybrid networks"
- "title": "Visualizing and understanding the effectiveness of BERT"
  "unique_context_marker": "[26]"
  "block_ids":
  - 2
  "intent_labels":
  - - "Model/Architecture Adoption"
    - "Prior Methods"
    - "Prior Methods"
    - "Prior Methods"
    - "Prior Methods"
  "topic_labels":
  - - "PLM-based fine-tuning strategies"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Pre-trained language models"
    - "Encoder-only transformers"
- "title": "How to fine-tune BERT for text classification?"
  "unique_context_marker": "[27]"
  "block_ids":
  - 2
  "intent_labels":
  - - "Prior Methods"
    - "Research Gap"
    - "Algorithm/Principle Adoption"
    - "Prior Methods"
    - "Prior Methods"
  "topic_labels":
  - - "PLM-based fine-tuning strategies"
    - "PLM-based fine-tuning strategies"
    - "Encoder-only transformers"
    - "PLM-based fine-tuning strategies"
    - "PLM-based fine-tuning strategies"
- "title": "Self-attention with relative position representations"
  "unique_context_marker": "[28]"
  "block_ids":
  - 3
  "intent_labels":
  - - "Model/Architecture Adoption"
    - "Model/Architecture Adoption"
    - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
  "topic_labels":
  - []
- "title": "Transformer-XL: Attentive language models beyond a fixed-length context"
  "unique_context_marker": "[29]"
  "block_ids":
  - 3
  "intent_labels":
  - - "Model/Architecture Adoption"
    - "Model/Architecture Adoption"
    - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
    - "Algorithm/Principle Adoption"
  "topic_labels":
  - []
- "title": "Non-projective dependency parsing using spanning tree algorithms"
  "unique_context_marker": "[30]"
  "block_ids":
  - 4
  "intent_labels":
  - []
  "topic_labels":
  - []
- "title": "A fast decoder for joint word segmentation and POS-tagging using a single discriminative model"
  "unique_context_marker": "[31]"
  "block_ids":
  - 7
  "intent_labels":
  - []
  "topic_labels":
  - []
- "title": "Improving chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data"
  "unique_context_marker": "[32]"
  "block_ids":
  - 7
  "intent_labels":
  - []
  "topic_labels":
  - []
- "title": "Decoupled weight decay regularization"
  "unique_context_marker": "[33]"
  "block_ids":
  - 7
  "intent_labels":
  - []
  "topic_labels":
  - []
- "title": "Incremental joint approach to word segmentation, POS tagging, and dependency parsing in chinese"
  "unique_context_marker": "[34]"
  "block_ids":
  - 10
  - 11
  - 12
  "intent_labels":
  - []
  - []
  - []
  "topic_labels":
  - - "Recurrent and hybrid networks"
    - "Graph neural methods for text"
    - "Recurrent and hybrid networks"
    - "Graph neural methods for text"
    - "Recurrent and hybrid networks"
  - - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
  - - "Other Topics"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Other Topics"
    - "Convolutional approaches"
- "title": "Randomized greedy inference for joint segmentation, POS tagging and dependency parsing"
  "unique_context_marker": "[35]"
  "block_ids": []
  "intent_labels": []
  "topic_labels": []
- "title": "Character-level dependency model for joint word segmentation, POS tagging, and dependency parsing in chinese"
  "unique_context_marker": "[36]"
  "block_ids":
  - 10
  - 12
  "intent_labels":
  - []
  - []
  "topic_labels":
  - - "Recurrent and hybrid networks"
    - "Graph neural methods for text"
    - "Recurrent and hybrid networks"
    - "Graph neural methods for text"
    - "Recurrent and hybrid networks"
  - - "Other Topics"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Other Topics"
    - "Convolutional approaches"
- "title": "Neural joint model for transition-based chinese syntactic analysis"
  "unique_context_marker": "[37]"
  "block_ids":
  - 10
  "intent_labels":
  - []
  "topic_labels":
  - - "Recurrent and hybrid networks"
    - "Graph neural methods for text"
    - "Other Topics"
    - "Other Topics"
    - "Other Topics"
- "title": "Is POS tagging necessary or even helpful for neural dependency parsing?"
  "unique_context_marker": "[38]"
  "block_ids":
  - 10
  "intent_labels":
  - []
  "topic_labels":
  - []
- "title": "Analyzing and integrating dependency parsers"
  "unique_context_marker": "[39]"
  "block_ids":
  - 11
  "intent_labels":
  - []
  "topic_labels":
  - - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
- "title": "Integrating graph-based and transition-based dependency parsers"
  "unique_context_marker": "[40]"
  "block_ids":
  - 12
  "intent_labels":
  - - "Domain Overview"
    - "Prior Methods"
    - "Resource Utilization"
    - "Other Intent"
    - "Prior Methods"
  "topic_labels":
  - - "Other Topics"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
- "title": "Transition-based dependency parsing with rich non-local features"
  "unique_context_marker": "[41]"
  "block_ids":
  - 12
  "intent_labels":
  - []
  "topic_labels":
  - - "Other Topics"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Recurrent and hybrid networks"
- "title": "Globally normalized transition-based neural networks"
  "unique_context_marker": "[42]"
  "block_ids": []
  "intent_labels": []
  "topic_labels": []
- "title": "An efficient algorithm for projective dependency parsing"
  "unique_context_marker": "[43]"
  "block_ids":
  - 12
  "intent_labels":
  - []
  "topic_labels":
  - - "Other Topics"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Recurrent and hybrid networks"
- "title": "Distilling an ensemble of greedy dependency parsers into one MST parser"
  "unique_context_marker": "[44]"
  "block_ids":
  - 12
  "intent_labels":
  - []
  "topic_labels":
  - - "Graph neural methods for text"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
- "title": "Neural probabilistic model for non-projective MST parsing"
  "unique_context_marker": "[45]"
  "block_ids":
  - 12
  "intent_labels":
  - []
  "topic_labels":
  - - "Other Topics"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Graph neural methods for text"
- "title": "Deep contextualized word embeddings in transition-based and graph-based dependency parsing–A tale of two parsers revisited"
  "unique_context_marker": "[46]"
  "block_ids":
  - 12
  "intent_labels":
  - []
  "topic_labels":
  - - "Pre-trained language models"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Encoder-only transformers"
    - "Pre-trained language models"
- "title": "A maximum entropy chinese character-based parser"
  "unique_context_marker": "[47]"
  "block_ids":
  - 12
  "intent_labels":
  - []
  "topic_labels":
  - - "Other Topics"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Other Topics"
    - "Convolutional approaches"
- "title": "Chinese parsing exploiting characters"
  "unique_context_marker": "[48]"
  "block_ids":
  - 12
  "intent_labels":
  - []
  "topic_labels":
  - - "Other Topics"
    - "Encoder-only transformers"
    - "Graph neural methods for text"
    - "Other Topics"
    - "Convolutional approaches"
