title: Multimodal Distillation Pre-Training Model for Ultrasound Dynamic Images Annotation
abstract: 'With the development of medical technology, ultrasonography has become
  an important diagnostic method in doctors’ clinical work. However, compared with
  the static medical image processing work such as CT, MRI, etc., which has more research
  bases, ultrasonography is a dynamic medical image similar to video, which is captured
  and generated by a real-time moving probe, so how to deal with the video data in
  the medical field and cross modal extraction of the textual semantics in the medical
  video is a difficult problem that needs to be researched. For this reason, this
  paper proposes a pre-training model of multimodal distillation and fusion coding
  for processing the semantic relationship between ultrasound dynamic Images and text.
  Firstly, by designing the fusion encoder, the visual geometric features of tissues
  and organs in ultrasound dynamic images, the overall visual appearance descriptive
  features and the named entity linguistic features are fused to form a unified visual-linguistic
  feature, so that the model obtains richer visual, linguistic cues aggregation and
  alignment ability. Then, the pre-training model is augmented by multimodal knowledge
  distillation to improve the learning ability of the model. The final experimental
  results on multiple datasets show that the multimodal distillation pre-training
  model generally improves the fusion ability of various types of features in ultrasound
  dynamic images, and realizes the automated and accurate annotation of ultrasound
  dynamic images.


  Index Terms —Annotation, knowledge distillation, multimodal distillation, pre-training
  model, transformer, ultrasound dynamic image.'
abstract_is_verbatim: true
segmented_markdown: '# Multimodal Distillation Pre-Training Model for Ultrasound Dynamic
  Images Annotation


  ## Abstract

  <block id="0">

  With the development of medical technology, ultrasonography has become an important
  diagnostic method in doctors’ clinical work. However, compared with the static medical
  image processing work such as CT, MRI, etc., which has more research bases, ultrasonography
  is a dynamic medical image similar to video, which is captured and generated by
  a real-time moving probe, so how to deal with the video data in the medical field
  and cross modal extraction of the textual semantics in the medical video is a difficult
  problem that needs to be researched. For this reason, this paper proposes a pre-training
  model of multimodal distillation and fusion coding for processing the semantic relationship
  between ultrasound dynamic Images and text. Firstly, by designing the fusion encoder,
  the visual geometric features of tissues and organs in ultrasound dynamic images,
  the overall visual appearance descriptive features and the named entity linguistic
  features are fused to form a unified visual-linguistic feature, so that the model
  obtains richer visual, linguistic cues aggregation and alignment ability. Then,
  the pre-training model is augmented by multimodal knowledge distillation to improve
  the learning ability of the model. The final experimental results on multiple datasets
  show that the multimodal distillation pre-training model generally improves the
  fusion ability of various types of features in ultrasound dynamic images, and realizes
  the automated and accurate annotation of ultrasound dynamic images.


  Index Terms —Annotation, knowledge distillation, multimodal distillation, pre-training
  model, transformer, ultrasound dynamic image.


  </block>

  ## I. INTRODUCTION

  <block id="1">

  IN THE process of medical ultrasound examination, the doctor generates objective
  and standardized description of the distribution, morphology, size, density and
  edge of internal human tissues and organs, through the real-time ultrasound images.
  Furthermore, descriptive diagnosis is manually summarized and concluded. At last,
  a complete ultrasound report is generated; meanwhile, the artificial annotation
  process from ultrasound image to textual description is completed. Due to certain
  differences in theoretical knowledge, reading method, reading sequence, and work
  experience of different doctors, different language descriptions may be interpreted
  for the same medical image, thus affecting patients’ diagnosis and treatment plan.


  With the advent of the artificial intelligence era, the fields of Computer Vision
  (CV) [1], which focuses on image data such as images and videos, and Natural Language
  Processing (NLP)[2], which focuses on textual data, have both achieved great success
  and gained a wide range of applications. Specifically in medicine, the field of
  Computer Vision mainly researches how to realize the techniques of target detection,
  image classification, image segmentation, and image alignment in medical images
  [3], [4], [5], [6]. Thereinto, medical image classification technology has already
  made considerable achievements, and some related technologies have already realized
  industrial landing, which are used to assist doctors in the diagnostic work of medical
  images. The field of Natural Language Processing mainly studies how to implement
  techniques such as structured extraction, text classification, and semantic comparison
  of electronic medical record texts [7], [8], [9]. The current research related to
  the structuring of electronic medical record text has also made good development,
  and has been practically applied to the named entity resolution of a large amount
  of unstructured text in hospital data centers for further use in search engines
  and knowledge graph presentation [10], [11].


  Li By combining gated convolutional neural networks (gated CNNs) and recurrent neural
  networks (RNNs) designed to learn spatio-temporal embedded representations in gene
  sequences to predict translation initiation sites (TIS) [12]. Guo presented an innovative
  deep spatial-temporal neural network model designed to predict Poly(A) signals in
  genomic sequences [13].


  However, most of the aforementioned fields of Computer Vision and Natural Language
  Processing focus on a single modality or deal with one-dimensional sequential data.
  Ultrasonography, on the other hand, needs to implement a cross modal conversion
  process from image to textual descriptions, find semantic associations that exist
  between multiple modalities of data, and realize textual annotation of image data.
  Although multimodal data can provide richer feature information, the heterogeneity
  of multimodality have brought new problems and challenges to the task of ultrasound
  image annotation, and have become the focus and hot issues for medical multimodal
  data processing and fusion researchers in recent years.


  Image Annotation is a comprehensive research direction combining Computer Vision
  and Natural Language Processing. The goal of image annotation is to realize the
  fusion of image semantic content and text semantics by computer analysis of image
  information and automatic generation of corresponding description words. The technique
  of Image Captioning has been further developed to describe an image through a text.
  Y. Mori in the first International Workshop on Intelligent Storage and Retrieval
  Management for Multimedia in 1999 proposed a method to establish a relationship
  between images and text, which achieved a certain degree of effectiveness by dividing
  each image uniformly into a process of sub-images containing keywords and vector
  quantizing the sub-images [14]. To further enhance the efficiency of image annotation,
  Vinyals O introduced deep learning techniques and proposed the Seq2Seq framework,
  which was originally applied in translation systems, to solve the image annotation
  problem by switching the original encoder for sentences from RNN to CNN and thus
  encoding the images, which has achieved better results on several public datasets
  [15]. Swinburne N C created a software module to parse medical images in DICOM format,
  generate annotations automatically, and apply it in a variety of PACS systems [16].
  Yang provides new ideas for solving the problem of image quality enhancement in
  low-light environments by combining the global feature capture capability of Transformer
  with the generative advantages of Generative Adversarial Networks (GANs) [17].


  With the continuous development of deep learning techniques, pre-training techniques
  have been introduced to visual language models. Xia proposed task modeling structure
  for textual continuous masks masking out a continuous sequence of words to obtain
  semantic and visual information [18]. Multimodal pre-training requires precise alignment
  and fusion of multimodal information to identify the correspondence between various
  types of modalities [19]. To enhance the cross modal alignment and zero-sample learning
  capability, the pre-training model can also adopt a contrast learning approach,
  where the distances of image-text pairs are computed after the image and text are
  feature extracted and encoded separately[20]. Fusion encoders also play an important
  role in visual language pre-training models [21]. Zhou proposed the unsupervised
  visual-linguistic pre-training model UVLP to learn strong joint visual and linguistic
  representations of unaligned text and image sources through retrieval-based multi-granularity
  alignment [22]. To further enhance the effect of multimodal pre-training model,
  multimodal distillation technique is introduced to help the student model to achieve
  higher efficiency through the teacher model [23]. To improve the training capability,
  knowledge graph is introduced to assist the visual language model to achieve better
  training effect [24].


  Considering the above background, this paper seeks to introduce deep learning techniques
  in the field of ultrasound medicine, design pre-training models, and study new methods
  of cross modal data processing and fusion of Computer Vision and Natural Language
  Processing. The significance of the study of automated annotation of ultrasound
  dynamic images is reflected in the improvement of the abstraction of image features
  and in the supplementation of existing techniques for intelligent processing of
  medical images. Specifically, the main contribution points of this paper are reflected
  in the following aspects:


  1) We propose the model of dynamic image language bidirectional encoder representations
  from transformers (DILBERT) to extend traditional medical image processing to dynamic
  image processing. This model is used to deal with the semantic relationships between
  ultrasound dynamic images and texts and carry out automatic and accurate annotation
  of ultrasound dynamic images precisely.


  2) Through Self-Attention Mechanism and Modality Fusion (MF), we design a fusion
  encoder to fuse the overall visual appearance of descriptive features and the named
  entity features, to form a unified visual-language feature. We propose a multimodal
  distillation framework that simultaneously migrates both response-based knowledge
  and relationship-based knowledge. The pre-training teacher model is used to constrain
  the corresponding network, reducing the training parameters and preventing the knowledge
  between the two networks from differing too much thus causing a negative optimization
  of the model performance. The DILBERT model can obtain richer aggregation and alignment
  capabilities of visual and verbal cues.


  3) We design pre-training and fine-tuning tasks of the DILBERT model and introduce
  the knowledge graph and human tissue and organ features as named entities and relations
  as prior knowledge for masking. The pre-training of the model is performed using
  four training tasks: Masked Language Modeling (MLM), Masked Description Classification
  (MDC), Masked Organ Classification (MOC), and Cross-Modal Matching (CMM). The pre-training
  range is reduced and the applicability of the actual scene of the ultrasound examination
  is strengthened.


  </block>

  ## II. RELATED WORKS

  <block id="2">


  </block>

  ### A. Visual Language Pre-Training Model

  <block id="3">

  Pre-training models are widely used in various fields of artificial intelligence.
  A single modality such as vision, language, and speech has a variety of pre-training
  models for practical applications. The visual-linguistic pre-training model [25]
  is a special form of multimodal pre-training model for fusing vision and language
  for pre-training to learn joint visual and linguistic representations for specific
  tasks such as visual annotation [26], visual question and answer [27], graphic and
  text matching [28], and visual interactions [29], and thus can be used to learn
  dynamic images and diagnosis in ultrasonography from the multimodal perspective
  of the joint characterization of text, and further to achieve automatic annotation
  of ultrasound dynamic images.


  The core of the visual language pre-training model is Modality Fusion (MF), which
  models internal of multimodal fusion to produce contextually joint representations
  of images and language, integrating encoded disparate modal information into a stable
  multimodal feature. MF usually employs dot product operations to fuse image and
  text feature vectors, which are mainly categorized into two types: Dual Stream Models
  (DSM) and Single Stream Models (SSM) [30], which are described below.


  1) Dual Stream Models: The dual stream model aims to map vision and language into
  the same semantic space. It is a pioneering approach to modal fusion. Instead of
  combining visual and verbal coded inputs, two independent encoders are used to learn
  high-level representations of vision and language, respectively, and the parameters
  are not shared between the two encoders; instead, cross modal interactions are realized
  through Co-Attention. The dual stream design allows the network depth and architecture
  to be adapted to each mode, and in addition to modal fusion within each modality,
  some studies have explicitly designed intermodal interactions between the two encoders
  to enable modal fusion at different encoding stages. Lu proposed ViLBERT based on
  dual stream structure. That is, image and text are encoded by separate encoders
  and then modal information is fused by cross modal encoders. Language and vision
  are fused through the attention mechanism module [31]. Sun proposed the CBT model
  to study the advantages and disadvantages of fusion structures in dual encoder structures[32].
  Li proposed the HERO model, which uses the double dual stream structure as ViLBERT,
  and the cross modal encoders are stacked in multiple layers to compute cross-attention
  for video-to-language and language-to-video, respectively [33]. In addition, Urooj
  proposed a three streams model based on the dual stream model to further enhance
  the input multimodal data [34].


  2) Single Stream Models: The single stream model aims to learn a joint representation
  that fuses multimodal inputs by Merged Attention. The images and text markers are
  linked and fed into the encoder. Since both modalities use the same set of parameters,
  the parameters are reduced by half relative to the dual stream model. Moreover,
  single stream modeling performs implicit internal of multimodal fusion, and is not
  constrained by the architectural design of the fusion phase in dual stream models.
  So most visual language pre-training models use this modality fusion scheme. Chen
  from Google Research proposed VideoBERT based on single stream structure. i.e.,
  image and text are encoded by the same encoder. Language and vision are fused through
  BERT module [25]. Su from Microsoft Research Asia proposed VL-BERT based on single
  stream model and introduced a new pre-trainable generic representation for visual-linguistic
  tasks [35]. Zhu from Baidu Research proposed ActBERT based on single stream model
  and leveraged global action information to catalyze mutual interactions between
  linguistic texts and local regional objects. It uncovers global and local visual
  clues from paired video sequences and text descriptions for detailed visual and
  text relation modeling. Due to the need to input visual information of “image regions”
  and “image descriptions” in ultrasound dynamic images, as well as language information
  of “named entity”, ActBERT’s Tangled Transformer (TNT) fusion encoder is introduced
  in this article. To further improve training efficiency, a knowledge distillation
  training model was constructed based on this architecture, which will be introduced
  in the next section [36].


  </block>

  ### B. Knowledge Distillation

  <block id="4">

  Knowledge distillation is a Teacher-Student training structure in which a trained
  teacher model usually provides knowledge and a student model is trained to acquire
  the teacher’s knowledge through distillation. It is commonly used to migrate and
  compress knowledge from a complex teacher model into a simple student model at the
  cost of a slight performance loss [37].


  Typically, knowledge distillation uses a larger T-value during training because
  of its small soft target variance, and the model is trained to pay more attention
  to smaller logical units so that the student model learns information about the
  relationship between these negative and positive samples. The relational information
  in the teacher model is called “Dark Knowledge”, and knowledge distillation model
  transfers “Dark Knowledge” from the teacher model to the student model by adjusting
  T to generate appropriate soft targets during the training process, thus improving
  the prediction performance of the student model. Knowledge distillation is set at
  T =1 in the testing phase, which creates a good level of differentiation due to
  the large differences in the soft targets for different logical unit values.


  Knowledge distillation can not only be used for model compression, it can also improve
  the performance of a complex model through optimization strategies such as mutual
  learning and self-learning. Meanwhile, its utilization of features such as unlabeled
  and cross modal data also has a significant improvement on model enhancement. Based
  on this, two technical directions, model compression based on knowledge distillation
  which was proposed by the researchers of our team [38] and model enhancement [39],
  are delineated on the basis of the application scenario, i.e., whether obtained
  network model is intended to be applied to resource-constrained devices. Since knowledge
  distillation has a model enhancement function, it is suitable for the scenarios
  of different modal data interaction and fusion, as described below.


  1) Cross Modal Distillation: In many practical applications, data usually exists
  in multiple modalities, and some data in different modalities are all describing
  the same thing or event, and cross modal distillation can be achieved using synchronized
  modal information. Often, many labeled large-scale datasets are available on the
  Web, and information from that dataset can be migrated to student models of different
  datasets through cross modal knowledge distillation. The modal information synchronized
  and aligned by the teacher can be used to compensate for the flow of information
  not otherwise available to the student network and continue to enhance the performance
  of the student network through training in knowledge distillation. Girdhar collects
  and labels large and clean still image datasets as a rich knowledge representation
  for teacher training and guided unlabeled video learning[40]. Yuan promote knowledge
  transfer from image semantic understanding to the task of text-to-image synthesis
  [41]. Tavakolian distills spatial-temporal information from each frame of a video
  into the image through frame adaptive weighting [42]. Duan proposed a novel network
  architecture aimed at efficient cross-modal information fusion through the Transformer
  model to improve the accuracy and generalization of semantic segmentation tasks
  [43]. Wang proposed a novel image fusion method that aims to optimize the fusion
  of infrared and visible images by semi-supervised transfer learning technique [44].


  2) Multimodal Distillation: Machine learning, like the way humans know the world,
  improves the performance of processing tasks by utilizing multimodal information,
  and the distillation of knowledge applied to multimodal learning is called multimodal
  distillation. Multimodal distillation extracts heterogeneous features from different
  modal data domains that can provide complementary information on the same topic,
  and associates them in the learning of knowledge distillation to provide diverse
  knowledge for the target task. Cioppa proposed to train a single network using multimodal
  information from different viewpoints of the same scene [45]. Wu extracted integrated
  features from videos with different modal data relationships to improve the performance
  of violence detection[46]. Li proposes an innovative strategy for the task of sentiment
  recognition, namely detached multimodal distillation (DMD), which aims to enhance
  the recognition performance by improving the multimodal data processing mechanism
  [47]. Radevski explores a multimodal knowledge distillation approach for first-person
  perspective (egocentric) action recognition tasks [48].


  </block>

  ## III. METHODS

  <block id="5">

  In this paper, Transformer Dynamic Image Language Bidirectional Encoder Representations
  from Transformers (DILBERT) is designed based on the visual language pre-training
  model, the overall framework is shown in Fig. 1. Firstly, a large amount of dynamic
  images are collected from online medical image datasets and ultrasound equipment
  in hospitals to generate an image database; secondly, data preprocessing is performed
  on the images, removing noise and normalization, and annotation of named entities
  is performed on the images ready to be trained; then, 3D-CNN module is used to extract
  “image description” visual features; finally, multimodal distillation and fusion
  coding of “image description” visual features, “image region” visual features and
  “named entity” linguistic features are used for feature learning. The pre-training
  of the model is performed using four training tasks: Masked Language Modeling (MLM),
  Masked Description Classification (MDC), Masked Organ Classification (MOC), Cross
  Modal Matching (CMM), and using the named entities in the knowledge graph that have
  a location relationship with the named entity or describe the event as the a priori
  knowledge for masking. The fine-tuning of the model uses the image classification
  results to match the associated “named entities”, optimizes the deep learning parameter
  computation, enhances the performance of the model, and further identifies the actual
  health conditions of the tissues and organs, as well as the semantic associations
  between the tissues and organs and the descriptive diagnostic text, so as to realize
  the automatic annotation of ultrasound dynamic images. The related research work
  of each module is described in detail below.


  </block>

  ### A. Fusion Encoder

  <block id="6">

  At the heart of the DILBERT model is Modality Fusion (MF), which models internal
  of multimodal fusion to produce contextual joint representations of images and text.
  Before multimodal fusion, the first step is to resolve the modal differences between
  ultrasound dynamic images and ultrasound text at different levels regarding dimension
  and structure using modal embedding, i.e., extracting features from each modality
  independently and then mapping the features into a shared feature space. Text needs
  to be tokenized before it can be embedded. Visual embedding aims to convert images
  into tokens whose multiple feature levels are textual with reference to textual
  embedding. Textual markers are discrete and arranged in a single dimension, but
  images have interrelated pixel values and exist in a high-dimensional space, so
  image tokenization is usually more complex than textual tokenization. After completing
  the task of labeling multiple modalities, this paper designs the fusion encoder
  to obtain the multimodal input information itself and its interdependence by reprogramming
  the workings of the self-attention mechanism in the single stream model of visual
  language pre-training. The working principle of the self-attention mechanism in
  the fusion encoder is as follows:


  1) Self-Attention Mechanism: The self-attention mechanism computes the semantic
  representation of entities and inter-entity dependencies in a sequence by associating
  different positions in the sequence of input information. The self-attention mechanism
  computes the data on the input and target side separately to capture the dependencies
  between the entities and then adds the self-attention value computed on the input
  side to the attention value computed on the target side to capture the dependencies
  between the entities on the input side and the target side. Therefore, the self-attention
  mechanism can not only obtain the dependency relationship between the entities of
  the input and the target, but also effectively obtain the dependency relationship
  between the entities of the input and the target itself.


  The workflow of the self-attention mechanism is described as follows: the bottom
  inputs x1, x2, x3, ... xt denote the input sequence data. Using image features as
  an example, x1 can represent the feature corresponding to the first target in the
  image. Firstly, embedding operation is conducted to initial data through embedding
  layer to get the vectors α1, α2, α3 ... αT; then, qi, ki, vi, i ∈ (1, 2, 3 ... T)
  are obtained by multiplying with the WQ, WK, WV matrices respectively. q1 with k1,
  k2, k3 ... kT respectively are used to compute the vector dot product to obtain
  α1,1, α1,2, α1,3, ... α1,T. Further, α1,1, α1,2, α1,3, ... α1,T are inputted into
  the Softmax function to obtain the attention between 0-1 weight values: c1,1, c1,2,
  c1,3, ... c1,T; the output b1 corresponding to the input x1 is obtained by multiplying
  c1,1, c1,2, c1,3, ... c1,T with v1, v2, v3, ... vT at the corresponding positions
  and then summing.


  Multi-head Attention mechanism is to have multiple sets of weight matrices for the
  input vector matrices, which extends the ability of the model to focus on different
  target vectors in the picture, and forms a multiplicity of subspaces at the attention
  layer, i.e., it has multiple sets of weight matrices, which allows mapping the input
  target vectors to different feature subspaces. Multiple sets of qi, ki, vi are obtained
  for the computation of multiple heads of attention. Taking the input α1 as an example,
  three outputs are b1_head, b2_head, b3_head obtained through the multi-head (head
  = 3) attention mechanism, and in order to obtain the output b1 corresponding to
  α1, b1_head, b2_head, b3_head is spliced using vector head-to-tail concatenation,
  and then b1 is obtained by linear transformation.


  2) Fusion Coding: The input of the single stream model of the visual language is
  respectively from three types of basic data: the visual information of “image area”,
  the visual information of “image description” and the language information of “named
  entity”. In this paper, a fusion encoder is designed by introducing the self-attention
  mechanism, which is used to input visual information into the language encoder and
  input the language information into the visual encoder to strengthen the interaction
  between visual and linguistic features, as the ActBERT [36]. The difference is that
  image regions, image descriptions and named entity are used in DILBERT instead of
  the global actions, local regional objects and linguistic descriptions in ActBERT.
  The fusion encoder performs the fusion operation by means of a multi-head self-attention
  mechanism. The formula is as follows:


  Cw = Multihead(W1_q^hl_a, Ww_k^hl_w, Ww_ν^hl_w)


  Cr = Multihead(W2_q^hl_a, Wr_k^hl_r, Wr_ν^hl_r)


  Where Cw is the fused linguistic representation and Cr is the fused regional feature.
  Cw uses a linear layer to obtain new key-value pairs and stacks them with the original
  a-transformer and r-transformer key values. In this way visual and verbal features
  are fused together.


  </block>

  ### B. Multimodal Distillation

  <block id="7">

  With the above design of fusion encoder, the multimodal information of visual language
  can be pre-trained after unified encoding. Due to the need to deal with data from
  multiple modalities, multimodal pre-training models, as opposed to single modal
  pre-training models, not only face the problem of pairing data from different modalities,
  but also generate more parameters after the training is completed. The DILBERT model
  designed in this paper improves the performance of student networks by using information
  from different modal data to provide complementary cues for the target task through
  knowledge distillation. Integrating information from multiple modal data improves
  model generalization.


  The teacher model and the student model have the same network architecture, the
  pre-training of the teacher model is completed first, and then the student model
  is guided to pre-training through soft labeling, after the pre-training phase is
  completed, the two networks begin to learn from each other, and the migrated knowledge
  is of two kinds, one is the response-based knowledge, and the other is the relationship-based
  knowledge. At the same time, the pre-training teacher model is used to constrain
  the corresponding network, preventing the knowledge between the two networks from
  differing too much and thus causing a negative optimization of the model performance.


  As the relevant computational variables and functions are defined: Let X = {x1,
  x2, ..., xn} be n samples from m categories, each corresponding to a true label
  labeled y = {y1, y2, ..., ym}. Label the logits output by the kth network Nk as
  zk(x) = {z1^k, z2^k, ..., zm^k}. Its output after the Softmax function is σi(zk(x),
  t). t is the temperature parameter.


  The loss function is defined based on the MTKD-SSR [38] proposed by the researchers
  of our team which is to migrate the response-based knowledge of network k′ to network
  k (k ≠ k′) when different networks learn from each other is defined as follows:


  L_KL(pk, pk′) = ∑_{x∈X} ∑_{i=1}^m σi(zk′(x), 1) log ( σi(zk′(x), 1) / σi(zk(x),
  1) )


  Due to the extensive relationship-based knowledge between the multimodal entities
  processed by DILBERT, when two networks learn from each other, different from MTKD-SSR
  [38], DILBERT additionally transfers relationship-based knowledge in addition to
  the response-based knowledge described above. Transfer knowledge of the relationship
  between two types of samples, a distance relationship between different samples
  and an angle relationship between different samples. Let sk^j = φk(xj) be the output
  of network Nk for sample xj at any layer. Where φk(·) is a feature mapping function
  of the network Nk. χT denotes the T-tuple of different samples. Therefore, the set
  of binary groups between samples is denoted as χ2 = {(xu, xν) | u ≠ ν}, and the
  set of ternary groups is denoted as χ3 = {(xu, xν, xw) | u ≠ ν ≠ w}.


  For the network, the function that captures the similarity of the distance between
  two samples in a binary group is:


  f(sk_u, sk_ν) = (1/π) || sk_u − sk_ν ||_2


  Where π = (1/|χ2|) ∑_{(xu,xv)∈χ2} || sk_u − sk_v ||_2 is normalization constant.
  Therefore, the distance-based loss function used to convert sample relationships
  between network Nk and network Nk′ is


  L_DD(xu, xν) = ∑_{(xu,xν)∈χ2} R( f(sk_u, sk_ν), f(sk′_u, sk′_ν) )


  Where R is the Huber loss reflecting the sample relationship, defined as:


  R(a, b) = { 1/2 (a−b)^2, if |a−b| ≤ 1; |a−b| − 1/2, otherwise }


  For the network Nk, the function that captures the similarity of angles between
  the three samples in a ternary group is:


  f(sk_u, sk_ν, sk_w) = cos∠(sk_u sk_ν sk_w) = ⟨ e_{uν}, e_{wν} ⟩


  Where e_{uν} = (sk_u − sk_ν)/|| sk_u − sk_ν ||_2, e_{wν} = (sk_w − sk_ν)/|| sk_w
  − sk_ν ||_2. Therefore, the angle-based loss function used to migrate the sample
  relationship between network Nk and network Nk′ is:


  L_AD(xu, xν, xw) = ∑_{(xu,xν,xw)∈χ3} R( f(sk_u, sk_ν, sk_w), f(sk′_u, sk′_ν, sk′_w)
  )


  Therefore, the loss function for migrating the relation-based knowledge of k′ to
  network k (k ≠ k′) when different networks learn from each other is defined as:


  L_RD = L_DD(xu, xν) + β1 L_AD(xu, xν, xw)


  Where β1 is the tuning parameter that controls the balance between the loss terms.
  The final loss function for mutual learning between the two networks is:


  L_k^MD = L_RD + β2 L_KL(pk, pk′)


  Where β2 is the tuning parameter that controls the balance between the loss terms.
  When the network undergoes self-learning, a pre-training teacher model is used to
  constrain the loss function of the corresponding network to be:


  L_k^SD(pt_k, p_{-t}_k) = ∑_{x∈X} ∑_{i=1}^m σ^t_i(zk,t) log ( σ^t_i(zk,t) / σ^t_i(zk,t)
  )


  For the classification task, the cross entropy between the output of network and
  the true label is:


  L_CE(y, pk) = − ∑_{x∈X} ∑_{i=1}^m y_i log( σi( zk(x), 1 ) )


  In summary, the objective loss function for training network Nk is:


  L_k^KD = α L_k^CE + β L_k^MD + γ L_k^SD


  Where α, β, γ are the tuning parameters that control the balance between the loss
  terms.


  </block>

  ### C. A Pre-Training Model for Multimodal Distillation

  <block id="8">

  In this paper, a DMILBERT pre-training model for multimodal distillation is designed.
  The input data are the “image description” visual feature of the ultrasound dynamic
  image, the “image area” visual feature of the ultrasound dynamic image, and the
  “named entity” linguistic feature of the electronic medical record. The DMILBERT
  pre-training model performs multimodal distillation and fusion coding of “image
  description” visual features, “image region” visual features and “named entity”
  linguistic features.


  The pre-training of the multimodal distillation model was performed using four training
  tasks, MLM, MDC, MOC, and CMM. The fine-tuning of the model employs the image classification
  results to match the associated named entities, to identify the actual health conditions
  of the tissues and organs, as well as the semantic associations between the tissues
  and organs and the descriptive diagnostic texts, to realize the automatic annotation
  of ultrasound dynamic images. The specific process is as follows:


  1) Model Pre-Training Phase: Masked modeling is a common task in pre-training models,
  which is essentially an inference task because masking and replying for keywords
  in descriptive text, such as quantifiers, adjectives, nouns, and actions are essentially
  inference tasks from different perspectives. The setup of MLM is consistent with
  other pre-training models: randomly masking or replacing a portion of tokens and
  learning a multilayer encoder to predict the original token through a representation
  of the model output. Bringing Masked Image Modeling (MIM) to the field of computer
  vision in the same way that MLM was done in natural language processing.


  Masked Language Modeling Task: The masked language modeling task in the DMILBERT
  model is similar to the task in natural language processing, by randomly masking
  textual information in a portion of the language so that unmasked information can
  be inferred from masked textual information. In addition to the randomized way of
  masking the named entities, the named entities with positional relationships and
  describing events in the relationship graph of the knowledge graph are further randomly
  selected for masking, which enhances the efficiency of the reasoning task in the
  pre-training model. The masked language modeling task helps the multimodal pre-training
  model to learn the fine-grained correlations between languages.


  Masked Description Classification Task: the masked named entities are further learned
  to predict the semantics represented by each masked named entity by the mask description
  classification task, which predicts the masked named entities based on the linguistic
  features of the organizing organ. The task of explicitly predicting cues for long-time
  action sequences that primarily tap into the language and discriminating the temporal
  order of named entities enhances the ability to categorize named entities in the
  pre-training model, which is further generalized to the fine-tuning task.


  Masked Organ Classification Task: The masked organ classification task works in
  a similar mode to the masked description classification task by randomly masking
  image regions in ultrasound dynamic images to learn the high-level semantics of
  tissues and organs in the masked regions of ultrasound dynamic images. The classification
  ability is further trained by predicting the classification of tissues and organs
  in the masked region by the absence of information about the tissues and organs
  in the masked region.


  Cross Modal Matching Task: Visual features in image regions and image descriptions
  naming entities are utilized to derive relationships between visual and linguistic
  entities. This task trains the model to learn from contextual descriptions while
  extracting relevant visual features to assist in text prediction, which provides
  the ability to align multimodal pre-training models on a coarse-grained level.


  2) Fine-Tuning of Mandates: The fine-tuning task outputs natural language descriptions
  of tissues and organs in the ultrasound dynamic image region, using the image classification
  results to match the associated “named entity” descriptions. The performance of
  the model is enhanced by optimizing the computation of deep learning parameters
  to further identify the actual health conditions of tissues and organs, as well
  as the semantic associations between tissues and organs and descriptive diagnostic
  texts, to realize the automatic annotation of ultrasound dynamic images.


  </block>

  ## IV. EXPERIMENTAL DETAILS

  <block id="9">


  </block>

  ### A. Experimental Basics

  <block id="10">

  1) Experimental Data: The experimental data sources were publicly available medical
  datasets on the Internet: the BUSI (Breast Ultrasound Images) dataset, which provided
  127 breast ultrasound reports [49], the CLUST (Challenge on Liver Ultrasound Tracking)
  dataset, which provided 64 liver ultrasound reports [50], the USSS (Ultrasound simulation
  & segmentation) dataset, which provided a total of 617 ultrasound reports of gallbladder,
  pancreas, spleen, kidney [51]; and internal ultrasound images obtained from the
  Affiliated Hospital of Jiangsu University (AHJU), the author’s institution. The
  AHJUUI dataset provided “Liver”, “Gallbladder”, “Pancreas”, “Spleen”, “Kidney”,
  “Thyroid”, and 500 ultrasound images and reports, and generated 3000 serialized
  images.


  As shown in the experimental data, four consecutive ultrasound dynamic images of
  medical ultrasound examination of breast nodules in the BUSI [49] dataset illustrate
  malignancy degrees. The sonographer can conclude that the breast nodule is malignant
  by synthesizing the ultrasound dynamic images. Consecutive ultrasound dynamic images
  from medical ultrasound examinations of thyroid nodules in the AHJUUI dataset show
  labeled thyroid gland, thyroid nodule, and the echogenicity status of the thyroid
  nodule. Ultrasound physicians can conclude that the thyroid nodules are benign by
  synthesizing the ultrasound dynamic images.


  The experimental data specifically includes two parts: ultrasound dynamic images
  and ultrasound reports. The format of the ultrasound dynamic images data is stored
  in the form of serialized images, and the format of the ultrasound report data is
  mainly stored in the form of unstructured text. It mainly contains 6 organs: mammary
  glands, liver, gallbladder, pancreas, spleen, and kidney.


  2) Baseline Methods: To evaluate the effectiveness, we compare the performance of
  our model with five representatives of visual language pre-training models both
  in dual stream and single stream.


  - VideoBERT [25] extends BERT for joint video-language learning with masked prediction,
  aligning data sans annotations, and enhancing cross-modal understanding and generation.

  - CBT [32] builds on the Bidirectional Transformer with contrastive learning, using
  self-attention for sequence dependency and contrastive strategies for discriminative
  frame representation.

  - HERO [33] was a hierarchical pre-training model with Cross-modal and Temporal
  Transformers for local and broad video context, plus tasks like Video-Subtitle Matching
  for alignment and comprehensive video usage.

  - MMFT-BERT [34] features separate BERTs for modalities fused via a transformer,
  excelling on TVQA with multimodal fusion at the feature level.

  - ActBERT [36] employs self-supervision for refined visual-linguistic interaction,
  using TNT to encode global actions, local objects, and language, deepening visual-textual
  relation modeling.


  The above five methods leverage self-supervised learning and transformers for joint
  video-text representation learning, enhancing multimodal understanding without explicit
  supervision.


  3) Experimental Environments: The experimental environment and the description of
  the use of the toolkit are provided in the experimental setup.


  4) Evaluation of Indicators: Sensitivity, Precision, and F1-Score were used as evaluation
  indicators. Data were calculated as True Positive (TP); True Negative (TN); False
  Positive (FP); and False Negative (FN) as:


  Sensitivity = TP / (TP + FN)


  Precision = TP / (TP + FP)


  F1-Score = 2 × Sensitivity × Precision / (Sensitivity + Precision)


  </block>

  ### B. Experimentation and Analysis of Automatic Annotation of Medical Images

  <block id="11">

  1) Setting: We use 13 as the initial task layer mapping, so that the student model
  and the teacher have 13 layer multimodal encoder for visual-textual. The parameter
  temperature t is fixed to 1, α = 0.2, β = 1, γ = 0.6. The training strategy adopts
  a stochastic gradient descent (SGD) optimizer that drives the quantity and weight
  decay for model optimization, where the momentum coefficient is initially set to
  0.9 and the weight decay coefficient is set to 0.0001. Set the initial learning
  rate to 0.24 and use a learning rate adjustment mechanism based on the function
  proposed in this article. The pre-trained model trains 800 epochs by default.


  2) Pre-Training of Multimodal Distillation Models: The multimodal distillation model
  in relation to the data changes during pre-training and validation shows that the
  text data dimensionality reduction interval in the training process is within the
  range of −0.88 to 0.82, and the image data dimensionality reduction interval during
  training is in the range of −0.51 to 0.52. The weights and biases data during training
  are in the range of 10000 to 3000, while during validation the weights and biases
  data remain in the range of 43400 to 43480.


  After the pre-training of the DMILBERT model, the matching relationship graph between
  the ultrasound image and the text of the ultrasound report is generated, which shows
  the recognition of the named entity and the image region under the conditions that
  the Transformer is set to 3-layer 8-head attention, 6-layer 6-head attention, and
  9-layer 9-head attention. The thickness of the line reflects the degree of association
  between the named entity and the image region. It can be found that the number of
  multi-heads attention improves the model efficiency better than the number of layers,
  and the DMILBERT pre-training model has the best recognition efficiency when the
  Transformer is configured with 9-layer 9-head attention.


  3) Knowledge Distillation Performance Comparison: In order to validate the effectiveness
  of the multimodal knowledge distillation pre-training model proposed in this paper,
  the benchmark method uses a simple BERT pre-training model. The batch size of the
  teacher model in knowledge distillation is set to 32, the maximum number of ultrasound
  serialized images is set to 128, and the maximum number of training rounds is 4.
  The batch size of the student model is set to 32, the maximum number of ultrasound
  serialized images is set to 128, the maximum number of training rounds is 4, the
  distillation temperature T is set to 5, 10, 20, and the equalization coefficients
  between the different losses in the distillation are set to 0.2, 0.5, 0.8.


  The performance of the DILBERT teacher model, DILBERT student model and the basic
  BERT model on the BUSI (Breast Ultrasound Images), CLUST & USSS and AHJUUI datasets
  indicates that the BERT model achieves relatively close accuracy in the BUSI, CLUST
  & USSS and AHJUUI datasets, and with the increase of the number of Transformer layers,
  the accuracy of the BERT model grows slightly, which indicates that the BERT model
  has a better stability. The DILBERT teacher model achieves better results than BERT
  in all three models, and as the number of Transformer layers increases, the DILBERT
  teacher model can achieve higher accuracy than BERT, suggesting that more Transformer
  layers can enhance the effect of pre-training. The DILBERT student model achieves
  the best accuracy in all 3 datasets, indicating that the multimodal knowledge distillation
  proposed in this paper allows the pre-training student model to obtain richer visual,
  verbal cues aggregation and alignment capabilities, and the multimodal knowledge
  distillation embodies a good model enhancement capability.


  4) Pre-Training Model Performance Comparison: The recall performance of the DILBERT
  model proposed in this paper compared with VideoBERT [25], CBT [32], HERO [33],
  MMFT-BERT [34], ActBERT [36] for the pre-training task with K = 10 on BUSI, CLUST
  & USSS and AHJUUI datasets shows that the R-values of the MLM, MDC, MOC, and CMM
  tasks on BUSI for all types of models are lower relative to the CLUST & USSS and
  AHJUUI datasets, indicating that all types of models are affected by the data noise
  and reduce their efficiency. The R-values of the various models in the MLM task
  are relatively high compared to the other tasks, indicating that the language modeling
  task can well help the models to learn the fine-grained relevance of textual information.
  The DILBERT model has the highest R-value for the MLM task for all 3 types of datasets,
  indicating that the positional relationships or descriptive events of the Knowledge
  Graph can effectively help the model to enhance the textual semantic comprehension.
  Except for the DILBERT model, the R-values of various other types of models in the
  MDC task are relatively low compared to other tasks, indicating that the classification
  of named entities in the text of ultrasound reports is challenging. The DILBERT
  model has the highest R-value for the MDC task of the CLUST & USSS and AHJUUI datasets,
  indicating that named entities can effectively enhance the model for the task of
  classifying descriptive text, but is also affected by noisy data. The DILBERT model
  has the highest R-value for the MOC task for the 3 types of datasets, indicating
  that the task is more reliant on the object detector and the region being recognized.
  The DILBERT model has the highest R-value in the CMM task for 3 types of datasets,
  indicating that the model’s learning ability is enhanced by introducing multimodal
  knowledge distillation, which strengthens the multimodal interactions and makes
  it more capable of visual-linguistic alignment, and helps the model to better learn
  the semantic associations between images and texts.


  5) Performance of Pre-Training Models on Automatic Annotation Tasks: The performance
  of the automatically labeled downstream task of the pre-training model for fine-tuning
  on the BUSI, CLUST & USSS and AHJUUI datasets shows that the performance of the
  DILBERT model proposed in this paper is higher than that of the other models on
  all three datasets, indicating that adequate pre-training provides strong support
  for the fine-tuning task, and that the generalization ability of the DILBERT model
  on all types of datasets optimizes the efficiency of parameter computation, enhances
  the performance of classification, strengthens the ability of recognizing the actual
  health status of tissues and organs in ultrasound images, and strengthens multimodal
  interaction between tissues, organs and descriptive diagnostic texts for the task
  of automatic annotation of ultrasound dynamic images.


  6) Analysis of Time Expenditure: In order to explore the time overhead scenario
  of the proposed method, the time consumed for training the proposed model in this
  paper with the comparison models on the BUSI, CLUST & USSS and AHJUUI datasets was
  recorded. All the experimental environments, such as hardware and software, remain
  the same, as do the deep learning tools and parameter settings. The training elapsed
  time of the model designed in this paper in each dataset is higher than that of
  the CBT, HERO, and MMFT-BERT models, close to the ActBERT model, and lower than
  that of the VideoBERT model, and the elapsed time is increased in parallel because
  the size of the AHJUUI dataset is larger than that of the BUSI and CLUST & USSS
  datasets. The time-consuming part of the model mainly involves two parts: the computation
  of the spatio-temporal hybrid attention mechanism and the knowledge distillation.
  Considering its improvement in the robustness of ultrasound dynamic image feature
  extraction and the efficiency of multi-modal model pre-training, the increased time
  overhead is still within an acceptable range, and in general, the model has the
  conditions to be put into practical application scenarios.


  </block>

  ## V. CONCLUSION

  <block id="12">

  In this paper, in order to investigate the relationship between a series of dynamic
  image representations captured after real-time probe movement in ultrasound medical
  examinations and the actual health conditions of tissues and organs, and the semantic
  correlation between the dynamic image sequences and descriptive diagnostic language,
  a pre-training model of multimodal distillation and fusion coding is proposed for
  automated annotation of ultrasound dynamic images. A pre-training model of multimodal
  distillation and fusion coding is used for multimodal feature enhancement learning
  by knowledge distillation of “image area” visual features, “image description” visual
  features and “named entity” linguistic features. Experimental results on multiple
  datasets show that the method proposed in this study can effectively improve the
  effectiveness of feature fusion and can significantly enhance the accuracy of automatic
  annotation of ultrasound dynamic images. The main future work focuses on the following
  three areas:


  1) Research on the knowledge representation of dynamic images. Aiming at the regular
  sequence composition of ultrasound dynamic images and the structured topological
  information of its tissues and organs, we consider adding formal semantic analysis
  to the existing model, in order to realize the knowledge representation of dynamic
  images and improve the interpretability of the model.


  2) The multimodal distillation pre-training model currently used plays a good role
  in automatic annotation of ultrasound dynamic images. However, how the model can
  be applied to automatic annotation of other medical dynamic images such as gastroscopy,
  enteroscopy, laryngoscopy, bronchoscopy, etc. is a direction that needs to be studied
  in the future.


  3) Improvement of multimodal distillation and fusion coding pre-training using deep
  neural networks, e.g., improvement of our model training using a large multimodal
  model, enhancement of multimodal distillation and fusion coding, further improvement
  of feature fusion, and improvement of the accuracy of automatic annotation of ultrasound
  dynamic images.

  </block>'
