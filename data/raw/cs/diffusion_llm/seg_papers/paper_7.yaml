title: 'MatchXML: An Efﬁcient Text-Label Matching Framework for Extreme Multi-Label
  Text Classiﬁcation'
abstract: The eXtreme Multi-label text Classiﬁcation (XMC) refers to training a classiﬁer
  that assigns a text sample with relevant labels from an extremely large-scale label
  set (e.g., millions of labels). We propose MatchXML, an efﬁcient text-label matching
  framework for XMC. We observe that the label embeddings generated from the sparse
  Term Frequency-Inverse Document Frequency (TF–IDF) features have several limitations.
  We thus propose label2vec to effectively train the semantic dense label embeddings
  by the Skip-gram model. The dense label embeddings are then used to build a Hierarchical
  Label Tree by clustering. In ﬁne-tuning the pre-trained encoder Transformer, we
  formulate the multi-label text classiﬁcation as a text-label matching problem in
  a bipartite graph. We then extract the dense text representations from the ﬁne-tuned
  Transformer. Besides the ﬁne-tuned dense text embeddings, we also extract the static
  dense sentence embeddings from a pre-trained Sentence Transformer. Finally, a linear
  ranker is trained by utilizing the sparse TF–IDF features, the ﬁne-tuned dense text
  representations, and static dense sentence features. Experimental results demonstrate
  that MatchXML achieves the state-of-the-art accuracies on ﬁve out of six datasets.
  As for the training speed, MatchXML outperforms the competing methods on all the
  six datasets.
abstract_is_verbatim: true
segmented_markdown: '# MatchXML: An Efﬁcient Text-Label Matching Framework for Extreme
  Multi-Label Text Classiﬁcation


  ## Abstract

  <block id="0">

  The eXtreme Multi-label text Classiﬁcation (XMC) refers to training a classiﬁer
  that assigns a text sample with relevant labels from an extremely large-scale label
  set (e.g., millions of labels). We propose MatchXML, an efﬁcient text-label matching
  framework for XMC. We observe that the label embeddings generated from the sparse
  Term Frequency-Inverse Document Frequency (TF–IDF) features have several limitations.
  We thus propose label2vec to effectively train the semantic dense label embeddings
  by the Skip-gram model. The dense label embeddings are then used to build a Hierarchical
  Label Tree by clustering. In ﬁne-tuning the pre-trained encoder Transformer, we
  formulate the multi-label text classiﬁcation as a text-label matching problem in
  a bipartite graph. We then extract the dense text representations from the ﬁne-tuned
  Transformer. Besides the ﬁne-tuned dense text embeddings, we also extract the static
  dense sentence embeddings from a pre-trained Sentence Transformer. Finally, a linear
  ranker is trained by utilizing the sparse TF–IDF features, the ﬁne-tuned dense text
  representations, and static dense sentence features. Experimental results demonstrate
  that MatchXML achieves the state-of-the-art accuracies on ﬁve out of six datasets.
  As for the training speed, MatchXML outperforms the competing methods on all the
  six datasets.


  I. INTRODUCTION

  The eXtreme Multi-label text Classiﬁcation (XMC) refers to learning a classiﬁer
  that can annotate an input text with the most relevant labels from an extremely
  large-scale label set (e.g., millions of labels). This problem has many real world
  applications, such as labeling a Wikipedia page with relevant tags [1], providing
  a customer query with related products in product search [2], and recommending relevant
  items to a customer in recommendation systems [3].


  To address the issue of the extremely large output space in XMC, the Hierarchical
  Label Tree (HLT) [2] has been proposed to effectively reduce the computational complexity
  from O(L) to O(logL), where L is the number of labels. Taking label embeddings as
  input, an HLT can be constructed by partition algorithms [2], [4] based on the K-means
  clustering. Prior works [2], [4], [5], [6], [7] have applied the Positive Instance
  Feature Aggregation (PIFA) to compute label embeddings, where one label embedding
  is the summation of the TF–IDF features of the text samples when the label is positive.
  However, the label embeddings generated from PIFA have several limitations. First,
  current machine learning algorithms are more efficient to process the data of small
  dense vectors than the large sparse vectors. Second, the TF–IDF features of text
  data, which are required by PIFA to generate the label embeddings, may not be always
  available and thus limits the applications of PIFA. Inspired by the word2vec [8],
  [9] in training word embeddings, we propose label2vec to learn the semantic dense
  label embeddings. We consider a set of labels assigned to a text sample as an unordered
  sequence, where each label can be treated as one word/token, and the Skip-gram model
  [8], [9] is applied to train the embedding for each label. The label2vec approach
  has better generalization than PIFA as it does not require the TF–IDF features.
  Besides, the dense label embeddings have smaller storage size that are more efficient
  to process by the downstream machine learning algorithms. Our experiments demonstrate
  that the dense label embeddings can capture the semantic label relationships and
  generate improved HLTs compared to the sparse label embeddings, leading to improved
  performance in the downstream XMC tasks.


  Most of the early works in XMC [2], [10], [11], [12], [13], [14], [15], [16], [17],
  [18], [19], [20], [21], [22] leverage the statistical Bag-Of-Words (BOW) or Term
  Frequency-Inverse Document Frequency (TF–IDF) features as the text representations
  to train a text classiﬁer. This type of text features is simple, but it can not
  capture the semantic meaning of text corpora due to the ignorance of word order.
  Recent works [5], [6], [23], [24], [25] explore deep learning approaches to learn
  the dense vectors as the text representations. These methods leverage the contextual
  information of words in text corpora to extract the dense text representations,
  leading to improved classiﬁcation accuracies. On the other hand, the recently proposed
  XR-Transformer [7] and CascadeXML [26] have showed that sparse TF–IDF features and
  dense text features are not mutually exclusive to each other, but rather can be
  leveraged together as the text representations to boost the performance. Inspired
  by this strategy, we generate the ﬁnal text representations by taking advantage
  of both sparse TF–IDF features and dense vector features, and we propose a novel
  method to improve the quality of dense vector features for XMC. Specifically, in
  the ﬁne-tuning stage of pre-trained encoder Transformer, we formulate the multi-label
  text classiﬁcation as a text-label matching problem in a bipartite graph. Through
  text-label alignment and label-text alignment in a bipartite graph, the ﬁne-tuned
  Transformer can yield robust and effective dense text representations. Besides the
  dense text representations ﬁne-tuned from the above-mentioned method, we also utilize
  the static dense sentence embeddings extracted from pre-trained Sentence Transformers,
  which are widely used in NLP for the tasks, such as text classiﬁcation, clustering,
  retrieval, and paraphrase detection, etc. Compared with the sparse TF-IDF representations,
  the static dense sentence embeddings can capture the semantic meaning and facilitate
  the downstream applications. In particular, we extract the static sentence embeddings
  from Sentence-T5 [27] and integrate them into our MatchXML. We have found that this
  approach is very effective as shown in our ablation study.


  The remainder of the paper is organized as follows. In Section II, we review the
  related works from the perspectives of extreme classiﬁcation, cross-modal learning
  and contrastive learning. The proposed method MatchXML is presented in Section III,
  where its main components: label2vec, hierarchical label tree, text-label matching,
  and linear ranker are introduced. Experimental results on six benchmark datasets
  are presented in Section IV, with comparisons to other algorithms currently in the
  literature. Conclusions and future work are discussed in Section V.


  </block>

  ## II. Related Works

  <block id="1">

  Extreme Classiﬁcation: A great number of works have been proposed to address the
  extreme classiﬁcation problem [19], [21], [22], [28], [29], [30], [31], [32], [33],
  [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47],
  which can be categorized to One-vs-All approaches, tree-based approaches, embedding-based
  approaches, and deep learning approaches. The One-vs-All approaches, such as PDSparse[13],
  train a binary classiﬁer for each label independently. To speed up the computation,
  these approaches leverage the negative sampling and parallel computing to distribute
  the training over multiple cores or servers. The tree-based approaches, such as
  FastXML [10], train a hierarchical tree structure to divide the label set into small
  groups. These approaches usually have the advantage of fast training and inference.
  The embedding-based approaches, such as SLEEC [11], seek to lower the computational
  cost by projecting the high-dimensional label space into a low-dimensional one.
  However, information loss during the compression process often undermines the classiﬁcation
  accuracy.


  Deep learning approaches leverage the raw text to learn semantic dense text representations
  instead of the statistical TF-IDF features. Recent works (e.g., X-Transformer [5],
  APLC-XLNet [25], LightXML [6]) ﬁne-tune the pre-trained encoder Transformers, such
  as BERT [48], RoBERTa [49] and XLNet [50], to extract the dense text features. Further,
  a clustering structure or a shallow hierarchical tree structure is designed to deal
  with the large output label space rather than the traditional linear classiﬁer layer.
  For example, XR-Transformer [7] proposes a shallow balanced label tree to ﬁne-tune
  the pre-trained encoder Transformer in multiple stages. The dense vectors extracted
  from the last ﬁne-tuning stage and sparse TF-IDF features are leveraged to train
  the ﬁnal classiﬁer. Compared with XR-Transformer, we generate the Hierarchical Label
  Tree by the label embeddings learned from label2vec rather than the TF-IDF features.
  Besides, we formulate the XMC task as a text-label matching problem to ﬁne-tune
  the dense text representations. In addition, we extract the static dense sentence
  embeddings from a pre-trained Sentence Transformer for the classiﬁcation task.


  Cross-Modal Learning: In the setting of text-label matching, we consider the input
  texts (i.e., sentences) as the text modality, while the class labels (i.e., 1, 2,
  3) as another label modality. Therefore, the line of research in cross-modal learning
  is relevant to our text-label matching problem. The cross-modal learning involves
  processing data across different modalities, such as text, image, audio, and video.
  Some typical Image-Text Matching tasks have been well studied in recent years, including
  Image-Text Retrieval [51], [52], Visual Question Answering [53], [54] and Text-to-Image
  Generation [55], [56], [57], [58], [59]. The general framework is to design one
  image encoder and one text encoder to extract the visual representations and textual
  representations, respectively, and then fuse the cross-modal information to capture
  the relationships between them. In contrast to the framework of Image-Text Matching,
  we develop one text encoder for the text data and one embedding layer to extract
  the dense label representations. Furthermore, the relationship between image and
  text in Image-Text Matching usually belongs to a one-to-one mapping, while the relationship
  between text and label in the context of XMC is a many-to-many mapping.


  Contrastive Learning: Another line of research in contrastive learning is also related
  to our proposed method. Recently, self-supervised contrastive learning [60], [61],
  [62] has attracted great attention due to its remarkable performance in visual representation
  learning. Typically, a positive pair of images is constructed from two views of
  the same image, while a negative pair of images is formed from the views of different
  images. Then a contrastive loss is designed to push together the representations
  of positive pairs and push apart the ones of negative pairs. Following the framework
  of self-supervised contrastive learning, supervised contrastive learning [63] constructs
  additional positive pairs by utilizing the label information. The application of
  the supervised contrastive loss can be found in recent works [64], [65], [66], [67]
  to deal with text classiﬁcation. In this paper, we leverage the supervised constrastive
  loss as the training objective for text-label matching, and we develop a novel approach
  to construct the positive and negative text-label pairs for XMC. MACLR [67] is a
  recent work that applies the contrastive learning for the Extreme Zero-Shot Learning,
  and thus is related to our MatchXML. However, there are two main differences between
  these two works. First, the contrastive learning paradigm in MACLR belongs to self-supervised
  contrastive learning, while MatchXML is a supervised contrastive learning method.
  Specifically, MACLR constructs the positive text-text pair, where the latter text
  is a sentence randomly sampled from a long input sentence, while MatchXML constructs
  the positive text-label pair, where the label is one of the class labels of the
  input text. Second, MACLR utilizes the Inverse Cloze Task which is a frequently
  used pre-training task for the sentence encoder, while MatchXML is derived from
  the Cross-Modal learning task.


  </block>

  ## III. Method

  <block id="2">


  </block>

  ### A. Preliminaries

  <block id="3">

  Given a training dataset with N samples {(xi, yi)}_{i=1}^N, where xi denotes text
  sample i, and yi is the ground truth that can be expressed as a label vector with
  binary values of 0 or 1. Let y_{i,l} for l ∈ {1,...,L}, denote the lth element of
  yi, where L is the cardinality of the label set. When y_{i,l} = 1, label l is relevant
  to text i, and otherwise not. In a typical XMC task, number of instances N and number
  of labels L can be at the order of millions or even larger. The objective of XMC
  is to learn a classiﬁer f(x, l) from the training dataset, where the value of f
  indicates the relevance score of text x and label l, with a hope that f can generalize
  well on test dataset {(x_j, y_j)}_{j=1}^{N_t} with a high accuracy.


  The training of MatchXML consists of four steps. In the first step, we train the
  dense label vectors by our proposed label2vec. In the second step, a preliminary
  Hierarchical Label Tree (HLT) is constructed using a Balanced K-means Clustering
  algorithm [4]. In the third step, a pre-trained Transformer model is fine-tuned
  recursively from the top layer to bottom layer through the HLT. Finally, we train
  a linear classiﬁer by utilizing all three text representations: (1) sparse TF-IDF
  text features, (2) the dense text representations extracted from the fine-tuned
  Transformer, and (3) the static dense sentence features extracted from a pre-trained
  Sentence Transformer. As for the inference, the computational cost contains the
  feature extraction of input text from the fine-tuned Transformer and the beam search
  guided by the trained linear classiﬁer through the refined HLT. Thus, the computational
  complexity of MatchXML inference can be expressed as


  O(T1 + k_b d log(L)),


  where T1 denotes the cost of extracting the dense text representation from the text
  encoder, k_b is the size of beam search, d is the dimension of the concatenated
  text representation, and L is the number of labels. The details of MatchXML are
  elaborated as follows.


  </block>

  ### B. label2vec

  <block id="4">

  The Hierarchical Label Tree (HLT) plays a fundamental role in reducing the computational
  cost of XMC, while the high-quality label embeddings is critical to construct an
  HLT that can cluster the semantically similar labels together. In this section,
  we introduce label2vec to train the semantic dense label embeddings for the HLT.
  Note that the training label set {yi}_{i=1}^N contains a large amount of semantic
  information among labels. We therefore treat the positive labels in yi as a label
  sequence, similar to the words/tokens in one sentence in word2vec. We then adopt
  the Skip-gram model to train the label embeddings, which can effectively learn high-quality
  semantic word embeddings from large text corpora. The basic mechanism of the Skip-gram
  model is to predict context words from a target word. The training objective is
  to minimize the following loss function:


  −log σ(w_t^T w_c) − Σ_{i=1}^k E_{z_i ∼ Z_T} [ log σ(−w_t^T w_{z_i}) ],


  where w_t and w_c denote the target word embedding and context word embedding, respectively,
  and z_i is one of the k negative samples. To have the Skip-gram model adapt to the
  label2vec task, we simply make several necessary modiﬁcations as follows. First,
  in word2vec the 2nk training target-context word pairs can be generated by setting
  a context window of size nk, consisting of nk context words before and after the
  target word. A small window size (i.e., nk = 2) tends to have the target word focusing
  more on the nearby context words, while a large window size (i.e., nk = 10) can
  capture the semantic relationship between target word and broad context words. The
  Skip-gram model adopts the strategy of dynamic window size to train word2vec. However,
  in label2vec there is no distance constraint between target label and its context
  labels since they are semantically similar if both labels co-occur in one training
  sample. Therefore, we set the window size nk to the maximum number of labels among
  all training samples. Second, the subsampling technique is leveraged to mitigate
  the imbalance issue between the frequent and rare words in word2vec since the frequent/stop
  words (e.g., “in”, “the”, and “a”) do not provide much semantic information to train
  word representations. In contrast, the frequent labels are usually as important
  as rare labels in XMC to capture the semantic relationships among labels in label2vec.
  Therefore, we do not apply the subsampling to the frequent labels in label2vec.


  </block>

  ### C. Hierarchical Label Tree

  <block id="5">

  Once the dense label vectors W = {w_i}_{i=1}^L are extracted from {yi}_{i=1}^N with
  label2vec, we build a Hierarchical Label Tree (HLT) of depth D from the label vectors
  W by a Balanced K-means Clustering algorithm [4]. In the construction of HLT, the
  link relationships of nodes between two adjacent layers are organized as 2D matrices
  C = {C^{(t)}}_{t=1}^D based on the clustering assignments. Then the ground truth
  label assignment of tth layer Y^{(t)} can be generated by the (t+1)th layer Y^{(t+1)}
  as follows:


  Y^{(t)} = binarize( Y^{(t+1)} C^{(t+1)} ).


  The original ground truth yi corresponds to Y^{(D)} in the bottom layer, and thus
  the ground truth label assignment Y^{(t)} can be inferred from the bottom layer
  to the top layer according to (3). Subsequently, we fine-tune the pre-trained Transformer
  in multiple stages from the top layer to the bottom layer through the HLT.


  </block>

  ### D. Text-Label Matching

  <block id="6">

  In this section, we present our text-label matching framework for XMC. In the fine-tuning
  stage, we consider the multi-label classiﬁcation as a text-label matching problem.
  We model this matching problem in a bipartite graph G(U, V^{(t)}, E), where U and
  V^{(t)} denote a set of text samples and the labels in the tth layer of HLT, respectively,
  and E is a set of edges connecting U and V^{(t)}. If text i has a positive label
  j, edge e_{ij} is created between them. A text node in U can have multiple edges
  connecting it to multiple label nodes in V^{(t)}. Vice versa, a label node in V^{(t)}
  can have multiple edges connecting it to multiple text nodes in U. We fine-tune
  a pre-trained encoder Transformer and the HLT from the top layer to the bottom layer
  in multiple stages. During training, we sample a mini-batch of training data, from
  which the text samples are fed to a text encoder to extract the text representations,
  and the corresponding labels are fed to an embedding layer to extract the label
  representations. We consider the text-label matching problem from two aspects: text-label
  alignment and label-text alignment.


  Text-Label Alignment: In the text-label matching setting, one text sample aligns
  with multiple positive labels and contrasts with negative labels in a mini-batch.
  We construct the set with multiple positive text-label pairs {(z_i, e_p)}, where
  p is a positive label of text i. Following the previous work [7], we also mine the
  hard negative labels (e.g., negative labels with high output scores) to boost the
  performance. We then generate the set with a number of negative text-label pairs
  {(z_i, e_n)}, where n is one of hard negative labels of text i. We utilize the dot
  product (z_i, e_j) as the quantitative metric to measure the alignment of the text-label
  pair. To align the text with labels, we train our model to maximize the alignment
  scores of positive text-label pairs and minimize the ones of negative text-label
  pairs. The loss function of text-label alignment is defined as


  L_{tl} = (1 / N_b) Σ_{i=1}^{N_b} (1 / |P1(i)|) Σ_{p ∈ P1(i)} − log ( exp( (z_i,
  e_p) / τ ) / Σ_{a ∈ A1(i)} exp( (z_i, e_a) / τ ) ),


  where N_b denotes the batch size, P1(i) is the set of indices of positive labels
  related to text i, |P1(i)| is its cardinality, A1(i) is the set of indices of positive
  and negative labels corresponding to text i, and τ ∈ R+ is a scalar temperature
  parameter.


  Label-Text Alignment: We also consider the label-text alignment in a reverse way
  for the text-label matching problem. In the above-mentioned text-label alignment,
  we mine a number of hard negative labels for each text to facilitate the training
  process. On the contrary, if we form the label set by combining all the positive
  labels and hard negative labels within a mini-batch, the computational cost is likely
  to increase notably due to the large cardinality of the label set. To reduce the
  computational cost, we construct the label set only from all the positive labels
  within a mini-batch. Similar to the previous text-label alignment, one label sample
  corresponds to several text samples and contrasts with the remaining text samples
  in the mini-batch. We generate the set with several positive label-text pairs {(e_i,
  z_p)}, where i is a positive label for text p. Otherwise, they form the set with
  a number of negative label-text pairs {(e_i, z_n)}, where i is a negative label
  for text n. To align the label with texts, we train our model to maximize the alignment
  scores of positive label-text pairs and minimize the ones of negative label-text
  pairs. Similarly, the loss function of label-text alignment is


  L_{lt} = (1 / M) Σ_{i=1}^M (1 / |P2(i)|) Σ_{p ∈ P2(i)} − log ( exp( (e_i, z_p) /
  τ ) / Σ_{a ∈ A2(i)} exp( (e_i, z_a) / τ ) ),


  where M is the number of positive labels in the mini-batch, P2(i) is the set of
  indices of positive text samples related to label i, |P2(i)| is its cardinality,
  and A2(i) is the set of indices of text samples within the mini-batch.


  Loss Function: The overall loss function of our text-label matching task is a linear
  combination of the two loss functions defined above


  L = λ L_{tl} + (1 − λ) L_{lt},


  with λ ∈ [0, 1]. Experiments show that the setting of hyperparameter λ has a notable
  impact on the performance of MatchXML, and we thus tune it for different datasets.


  </block>

  ### E. Linear Ranker

  <block id="7">

  Once the multi-stage fine-tuning with (6) is completed, we extract the dense text
  representations from the text encoder. The extracted dense representations are then
  concatenated with the static dense sentence embeddings from the Sentence Transformer
  and the sparse TF-IDF features as the ﬁnal text representations {˜x_i}_{i=1}^N,
  which are used to train a linear ranking model based on XR-LINEAR [4]. Specifically,
  let W^{(t)} denote the learnable parameter matrix of the ranker corresponding to
  the tth layer of HLT, ˆM^{(t)} denote the matrix of sampled labels by the combination
  of the Teacher-Forcing Negatives (TFN) and Matcher-Aware Negatives (MAN), Y^{(t)}
  denote the label assignment at the tth layer of HLT. The linear ranker at the tth
  layer can be optimized as


  arg min_{W^{(t)}} Σ_{ℓ: ˆM^{(t)}_{i,ℓ} ≠ 0} L( Y^{(t)}_{iℓ}, W^{(t)⊤}_ℓ ˜x_i ) +
  α || W^{(t)} ||^2,


  where α is the hyperparameter that balances the classiﬁcation loss and the L2 regularization
  on the parameter matrix W^{(t)}.


  In summary, the training procedure of MatchXML is provided in Algorithm 1.


  Algorithm 1: MatchXML Training.


  ```

  Input: Training dataset {X,Y} = {(x_i,y_i)}_{i=1}^N, TF-IDF features {X̄} = {(x̄_i)}_{i=1}^N,
  static dense sentence embeddings {X̌} = {(x̌_i)}_{i=1}^N, Skip-gram model h, text
  encoder g, the depth of HLT D

  Output: Optimized text encoder g and the hierarchical linear ranker {R^{(t)}}_{t=1}^D


  1: Generate label pairs {(l^k_i, l^k_j)}_{k=1}^K from {Y} = {y_i}_{i=1}^N

  2: for {1,... , # of training epochs} do

  3:   for {1, ... , # of training steps} do

  4:     Sample a mini-batch of label pairs {(l_i,l_j)}

  5:     Update Skip-gram model h to minimize (2)

  6:   end for

  7: end for

  8: Obtain dense label vectors W = {w_i}_{i=1}^L ← h

  9: {C^{(t)}}_{t=1}^D ← Balanced K-means Clustering(W)

  10: Get hierarchical ground truth label assignment {Y^{(t)}}_{t=1}^D by (3)

  11: for {1,... ,D} do

  12:   Initialize label embedding layer E^{(t)} by the Bootstrap

  13:   for {1,... , # of training steps} do

  14:     Sample a mini-batch of training samples {(x_i, y^{(t)}_i)}

  15:     Construct text-label pairs {(z_i, e_j)}

  16:     Construct label-text pairs {(ê_i, ẑ_j)}

  17:     Update Encoder g and Embedding E^{(t)} to minimize (6)

  18:   end for

  19: end for

  20: Obtain dense text features X̂ = {x̂_i}_{i=1}^N ← g(X)

  21: Obtain final text features X̃ = {x̃_i}_{i=1}^N ← Concat(X̂, X̄, X̌)

  22: for {1,... ,D} do

  23:   Train the linear ranker R^{(t)} of the tth layer by (7)

  24: end for

  ```


  </block>

  ## IV. Experiments

  <block id="8">

  We conduct experiments to evaluate the performance of MatchXML on six public datasets
  [68], including EURLex-4K, Wiki10-31K, AmazonCat-13K, Wiki-500K, Amazon-670K, and
  Amazon-3M, which are the same datasets used by XR-Transformer [7]. The statistics
  of these datasets can be found in Table I. It is well-known that the label distribution
  of the XMC datasets follows the power (Zipf’s) law, where most of the probability
  mass is covered by a small fraction of the label set. As for the text distribution,
  each document is categorized by a different number of labels, and this distribution
  doesn’t follow a particular standard form. This can be observed from Fig. 2, where
  the label and text distributions of Amazon-670K and Amazon-3M are provided.


  We consider EURLex-4K, Wiki10-31K, and AmazonCat-13K as medium-scale datasets, while
  Wiki-500K, Amazon-670K, and Amazon-3M as large-scale datasets. We are more interested
  in the performance on large-scale datasets since they are more challenging XMC tasks.


  </block>

  ### A. Evaluation Metrics

  <block id="9">

  The widely used evaluation metrics for XMC are the precision at k (P@k) and ranking
  quality at k (nDCG@k), which are defined as


  P@k = (1 / k) Σ_{l ∈ rank_k(ŷ)} y_l,


  DCG@k = (1 / k) Σ_{l ∈ rank_k(ŷ)} y_l / log(l + 1),


  nDCG@k = DCG@k / (Σ_{l=1}^{min(k, ||y''||)} 1 / log(l+1) ),


  where y ∈ {0,1}^L is the ground truth label, ŷ is the predicted score vector, and
  rank_k(ŷ) returns the k largest indices of ŷ, sorted in descending order.


  For datasets that contain a large percentage of head (popular) labels, high P@k
  or nDCG@k may be achieved by simply predicting well on head labels. For performance
  evaluation on tail (infrequent) labels, the XCM methods are recommended to evaluate
  with respect to the propensity-scored counterparts of the precision P@k and nDCG
  metrics (PSP and PSnDCG), which are defined as


  PSP@k = (1 / k) Σ_{l ∈ rank_k(ŷ)} y_l / p_l,


  PSDCG@k = (1 / k) Σ_{l ∈ rank_k(ŷ)} y_l / (p_l log(l + 1)),


  PSnDCG@k = PSDCG@k / (Σ_{l=1}^k 1 / log(l+1) ),


  where p_l is the propensity score of label l that is used to make metrics unbiased
  with respect to missing labels [68]. For consistency, we use the same setting as
  XR-Transformer [7] for all datasets.


  Following the prior works, we also record the Wall-clock time of our program for
  speed comparison.


  </block>

  ### B. Experimental Settings

  <block id="10">

  We train the dense label embeddings by using the Skip-gram model of the Gensim library,
  which contains an efficient implementation of word2vec as described in the original
  paper [8]. We take the label sequences {y_i}_{i=1}^N of training data as the input
  corpora, and set the dimension of label vector to 100 and number of negative label
  samples to 20. In word2vec, some rare words would be ignored if the frequency is
  less than a certain threshold. We keep all the labels in the label vocabulary regardless
  of the frequency. The settings of the Skip-gram model for the six datasets are listed
  in Table II.


  Following the prior works, we utilize BERT [69] as the major text encoder in our
  experiments. Instead of using the same learning rate for the whole model, we leverage
  the discriminative learning rate [25], [70] to fine-tune our model, which assigns
  different learning rates for the text encoder and the label embedding layer. Following
  XR-Transformer, we use different optimizers AdamW [71] and SparseAdam for the text
  encoder and the label embedding layer, respectively. Since the size of parameters
  in the label embedding layer can be extremely large for large datasets, the SparseAdam
  optimizer is utilized to reduce the GPU memory consumption and improve the training
  speed. Further, prior Transformer-based approaches [5], [6], [25] have shown that
  the longer input text usually improves classiﬁcation accuracy, but leads to more
  expensive computation. However, we find that the classiﬁcation accuracy of MatchXML
  is less sensitive to the length of input text since MatchXML utilizes both dense
  feature vectors extracted from Transformer and the TF-IDF features for classiﬁcation.
  We therefore truncate the input text to a reasonable length to balance the accuracy
  and speed. In the multi-stage fine-tuning process, we only apply the proposed text-label
  matching learning in the last stage, while we keep the original multi-label classiﬁcation
  learning for the other fine-tuning stages. As shown in Table III, we set different
  learning rates for the text encoder and the label embedding layer in each fine-tuning
  stage. There is a three-stage process for fine-tuning the Transformer on five datasets,
  including EURLex-4K, Wiki10-31K, AmazonCat-13K, Amazon-670K, and Amazon-3M, and
  a four-stage process on Wiki-500K. Table IV provides the further details of the
  hyperparameters. We extract the static sentence embeddings from the pre-trained
  Sentence-T5 model [27].


  We compare our MatchXML with 12 state-of-the-art (SOTA) XMC methods: AnnexML [16],
  DiSMEC [15], PfastreXML [12], Parabel [2], eXtremeText [18], Bonsai [20], XML-CNN
  [23], XR-Linear [4], AttentionXML [24], LightXML [6], APLC-XLNet [25], and XR-Transformer
  [7]. For deep learning approaches (XML-CNN, AttentionXML, LightXML, APLC-XLNet,
  XR-Transformer, MatchXML), we list the results of the single model for a fair comparison.
  We also provide the results of ensemble model. The results of the baseline methods
  are cited from the XR-Transformer paper. For parts of the results that are not available
  in XR-Transformer, we reproduce the results using the source code provided by the
  original papers. The original paper of APLC-XLNet has reported the results of another
  version of datasets, which are different from the ones in XR-Transformer. We therefore
  reproduce the results of APLC-XLNet by running the source code on the same datasets
  as XR-Transformer. Our experiments were conducted on a GPU server with 8 Tesla V100
  GPUs and 64 CPUs, which has the same number of Tesla V100 GPUs and CPUs as the AWS
  p3.16xlarge utilized by XR-Transformer.


  </block>

  ### C. Experimental Results

  <block id="11">

  Classification Accuracy: Table V shows the classiﬁcation accuracies of our MatchXML
  and the baseline methods over the six datasets. Overall, MatchXML has achieved state-of-the-art
  results on five out of six datasets. Especially, on three large-scale datasets:
  Wiki-500K, Amazon-670K, and Amazon-3M, the gains are about 1.70%, 1.73% and 1.62%
  in terms of P@1, respectively, over the second best results. Compared with the baseline
  XR-Transformer, MatchXML has a better performance in terms of precision on all the
  six datasets. For AmazonCat-13K, our approach has achieved the second best result,
  with the performance gap of 0.05% compared with LightXML. Note that the number of
  labels for this dataset is not large (about 13K), indicating that it can be handled
  reasonably well by the linear classiﬁer in LightXML, while our hierarchical structure
  is superior when dealing with datasets with extremely large label outputs.


  Results of Ensemble Models: We have the similar ensemble strategy as XR-Transformer.
  That is, three pre-trained text encoders (BERT, RoBERTa, XLNet) are utilized together
  as the ensemble model for three small datasets, including EURLex-4K, Wiki10-31K,
  and AmazonCat-13K; and one text encoder with three different Hierarchical Label
  Trees are formed the ensemble model for three large datasets, including Wiki-500K,
  Amazon-670K, and Amazon-3M. As shown in Table VI, our MatchXML again achieves state-of-the-art
  results on four datasets: Wiki10-31K, Wiki-500K, Amazon-670K, and Amazon-3M in terms
  of three metrics P@1, P@3 and P@5, which is consistent with the results of the single
  model setting. For dataset EURLex-4K, P@1 and P@3 of our approach are the best,
  similar to the single model results, while the P@5 is the second best with a slight
  performance gap of 0.15, compared with the best result. For AmazonCat-13K, P@1 of
  our approach achieves the best result, while P@3 and P@5 are the second best, similar
  to the ones in single model.


  Table VII shows the performances in terms of the ranking metric nDCG@k of our MatchXML
  and the baselines over the six datasets. Similarly, our MatchXML achieves state-of-the-art
  results on all the six datasets. For the three medium-scale datasets, EURLex-4K,
  Wiki10-31K, AmazonCat-13K, the performance gains of nDCG@1 are about 0.65%, 1.0%
  and 0.12% over the second best results, respectively. For the three large-scale
  datasets: Wiki-500K, Amazon-670K and Amazon-3M, the gains are about 1.23%, 1.62%
  and 1.66% over the second best results, respectively.


  Results of Propensity Scored Precision: We compute the propensity scored precision
  (PSP@k) to measure the performance of MatchXML on tail labels. The results of the
  baselines: PfastreXML and Parabel are cited from the ofﬁcial website. The results
  reported in the XR-Transformer paper are computed using a different version of source
  code. We reproduce the results of XR-Transformer and compute the PSP@k using the
  ofﬁcial source code. As shown in Table VIII, our MatchXML again achieves state-of-the-art
  results on two out of three large datasets Wiki-500K and Amazon-670K in terms of
  three metrics PSP@1, PSP@3 and PSP@5. For Amazon-3M, our approach has achieved the
  second best performance. Note that Parabel has developed speciﬁc techniques to boost
  the performance on tail labels, and thus has the best performance on tail labels
  of Amazon-3M. However, as shown in Table V, the performance of Parabel on all the
  labels is about 6% lower than our approach.


  Computation Cost: Table IX reports the training costs of our MatchXML and other
  deep learning based approaches. The baseline results of training time are cited
  from XR-Transformer. For the unavailable training time of a single model, we calculate
  it by dividing the training time of ensemble model by the number of models in the
  ensemble. In XR-Transformer, 13.2 hour is the reported training time of the ensemble
  of three models for AmazonCat-13K. We have checked the sequence length (which is
  256) and the number of training steps (which is 45,000). We believe this cost should
  be the training time of single model. Overall, our approach has shown the fastest
  training speed on all the six datasets. We fine-tune the text encoder in three stages
  from the top layer to the bottom layer through the HLT. Furthermore, we leverage
  several training techniques, such as discriminative learning rate, small batch size
  and less training steps, to improve the convergence rate of our approach. Our MatchXML
  has the same strategy for inference as XR-Transformer. The inference time on six
  datasets can be found in Appendix A.4.2 of XR-Transformer [7].


  </block>

  ### D. Ablation Study

  <block id="12">

  The framework of our MatchXML follows the training procedure as the baseline XR-Transformer,
  including the construction of HLT, fine-tuning the encoder Transformer from the
  top to bottom layers through the HLT, and training a linear classiﬁer. Besides,
  we have proposed three novel techniques to boost the performance, namely label2vec
  to learn the dense label embeddings for the HLT construction, text-label matching
  for fine-tuning the encoder Transformer, and extraction of static dense text embeddings
  from pre-trained Sentence Transformer. In the ablation study, we set up our technical
  contribution one by one and report the experimental results to show the effectiveness
  of each component. The performance of our base model is comparable to or slightly
  better than the baseline XR-Transformer, since we have leveraged some techniques
  to speed up the training.


  Performance of label2vec: Table XII reports the performance comparison of label2vec
  (number 2) and TF-IDF (number 1) in terms of precision for the downstream XMC tasks.
  On the small datasets, e.g., EURLex-4K, Wiki10-31K and AmazonCat-13K, the performances
  of label embeddings from label2vec are comparable to the ones from TF-IDF features.
  However, on the large datasets, e.g., Wiki-500K, Amazon-670K and Amazon-3M, label2vec
  outperforms TF-IDF, indicating that a large training corpus is essential to learn
  high-quality dense label embeddings. Our experimental results show that label2vec
  is more effective than TF-IDF to utilize the large-scale datasets.


  Table X reports the training time of label2vec on the six datasets. The training
  of label2vec is highly efficient on five of them, including EURLex-4K, Wiki10-31K,
  AmazonCat-13K, Wiki-500K and Amazon-670K, as the cost is less than 0.3 hours. The
  training time on Amazon-3M is about 3.6 hours, which is the result of large amount
  of training label pairs. As shown in Table I, the number of instances N_train and
  the average number of positive labels per instance L are the two factors that determine
  the size of training corpus. Note that we do not add the training time of label2vec
  into the classiﬁcation task since we consider the label2vec task as the preprocessing
  step for the downstream tasks.


  Table XI compares the sizes of label embedding from label2vec and TF-IDF. The dense
  label vectors have much smaller size than that of the sparse TF-IDF label representations.
  Especially, on the large dataset, such as Wiki-500K, the size of label embeddings
  can be reduced by 35× (from 7,109.2MB to 200.4MB), which beneﬁts the construction
  of HLT significantly.


  Performance of Text-Label Matching: Table XII reports the performance of our text-label
  matching (number 3) on the six datasets. The baseline objective is the weighted
  squared hinge loss [7] (number 2). Our text-label matching approach outperforms
  the baseline method on ﬁve out of six datasets, including EURLex-4K, AmazonCat-13K,
  Wiki-500K, Amazon-670K and Amazon-3M. For Wiki10-31K, the metric P@1 is still better
  than the baseline, while P@3 and P@5 are slightly worse. On the three large-scale
  datasets, the text-label matching has achieved the largest gain of about 0.91% on
  Amazon-670K, while the small gain of about 0.16% on Amazon-3M.


  Performance of Static Sentence Embedding: Table XII also reports the performance
  of static dense sentence embedding (number 4) on the six datasets. The technique
  has achieved performance gains in 16 out of 18 metrics over the six datasets, with
  two performance drops of P@3 and P@5 on EURLex-4K. On the three large-scale datasets:
  Wiki-500K, Amazon-670K and Amazon-3M, the performance gains in P@1 are 0.66%, 0.39%
  and 0.95%, respectively. There are three types of text features in our proposed
  MatchXML: sparse TF-IDF features, dense text features fine-tuned from pre-trained
  Transformer, and the static dense sentence embeddings extracted from Sentence-T5.
  The sparse TF-IDF features contains the global statistical information of input
  text, but it does not capture the semantic information. The dense text features
  fine-tuned from pre-trained Transformer are likely to lose parts of textual information
  due to the truncation operation (i.e., context window size of 512 tokens), while
  the static dense sentence embeddings can support much longer text sequence than
  the fine-tuned text embeddings from the encoder Transformer. Therefore, the static
  dense sentence embeddings can be considered as an effective complement to the sparse
  TF-IDF features and dense text features fine-tuned from pre-trained Transformer.
  As shown in Table XII (number 4), including the static dense sentence embeddings
  boosts the performance of MatchXML consistently over the sparse TF-IDF baselines
  and the fine-tuned dense text feature baselines.


  </block>

  ## V. Conclusion

  <block id="13">

  This paper proposes MatchXML, a novel text-label matching framework, for the task
  of XMC. We introduce label2vec to train the dense label embeddings to construct
  the Hierarchical Label Tree, where the dense label vectors have shown superior performance
  over the sparse TF-IDF label representations. In the fine-tuning stage of MatchXML,
  we formulate the multi-label text classiﬁcation as the text-label matching problem
  within a mini-batch, leading to robust and effective dense text representations
  for XMC. In addition, we extract the static sentence embeddings from the pre-trained
  Sentence Transformer and incorporate them into our MatchXML to boost the performance
  further. Empirical study has demonstrated the superior performance of MatchXML in
  terms of classiﬁcation accuracy and training speed over six benchmark datasets.
  It is worthy mentioning that although we propose MatchXML in the context of text
  classiﬁcation, our framework is general and can be extended readily to other modalities
  for XMC, including image, audio, and video, etc. as long as a modality-specific
  encoder is available. The training of MatchXML consists of four stages: training
  of label2vec, construction of HLT, fine-tuning the text encoder, and training a
  linear classiﬁer. As of future work, we plan to explore an end-to-end training approach
  to improve the performance of XMC further.

  </block>'
