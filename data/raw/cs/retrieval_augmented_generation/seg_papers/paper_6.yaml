title: Improving Deep Assertion Generation via Fine-Tuning Retrieval-Augmented Pre-trained
  Language Models
abstract: Unit testing validates the correctness of the units of the software system
  under test and serves as the cornerstone in improving software quality and reliability.
  To reduce manual efforts in writing unit tests, some techniques have been proposed
  to generate test assertions automatically, including deep learning (DL)-based, retrieval-based,
  and integration-based ones. Among them, recent integration-based approaches inherit
  from both DL-based and retrieval-based approaches and are considered state-of-the-art.
  Despite being promising, such integration-based approaches suffer from inherent
  limitations, such as retrieving assertions with lexical matching while ignoring
  meaningful code semantics, and generating assertions with a limited training corpus.
  In this paper, we propose a novel Retrieval-Augmented Deep Assertion Generation
  approach, namely RetriGen, based on a hybrid assertion retriever and a pre-trained
  language model (PLM)-based assertion generator. Given a focal-test, RetriGen first
  builds a hybrid assertion retriever to search for the most relevant test-assert
  pair from external codebases. The retrieval process takes both lexical similarity
  and semantical similarity into account via a token-based and an embedding-based
  retriever, respectively. RetriGen then treats assertion generation as a sequence-to-sequence
  task and designs a PLM-based assertion generator to predict a correct assertion
  with historical test-assert pairs and the retrieved external assertion. Although
  our concept is general and can be adapted to various off-the-shelf encoder-decoder
  PLMs, we implement RetriGen to facilitate assertion generation based on the recent
  CodeT5 model. We conduct extensive experiments to evaluate RetriGen against six
  state-of-the-art approaches across two large-scale datasets and two metrics. The
  experimental results demonstrate that RetriGen achieves 57.66% and 73.24% in terms
  of accuracy and CodeBLEU, outperforming all baselines with an average improvement
  of 50.66% and 14.14%, respectively. Furthermore, RetriGen generates 1598 and 1818
  unique correct assertions that all baselines fail to produce, 3.71X and 4.58X more
  than the most recent approach EditAS. We also demonstrate that adopting other PLMs
  can provide substantial advancement, e.g., four additionally-utilized PLMs outperform
  EditAS by 7.91%∼12.70% accuracy improvement, indicating the generalizability of
  RetriGen. Overall, our study highlights the promising future of fine-tuning off-the-shelf
  PLMs to generate accurate assertions by incorporating external knowledge sources.
abstract_is_verbatim: true
segmented_markdown: '# Improving Deep Assertion Generation via Fine-Tuning Retrieval-Augmented
  Pre-trained Language Models


  ## Abstract

  <block id="0">

  Unit testing validates the correctness of the units of the software system under
  test and serves as the cornerstone in improving software quality and reliability.
  To reduce manual efforts in writing unit tests, some techniques have been proposed
  to generate test assertions automatically, including deep learning (DL)-based, retrieval-based,
  and integration-based ones. Among them, recent integration-based approaches inherit
  from both DL-based and retrieval-based approaches and are considered state-of-the-art.
  Despite being promising, such integration-based approaches suffer from inherent
  limitations, such as retrieving assertions with lexical matching while ignoring
  meaningful code semantics, and generating assertions with a limited training corpus.
  In this paper, we propose a novel Retrieval-Augmented Deep Assertion Generation
  approach, namely RetriGen, based on a hybrid assertion retriever and a pre-trained
  language model (PLM)-based assertion generator. Given a focal-test, RetriGen first
  builds a hybrid assertion retriever to search for the most relevant test-assert
  pair from external codebases. The retrieval process takes both lexical similarity
  and semantical similarity into account via a token-based and an embedding-based
  retriever, respectively. RetriGen then treats assertion generation as a sequence-to-sequence
  task and designs a PLM-based assertion generator to predict a correct assertion
  with historical test-assert pairs and the retrieved external assertion. Although
  our concept is general and can be adapted to various off-the-shelf encoder-decoder
  PLMs, we implement RetriGen to facilitate assertion generation based on the recent
  CodeT5 model. We conduct extensive experiments to evaluate RetriGen against six
  state-of-the-art approaches across two large-scale datasets and two metrics. The
  experimental results demonstrate that RetriGen achieves 57.66% and 73.24% in terms
  of accuracy and CodeBLEU, outperforming all baselines with an average improvement
  of 50.66% and 14.14%, respectively. Furthermore, RetriGen generates 1598 and 1818
  unique correct assertions that all baselines fail to produce, 3.71X and 4.58X more
  than the most recent approach EditAS. We also demonstrate that adopting other PLMs
  can provide substantial advancement, e.g., four additionally-utilized PLMs outperform
  EditAS by 7.91%∼12.70% accuracy improvement, indicating the generalizability of
  RetriGen. Overall, our study highlights the promising future of fine-tuning off-the-shelf
  PLMs to generate accurate assertions by incorporating external knowledge sources.


  </block>

  ## 1 INTRODUCTION

  <block id="1">

  Unit testing has become a pivotal and standard practice in software development
  and maintenance, serving as a fundamental phase in improving software quality and
  reliability [11, 29]. This practice involves writing unit tests to ensure that individual
  components (e.g., methods, classes, and modules) perform as per their designated
  specifications and usage requirements. Unlike integration and system testing [6],
  which evaluate the system’s overall functionality, unit testing is dedicated to
  ensuring that individual components of the system operate as intended by the developer,
  enabling early detection and diagnosis of issues during software development [43].
  Besides, well-designed unit tests enhance the quality of production code, minimize
  the costs of software failures, and facilitate debugging and maintenance processes
  [12, 20, 39].


  Despite the significant benefits of unit testing, it is non-trivial and time-consuming
  to construct effective unit tests manually. For example, as shown in a prior report
  [9], software developers usually spend more than 15% of their time on writing unit
  test cases. Thus, a substantial amount of research has been dedicated to automated
  unit test generation, such as Randoop [35] and EvoSuite [15]. A typical unit test
  usually consists of two key components: (1) the test prefix involving a sequence
  of statements that configure the unit under test to reach a particular state, and
  (2) the test oracle containing an assertion that defines the expected outcome in
  that state [11]. Nevertheless, these test generation tools often focus on creating
  tests that achieve high coverage, while struggling to understand the intended program
  behavior and produce meaningful assertions. For instance, Almasi et al. [5] conduct
  an industrial evaluation of unit tests generated by EvoSuite, highlighting that
  assertions in manually written tests, in contrast to automatically generated ones,
  are more meaningful and practical.


  To tackle the assertion problem suffered by existing unit test generation tools,
  an increasing number of assertion generation (AG) techniques [45, 55, 59] have been
  proposed. Such AG techniques can be categorized into three groups: deep learning
  (DL)-based, retrieval-based, and integration-based ones. In particular, DL-based
  AG techniques (e.g., ATLAS [55]) handle the assertion generation problem as a neural
  machine translation (NMT) task with sequence-to-sequence learning. For example,
  ATLAS [55] takes a focal method (i.e., the method under test) and its test prefix
  as input to generate an assertion from scratch. For consistency with prior research
  [45], we refer to the input as focal-test, and the input-output pair as a test-assert
  pair. However, DL-based AG techniques are restricted by the quality and amount of
  historical test-assert pairs for training, e.g., a small corpus with only 156,760
  samples for ATLAS. In contrast to DL-based AG techniques, given an input focal-test,
  retrieval-based AG techniques (e.g., IRar, RAHadapt and RANNadapt [59]) retrieve
  the most similar focal-test from a codebase and adapt its assertion to obtain the
  desired outcome. However, retrieval-based AG techniques face difficulties in retrieving
  accurate assertions based on only lexical similarity, which is sensitive to the
  choice of the identifier naming of source code while ignoring the meaningful code
  semantics.


  Recently, integration-based AG techniques (e.g., Integration [59] and EditAS [45])
  leverage both DL-based and retrieval-based approaches as basic components to retrieve
  or generate assertions. For example, Integration first utilizes IRar to retrieve
  similar assertions, and use ATLAS to generate new assertions from scratch for those
  deemed appropriate. Meanwhile, EditAS trains a neural model to generate edit actions
  for similar assertions retrieved by IRar. Despite their impressive performance,
  integration-based AG techniques still inherit the limitations of its two components,
  i.e., retrieving assertions by lexical similarity while overlooking code semantics,
  and generating assertions with a small training corpus.


  In this paper, we propose a novel retrieval-augmented AG approach called RetriGen
  equipped with a hybrid assertion retriever and a pre-trained language model (PLM)-based
  assertion generator to address limitations of prior work. Our work is inspired by
  the opportunity to integrate the well-known plastic surgery hypothesis [7] with
  recent PLMs [52] in the field of assertion generation. To this end, we automate
  the plastic surgery hypothesis by fine-tuning retrieval-augmented PLMs, i.e., retrieving
  similar assertions from open-source projects to assist in fine-tuning PLMs for new
  assertion generation. Particularly, given a focal-test, RetriGen builds a hybrid
  retriever to jointly search for the most relevant focal-test and its assertion from
  external codebases. The hybrid retriever consists of a sparse token-based retriever
  and a dense embedding-based retriever to consider both the lexical and semantic
  similarity of test-assert pairs in a unified manner. RetriGen then uses off-the-shelf
  PLMs as a backbone of the generator to perform the assertion generation task by
  fine-tuning it with the focal-test and the retrieved external assertion as input.
  RetriGen is conceptually generalizable to various encoder-decoder PLMs, and we implement
  RetriGen on top of the recent code-aware CodeT5 model. While the retrieval-augmented
  generation pipeline has been explored in previous code-related studies [34, 37,
  51], we are the first to investigate its effectiveness for assertion generation
  by utilizing a hybrid retriever to fine-tune off-the-shelf PLMs with external knowledge
  sources. The distinctions between RetriGen and previous AG approaches mainly lie
  in both the retriever and the generator. First, prior work utilizes an assertion
  retriever (e.g., Jaccard similarity [45]) based on lexical matching, while RetriGen
  builds a hybrid retriever to search for relevant assertions jointly, demonstrating
  superiority over a single retriever. Besides, prior work trains an assertion generator
  with a basic encoder-decoder model (e.g., RNNs [55]) from a limited number of labeled
  training data, while RetriGen is built upon off-the-shelf language models, which
  are pre-trained from various open-source projects in the wild to obtain general
  knowledge about programming language, thus generating optimal vector representations
  for unit tests.


  We select six state-of-the-art AG approaches as baselines, including one DL-based
  approach (i.e., ATLAS), three retrieval-based approaches (i.e., IRar, RAHadapt,
  and RANNadapt), two integration-based approaches (i.e., Integration and its most
  recent follow-up EditAS). We conduct extensive experiments to compare RetriGen with
  these baselines on two widely adopted datasets Data_old and Data_new in terms of
  both accuracy and CodeBLEU. The experimental results show that RetriGen outperforms
  all baselines across all datasets and metrics, with average accuracy and CodeBLEU
  of 57.66% and 73.24%, setting new records in the AG field. The average improvement
  against these baselines in accuracy and CodeBLEU score reached 50.66% and 14.14%
  on the two datasets. Besides, RetriGen generates 1598 and 1818 unique assertions
  that all baselines fail to predict, significantly improving the most recent approach
  EditAS by 3.71X and 4.58X, respectively, demonstrating its superior complementarity
  with existing approaches. Furthermore, We further explore the influence of each
  component and observe that all components positively contribute to the performance
  of RetriGen, e.g., an accuracy improvement of 7.73% brought by the hybrid retriever.
  Finally, we implement RetriGen with CodeBERT, GraphCodeBERT, UniXcoder and CodeGPT,
  and find these variants achieve an accuracy of 57.69%∼60.25% and 46.96%∼48.58% on
  Data_old and Data_new, outperforming the most recent approach EditAS by 7.91%∼12.70%
  and 5.86%∼9.51%, respectively. The results show that RetriGen is universal and is
  effectively adapted to different PLMs with sustaining advancements, highlighting
  the applicability and generalizability of RetriGen in practice.


  To sum up, the contributions of this paper are as follows:

  - We introduce a generic yet effective retrieval-augmented assertion generation
  paradigm in the unit testing scenario with a hybrid assertion retriever and a PLM-based
  assertion generator. The framework is generic and can be integrated with different
  encoder-decoder PLMs.

  - We propose RetriGen, which utilizes a sparse token-based retriever and a dense
  embedding-based retriever to jointly search for the relevant assertion based on
  both lexical and semantic matching. We adopt a code-aware PLM CodeT5 as the foundation
  model for the assertion generator. To the best of our knowledge, RetriGen is the
  first attempt to fine-tune recent PLMs with the advance of external codebases for
  the crucial assertion generation task.

  - We extensively evaluate RetriGen against six prior AG approaches on two datasets
  and two metrics. The experimental results demonstrate that RetriGen significantly
  outperforms all baselines, with an average accuracy improvement of 58.81% and 42.51%
  on Data_old and Data_new.

  - We release the relevant materials in our experiment to facilitate follow-up AG
  studies at the repository, including datasets, scripts, models, and generated assertions.


  </block>

  ## 2 BACKGROUND AND RELATED WORK

  <block id="2">


  </block>

  ### 2.1 Deep Assertion Generation

  <block id="3">

  With the success of deep learning (DL), researchers have increasingly been utilizing
  advanced DL techniques to automate a variety of software engineering tasks [54,
  58, 62]. For example, Watson et al. [55] propose ATLAS, the first DL-based assertion
  generation approach that utilizes deep neural networks to learn correct assertions
  from existing test-assert pairs. Yu et al. [59] propose two retrieval-based approaches
  to generate assertions by searching for similar assertions given a focal-test, namely
  IR-based assertion retrieval (IRar) and retrieved-assertion adaptation (RAAadapt).
  IRar retrieves the assertion with the highest similarity to the given focal-test
  using measures like Jaccard similarity. As IRar may not always retrieve completely
  accurate assertions, RAAadapt attempts to revise the retrieved assertions by replacing
  tokens within them. Two strategies are proposed for determining replacement values,
  i.e., a heuristic-based approach RAHadapt and a neural network-based approach RANNadapt.
  Furthermore, Yu et al. [59] propose an integration-based approach called Integration
  by integrating IR and DL techniques. Building on Integration, Sun et al. [45] introduce
  EditAS by searching for a similar focal-test, which is then modified by a neural
  model. Unlike prior AG studies retrieving relevant assertions [59] with token matching
  or training generator from scratch [55] with historical test-assert pairs, our work
  attempts to utilize PLMs as an assertion generator with the help of a hybrid assertion
  retriever for more effective retrieval and generation.


  The literature has also witnessed several studies [32, 33, 48], exploring the use
  of PLMs like T5 [40] in the field of assertion generation. For instance, Mastropaolo
  et al. [32] pre-train a T5 model and fine-tune it on four code-related tasks: bug-fixing,
  mutant generation, assertion generation, and code summarization. These studies typically
  pre-train language models from scratch and fine-tune them on multiple downstream
  tasks. In contrast, our work attempts to propose a specific AG approach by leveraging
  off-the-shelf PLMs. Nashid et al. [34] propose CEDAR, a large language model (LLM)-based
  approach to apply retrieval-based demonstration selection strategy for program repair
  and assertion generation. RetriGen and CEDAR are fundamentally distinct regarding
  their learning paradigms, retrievers, and generators. First, CEDAR utilizes few-shot
  learning with prompt engineering, while RetriGen leverages fine-tuning with augmented
  inputs. Second, CEDAR utilizes a single retriever, while RetriGen builds a hybrid
  retriever to identify relevant assertions jointly, indicating superiority over a
  single retriever. Third, CEDAR queries APIs from a black-box LLM Codex with 12 billion
  parameters, whereas RetriGen fine-tunes an open-source PLM CodeT5 with only 220
  million parameters. Recently, Zhang et al. [67] conduct an empirical study to explore
  the potential of several PLMs for generating assertions in a fine-tuning scenario.


  </block>

  ### 2.2 Pre-trained Language Model

  <block id="4">

  Recently, researchers have explored the capabilities of PLMs to revolutionize various
  code-related tasks [13, 64], such as code review [28, 49], test generation [8, 17,
  25, 42, 46, 53, 57, 61] and program repair [56, 63, 66].


  There exist three main categories based on model architectures. (1) Encoder-only
  PLMs, e.g., CodeBERT [14], train the encoder part of the Transformer with masked
  language modeling, thus suitable for code understanding. (2) Decoder-only PLMs,
  e.g., CodeGPT [31], train the decoder part of the Transformer with unidirectional
  language modeling, thus suitable for auto-regressive generation. (3) Encoder-decoder
  PLMs, e.g., CodeT5 [52], train both encoder and decoder parts of the Transformer
  with denoising objectives, thus suitable for sequence-to-sequence generation.


  In our work, the foundation model selection of RetriGen is limited to encoder-decoder
  PLMs, as assertions are generated in a sequence-to-sequence learning manner. Following
  prior work [16, 51, 65, 69], we consider CodeT5, a generic code-aware language model
  that is pre-trained on a large code corpus, achieving state-of-the-art performance
  in both code understanding and generation tasks. CodeT5 is the most popular encoder-decoder
  PLM that is adopted by previous fine-tuning-based sequence-to-sequence code generation
  tasks [50, 60, 70]. Besides, CodeT5 is trained with CodeSearchNet [31] without test
  code snippets, thus avoiding the data leakage issue.


  </block>

  ### 2.3 Information Retrieval for SE

  <block id="5">

  Information Retrieval (IR) refers to the process of identifying relevant information
  from a large collection of data. IR has been extensively applied to a range of code-related
  tasks, such as fault localization [10] and test case prioritization [38]. Such approaches
  search for the object that best matches the given query from the database based
  on different similarity coefficients, such as Jaccard similarity. Besides, the literature
  has seen an increasing number of studies performing generation tasks with the retrieval-augmented
  paradigm, such as code repair [34, 51] and code summarization [27, 37]. This paradigm
  improves the quality of generated results by grounding the model with external knowledge
  sources to supplement the PLM’s internal representation of information [26]. In
  our work, inspired by the intuition of retrieval-augmented generation in the PLM
  domain, we focus on the assertion generation problem and propose RetriGen to fine-tune
  a PLM-based assertion generator with a hybrid assertion retriever.


  </block>

  ## 3 APPROACH

  <block id="6">

  The framework overview of RetriGen consists of three phases. In the assertion retrieval
  phase, RetriGen identifies a similar assertion from the external codebase based
  on lexical and semantical similarity calculated by a token-based retriever and an
  embedding-based retriever. In the assertion generator training phase, RetriGen is
  first pre-trained with millions of code snippets from open-source projects and then
  fine-tuned with the retrieval-augmented labeled pairs, i.e., the focal-test and
  the retrieved assertion as input and the correct assertion as output. In the assertion
  inference phase, after the generator is well trained, RetriGen leverages the beam
  search strategy to generate a ranked list of candidate assertions, and return the
  one with the highest probability of being correct.


  </block>

  ### 3.1 Problem Definition

  <block id="7">

  Similar to the DL-based approach ATLAS, RetriGen regards assertion generation as
  an NMT task based on the encoder-decoder Transformer architecture. Suppose D = (FTi,
  Ai) |D|_{i=1} be a unit testing dataset consisting of |D| test-assert pairs, where
  FTi and Ai are the i-th focal-test and the corresponding assertion, respectively.
  The sequence-to-sequence assertion generator attempts to predict Ai from FTi in
  an auto-regressive manner, which is formally defined as follows:


  Definition 1. Deep Assertion Generation.

  Given a focal-test input FTi = [ft1, · · ·, ftm] with m code tokens and an assertion
  output Ai = [a1, . . . , an] with n code tokens, the problem of assertion generation
  is defined as maximizing the conditional probability, i.e., the likelihood of Ai
  being the correct assertion:

  Pθ(Ai | FTi) = ∏_{j=1}^{n} Pθ(aj | a1, · · ·, a_{j-1}; ft1, · · ·, ftm)


  However, different from ATLAS generating new assertions from scratch, RetriGen takes
  the focal-test and an additional retrieved assertion as input and the correct assertion
  as output. Thus, on top of Definition 1, suppose C = (FT''_{j}, A''_{j}) |C|_{j=1}
  be an external codebase containing a large collection of historical test-assert
  pairs, where FT''_{j} and A''_{j} denote the j-th previous focal-test, and the corresponding
  assertion. Then, with the codebase C, the retrieval-augmented deep assertion generation
  can be defined as follows:


  Definition 2. Retrieval-Augmented Deep Assertion Generation.

  Given a focal-test FTi in D, the retriever searches for the most relevant focal-test
  FT''_{j} from the codebase C, and its assertion A''_{j} = [a''_{1}, · · ·, a''_{z}]
  with z code tokens. Then the original focal-test input FTi is augmented with the
  retrieved assertion to form a new input sequence \^{FTi} = FTi ⊕ A''_{j}, where
  ⊕ denotes the concatenation operation. Finally, the assertion generator attempts
  to generate Ai from \^{FTi} by learning the following probability:

  Pθ(Ai | \^{FTi}) = ∏_{j=1}^{n} Pθ(aj | a1, · · ·, a_{j-1}; FTi (Original); a''_{1},
  · · ·, a''_{z} (Augmented))


  </block>

  ### 3.2 Hybrid Assertion Retrieval Component

  <block id="8">

  RetriGen utilizes a hybrid strategy that integrates a token-based retriever and
  an embedding-based retriever, allowing it to account for both the lexical and semantic
  similarities of test-assert pairs.


  #### 3.2.1 Token-based Retriever.

  RetriGen utilizes the sparse strategy IR as the token-based retriever to identify
  an assertion that closely resembles the query focal-test through lexical matching,
  which has been proven to be effective in previous AG studies [45, 59]. In particular,
  RetriGen tokenizes all focal-tests in both the dataset D and the codebase C, and
  eliminates duplicate tokens to enhance the efficiency of the retrieval process.
  Given a query focal-test FTi in D, RetriGen computes the lexical similarity between
  FTi and all focal-tests in C using the Jaccard coefficient as the similarity measure.
  The Jaccard coefficient is a commonly employed metric for assessing the similarity
  between two sparse vector representations by considering their overlapping and unique
  tokens. The calculation of Jaccard similarity is defined as:

  Jac(FTi, FTj) = |S(FTi) ∩ S(FTj)| / |S(FTi) ∪ S(FTj)|

  where S(FTi) and S(FTj) represent the sets of code tokens for the two focal-tests
  FTi and FTj, respectively.


  #### 3.2.2 Embedding-based Retriever.

  RetriGen utilizes the pre-trained CodeLlama [41] as the embedding-based retriever,
  to identify the most similar assertion based on semantic similarity. CodeLlama is
  an advanced language model pre-trained with common programming languages that support
  code understanding and generation tasks. Unlike previous studies training the dense
  retriever from scratch [24], we directly employ the checkpoint of CodeLlama from
  Hugging Face without any fine-tuning in our work as it is already trained with a
  mass of testing code to get meaningful vector embeddings for the query focal-test
  in the unit testing scenario.


  In particular, RetriGen first splits the source code of the focal-test into a sequence
  of code tokens and utilizes CodeLlama to convert these tokenized elements into vector
  representations. RetriGen then computes the Cosine similarity between the embeddings
  of two focal-tests to assess their semantic relevance. Cosine similarity has been
  widely employed in prior research for measuring the semantic relationship between
  two dense vectors [36]. It is calculated by taking the cosine of the angle between
  two vectors, which is the dot product of the vectors divided by the product of their
  magnitudes. The Cosine similarity calculation is:

  Cos(FTi, FTj) = FTi · FTj / (||FTi|| ||FTj||)

  where FTi and FTj represent the embeddings of focal-tests FTi and FTj.


  #### 3.2.3 Hybrid Retriever.

  Our sparse token-based retriever primarily relies on identifier naming in the source
  code, making it more sensitive to lexical choices rather than the underlying code
  semantics. In contrast, our dense embedding-based retriever focuses more on the
  semantic representation of the code rather than its literal syntax. Thus, to incorporate
  both lexical and semantic information, we adopt a hybrid approach that integrates
  the two retrievers in a unified manner. The combined similarity score between two
  focal-tests is computed as:

  Sim(FTi, FTj) = Jac(FTi, FTj) + λ Cos(FTi, FTj)

  where λ serves as a weighting factor to balance the contributions of the two retrievers,
  with its default value set to 1. Based on this combined similarity score, we select
  the top-1 relevant test-assert pair (FT''_{i}, A''_{i}) to guide the assertion generator
  for generating assertions.


  </block>

  ### 3.3 Assertion Generation Component

  <block id="9">

  RetriGen utilizes CodeT5 as the foundation model to generate the assertion Ai, leveraging
  both the focal-test FTi and the externally retrieved assertion A''_{j} as input.
  The assertion generation component is general to a mass of existing PLMs, and we
  select CodeT5 as it is a code-aware encoder-decoder PLM optimized for source code
  and has demonstrated effectiveness in various sequence-to-sequence code generation
  tasks, such as program repair [16, 65].


  Input Representation. As discussed in Section 3.1, the retrieval-augmented focal-test
  input to RetriGen is represented as \^{FTi} = FTi ⊕ A''_{j} with the concatenation
  operation ⊕. Thus, the generator is trained to learn the transformation patterns
  from the retrieval-augmented input \^{FTi} to the output Ai by the sequence-to-sequence
  learning.


  Model Architecture. The generation model of RetriGen is composed of an encoder stack
  and a decoder stack, culminating in a linear layer with softmax activation. First,
  RetriGen splits the source code of the input \^{FTi} into subwords using a code-specific
  Byte-Pair Encoding (BPE) tokenizer, which has been pre-trained on eight widely used
  programming languages and is specifically designed for tokenizing source code [52].
  The BPE tokenizer is able to break down rare tokens into meaningful subword units
  based on their frequency distribution to address the common Out-Of-Vocabulary (OOV)
  problem in code-related tasks. Second, RetriGen employs a word embedding stack to
  generate representation vectors for tokenized focal-test functions, allowing it
  to capture the semantic meaning of the code tokens as well as their positional relationships
  within the code snippet. Third, RetriGen inputs the vectors into an encoder stack
  to obtain the hidden state, which is subsequently passed to a decoder stack for
  further processing. Finally, the output from the decoder stack is passed through
  a linear layer with softmax activation, generating a probability distribution over
  the vocabulary.


  Loss Function. The assertion generation model receives \^{FTi} as input and produces
  the correct assertion Ai by learning the mapping between \^{FTi} and Ai. Thus, the
  model’s parameters are adjusted through the training dataset, with the goal of improving
  the mapping by maximizing the conditional probability Pθ(Ai | \^{FTi}). RetriGen
  employs the common cross entropy as the loss function to train the assertion generator
  L, which is widely adopted in previous code-related studies [14, 52]. The cross-entropy
  loss is minimized between each position in the predicted assertion and each position
  in the ground-truth assertion, defined as:

  L = − ∑_{i=1}^{|D|} log(Pθ(Ai | \^{FTi}))


  </block>

  ### 3.4 Assertion Inference

  <block id="10">

  During the model inference phase, once the generation model is effectively trained,
  given a focal-test as the input, RetriGen employs the beam search strategy to produce
  a ranked list of assertion candidates based on the vocabulary’s probability distribution.
  In particular, beam search [55] is a widely used strategy for identifying the highest-scoring
  candidate assertions by progressively prioritizing the top-k probable tokens according
  to their predicted likelihood scores. The correctness of the generated candidate
  assertion can be automatically evaluated by comparing it with ground-truth assertions
  or manually inspected by test experts for deployment in the unit testing pipeline.


  </block>

  ## 4 EXPERIMENTAL SETUP

  <block id="11">


  </block>

  ### 4.1 Research Questions

  <block id="12">

  To evaluate the performance of RetriGen, we address the following three research
  questions (RQs):

  - RQ1: How does RetriGen compare to state-of-the-art assertion generation approaches?

  - RQ2: To what extent do different choices influence the overall effectiveness of
  RetriGen?

  - RQ3: What is the generalizability of RetriGen when utilizing other advanced PLMs?


  </block>

  ### 4.2 Datasets

  <block id="13">

  Following EditAS [45], we utilize two popular large-scale datasets, i.e., Data_old
  and Data_new, to evaluate RetriGen and the baselines. These datasets have been widely
  used in previous deep assertion generation studies [21, 45, 55, 59], including all
  of our baselines. A brief introduction to both datasets is provided below.


  (1) Data_old. Data_old is initially constructed by Watson et al. [55] to propose
  ATLAS [55], the first deep assertion generation approach. Watson et al. [55] collect
  a dataset of 2.5 million test methods within GitHub, each comprising test prefixes
  and their corresponding assertion statements. In Data_old, every test method is
  linked to a specific focal method, which represents the production code being tested.
  Watson et al. then perform preprocessing to remove test methods with token lengths
  exceeding 1K and to filter out assertions that contain unknown tokens not present
  in the focal test or the vocabulary.


  (2) Data_new. However, Data_old excludes assertions containing unknown tokens to
  oversimplify the assertion generation problem, thus being unsuitable to reflect
  the real-world data distribution. Thus, Yu et al. [59] address this issue by creating
  an extended dataset, referred to as Data_new, which includes additional 108,660
  samples that are excluded due to unknown tokens in Data_old.


  As a result, Data_old and Data_new contain a total of 156,760 and 265,420 samples,
  respectively. These datasets are divided into training, validation, and test sets
  using an 8:1:1 ratio, as done by Watson et al. [55] and Yu et al. [59]. In this
  paper, we strictly adhere to the replication package provided by prior work [45,
  55, 59].


  </block>

  ### 4.3 Baselines

  <block id="14">

  To address the above-mentioned RQs, we compare RetriGen against six state-of-the-art
  AG approaches, including DL-based, retrieval-based, and integration-based ones.
  First, we include ATLAS, the first and classical AG technique that utilizes a sequence-to-sequence
  model to generate assertions from scratch. ATLAS is the most relevant baseline as
  both ATLAS and RetriGen consider assertion generation as an NMT task based on the
  encoder-decoder Transformer architecture. Second, we incorporate three existing
  retrieval-based AG techniques: IRar, RAHadapt, and RANNadapt. Finally, we consider
  one state-of-the-art integration-based approach Integration, and its most recent
  follow-up EditAS, as detailed in Section 2.1. It is worth noting that we exclude
  CEDAR as a baseline mainly due to the severe data leakage issue of the utilized
  black-box LLM Codex. LLMs are proprietary, and their training details (e.g., pre-training
  datasets) are not publicly disclosed, making it difficult to ensure whether models
  have been exposed to the evaluation dataset during training [44, 47, 68].


  </block>

  ### 4.4 Evaluation Metrics

  <block id="15">

  Following prior AG work [45, 55, 59], we consider two metrics to evaluate the performance
  of RetriGen and baselines, which are widely-adopted in code-related studies [49,
  51, 65].


  Accuracy. Accuracy is defined as the proportion of focal tests for which assertions
  generated by AG approaches match the ground-truth ones, and is utilized by all baseline
  methods [45, 55, 59]. An assertion is deemed correct only when it precisely aligns
  with the ground truth at the token level.


  CodeBLEU. Apart from accuracy, BLEU is utilized by previous AG studies [55, 59]
  to evaluate how closely the generated assertion matches the ground truth. However,
  BLEU is initially developed for natural language through token-level matching, overlooking
  significant lexical and semantic aspects of source code. In this work, we consider
  a code-aware variant of BLEU, i.e., CodeBLEU, which is designed particularly for
  code generation tasks [31]. Compared with BLEU, CodeBLEU is more suitable for the
  AG task as it further incorporates lexical similarity with the AST information and
  semantic similarity with a data-flow structure.


  </block>

  ### 4.5 Implementation Details

  <block id="16">

  We implement our experiments, including RetriGen, using the PyTorch framework [4].
  We employ the Hugging Face implementation [1] of the studied PLMs and all hyperparameters
  are taken from default values. We utilize the training set as the search codebase,
  consistent with prior deep AG studies [45, 59]. To prevent data leakage, we perform
  queries on the datasets to verify that there is no overlap between the evaluation
  and training sets. This strategy ensures that the retriever and generator are not
  privy to ground-truth assertions. During training, we set the batch size to 8, the
  maximum lengths of input to 512, the maximum lengths of output to 256, and the learning
  rate to 2e-5. All training and evaluations are performed on one Ubuntu 20.04 server
  with two NVIDIA GeForce RTX 4090 GPUs.


  </block>

  ## 5 EVALUATION AND RESULTS

  <block id="17">


  </block>

  ### 5.1 RQ1: Comparison with State-of-the-arts

  <block id="18">

  Experimental Design. In RQ1, we aim to evaluate the effectiveness of assertions
  generated by RetriGen. We compare RetriGen with six state-of-the-art AG techniques,
  including ATLAS, IRar, RAHadapt, RANNadapt, Integration and EditAS. To ensure a
  fair comparison, we employ the same training set to train baselines and RetriGen,
  and we provide the same evaluation set to all approaches for evaluation. We calculate
  the accuracy and CodeBLEU of all baselines and RetriGen by comparing generated assertions
  and human-written assertions. According to prior work [45, 59], we directly utilize
  reported results from their original paper rather than re-executing baselines, thereby
  minimizing potential risks.


  Results and Analysis. Overall, RetriGen achieves a remarkable performance with a
  prediction accuracy of 57.66% and a CodeBLEU score of 73.24% on average for two
  datasets, with an improvement of 50.66% and 14.14% against all previous baselines,
  respectively.


  In particular, when compared with the DL-based approach ATLAS, RetriGen achieves
  a prediction accuracy of 64.09% and 51.22% on the Data_old and Data_new datasets,
  yielding a remarkable improvement of 103.98% and 136.47%, respectively. Similarly,
  RetriGen exhibits substantial gains in the CodeBLEU metric, achieving improvements
  of 25.46% and 75.89% on average. It is worth noting that ATLAS is the most relevant
  to our RetriGen, as both approaches address assertion generation as an NMT task
  based on the encoder-decoder Transformer architecture. The significant improvement
  against ATLAS indicates the advantage of our PLM-based assertion generator, which
  is pre-trained with millions of code snippets, thus getting rid of the limited number
  of training samples. When compared with retrieval-based approaches, RetriGen attains
  an average accuracy improvement of 55.95%, 42.81%, and 37.76% against IRar, RAHadapt,
  and RANNadapt across the two datasets. Similarly, in terms of the CodeBLEU metric,
  the improvement still reaches 9.37%, 7.43%, and 8.08%. When compared with the two
  advanced integration-based approaches Integration and EditAS, RetriGen improves
  the prediction accuracy by 37.71% and 19.88% on Data_old, and 21.37% and 15.46%
  on Data_new, respectively.


  Effectiveness on Different Assertion Types. RetriGen outperforms all baselines on
  all standard JUnit assertion types across both datasets. For example, for the most
  common Equals type, RetriGen correctly generates substantially more assertions on
  two datasets, with a prediction accuracy rate improvement over the best-performing
  EditAS. Besides, in the case of Other assertion type, RetriGen outperforms existing
  baselines, except in the Data_old dataset, which has only two samples and is too
  small to yield convincing results. In summary, the experimental results demonstrate
  the generality of RetriGen in generating various types of assertions, including
  both standard and non-standard JUnit types.


  Overlap Analysis. To explore the extent to which RetriGen complements prior studies,
  we analyze the number of overlapping assertions generated by different AG techniques.
  RetriGen generates 1598 unique assertions that other AG approaches fail to generate
  on Data_old, which are substantially more than ATLAS, IRar, Integration, and EditAS.
  Similarly, on the Data_new dataset, RetriGen showcases its effectiveness by generating
  1818 unique assertions. Overall, the results demonstrate the superior capability
  of RetriGen in generating unique assertions, indicating the potential to complement
  existing AG techniques.


  Answer to RQ1: Our comparison results demonstrate that: (1) RetriGen significantly
  outperforms all baselines in terms of accuracy and CodeBLEU across both datasets,
  e.g., with an average accuracy improvement of 58.81% on Data_old and 42.51% on Data_new;
  (2) RetriGen consistently outperforms all baselines on all standard JUnit assertion
  types across both datasets, e.g., improving EditAS by significant margins in generating
  assertions for the Equals type; (3) RetriGen generates a large number of unique
  assertions that all baselines fail to generate across both datasets, e.g., 1598
  and 1818 unique ones on Data_old and Data_new.


  </block>

  ### 5.2 RQ2: Impact Analysis

  <block id="19">

  Experimental Design. In RQ2, we attempt to investigate the impacts of different
  components in RetriGen. RetriGen employs a hybrid retriever to search for a relevant
  test-assert pair, which consists of a token-based retriever and an embedding-based
  retriever. To illustrate the importance of each component, we compare RetriGen with
  three of its variants: (1) RetriGennone that generates assertions without any retriever,
  which is similar to ATLAS; (2) RetriGentoken that generates assertions with only
  a token-retriever; (3) RetriGenembed that generates assertions with only an embedding-retriever.


  Results and Analysis. We find that RetriGen outperforms all variants in terms of
  accuracy and CodeBLEU across both datasets, indicating the benefits of each component.
  For example, on the Data_old dataset, the hybrid retriever of RetriGen improves
  the prediction accuracy by substantial margins against RetriGennone, RetriGentoken
  and RetriGenembed, respectively. Similarly, the average improvement reaches notable
  percentages under the remaining three settings, i.e., accuracy on Data_old, CodeBLEU
  on Data_old and Data_new. Second, we find that without any retriever, RetriGen still
  achieves remarkable performance, with a prediction accuracy of 58.71% and 48.19%
  on two datasets. Compared with the most relevant baseline ATLAS, the improvement
  reaches large percentages, highlighting RetriGen’s advanced capability in overcoming
  the challenges posed by limited training datasets. Although the improvement of RetriGen
  over RetriGentoken and RetriGenembed is not as significant as its improvement over
  RetriGennone, given that three variants have achieved state-of-the-art results,
  the improvement brought by our hybrid retriever is valuable in practice, leading
  to a new higher baseline of deep assertion generation performance.


  Answer to RQ2: Our impact analysis indicates that all components (such as token-based
  and embedding-based retrievers) positively contribute to the effectiveness of RetriGen
  across two metrics, leading to new records on two widely adopted datasets.


  </block>

  ### 5.3 RQ3: Generalizability of RetriGen

  <block id="20">

  Experimental Design. RQ1 and RQ2 have proven that RetriGen outperforms the baselines
  when utilizing CodeT5 as the foundational model. To further explore the impact of
  various PLMs on the performance of RetriGen, we utilize four additional advanced
  PLMs for the assertion generation task: CodeBERT, GraphCodeBERT, UniXcoder, and
  CodeGPT. Particularly, CodeBERT [14] is a bi-modal PLM for both programming language
  and natural language understanding, pre-trained with masked language modeling and
  replaced token detection. GraphCodeBERT [19] is a successor of CodeBERT by incorporating
  two structure-aware pre-training tasks, i.e., edge prediction and node alignment.
  UniXcoder [18] is a unified cross-modal PLM designed to enhance both code understanding
  and generation capabilities with two pre-training tasks, i.e., multi-modal contrastive
  learning, and cross-modal generation. All these PLMs are pre-trained with source
  code, publicly accessible, and medium-scale, thus suitable for fine-tuning in the
  AG task.


  To implement RetriGen, for encoder-decoder PLMs, like UniXcoder, we use Test-Assertion
  Pairs to train both encoder and decoder components, thus learning hidden mappings
  between two sequences. For encoder-only PLMs, like CodeBERT and GraphCodeBERT, we
  initialize a new decoder from scratch to construct an encoder-decoder architecture
  for fine-tuning. For decoder-only PLMs, like CodeGPT, we directly leverage GPT’s
  capabilities to generate subsequent assertions from previous focal-methods in an
  auto-regressive manner.


  Results and Analysis. RetriGen consistently achieves impressive performance across
  all PLMs and two datasets with an average accuracy and CodeBLEU score. Particularly,
  the default generator of RetriGen (i.e., CodeT5) achieves better performance than
  the other four PLMs for all metrics and datasets. For example, on the Data_old dataset,
  the prediction accuracy improvement against CodeBERT, GraphCodeBERT, UniXcoder,
  and CodeGPT reaches notable percentages. The possible reason for the advance of
  CodeT5 lies in the model architecture. Similar to ATLAS, CodeT5 features an encoder-decoder
  Transformer architecture, which has been shown to effectively support code generation
  tasks in prior work [52]. However, encoder-only PLMs (such as GraphCodeBERT) necessitate
  a decoder for generation tasks, where the decoder starts from scratch and does not
  leverage the pre-training dataset. Thus, it is natural and effective to employ CodeT5
  as the backbone of RetriGen.


  When comparing different PLMs against previous AG approaches, we find that all PLMs
  are able to achieve impressive performance. For example, five investigated PLMs
  achieve notable prediction accuracy on Data_old and Data_new, outperforming the
  most recent baseline EditAS by various margins. Our analysis reveals that the observed
  improvements over previous baselines primarily stem from both our assertion generator
  and retriever. The generator leverages extensive codebases to learn more meaningful
  vector representations for unit tests (e.g., the pre-training data of CodeT5 comprises
  2.3 million functions from CodeSearchNet [31]), whereas baselines are trained only
  on a restricted corpus of test-assert pairs. Besides, the retriever identifies relevant
  assertions with both lexical and semantic matching, which is valuable for guiding
  the generator in generating correct assertions.


  Answer to RQ3: Our comparison results demonstrate that: (1) RetriGen is general
  to different PLMs and sustainably achieves state-of-the-art performance; (2) the
  default assertion generator (i.e., CodeT5) is natural and quite effective in the
  unit assertion generation scenario.


  </block>

  ## 6 DISCUSSION

  <block id="21">

  Case Study. To further illustrate the retrieval and generation capabilities of RetriGen,
  we present two examples of assertions from real-world projects. One example illustrates
  a unique assertion from the OWLAPI project [3], which is only correctly generated
  by RetriGen, but all baselines fail to. In this example, IRar retrieves similar
  assertions based on lexical matching, and returns an assertion within the same class
  as the input focal-test, although they are not responsible for testing similar functionalities.
  Thus, IRar and RANNadapt directly return the wrong assertion. RAHadapt and EditAS
  also fail to produce correct assertions as they make modifications to the wrong
  assertion type. In contrast, RetriGen, which relies on lexical and semantic matching,
  accurately identifies a similar assertion from another class file. Despite significant
  differences in lexical similarity, the two assertions share the same assertion type
  (i.e., assertNotNull) and parameter setting (i.e., objectInstance.method()). Thus,
  RetriGen is able to capture the edit patterns between the two focal-tests, and performs
  the appropriate modifications on the retrieved assertion to generate the correct
  assertion.


  Similarly, another example presents a real-world case from OpenDDAL [2], in which
  RetriGen and all baselines retrieve the same assertion for the given focal-test.
  The retrieved assertion is almost accurate, with only one parameter being corrected,
  as it targets the same focal-method with the query input. However, existing approaches
  fail to assume this retrieved assertion is correct and directly return it as the
  final output. Benefiting from the code understanding capability of the utilized
  PLM-based generator, RetriGen successfully captures the semantic differences between
  the retrieved test prefix and the input test prefix and applies the corresponding
  edit operations to generate the correct assertion.


  Potential of Large Language Models. As mentioned in Section 3, we consider encoder-decoder
  PLMs as foundation models of RetriGen and select CodeT5 because it is quite effective
  and the most popular PLM that is fine-tuned to support sequence-to-sequence code
  generation tasks. We notice that recent LLMs have been released with powerful performance,
  such as CodeLlama [41]. Most prior studies employ such LLMs in a zero-shot or few-shot
  setting, as fine-tuning these models with billions (or even more) of parameters
  is unaffordable due to device limitations [50]. In this section, we attempt to explore
  the preliminary potential of integrating LLMs with RetriGen. Due to device limitations,
  we select CodeLlama-7B as the foundation model to generate assertions without using
  retrieved assertions. The results demonstrate that CodeLlama-7B is able to achieve
  71.42% accuracy, which is 11.44% better than RetriGen (64.09%). It is worth noting
  that the improvement is valuable as we do not perform the retrieval-augmented process,
  while our hybrid retriever can significantly enhance the prediction results, e.g.,
  an improvement of 9.16% on the Data_old dataset. Thus, we are confident that RetriGen
  is able to achieve better results when equipped with recent LLMs. While the potential
  of LLMs like CodeLlama is evident, their deployment raises concerns about computational
  efficiency in real-world applications. For example, fine-tuning billion-level LLMs
  may require access to high-performance GPU clusters, making the process unaffordable
  for many research and industrial teams. Thus, in the future, researchers can systematically
  analyze the trade-offs between performance gains and computational costs when integrating
  larger LLMs into RetriGen. Besides, it is crucial to employ optimization strategies
  (parameter-efficient fine-tuning) to reduce computational overhead without sacrificing
  significant performance. Overall, the promising results motivate us to further explore
  the capabilities of RetriGen with newly-released LLMs while balancing trade-offs
  between effectiveness gains and efficiency.


  Comparison with CEDAR. CEDAR [34] is a prompt-based few-shot learning strategy which
  queries Codex for both assertion generation and program repair. We exclude CEDAR
  as a baseline in Section 4.3 mainly due to the significant data leakage issue of
  the utilized black-box LLM Codex. To provide an extended comparison, we reuse the
  reported results of CEDAR and execute RetriGen on the same ATLAS dataset for a direct
  comparison. Given that CEDAR uses Codex and Codex is not open-source, we chose CodeLlama-7B
  as the foundation model for generating assertions in RetriGen. The results demonstrate
  that, equipped with CodeLlama-7B, RetriGen is able to achieve 75.30% accuracy on
  the ATLAS dataset, comparable to CEDAR’s reported results. It is important to note
  that CEDAR relies on six retrieved assertions to guide Codex in generating assertions,
  while we fine-tune CodeLlama-7B with only one assertion. In fact, RetriGen and CEDAR
  are orthogonal, representing cutting-edge approaches in model utilization strategies:
  fine-tuning and few-shot learning. In this work, we implement RetriGen in a fine-tuning
  manner instead of few-shot learning primarily due to concerns about data leakage.
  The fine-tuning strategy enables us to choose much smaller open-source models that
  can be more effectively adapted to specific domains while mitigating data leakage
  concerns.


  Potential of Fault Detection Capabilities. We attempt to investigate the potential
  of generated assertions in uncovering real-world bugs. Following prior studies [11,
  22, 29], we utilize EvoSuite [15] to generate test cases for Defects4J [23], which
  includes 835 bugs from 17 real-world Java projects. Given EvoSuite’s reliance on
  randomized algorithms, we execute it 10 times per bug with different seeds to generate
  the final test cases. We exclude test cases that involve exception behavior, focusing
  specifically on assertion generation. We then remove assertion statements generated
  by EvoSuite and apply RetriGen to predict the test assertions. To assess the performance
  of bug detection, we run the complete test cases on both buggy and fixed versions
  of all programs. A bug is considered detected if the test case fails on the buggy
  version but passes on the fixed one. We compare RetriGen with the best-performing
  baseline EditAS due to dynamic execution overhead and computational resources. Both
  RetriGen and EditAS are implemented using Data_new as the training and retrieval
  corpus to ensure a fair comparison. Our results show that RetriGen and EditAS detect
  33 and 21 real-world bugs, respectively, with 20 and 8 unique detections. RetriGen
  surpasses EditAS by 57.14% in total detected bugs and by 150% in unique detections,
  highlighting RetriGen’s potential in detecting bugs.


  Practical Deployment. RetriGen is designed to predict appropriate assertion statements
  when provided with focal methods and test prefixes. In practice, RetriGen can be
  deployed as a complement to automated test case generation tools or as a code completion
  plugin for developers. First, existing automated test generation tools (e.g., EvoSuite)
  are skilled at generating test prefixes with high code coverage but often struggle
  to understand the semantics of focal methods and generate meaningful assertions.
  RetriGen can address this gap by accepting a test prefix generated by existing tools
  along with its focal method as inputs and generating appropriate assert statements.
  In this context, RetriGen is not a replacement for existing automated test generation
  tools but a complementary technique that enhances their effectiveness. Our preliminary
  results on Defects4J demonstrate that when provided with focal methods and the test
  prefixes generated by EvoSuite, RetriGen can generate meaningful assertions capable
  of detecting real-world bugs. Second, RetriGen can be integrated into an IDE as
  a code completion plugin for developers, offering suggested assertion statements
  while they write test code. For instance, when a developer needs to validate the
  correctness of software units, they begin by writing a test prefix, i.e., a sequence
  of call statements that invoke the specific behavior of the unit under test. RetriGen
  then assists by automatically generating the necessary test assertions based on
  the context of the focal method and the test prefix. This plugin is particularly
  valuable because manually crafting effective assert statements requires a deep understanding
  of the program’s functionality and specifications. Our experimental results on Data_old
  demonstrate that RetriGen predicts a large portion of correct assertions when given
  the focal method under test and a developer-written test prefix. This result highlights
  the potential of RetriGen in automated code completion scenarios, where developers
  can easily select appropriate assert statements recommended by RetriGen, significantly
  reducing the manual effort involved in writing assertions.


  </block>

  ## 7 THREATS TO VALIDITY

  <block id="22">

  The first threat to validity pertains to the evaluation metrics. In this work, we
  strictly follow the evaluation protocols established in prior work [45, 55, 59]
  and employ prediction accuracy to measure the effectiveness of the generated assertion.
  However, we are unable to execute generated assertions due to the limitations of
  the utilized datasets (only containing focal and test methods) [30], making it impossible
  to employ dynamic metrics such as defect detection rate. To mitigate this threat,
  we utilize Defects4J to assess whether the generated assertions can detect real-world
  bugs. Besides, since all baselines omit efficiency metrics, such as training or
  inference times in their evaluation, we cannot provide a direct comparison with
  baselines in this regard. To mitigate this threat, we include a code-aware metric,
  CodeBLEU, that has not yet been adopted in prior AG work. In the future, we will
  conduct an extensive evaluation of such deep assertion generation approaches in
  terms of computational costs.


  The second threat to validity arises from the possibility of data leakage in PLMs.
  PLMs are usually pre-trained with a mass of open-source projects, which may contain
  the test methods from the two AG datasets. To address the concern, we query the
  pre-training datasets (e.g., CodeSearchNet [31]) of studied PLMs and find such PLMs
  do not have access to any test cases (including assertions) during pre-training.
  It is worth noting that the data leakage concern motivates our choice of open-source
  PLMs instead of black-box PLMs or LLMs. Therefore, we believe that this concern
  does not significantly impact our conclusions.


  The third threat to validity is the potential limitation of our findings in generalizing
  to other benchmarks, programming languages, and PLMs. First, the two benchmarks
  may not fully capture the diversity of assertion patterns or coding styles across
  various domains. To mitigate the threat, we also utilize Defects4J to evaluate the
  fault detection capabilities of RetriGen in a more realistic assessment setting.
  Second, we only evaluate RetriGen on Java programs. However, we believe that the
  impact of this threat is relatively minor because (1) Java is the most targeted
  language in the AG field [21]; (2) adopted datasets are widely-utilized and sufficiently
  large-scale to yield reliable conclusions; and (3) RetriGen is language-agnostic
  to support multiple languages naturally. Third, we implement RetriGen with the recent
  PLM CodeT5. To mitigate the threat, we select four other PLMs from three types of
  model architectures. Considering that selected PLMs cover different architectures,
  organizations, parameter sizes, and pre-training datasets, we are confident in extending
  our findings to newly released larger LLMs. In the future, we attempt to explore
  the performance of RetriGen with more benchmarks, programming languages and PLMs.


  </block>

  ## 8 CONCLUSION AND FUTURE WORK

  <block id="23">

  In this work, we present RetriGen, a novel retrieval-augmented assertion generation
  (AG) approach that leverages the advances of external codebases and pre-trained
  language models (PLMs). RetriGen first builds a hybrid retriever to search the most
  relevant assertion for a query focal-test from external codebases by both the lexical
  and semantic similarity. RetriGen then utilizes off-the-shelf CodeT5 as the assertion
  generator to predict assertions fine-tuned with both focal-tests and additional
  retrieved assertions. The experimental results on two datasets show the superior
  performance of RetriGen against all six baselines on two metrics, e.g., achieving
  57.66% and 73.24% in terms of accuracy and CodeBLEU, outperforming all state-of-the-art
  AG techniques by 50.66% and 14.14% on average. We also demonstrate that RetriGen
  is able to generate a mass of unique assertions that all baselines fail to generate,
  e.g., 1598 and 1818 ones on two datasets, 3.71X and 4.58X more than the most recent
  technique EditAS. In the future, we plan to explore the applicability of RetriGen
  with larger PLMs, other programming languages, and datasets.

  </block>'
