title: Evaluating Retrieval Quality in Retrieval-Augmented Generation
abstract: Evaluating retrieval-augmented generation (RAG) presents challenges, particularly
  for retrieval models within these systems. Traditional end-to-end evaluation methods
  are computationally expensive. Furthermore, evaluation of the retrieval model’s
  performance based on query-document relevance labels shows a small correlation with
  the RAG system’s downstream performance. We propose a novel evaluation approach,
  eRAG, where each document in the retrieval list is individually utilized by the
  large language model within the RAG system. The output generated for each document
  is then evaluated based on the downstream task ground truth labels. In this manner,
  the downstream performance for each document serves as its relevance label. We employ
  various downstream task metrics to obtain document-level annotations and aggregate
  them using set-based or ranking metrics. Extensive experiments on a wide range of
  datasets demonstrate that eRAG achieves a higher correlation with downstream RAG
  performance compared to baseline methods, with improvements in Kendall’s τ correlation
  ranging from 0.168 to 0.494. Additionally, eRAG offers significant computational
  advantages, improving runtime and consuming up to 50 times less GPU memory than
  end-to-end evaluation.
abstract_is_verbatim: true
segmented_markdown: '# Evaluating Retrieval Quality in Retrieval-Augmented Generation


  ## ABSTRACT

  <block id="0">

  Evaluating retrieval-augmented generation (RAG) presents challenges, particularly
  for retrieval models within these systems. Traditional end-to-end evaluation methods
  are computationally expensive. Furthermore, evaluation of the retrieval model’s
  performance based on query-document relevance labels shows a small correlation with
  the RAG system’s downstream performance. We propose a novel evaluation approach,
  eRAG, where each document in the retrieval list is individually utilized by the
  large language model within the RAG system. The output generated for each document
  is then evaluated based on the downstream task ground truth labels. In this manner,
  the downstream performance for each document serves as its relevance label. We employ
  various downstream task metrics to obtain document-level annotations and aggregate
  them using set-based or ranking metrics. Extensive experiments on a wide range of
  datasets demonstrate that eRAG achieves a higher correlation with downstream RAG
  performance compared to baseline methods, with improvements in Kendall’s τ correlation
  ranging from 0.168 to 0.494. Additionally, eRAG offers significant computational
  advantages, improving runtime and consuming up to 50 times less GPU memory than
  end-to-end evaluation.


  </block>

  ## 1 INTRODUCTION

  <block id="1">

  Retrieval-augmented generation (RAG) has emerged as a prominent approach in natural
  language processing, combining the strengths of retrieval and generation models
  [35], with use cases in decreasing hallucination [1, 29], knowledge-grounding [9,
  16, 34], and personalization [25, 26]. Evaluating RAG systems is important as it
  ensures the effectiveness of integrating retrieval-based methods with generative
  models [10, 23]. Traditionally, RAG evaluation has primarily relied on end-to-end
  assessment, which entails comparing the generated output with one or more ground
  truth references [20]. While this is crucial, it presents several limitations, especially,
  for evaluating retrieval models in RAG systems.


  First, end-to-end evaluation lacks transparency regarding which retrieved document
  contributed to the generated output, hindering interpretability of the system’s
  behavior. Secondly, it is resource-intensive, consuming significant time and computational
  power, particularly when dealing with a large set of retrieval results consumed
  by the LLM. To process long input sequences resulting from the utilization of all
  retrieved documents by the LLM, GPUs with substantial memory capacities are essential
  for end-to-end evaluation. Moreover, many ranking systems rely on interleaving (i.e.,
  replacing one or more documents in the result list) for evaluation and optimization,
  which further complicates the evaluation, as slight variations in retrieval results
  necessitate re-computation of the RAG pipeline. Finally, optimizing ranking models
  often requires document-level feedback, such as user clicks [3, 6]. However, end-to-end
  evaluation only provides list-level feedback for the retrieval results. That said,
  this paper studies retrieval evaluation in RAG.


  Human annotations can be a potential solution for evaluating retrieval models in
  RAG, however, accurate annotations are often challenging and costly to obtain. More
  recently, with the emergence of large language models (LLMs) and their advanced
  capabilities in reasoning and text comprehension, they have been utilized to annotate
  documents for retrieval evaluation [10, 23]. Nevertheless, these approaches predominantly
  evaluate the retriever in RAG systems based on human preferences, whereas the primary
  objective of the retrieval model in RAG is to serve the LLM that leverages the retrieved
  results [35]. That said, our extensive investigation on a diverse set of RAG systems
  for open-domain question answering, fact verification, and dialogue systems reveals
  that employing human annotations, such as the provenance labels in the KILT benchmark
  [20], for evaluating the retrieval models within a RAG system exhibits only a minor
  correlation with the downstream RAG performance. This indicates a lack of meaningful
  relationship between the evaluated metrics and the downstream performance of RAG.


  In this paper, we propose eRAG, a new approach for evaluating retrievers in RAG
  systems, where we apply the LLM in RAG system on each document in the retrieval
  result list individually and use the LLM’s output to provide document-level annotations.
  These annotations can be obtained using any arbitrary downstream task metric, such
  as accuracy, exact match, or ROUGE [17]. We can then apply a set-based or ranking
  metric as an aggregation function to obtain a single evaluation score for each retrieval
  result list.


  We evaluate our proposed approach on question answering, fact-checking, and dialogue
  generation from the knowledge-intensive language tasks (KILT) benchmark [20]. Our
  results demonstrate that our proposed approach achieves the highest correlation
  with the downstream performance of the RAG system in comparison with the baselines.
  Specifically, we observe an absolute improvement in Kendall’s tau correlation ranging
  between 0.168 and 0.494 across the evaluated datasets. Furthermore, we investigate
  the impact of different retrieval augmentation methods, the quantity of retrieved
  documents, and the LLM size on correlation. Finally, we demonstrate that our approach
  offers significant computational advantages, consuming up to 50 times less memory
  compared to end-to-end evaluation. To facilitate research in this domain, we make
  eRAG’s implementation publicly available at: https://github.com/alirezasalemi7/eRAG.


  </block>

  ## 2 EVALUATING RETRIEVERS IN RAG

  <block id="2">

  Generally, two predominant methods are used for obtaining relevance labels for retrieval
  evaluation. The first approach involves human judgment to assess the relevance of
  a query to documents within a corpus. The main issue with this approach is that
  human annotation can be costly and is often impractical for evaluating all documents
  in a corpus [28]. Moreover, human annotation relies on human preferences to judge
  the relevance of documents to a query. However, a document deemed relevant based
  on human preferences may not be useful for an LLM in fulfilling its task.


  The second approach utilizes the downstream ground truth output associated with
  the query to provide weak relevance labels. In this method, a retrieved document
  containing the downstream ground truth is considered relevant [8, 14, 24, 27]. This
  method also presents its own challenges. This approach is impractical, particularly
  in scenarios where the task involves long-text generation or text classification,
  as downstream task labels might not exist within documents. Also, one document can
  be useful for an LLM in fulfilling its task without containing the ground truth
  labels.


  Even though we are not aware any work that use LLMs for evaluating retrieval models
  in RAG, LLMs can be leveraged to label documents based on their relevance to a query.
  Inspired by Thomas et al. [30], the LLM functions as a binary classifier, indicating
  whether a document is relevant to the query or not. The mentioned challenges persist
  even with the judgment of LLMs, especially if the LLM responsible for labeling differs
  from the LLM in the RAG pipeline. Besides, employing LLMs as judges in this scenario
  can pose challenges due to the computational cost of running them on a large set
  of retrieved documents and memory constraints.


  To mitigate these problems, we propose eRAG, a novel approach that involves utilizing
  the LLM in RAG system itself as the arbiter for generating labels to evaluate the
  retrieval model.


  </block>

  ### Using Downstream Large Language Model in RAG as Document Annotator

  <block id="3">

  Consider a retrieval model R that produces a ranked list R_k with k documents for
  the LLM M tasked with performing a specific task, utilizing a downstream evaluation
  function E_M. The LLM M takes a ranked list of documents as its input along with
  the query q, and generates an output represented as ȳ = M(q, R_k). For the documents
  in R_k, we feed each document individually to the LLM M with the query and evaluate
  the generated answer to create the label for each document, expressed as:

  G_q[d] = E_M(M(q, {d}), y) : ∀d ∈ R_k (1)

  where y is the expected downstream output for the query. We can employ the created
  G_q to utilize any ranking metric to evaluate R. Note that the runtime cost of a
  vanilla transformer [32] scales quadratically with its input length. Consequently,
  for end-to-end evaluation, the cost of running a transformer on a ranked list with
  k documents, with an average length of d, to generate an output with length l is
  O(l k^2 d^2). Conversely, in our approach, as each document is individually fed
  to the LLM for k times, the cost is O(l k d^2), proving to be more efficient than
  end-to-end evaluation.


  </block>

  ### Retrieval Evaluation Metrics

  <block id="4">

  For a ranked list R_k, comprising k retrieved documents generated by a retrieval
  model R, an evaluation metric E_R assigns a score E_R(R_k, G_q) ∈ [0, 1], by comparing
  the ranked list with the relevance scores G_q, which is a function that maps each
  document to a scalar relevance score for the document with respect to the query
  q (i.e., G_q(d) = s_d). Various definitions exist for the evaluation metric E_R;
  in this paper, we examine Precision (P), Recall (R), Mean Average Precision (MAP),
  Mean Reciprocal Rank (MRR) [2], Normalized Discounted Cumulative Gain (NDCG) [11],
  and Hit Rate. Note that when dealing with non-binary relevance labels, precision
  considers the average value of relevance labels, while Hit Ratio considers the maximum
  value among them.


  </block>

  ## 3 EXPERIMENTS

  <block id="5">


  </block>

  ### 3.1 Setup

  <block id="6">

  Datasets and Evaluation. We use Natural Questions (NQ) [15], TriviaQA [13], HotpotQA
  [33], FEVER [31], and Wizard of Wikipedia (WoW) [4] datasets from the KILT [20]
  benchmark. Due to the unavailability of ground truth labels for the test set, we
  utilize the publicly accessible validation set. As the retrieval corpus, we employ
  the Wikipedia dump of the KILT benchmark and adhere to the preprocessing outlined
  by Karpukhin et al. [14], where each document is segmented into passages, each constrained
  to a maximum length of 100 words. The concatenation of the article title and passage
  is used as a document. The KILT benchmark furnishes document-level relevance labels
  (called Provenance) for its datasets, and these are employed for evaluating retrieval
  performance. In line with our preprocessing method, we define all passages within
  a positive document as positive passages for our evaluation. For relevance evaluation
  using an LLM, we employ Mistral1 [12] to annotate each document within the retrieved
  list, determining whether it is relevant to the query or not. We adopt the metrics
  recommended by the KILT benchmark, namely Exact Match (EM) for NQ, TriviaQA, and
  HotpotQA, Accuracy for FEVER, and F1 for the WoW dataset.


  Experiments Configuration. In all experiments, unless explicitly stated otherwise,
  we employ T5-small [21] with Fusion-in-Decoder (FiD) [9] as the LLM. We employ AdamW
  [19] with a weight decay of 10−2 and a learning rate of 5 × 10−5 for 10 epochs,
  incorporating linear warmup for the initial 5% of training steps. The effective
  batch size is set to 64. Each model is trained using an A100 Nvidia GPU. For document
  retrieval during training, we utilize BM25 [22] implemented in Pyserini [18] to
  retrieve 50 documents to augment the input with them. For fast vector search in
  dense retrieval with Contriever [7], we use Faiss [5] flat index.


  </block>

  ### 3.2 Main Findings

  <block id="7">

  How do different retrieval evaluation methods correlate with the end-to-end downstream
  performance in RAG?. To compare the different evaluation strategies for evaluating
  retriever in RAG, we report the correlation between the scores generated for each
  method and the downstream performance of the LLM (i.e., T5-small with FiD and 50
  retrieved documents). The results indicate that eRAG attains the highest correlation
  compared to other evaluation approaches. Furthermore, the results show that regardless
  of the retrieval model employed, eRAG consistently outperforms others in terms of
  correlation with the LLM’s downstream performance. Interestingly, the most common
  approaches, KILT Provenance and Annotation with LLMs, that are, document-level relevance
  labels and using LLMs to assign a relevance label to each retrieved document, have
  the lowest correlation with the downstream performance of the LLM. This finding
  confirms that the LLM as the consumer of the retrieved results in RAG is the best
  judge for the performance of the retrieval model.


  How do different retrieval evaluation methods in RAG perform as the size of retrieval
  results increases? To address this, we varied the number of retrieved documents
  and computed the correlation between the metric with highest correlation for each
  method at each specified number of retrieved documents and the downstream performance
  of the LLM given that number of retrieved documents. For the sake of space, we limit
  our experiments to three datasets: NQ for question answering, FEVER for fact-checking,
  and WoW for long-text generation. The outcomes of this experiment reveal that irrespective
  of the quantity of retrieved documents, our suggested evaluation strategy consistently
  exhibits a higher correlation with the downstream performance of the LLM. Furthermore,
  the results illustrate that augmenting the number of retrieved documents leads to
  a decline in correlation—an intuitive observation, as all metrics assess each document-relevance
  label independently for scoring a ranked list, while the LLM uses information from
  the entirety of these documents to accomplish its task.


  How does our method correlate with the downstream RAG performance as the size of
  large language models increases? In addressing this question, we computed the correlation
  between our retrieval evaluation strategy and the downstream performance of the
  LMs with two distinct sizes (i.e., T5-small with FiD consisting of 60M and T5-base
  with FiD consisting of 220M parameters). For the sake of space, we limit our experiments
  to three datasets: NQ for question answering, FEVER for fact-checking, and WoW for
  long-text generation. The results indicate that, for certain datasets, there is
  a higher correlation with the smaller LLM, while for others, a higher correlation
  is observed with the larger model. Nonetheless, in none of the cases is there a
  significant difference between the correlations, suggesting that the proposed approach
  is effective regardless of the LLM size.


  How does different retrieval-augmentation approaches affect the correlation between
  eRAG and the downstream RAG performance? We applied eRAG to two LLMs. One LLM utilizes
  In-Prompt Augmentation (IPA), where the retrieved results are appended to the input
  of the LLM. The other LLM employs Fusion-in-Decoder (FiD) [9], wherein each retrieved
  document is individually processed by the encoder, and subsequently, the representations
  for all documents are concatenated together and fed to the decoder. For the sake
  of space, we limit our experiments to NQ for question answering, FEVER for fact-checking,
  and WoW for long-text generation. The correlation between eRAG and the outputs of
  each LLM indicates that although there is no significant difference between the
  correlation of eRAG with IPA and FiD LLMs, eRAG consistently exhibits a higher correlation
  with the FiD. This observation can be elucidated by considering the distinction
  between IPA and FiD methodologies. In IPA, all documents are concatenated together
  and then presented as a single input to the LLM. In contrast, FiD processes each
  document individually by feeding them separately to the LLM’s encoder. Given that
  our approach aligns more closely with FiD, we believe this alignment is a contributing
  factor to the higher correlation between eRAG and the downstream performance of
  FiD.


  How much more efficient is eRAG compared to the end-to-end evaluation? Here, we
  consider two factors: inference time and memory consumption. For inference time,
  we compare the total time required for end-to-end evaluation to generate scores
  with the total time used by eRAG. In this experiment, we opt for the batch size
  of each approach to be as large as possible, maximizing the utilization of the entire
  GPU memory. The findings indicate that, on average, eRAG is 2.468 times faster than
  end-to-end evaluation. Further elaborating, the speedup for eRAG ranges from 1.232
  to 3.252 times compared to end-to-end evaluation across the datasets, where the
  least speedup is for the long-text generation task (i.e., WoW).


  To compare memory consumption between eRAG and end-to-end evaluation, we conducted
  two experiments. First, we compared the maximum memory required by end-to-end evaluation
  to assess a query with the maximum memory demanded by eRAG for the same evaluation.
  To carry out this comparison, we configured the batch size for end-to-end evaluation
  to 1, while for eRAG, we set it to the same number of documents used for one query
  by end-to-end evaluation (we call this query-level configuration). In the subsequent
  experiments, we set both batch sizes to 1 to assess the extent to which eRAG demonstrates
  superior memory efficiency compared to end-to-end evaluation under the most efficient
  configuration (we call this document-level configuration). The findings indicate
  that in the query-level configuration, eRAG exhibits between 7 to 15 times greater
  memory efficiency compared to end-to-end evaluation. Furthermore, in the document-level
  configuration, this efficiency gap widens, with eRAG demonstrating 30 to 48 times
  more memory efficiency than end-to-end evaluation across different dataset. In summary,
  these experiments suggest that eRAG is more efficient than end-to-end evaluation
  of a vanilla transformer, excelling in both inference time and memory utilization.


  </block>

  ## 4 CONCLUSION

  <block id="8">

  This paper explores various approaches for evaluating retrieval models within a
  RAG pipeline. Additionally, it introduces eRAG, a novel approach for evaluating
  retrieval models in the RAG pipeline. eRAG leverages the per-document performance
  of the LLM on the downstream task to generate relevance labels. The findings suggest
  that the proposed approach exhibits significantly higher correlation with the downstream
  performance of the LLM. Furthermore, eRAG demonstrates greater efficiency than end-to-end
  evaluation in terms of both memory consumption and inference time.

  </block>'
