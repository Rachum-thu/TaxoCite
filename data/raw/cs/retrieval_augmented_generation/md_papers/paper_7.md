# Parametric Retrieval Augmented Generation

## Abstract
Retrieval-augmented generation (RAG) techniques have emerged as a promising solution to enhance the reliability of large language models (LLMs) by addressing issues like hallucinations, outdated knowledge, and domain adaptation. In particular, existing RAG methods append relevant documents retrieved from external corpus or databases to the input of LLMs to guide their generation process, which we refer to as the in-context knowledge injection method. While this approach is simple and often effective, it has inherent limitations. Firstly, increasing the context length and number of relevant documents can lead to higher computational overhead and degraded performance, especially in complex reasoning tasks. More importantly, in-context knowledge injection operates primarily at the input level, but LLMs store their internal knowledge in their parameters. This gap fundamentally limits the capacity of in-context methods. To this end, we introduce Parametric retrieval-augmented generation (Parametric RAG), a new RAG paradigm that integrates external knowledge directly into the parameters of feed-forward networks (FFN) of an LLM through document parameterization. This approach not only saves online computational costs by eliminating the need to inject multiple documents into the LLMs‚Äô input context, but also deepens the integration of external knowledge into the parametric knowledge space of the LLM. Experimental results demonstrate that Parametric RAG substantially enhances both the effectiveness and efficiency of knowledge augmentation in LLMs. Also, it can be combined with in-context RAG methods to achieve even better performance.

## Keywords
Large Language Model, Retrieval Augmented Generation, Knowledge Representation, Parametric Information Representation

## 1 Introduction
Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of information retrieval (IR) and natural language processing (NLP) tasks [5, 6, 11, 36, 48, 55]. Despite these successes, a critical limitation remains: once training is complete, an LLM‚Äôs internal knowledge becomes effectively static, making it challenging to incorporate newly emerging information or knowledge not included in its pre-training data. To address this challenge, retrieval-augmented generation (RAG) has emerged as a prominent solution. RAG enables LLMs to dynamically access and utilize information beyond their pre-trained parameters by retrieving relevant information from an external corpus, thus improving their adaptability and performance [4, 12, 16, 18, 23, 44‚Äì46].

Existing studies have explored various aspects of the RAG pipeline, considering factors such as retrieval timing [2, 18, 44, 45], document selection [21, 58], and external knowledge organization [10, 15, 32]. While these innovations improve different stages of the pipeline, all RAG methods, regardless of their variations, share a common characteristic: they inject external knowledge by directly adding passages or documents into the input context of LLMs, which we refer to as the in-context knowledge injection.

Although this in-context knowledge injection approach is straightforward and often effective, recent studies have highlighted several limitations of this paradigm. First, injecting knowledge through input prompts will inevitably increase the context length. Long context not only introduces extra computational overhead and latency for LLM inference, but also hurts the performance of LLMs in understanding and utilizing external knowledge, especially in tasks that involve complex reasoning [22, 26]. Second, more importantly, the way LLMs process information in context is fundamentally different from the way they utilize internal knowledge stored in their parameters. Studies have shown that LLMs store most of their knowledge within the parameters of their neural network architecture (e.g., the parameters of their feed-forward network layers) [31, 59]. Adding passages or documents in the input context could only affect the online computation of key-value (KV) pairs in the attention networks of LLMs, but not the model‚Äôs stored parameters, where its knowledge is encoded [59]. This means that LLMs may never be able to utilize external knowledge as effectively as they use their internal knowledge in in-context RAG methods. A straightforward solution to this problem is to conduct supervised fine-tuning (SFT) with retrieved documents, thereby incorporating relevant knowledge directly into the LLM‚Äôs parameters. However, such SFT-based methods are considered suboptimal as they require substantial computational resources and GPU memory, making it impractical to inject relevant documents online for every query. In addition, they can negatively affect the original ability of the LLM to follow instructions [7, 54] and lack the flexibility of in-context methods, which allow external knowledge to be added or removed on the fly.

The observations above inspire us to raise the following research question: Is it possible to inject external knowledge into LLM parameters effectively, efficiently, and flexibly for retrieval-augmented generation?

To this end, we introduce a new RAG paradigm, Parametric Retrieval Augmented Generation (Parametric RAG), which directly injects the external knowledge into the Feed-Forward Networks (FFN) of an LLM. Specifically, our approach begins with an offline preprocessing phase that parameterizes each document from the external corpus, transforming them into a small set of parameters (usually a few MB per document) that can be directly integrated into the downstream LLM. We refer to this set of parameters as the parametric representation of the document. In the inference phase, we conduct retrieval augmented generation following a Retrieve-Update-Generate (RUG) workflow. The Retrieve step retrieves top-n documents from the external corpus based on the input prompt following the same procedure used by the existing RAG pipeline. Then, the Update step uses the parametric representations of the retrieved documents to update the LLM. Finally, in the Generate step, we use the updated LLMs to conduct inference directly based on the original input prompt.

Theoretical and empirical analysis show that our Parametric RAG method has superior inference efficiency and outperforms state-of-the-art in-context methods on several RAG benchmarks that involve tasks with complex reasoning. While the preprocessing phase of Parametric RAG introduces an offline computational overhead, this cost is affordable and even negligible compared to the online cost of large-scale inference requests, leading to long-term savings in terms of power and carbon footprints. Also, similar to in-context RAG, our method can adapt to various numbers of input documents on the fly. Furthermore, our proposed Parametric RAG pipeline is in parallel with existing in-context methods. As shown in our experiments, combining our methods with in-context RAG could produce even better performance on the benchmark datasets. This indicates that parametric knowledge injection could be a fruitful direction for the future development of the RAG system.

In summary, this paper makes the following key contributions:
- We propose Parametric RAG, a new RAG paradigm that integrates external knowledge directly into LLM‚Äôs parameters.
- We propose an offline method to build parametric document representation and a Retrieve-Update-Generate pipeline to conduct Parametric RAG on the fly.
- We conduct extensive experiments to compare the state-of-the-art in-context RAG methods with our method to demonstrate the potential of Parametric RAG in terms of effectiveness and efficiency.

## 2 Related Work
Large language models have shown remarkable performance across diverse applications. However, their inherent knowledge often proves insufficient for tackling knowledge-intensive tasks, underscoring the necessity of integrating external knowledge for robust performance in these contexts. One prominent approach to address this gap is Retrieval-Augmented Generation (RAG), which improves LLMs by integrating relevant external knowledge sources [4, 8, 12, 16, 18, 23, 37, 42, 50, 51]. In the traditional RAG framework, an external retriever [9, 27, 34, 40, 41, 43, 60] or a more complex retrieval system [24, 35] retrieves relevant documents based on a query. These documents are then appended to the LLM‚Äôs input context, allowing the LLM to leverage knowledge beyond its training data [23].

Building upon the traditional RAG framework, several extensions have been proposed to improve its effectiveness and efficiency. One such extension, Adaptive RAG [17, 52], introduces adaptive retrieval strategies that actively adjust the retrieval pipeline based on the complexity of the query to improve the adaptability of RAG in different tasks. From another angle, to make in-context knowledge injection more effective in RAG scenarios, IR-CoT [49] designs prompt templates specifically tailored for RAG that demonstrate how to perform chain-of-thought (CoT) reasoning based on the given passage. Each sentence in the CoT reasoning content is then applied to retrieve more relevant documents. Another research direction, GraphRAG [10, 15, 32], employs pre-constructed knowledge graphs to retrieve graph elements that encapsulate relational knowledge relevant to the query. GraphRAG has demonstrated enhanced performance, particularly in tasks that require structured, relational information. In the context of long-form generation, where the LLM‚Äôs informational needs may change during the generation process, dynamic RAG techniques have been developed to actively decide when and what to retrieve during the generation process [3, 19, 25, 44, 45, 57]. For example, FLARE [19] triggers the retrieval module when the model‚Äôs token prediction probability falls below a predefined threshold. Similarly, DRAGIN [45] models the real-time information needs of the LLM, generating queries based on the model‚Äôs internal state and preceding context to fetch relevant external knowledge dynamically.

To summarize, existing RAG approaches have explored various aspects of the RAG pipeline, considering factors such as retrieval timing [3, 19, 25, 44, 45, 57], prompt template for in-context knowledge injection [49, 53], document selection [58], and external knowledge organization [10, 15, 32]. While these innovations improve different stages of the pipeline, all RAG methods, regardless of their variations, share a common characteristic at the knowledge injection level: relevant passages or documents are appended directly to the LLM‚Äôs input context to inject external knowledge. In contrast, our proposed Parametric RAG paradigm diverges from all the existing RAG frameworks by directly injecting documents into the LLM‚Äôs parameters. This shift in knowledge integration addresses the inherent limitations of the in-context knowledge injection methods employed in all existing RAG frameworks.

## 3 Methodology
In this section, we introduce our proposed Parametric RAG framework. This section begins by formulating the problem and providing an overview of the Parametric RAG framework (¬ß3.1). Next, we introduce the Offline Document Parameterization process (¬ß3.2), which transforms documents into parametric representations through Document Augmentation and Parametric Document Encoding. Finally, we introduce the Online Inference procedure (¬ß3.3), where the parametric representations are retrieved, merged, and integrated into the LLM to generate responses.

### 3.1 Problem Formulation and Overview
This subsection introduces the problem formulation of the RAG task and provides an overview of our proposed Parametric RAG pipeline. Consider an LLM (denoted as L) with base parameters ùúÉ. Given a user query q, we aim to generate an accurate response using an external corpus K. Formally, the corpus K is defined as: K = {d1, d2, . . . , dN}, where each di represents a text chunk, such as documents, Wikipedia articles, or passages (for convenience, we refer to each di as ‚Äòdocument‚Äô in the following sections). The system contains a retrieval module R that calculates the relevance score of each document {Sd1, Sd2, . . . , SdN} corresponding to the query q. Traditional RAG paradigms select the top k documents with the highest relevance scores as relevant external knowledge and append them to the input context of the L. This process is typically guided by a prompt template that instructs L to generate the response based on the provided knowledge.

In contrast to the in-context RAG paradigm that injects relevant documents into the LLM‚Äôs input context, in Parametric RAG, we propose to insert documents directly into the parameters of L. To achieve this, the Parametric RAG framework is designed with two stages: an offline document parameterization stage and an online inference stage with a Retrieve-Update-Generate workflow.

Offline document Parameterization. In this step, we offline transform each document in K into a parametric representation, thereby forming a set of parameters known as the Parametric Corpus KP. Specifically, we define:
KP = {pi | pi = f_phi(di), i = 1, 2, . . . , N},
where f_phi is a mapping function that converts each document di into its corresponding parametric representation pi. We define parametric representations pi to possess the following properties:
(1) The parameters pi can be plugged into the feed-forward network weights of the LLM.
(2) After inserting the parametric representation pi into L, the LLM can effectively comprehend the knowledge contained within the corresponding document di.
(3) Different document parameters pi can be merged through specific algorithms, and after merging, the LLM can grasp the combined knowledge corresponding to the merged documents.

Online Inference. During the online inference process, our method first merges the parametric representations corresponding to the retrieved top-k documents and then plugs the merged parameters into the LLM. Subsequently, the updated LLM is used to answer the user‚Äôs question. This overall framework allows for more efficient and effective knowledge injection, overcoming the limitations of traditional RAG by leveraging parameterized representations of external knowledge.

### 3.2 Offline Document Parameterization
In this subsection, we describe the detailed process of offline parameterizing each document in the corpus K during the pre-processing phase. Given a document di and an LLM L, our objective is to construct a parametric representation pi of the document. This representation enables L to effectively comprehend and utilize the knowledge contained in di during inference. To achieve this, we propose a two-step approach: Document Augmentation and Parametric Document Encoding. These steps are combined to generate robust and informative parametric representations for each document.

#### 3.2.1 Document Augmentation
Existing studies have shown that effectively incorporating factual knowledge from external documents into LLMs requires more than simply pre-training the model on raw text via standard next-token prediction. For example, Allen-Zhu and Li [1] find that after being trained repeatedly on a given document, LLMs can memorize its content but fail to extract and apply this knowledge effectively in downstream tasks such as question answering. To address this issue, Allen-Zhu and Li propose two key strategies: (1) incorporating question-answer (QA) pairs derived from the document during training and (2) augmenting the document through multiple rewrites that express the same factual content in different forms. Their findings indicate that these two approaches enable LLMs to internalize knowledge to support accurate application in downstream tasks rather than reproducing the original text token-by-token.

Building upon the insights discussed above, we introduce the document augmentation process consisting of two steps: Document Rewriting and QA Pair Generation, to construct robust and informative parametric representations for documents. Specifically, for each document, we prompt the LLM L to rewrite the content multiple times using different wording, styles, or organizational structures. Formally, each document di is transformed into multiple rewritten documents {di1, di2, . . . , din}, which preserve the original facts but vary in language expression. Once each document has been rewritten into multiple documents, we prompt L again to generate question-answer (QA) pairs based on the original document di. For each document di, L produces a set of questions and their corresponding answers: {(qi1, ai1), (qi2, ai2), . . . , (qim, aim)}, where m is a tunable hyperparameter representing the number of QA pairs we aim to generate per document. Integrating multiple rewrites with corresponding QA pairs transforms each document di into a more comprehensive resource Di that preserves its original factual content while incorporating diverse linguistic variations. Formally:
Di = {(dik, qij, aij) | 1 ‚â§ k ‚â§ n, 1 ‚â§ j ‚â§ m},
where each (dik, qij, aij) triple corresponds to a rewritten document dik from the original document di, coupled with a question qij and its respective answer aij.

#### 3.2.2 Parametric Document Encoding
In this subsection, we introduce the second step of the offline document parameterization pipeline, where we leverage the augmented dataset Di to train the parametric representation pi for each document di. Specifically, we initialize these parametric representations as low-rank matrices corresponding to the feed-forward network (FFN) parameter matrix W of the LLM L, following the LoRA approach [14]. This design allows each document di to be associated with independently trained low-rank parameters, allowing the model to internalize the knowledge specific to di in a parameter-efficient manner.

Suppose the Transformer layers in L have a hidden dimension h, and the feed-forward network (FFN) within each layer has an intermediate dimension k. Consequently, each FFN layer of L contains a weight matrix W ‚àà R^{h√ók}. To incorporate document-specific knowledge, we introduce low-rank matrices A and B such that
W' = W + ŒîW = W + A B^‚ä§,
where A ‚àà R^{h√ór} and B ‚àà R^{k√ór}, with r ‚â™ min(h, k). The original weight matrix W is kept fixed, while A and B are the only trainable parameters for that layer. We denote these newly introduced parameters as ŒîŒ∏ = {A, B}. By combining the pre-trained weights W and ŒîŒ∏, the model obtains the knowledge from the selected document. Each document in the corpus is associated with its instance of ŒîŒ∏.

For each document di, we train its corresponding parametric representation ŒîŒ∏ using its corresponding augmented dataset Di. Recall from above that Di contains triplets (dik, qij, aij). For each triplet, we concatenate dik, qij, and aij into a token sequence:
x = [ dik ‚äï qij ‚äï aij ],
where [ ¬∑ ‚äï ¬∑ ] indicates concatenation. Let T be the total number of tokens in x. We adopt a standard sequential language modeling objective to ensure that the LLM internalizes knowledge from the entire augmented text (i.e., both the documents and QA pairs). Specifically, we optimize:
min_{ŒîŒ∏} ‚àë_{(dik,qij,aij) ‚àà Di} ‚àë_{t=1}^T ‚àí log P_{Œ∏+ŒîŒ∏}( x_t | x_{<t} ),
where Œ∏ are the frozen pretrained parameters of the LLM, and ŒîŒ∏ = {A, B} are the trainable low-rank matrices introduced above. The innermost summation is taken over all tokens x_t in the concatenated input sequence. This design inherently encourages the LLM to internalize the factual details in the documents into its parameters during training. Although the generated question-answer pairs do not directly cover all the facts within the document, repeated training on the documents‚Äô tokens allows the model to reinforce its understanding of the textual content. Consequently, once the training is complete, the parametric representation ŒîŒ∏ serves as a lightweight document-specific knowledge representation that can be directly added to the original model L at inference time.

Notably, this entire process can be conducted offline, as each document (or batch of documents) is processed to produce its respective low-rank representation ŒîŒ∏. At inference time, we only need to load the LoRA parameters corresponding to the specific document(s) rather than appending the document directly into the LLM‚Äôs context. The computational cost of loading these parametric representations constitutes only a minimal portion, approximately 1% of the computation required to decode a single token.

#### 3.2.3 Discussion on LoRA Initialization
In our proposed training framework, the LoRA parameters for each document di are initialized randomly without any warm-up stage. This choice is deliberate and aligns with our goal of developing a general-purpose parametric knowledge injection method rather than one tailored to specific downstream tasks. If not explicitly mentioned otherwise, we initialize LoRA with random values. However, randomized LoRA initialization is not necessarily the most effective way to train parametric document representations. For example, we could pre-train the random LoRA with a couple of few-shot examples following the same method described above and save the LoRA weight W_{warm-up} to initialize the training of each document‚Äôs LoRA (i.e., the document‚Äôs parametric representation). Our experiment (¬ß 5.2) demonstrates that this warm-up process can significantly improve the performance compared to random initialization on RAG tasks, indicating that a task-aware initialization can further enhance the effectiveness of parametric knowledge injection for specific downstream tasks. Yet, we use random initialization for LoRA if not mentioned explicitly for simplicity and broad applicability across various tasks.

### 3.3 Online Inference
In the previous stage (¬ß3.2.2), we generated a set of document-specific low-rank parameters for each document in the corpus K. This section describes how these parameters are utilized for RAG pipelines. Given a user query q, our proposed Parametric RAG pipeline proceeds through three steps: Retrieve, Update, and Generate. The following section details each step and illustrates the underlying mathematical operations.

#### 3.3.1 Retrieve
We first use a retriever R to calculate a relevance score Sd_i for each document di ‚àà K to the query q. We then select the top-k documents with the highest relevance scores, denoted as {d1, d2, . . . , dk} ‚äÜ K as the relevant external knowledge. Each retrieved document di has a corresponding parametric representation, i.e., a pair of low-rank matrices (Ai, Bi), previously obtained by the procedure described in ¬ß3.2.2.

#### 3.3.2 Update
After retrieval, we merge the low-rank matrices from the top-k retrieved documents to form a single plug-in module for the LLM. Following the setting of LoRA [14] convention, we use a scalar scaling factor Œ± to modulate the final update. The merged weight update, ŒîW_merge, is computed by summing over all retrieved documents:
ŒîW_merge = Œ± ¬∑ ‚àë_{j=1}^k A_j B_j^‚ä§.
Intuitively, ŒîW_merge combines the knowledge from multiple relevant documents into a single low-rank update that can be applied to the LLM‚Äôs base parameters. Once we obtain ŒîW_merge, we update the original feed-forward weight W by: W' = W + ŒîW_merge, thus yielding the final set of parameters for that layer at inference time. Conceptually, W' encodes the base model‚Äôs knowledge plus the aggregated knowledge from the top-k retrieved documents.

#### 3.3.3 Generate
After updating all feed-forward layers in the Transformer with ŒîW_merge, we obtain a temporary model L' (Œ∏'), where Œ∏' represents the updated model parameters, which are obtained by incorporating the merged low-rank parameters for all retrieved documents. We can then directly use L' to generate the final response to the query q using a standard left-to-right decoding process.

### 3.4 Discussion on Time/Space Efficiency

#### 3.4.1 Computation Cost
The computation cost of our method can be divided into offline preprocessing cost and online inference cost. The offline cost primarily arises from the Parametric Document Encoding (¬ß3.2.2). Let |d| be the average number of tokens in a document d, and h be the hidden dimension size of the LLM. The computational complexity of a typical decoder-only LLM is O(|d|^2 h + |d| h^2), where the attention layers complexity is O(|d|^2 h) plus the FFN layers O(|d| h^2). Theoretically, our method only introduces a constant coefficient change on the number of tokens processed, thus the overall time complexity remains O(|d|^2 h + |d| h^2).

Based on our implementation settings detailed in ¬ß4.3, the Data Augmentation process takes the original document d as input and subsequently generates approximately 2|d| new tokens. This process requires computational costs equivalent to a forward pass over 3|d| tokens, including the decoding of 1|d| tokens and the inference of 2|d| tokens. Training LoRA parameters on these augmented tokens requires a forward pass over 3|d| tokens and a backward pass equivalent to processing 6|d| tokens (typically about twice the forward-pass cost), resulting in an overall computational cost equivalent to processing 9|d| tokens. Adding the 3|d| tokens from the Document Augmentation phase, the total computational cost is approximately the cost of decoding 12|d| tokens in the LLM.

The online inference cost mainly depends on the number of input and output tokens. For simplicity, we focus on input tokens and ignore the variance in output tokens since they vary significantly from tasks and LLMs. Let |q| represent the length of input prompt/question q, and t be the number of retrieved documents. Since the time needed to load the LoRA parameters for t documents is negligible, the inference time complexity of our method is O(|q|^2 h + |q| h^2). In contrast, the time complexity of in-context RAG methods is O((t |d| + |q|)^2 h + (t |d| + |q|) h^2), which means that our method can save O(t^2 |d|^2 h + t |d| |q| h + t |d| h^2) time for online inference. Empirically, suppose that the lengths of q and d are approximately the same and significantly smaller than the hidden dimension of the LLM (e.g., about 4096 for LLaMA-8B), and we retrieve t = 6 documents for each q, then our method can roughly save 6|d| tokens in inference. Compared to its offline cost, this means that Parametric RAG is more cost-friendly than in-context RAG when the number of queries is more than twice that of documents in the life cycle of the service.

In summary, while the offline preprocessing step in Parametric RAG introduces additional computational overhead compared to traditional RAG, our analysis demonstrates that, when the system handles a large number of queries, Parametric RAG can provide a more carbon-efficient solution for large-scale RAG systems.

#### 3.4.2 Storage Overhead
In Parametric RAG, storage overhead comes from the Parametric Representation of each document, which consists of low-rank matrices from the FFN layer. Let r be the LoRA rank, n be the number of Transformer layers, h be the hidden size, and l the intermediate size of FFN, then the number of parameters in the Parametric Representation of a document is 2 n r (h + l). For example, with the LLaMA3-8B model (32 layers, hidden size 4096, intermediate size 14336), we need to store approximately 2.36M extra parameters (with r = 2 as used in our experiments). Stored at 16-bit precision, this requires around 4.72MB per document.

While the storage requirements for Parametric RAG may seem substantial compared to the raw documents, there are multiple methods to reduce its cost in practice. For example, previous studies find that the access of information in real user traffic follows a long-tail distribution [38]. Taking Google as an example, about 96.55% of Web pages receive zero traffic, and only 1.94% get one to ten visits per month [39]. Therefore, creating parametric representations for a tiny set of head documents can serve the majority of user requests, which significantly reduces the storage cost of Parametric RAG. Also, as shown in our experiments, Parametric RAG can be used with in-context RAG together for downstream tasks. Thus, it can serve as a natural boost to existing RAG methods without breaking their system pipelines.

## 4 Experimental Setup
In this section, we detail the experimental framework used to evaluate Parametric RAG. We begin with the introduction of our selected benchmark datasets (¬ß4.1). Next, we introduce our selected baseline methods (¬ß4.2) and implementation. Finally, we provide implementation details regarding our parameterization method, retrieval strategy, and inference settings (¬ß4.3).

### 4.1 Benchmarks and Metrics
We evaluate our approach on diverse benchmark datasets, each designed to assess different reasoning capabilities, such as multi-hop reasoning and commonsense inference. Specifically, we select the following datasets:
- 2WikiMultihopQA (2WQA) [13] is a dataset designed to test the model‚Äôs ability to perform multi-hop reasoning by integrating information across multiple Wikipedia passages.
- HotpotQA (HQA) [56] also focuses on evaluating multi-hop reasoning skills, requiring models to combine information from different contexts to address a single query.
- PopQA (PQA) [28] assesses factual question answering, challenging the model‚Äôs ability to recall accurate knowledge and resolve ambiguity in entity representation.
- ComplexWebQuestions (CWQ) [47] involves answering multi-step, web-based questions, further testing the model‚Äôs capacity to retrieve and reason over large-scale web content.

For evaluation metrics, we use the F1 score to evaluate performance on question-answering tasks, as it captures the balance between precision and recall by accounting for partially correct answers. Both 2WQA and HQA categorize questions based on reasoning types, with 2WQA divided into four categories and HQA into two. To comprehensively compare the performance of P-RAG and other RAG baselines across different reasoning tasks, our main experimental table presents the performance for each sub-task separately, using the first 300 questions from each sub-dataset. The table also presents the overall performance of each RAG baseline on the two datasets in the ‚ÄúTotal‚Äù column. Since the original datasets contain uneven distributions of question types, the ‚ÄúTotal‚Äù column is not a simple average of the sub-dataset performances.

### 4.2 Baselines
We choose the following RAG baselines for comparison:
- Standard RAG. This RAG method directly appends the top retrieved documents to the LLM‚Äôs input prompt. The prompt explicitly instructs the LLM to refer to the provided documents when answering the question and also includes instructions on the output format expected from the model.
- DA-RAG incorporates the augmented documents and QA pairs using the Data Augmentation method introduced in ¬ß3.2.1. This baseline aims to demonstrate that the performance improvement observed in Parametric RAG does not stem from the data augmentation phase but from the in-parameter knowledge injection.
- FLARE [19] is a multi-round retrieval augmentation method that triggers retrieval each time it encounters an uncertain token. When the retrieval module is triggered, the last generated sentence without the uncertain tokens is defined as the query.
- DRAGIN [45] is a multi-round retrieval augmentation method. It triggers retrieval when an uncertain token has semantic meaning and also has a strong influence on the following tokens. When the retrieval module is triggered, it formulates the query based on the model‚Äôs internal state and preceding context.
- P-RAG directly injects relevant documents into the LLM‚Äôs parameters through document parameterization, enabling efficient RAG without increasing the input context length.
- Combine Both. This baseline combines the in-context RAG method with P-RAG, leveraging both in-context and parametric knowledge injection. This baseline aims to evaluate whether the fusion of these approaches leads to better performance.

For P-RAG and all the baselines, we use the same retriever and select the top 3 retrieved documents as relevant. To ensure a fair comparison, we ensured that P-RAG and all the baselines share the same prompt template under the same dataset.

### 4.3 Implementation Details
In this subsection, we introduce the specific implementation of our experiments:

Base Models. We implement Parametric RAG using open-source pre-trained LLMs. To ensure the broad effectiveness of P-RAG across different models, we selected LLMs of varying scales and from different series, including Qwen2.5-1.5B-Instruct [55], LLaMA-3.2-1B-Instruct [29], and Llama-3-8B-Instruct [30]. All experiments were conducted using PyTorch on NVIDIA A100 GPUs with 40GB of memory.

Preprocessing and Parameterization. Consistent with prior works [19, 20, 44, 45], we utilize Wikipedia dumps as our external knowledge corpus, specifically adopting the dataset proposed by DPR [20]. For document augmentation, each document is rewritten once, and three QA pairs are generated based on the document (using the downstream LLM, if not mentioned explicitly). In the LoRA fine-tuning process, the learning rate was set to 3 √ó 10^‚àí4, and the training epoch was set to 1. The LoRA modules were exclusively integrated into the feed-forward network (FFN) matrices, excluding the query, key, and value (QKV) matrices. The scaling factor Œ± was configured to 32, LoRA rank r was set to 2, and no dropout was applied during training to ensure stability and full utilization of the parameter updates. The LoRA weight is randomly initialized following the setting of the original LoRA paper [14].

Retrieval Module. Recent studies on retrieval-augmented generation (RAG) [33] reveal that BM25 performs on par with, or even outperforms, state-of-the-art dense models in some scenarios. Given its strong performance, simplicity, and low computational cost, we adopt BM25 as the retriever for our approach. We use Elasticsearch as the backend for implementing BM25, with detailed configuration settings and instructions available in our code repository.

Generation Configuration. All experiments are conducted using the publicly released Hugging Face implementations of LLaMA and Qwen. We adopt the default hyperparameters and chat template provided in the official Huggingface repository, with the only modification being the use of greedy decoding to ensure the reproducibility of our reported results.

## 5 Experiments

### 5.1 Main Experiment
In this section, we present the main experimental result and an in-depth analysis of our proposed Parametric RAG compared with other RAG baselines and a combined setting that leverages both parametric and in-context knowledge injection. The experimental results are presented and we provide the following analysis:

(1) Overall Analysis. P-RAG outperforms existing RAG frameworks in most of the benchmarks and LLMs evaluated. This trend is especially obvious for Qwen-1.5B and LLaMA-8B. The improvements suggest that the incorporation of knowledge into model parameters can enhance the overall performance of the RAG pipeline, enabling the model to recall and reason over the injected knowledge more effectively. Furthermore, since these gains are observed in models from different series and parameter sizes, the results underscore the robustness and broad applicability of P-RAG.

(2) Comparison with DA-RAG. DA-RAG incorporates all the content generated during the Document Augmentation phase into the context, whereas our proposed P-RAG consistently outperforms DA-RAG across all settings. This result demonstrates that the performance improvement observed in Parametric RAG does not stem from the document augmentation phase, but from the in-parameter knowledge injection paradigm.

(3) Impact of Model Scale on P-RAG. The performance gap between P-RAG and other RAG baselines is noticeably more significant when moving from the LLaMA-1B model to LLaMA-8B. This discrepancy indicates that parametric injection becomes even more beneficial in larger-scale models because larger models can better leverage internalized document knowledge.

(4) Combine In-context RAG and P-RAG. The combined use of parametric and in-context RAG methods (Combine Both) yields the highest overall performance across various datasets and base LLMs. This result highlights that in-parameter knowledge injection is not in conflict with traditional RAG methods based on in-context knowledge injection. Consequently, our proposed document parameterization approaches can be seamlessly integrated for downstream tasks, allowing Parametric RAG to enhance existing RAG systems without disrupting their pipelines.

(5) Other Findings. Both DRAGIN and FLARE underperform significantly when applied to Qwen-2.5-1.5B. Our analysis suggests that Qwen-2.5-1.5B tends to produce highly confident answers regardless of uncertainty. Since these dynamic RAG frameworks rely on confidence to trigger retrieval, they rarely activate on Qwen-2.5-1.5B. This highlights a key limitation of uncertainty-based triggers and underscores the need for more robust mechanisms in dynamic RAG frameworks.

### 5.2 Impact of LoRA Weight Initialization
To investigate the impact of LoRA weight initialization strategies on our proposed Parametric RAG framework, we conducted an ablation study using two initialization strategies:

(1) Random Initialization (P-RAG Rand.): The LoRA weights are initialized randomly without any pretraining or warm-up, which represents the default setting for our proposed Parametric RAG approach. All the Parametric RAG experimental results reported in other sections in this paper are based on this setting.

(2) Warm-Up Initialization (P-RAG Warm.): LoRA weights are pre-trained using a set of 600 sampled question-answer (QA) pairs. These QA pairs are selected from the training sets of our chosen benchmarks and are distinct from the test questions to ensure no data leakage. The pre-training process involves training the LoRA parameters using the standard next-token prediction method on the concatenated tokens of the QA pairs. The pre-trained LoRA parameters are saved as initialization for the Document Parameterization phase.

The experimental results clearly indicate that across different model series and scales, as well as diverse datasets, the warm-up initialization strategy (P-RAG Warm.) consistently outperforms random initialization (P-RAG Rand.). This demonstrates the effectiveness of task-aware pretraining in enhancing the Parametric RAG pipeline. Furthermore, the observed improvements across varying model sizes confirm the scalability and generality of this approach. The superior performance of the warm-up approach in downstream tasks can be attributed to two key factors. First, it effectively aligns the additional LoRA parameters with the base LLM before document parameterizing, ensuring a smoother integration of knowledge. Second, it facilitates the incorporation of task-relevant knowledge, including output formats and generation patterns, which are critical for enhancing the quality of response in certain tasks. This finding suggests that in practical Parametric RAG applications where the downstream task is fixed, warming up the LoRA parameters for the task offers a promising approach to boost effectiveness. It is important to note that our main experiments (as well as all other experiments in this paper) were conducted using random initialization without any task-specific optimizations or dataset-specific tuning. This further highlights the strong generalization capability of our proposed Parametric RAG paradigm.

These findings also highlight a broader insight: embedding few-shot examples either in the model‚Äôs context or directly into its parameters leads to improved downstream task performance. Interestingly, our proposed parametric information representation method offers compatibility with few-shot in-context learning, enabling a combination of parametric and in-context knowledge augmentation.

### 5.3 Impact of Document Augmentation
To investigate the individual contributions of the rewriting and question-answer (QA) generation steps in the document augmentation process, we conduct a series of ablation experiments by removing (1) both rewriting and QA, (2) QA alone, and (3) rewriting alone. The observations are as follows:

(1) When neither rewriting nor QA generation is employed, the performance consistently degrades significantly across all evaluated tasks and models. This reduction suggests that simply training the LLM on the selected document via the next token prediction task without any form of data augmentation leads to insufficient internalization of facts by the model.

(2) Removing either QA or rewriting alone yields better results than removing both, indicating that each step offers distinct benefits. However, we notice that removing QA leads to a more significant performance decline than removing rewriting. This observation suggests that QA pair generation is more crucial for pushing the model to recall and apply factual information while rewriting offers valuable diversity in phrasing and structure and benefits the overall performance.

(3) Incorporating both rewriting and QA results in the strongest overall performance on most of the evaluated tasks and models. These findings reinforce that rewriting and QA generation play complementary roles.

In general, this ablation study indicates that both rewriting and QA generation significantly enhance the performance of document parameterizing. Their integration produces the best performance. Rewriting expands the coverage and diversity of context, while QA explicitly encourages the model to encode the knowledge of the selected document in a necessary way to apply the knowledge for downstream tasks. Therefore, it is advisable to incorporate both components of our document augmentation for effective internalization of knowledge.

### 5.4 Impact of Data-augmentation Model
To evaluate the impact of the choice of LLM in the Document Augmentation phase, we conducted an ablation study comparing different configurations of the model used for document rewriting and QA pair generation. In our default setting, we use the same LLM for both the document augmentation process and the downstream task. However, to explore whether the performance of our method is sensitive to this choice, we tested alternative configurations. Specifically, we tested different model sizes by using both smaller and larger LLMs for document augmentation and QA pair generation. The experimental results indicate that our framework demonstrates an insensitivity to the choice of data augmentation model. Performance remains consistent across different setups, regardless of whether a smaller, larger, or the same model as the generator is used for data augmentation. Importantly, using a small model for augmentation yields comparable results to employing significantly larger models, indicating that the augmentation step does not require high-capacity models to be effective. Similarly, when the same model is used for both generation and augmentation, the outcomes are indistinguishable from those where separate models are employed.

### 5.5 Runtime Analysis
We present the inference time for the LLaMA3-8B model across various RAG baselines on 2WikiMultihopQA (2WQA) and ComplexWebQuestions (CWQ). This evaluation simulates the online inference latency for answering a question using different RAG approaches. All experiments were conducted on the same GPU server to ensure consistent evaluation conditions. The experimental results indicate that P-RAG reduces the time per question by 29% to 36% compared to Standard RAG. Notably, the Combine Both baseline, which showed the best performance and significant improvements in the main experiment, requires almost the same online computation time as the Standard RAG method. In contrast, multi-round RAG frameworks like DRAGIN and FLARE exhibit significantly higher latency for answering a question compared to single-round methods.

For both P-RAG and Combine Both baselines, we present the inference times separately from the time required for merging and loading the LoRA (0.32s). This distinction arises because, in our current implementation, the time spent on merging and loading the LoRA significantly exceeds theoretical expectations. The floating-point operations involved in the LoRA operation step contribute less than 1% to the total computational cost of generating a response [14], but the latency of memory loading and data communications in our current implementation is far from perfect. We believe this latency can potentially be addressed through engineering optimizations. It is important to note that our analysis primarily emphasizes the relative time ratios and trends across the different methods, as actual application times and latencies can vary depending on hardware configurations, such as CPU, GPU, memory, and storage.

## 6 Conclusion and Future Directions
This work introduces Parametric RAG, a novel framework that addresses the limitations of in-context knowledge augmentation by parameterizing external documents. Parametric RAG infuses these parameterized documents directly into the model, reducing contextual overload and online computational costs while maintaining robust performance. Our experiments on multiple benchmarks demonstrate that Parametric RAG outperforms traditional retrieval-augmented generation methods across different LLMs. Ultimately, Parametric RAG offers a more efficient and scalable pathway to integrate external knowledge into LLMs, paving the way for further innovation in parametric-based knowledge augmentation.

Despite its significant potential, Parametric RAG presents several challenges that warrant further investigation. First, the current parameterization process is computationally intensive, and the parametric representations of each document are substantially larger than plain text. Future work could explore more methods to improve computational and storage efficiency, making the parameterization process more scalable. Second, the parameterized documents are currently tied to specific LLMs, restricting their ability to generalize across different models. Developing universal, model-agnostic representations could significantly enhance flexibility and reuse across diverse systems. Finally, we believe the potential applications of information parameterization can be extended beyond RAG. For instance, LLM-based agents could benefit from parameterizing the agent‚Äôs profiles and configuration, which could alleviate context-length constraints and improve online computational efficiency. By addressing these challenges, future research could unlock more potential for the Parametric RAG paradigm.