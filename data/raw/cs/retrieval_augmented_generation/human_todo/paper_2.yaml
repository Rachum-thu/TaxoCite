title: 'FoRAG: Factuality-optimized Retrieval Augmented Generation for Web-enhanced Long-form Question Answering'
blocks:
- block_id: 0
  content: Retrieval Augmented Generation (RAG) has become prevalent in question-answering (QA) tasks due to its ability of
    utilizing search engine to enhance the quality of long-form question-answering (LFQA). Despite the emergence of various
    open source methods and web-enhanced commercial systems such as Bing Chat, two critical problems remain unsolved, i.e.,
    the lack of factuality and clear logic in the generated long-form answers. In this paper, we remedy these issues via a
    systematic study on answer generation in web-enhanced LFQA. Specifically, we first propose a novel outline-enhanced generator
    to achieve clear logic in the generation of multifaceted answers and construct two datasets accordingly. Then we propose
    a factuality optimization method based on a carefully designed doubly fine-grained RLHF framework, which contains automatic
    evaluation and reward modeling in different levels of granularity. Our generic framework comprises conventional fine-grained
    RLHF methods as special cases. Extensive experiments verify the superiority of our proposed Factuality-optimized RAG (FoRAG)
    method on both English and Chinese benchmarks. In particular, when applying our method to Llama2-7B-chat, the derived
    model FoRAG-L-7B outperforms WebGPT-175B in terms of three commonly used metrics (i.e., coherence, helpfulness, and factuality),
    while the number of parameters is much smaller (only 1/24 of that of WebGPT-175B). Our datasets and models are made publicly
    available for better reproducibility.
  citations: []
- block_id: 1
  content: 'Retrieval Augmented Generation (RAG), a technique that augments Large Language Models (LLMs) with a retriever
    by appending the retrieved relevant passages to the current context [32], has recently attracted considerable research
    attention [6, 16]. The access to search engine supplements massive and latest knowledge to LLMs, boosting their performance
    on various knowledge intensive tasks, such as open domain dialogue [45] and question answering (QA) [43].


    Following this paradigm, many web-enhanced commercial systems have been developed, such as Bing Chat and perplexity.ai.
    They generate answers to user queries in natural language with reference to web pages, which we refer to as the web-enhanced
    long-form question-answering (LFQA) task. Although these systems can generate coherent and helpful answers, recent researches
    have revealed the low factuality issue of these systems, such that only about half of the statements generated are fully
    supported by the retrieved references [14, 28]. This poses an unignorable threat to the trustworthiness of existing web-enhanced
    LFQA systems.


    Despite its prevalence, there lacks an effective method to optimize factuality in web-enhanced LFQA as far as we are concerned.
    There are two intrinsic difficulties. First, previous studies mostly rely on human evaluation [28, 31, 34], which is generally
    expensive to acquire. In web-enhanced LFQA task, factuality is even more time-consuming and difficult to manually annotate
    compared to other language generation tasks, since it involves comparing the factual details of two lengthy texts [50].
    Second, the most commonly used fine-tuning method for human preference alignment, i.e., Reinforcement Learning from Human
    Feedback (RLHF), conventionally adopts the holistic reward, such that each answer only has a single evaluation score.
    Such a reward provides a relatively sparse training signal, which undermines the reliability of RLHF [40, 50]. In web-enhanced
    LFQA, the sparsity problem is even exaggerated, as the answers are in long form.


    Besides the above factuality issue, different from conventional QA tasks with short answers, web-enhanced LFQA poses extra
    challenges due to the pervasive ambiguity of many real-world questions. A desirable answer to these questions is preferred
    to be multifaceted [2], which requires organizing and consolidating information from multiple aspects and references [25].
    The problem might be one possible reason why existing open source methods such as WebGLM [29] have no explicit improvement
    over closed source methods such as WebGPT-175B.


    To resolve the above issues, in this paper, we conduct a systematic study of web-enhanced LFQA. Specifically, we first
    propose a novel outline-enhanced generator, which achieves clear logic in the generation of multifaceted answers. We then
    propose an innovative factuality optimization approach based on a novel doubly fine-grained RLHF framework. Specifically,
    we design new automatic evaluation and reward modeling steps in different granularities, which allows to optimize factuality
    for RAG in a flexible way. Our generic method contains several existing fine-grained RLHF methods as special cases.


    Extensive experiments demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance
    on both Chinese and English benchmarks. Specifically, the outline-enhanced generator significantly improves the coherence
    and helpfulness, while the doubly fine-grained factuality optimization method substantially promotes the factuality on
    both answer and sentence levels. Remarkably, applying our method to Llama2-7B-chat yields a fine-tuned model FoRAG-L-7B,
    which, for the first time, surpasses the WebGPT-175B on coherence, helpfulness, and factuality, while the number of parameters
    of FoRAG-L-7B is much smaller (only 1/24 of that of WebGPT-175B).


    The contributions of this work are summarized as follows:

    - We propose a new outline-enhanced generator to promote a clear logic of long answer generation in RAG, which significantly
    improves the coherence and helpfulness of the generated answers. Two large-scale outline-enhanced LFQA datasets are accordingly
    constructed.

    - We propose a novel factual optimization method for web-enhanced RAG based on a novel doubly fine-grained RLHF framework,
    which eschews the need of expensive human annotation.

    - We conduct extensive experiments to show that our method achieves state-of-the-art performance on both Chinese and English
    benchmarks. Notably, the FoRAG-L-7B model fine-tuned by our method outperforms WebGPT-175B on coherence, helpfulness,
    and factuality, with only 1/24 of the parameters in WebGPT-175B. Both datasets and models are publicly available for better
    reproducibility.'
  citations:
  - marker: '[2]'
    intent_label: Domain Overview
    topic_label: Generation Quality
  - marker: '[6]'
    intent_label: Domain Overview
    topic_label: Retrieval Integration for Generation
  - marker: '[14]'
    intent_label: Research Gap
    topic_label: Generation Quality
  - marker: '[16]'
    intent_label: Domain Overview
    topic_label: Retrieval Integration for Generation
  - marker: '[25]'
    intent_label: Domain Overview
    topic_label: Context Curation and Compression
  - marker: '[28]'
    intent_label: Research Gap
    topic_label: Generation Quality
  - marker: '[29]'
    intent_label: Research Gap
    topic_label: NLP Applications
  - marker: '[31]'
    intent_label: Research Gap
    topic_label: Evaluation Aspects and Tools
  - marker: '[32]'
    intent_label: Domain Overview
    topic_label: Input-Layer Integration
  - marker: '[34]'
    intent_label: Research Gap
    topic_label: Evaluation Aspects and Tools
  - marker: '[40]'
    intent_label: Research Gap
    topic_label: Independent Training
  - marker: '[43]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[45]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[50]'
    intent_label: Research Gap
    topic_label: Evaluation Aspects and Tools
- block_id: 2
  content: 'In this section, we review prior work in three related fields, i.e., open-domain question answering, retrieval
    augmented generation, as well as web-enhanced LFQA.


    Open-domain Question Answering. In the field of open-domain QA, traditional efforts have centered around reading comprehension
    techniques, with foundational datasets like SQuAD [38] providing human-written questions and extracted answers. Subsequent
    datasets, including Natural Questions [26], TriviaQA [20], and CoQA [41], continue this trend but largely cater to brief
    answers. Recognizing the value of more informative, long-form responses, recent initiatives such as ELI5 [12] have begun
    to compile questions demanding detailed answers, prompting research into advanced generative techniques to meet this need.


    Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) enhances language model (LM) performance by integrating
    external knowledge retrieval with in-context learning. The knowledge retrieval techniques in RAG include sparse methods
    such as BM25 and TF-IDF and dense retrieval systems, including DPR [22] and Contriever [18]. To utilize the retriever,
    various methods are proposed. REALM proposes a joint optimization of retriever and language modeling [16]. Retro uses
    a frozen retriever to enhance the generation ability model with a novel chunked cross-attention mechanism [6]. Atlas studies
    the few-shot ability for RAG models. Others combine black-box LMs with custom or fine-tuned retrieval systems [39, 42].
    Different from these work, we treat the retrieval step as a black box and focus on improving the generation quality given
    the query and retrieved passages.


    Web-enhanced LFQA. The web-enhanced LFQA takes a new approach to QA tasks which utilizes the retrieval ability of search
    engine to improve the generation performance on long-form QA. Closely related to our work, WebGPT [34] uses the questions
    from ELI5 and explores the ability of LLMs in navigating through the web, retrieving passages from web, and generating
    long-form responses. Despite its notable capabilities, the dataset, and models are not accessible to the public. Following
    this idea, WebCPM [37] builds and releases an open-source web-enhanced LFQA model in Chinese. WebGLM [29] provides an
    more efficient and cost-effective method by replacing the expert annotation with evaluations using LLMs and utilizing
    a non-interactive way to use search engine. However, its resulting model, the WebGLM-10B does not outperforms the WebGPT-175B.
    Compared to these works, we consider optimizing the logic structure and factuality of the generation, which has not been
    studied in web-enhanced LFQA as far as we know.'
  citations:
  - marker: '[6]'
    intent_label: Prior Methods
    topic_label: Intermediate-Layer Integration
  - marker: '[12]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[16]'
    intent_label: Prior Methods
    topic_label: Joint Training
  - marker: '[18]'
    intent_label: Prior Methods
    topic_label: Retriever Type
  - marker: '[20]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[22]'
    intent_label: Prior Methods
    topic_label: Retriever Type
  - marker: '[26]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[29]'
    intent_label: Research Gap
    topic_label: NLP Applications
  - marker: '[34]'
    intent_label: Research Gap
    topic_label: NLP Applications
  - marker: '[37]'
    intent_label: Prior Methods
    topic_label: NLP Applications
  - marker: '[38]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[39]'
    intent_label: Prior Methods
    topic_label: Parameter-Inaccessible Generators (Black-box)
  - marker: '[41]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[42]'
    intent_label: Prior Methods
    topic_label: Parameter-Inaccessible Generators (Black-box)
- block_id: 3
  content: 'In this section, we briefly review the RAG pipeline in web-enhanced LFQA task, which for simplicity of presentation,
    we adopt the term web-enhanced RAG to describe in the sequel.


    In web-enhanced RAG, for a given user input x, the system first utilizes a web search engine to retrieve a list of relevant
    website URLs, then crawls the websites and extracts the relevant text segments z, which are usually called reference or
    context for generation [37]. This extraction is commonly done by first segmenting the web pages into text segments and
    then using pre-trained dense retrievers to extract the top-k segments [29].


    After deriving the context z, the RAG system generates an answer y based on the context z and the user query x. Following
    [40], the response generation can be formulated as a Markov Decision Process (MDP) <S, A, R, P, γ>. In such a process,
    each episode starts with a sampled state s ∈ S, where s = (x, z) is a prompt that contains a query x and a relevant context
    z (here the parenthesis denotes string concatenation). At each step t during this episode, the state s_t = (x, z, a1,
    ..., a_{t-1}) is described by the query x, the context z, and all the previously generated tokens (a1, ..., a_{t-1}),
    which is denoted a_{t-1} for short. Given the state s_t, the LLM, denoted by π_θ, defines a probability distribution a
    ∼ π_θ(·|s_t) over all tokens a ∈ A conditioned on the current state s_t, where θ denotes the trainable parameters of the
    LLM. After generating the specific token a_t ∈ A, the state will transit to s_{t+1} = (s_t, a_t) at the next time step
    t+1 by appending the latest generated a_t token to the current state s_t. This episode terminates when the length t exceeds
    a pre-defined threshold T or an end-of-sequence token is generated. In the above definition of MDP, the parameter γ is
    a discount factor.


    In most of the language generation tasks, we have a task specific evaluation metric R(a_T, x, z) that depends on the final
    context s_T = (x, z, a1, ..., a_T) which consists of the generated sequence a_T and the initial context x, z, which is
    typically given at the end of sequence to reflect the quality of the generated sequence, e.g., whether the sequence is
    helpful or harmless [4]. Depending on the evaluation granularity, R(a_T, x, z) might be a scalar or a vector or even a
    matrix, we denotes the three cases by r, r (bold), R (capital bold), respectively.


    Besides helpfulness or harmlessness, one crucial criterion of response generation in RAG is factuality (or verifiability),
    which leverages the extent to which the generated response is trustful. In general, the response y is considered to be
    truthful if its contents are factually consistent with the retrieved text z [48]. In most previous works [33, 47], factuality
    is mainly evaluated by human annotation or via the NLI model (i.e., whether the context can entail the information contained
    by the response) [53] or general purpose LLMs [17, 21, 27], such as ChatGPT [36] or GPT4 [35].'
  citations:
  - marker: '[4]'
    intent_label: Problem Formulation
    topic_label: Generation Quality
  - marker: '[17]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[21]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[27]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[29]'
    intent_label: Prior Methods
    topic_label: Retriever Type
  - marker: '[33]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[35]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[36]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[37]'
    intent_label: Problem Formulation
    topic_label: Input-Layer Integration
  - marker: '[40]'
    intent_label: Problem Formulation
    topic_label: Training Strategies
  - marker: '[47]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[48]'
    intent_label: Problem Formulation
    topic_label: Generation Quality
  - marker: '[53]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
- block_id: 4
  content: In this section, we propose the Outline-Enhanced generation technique, which is able to generate well-structured
    responses of high quality. The outline-enhanced generator takes a two-stage generation, where the generator first generates
    an organizational pattern and outline to improve the logic structure. In the following, we describe our technique and
    the corresponding construction of two outline-enhanced LFQA datasets.
  citations: []
- block_id: 5
  content: 'In most existing open-source methods [29, 34, 37], the responses are directly generated, i.e., the retrieved contents
    are concatenated with the original query and fed into a generation model using certain prompt template. Compared to those
    generated by closed-source methods, these responses are shorter and often found unorganized, lacking a clear logical structure.


    To enhance the performance, one possible way is to make the responses more organized. Indeed, some researchers have found
    that carefully designed prompts that comprise task descriptions and a few demonstrations will improve the quality of the
    generated responses on various tasks [7]. For example, the technique of “Let’s think step by step" [23] substantially
    improves the performance by encouraging the chain-of-thought reasoning ability.


    Inspired by the above works, we introduce the outline-enhanced technique into response generation. Our proposed generator
    includes an outline stage and an expansion stage, which aligns with the intuition that when answering a question, human
    usually first outlines and organizes the answer before expanding each point. Specifically, to generate high-quality output
    with a clear logic flow, we prompt the model to first output an outline of the final answer, and then concatenate the
    draft into the prompt to generate the full response. In the following, we elaborate the two stages in detail.


    Outline Stage. In this stage, the generator first drafts an outline of the answer using an outline template, with the
    user query x and context z as input. The outline template guides the LLM to first consider which organizational pattern
    is best suitable to the current question, e.g., “cause and effect" or “compare and contrast". Then the LLM uses the organizational
    pattern to output an outline. For example, when the selected pattern is “compare and contrast", the generated outline
    will include various perspectives that will later be used to expand on the similarities and differences.


    Expansion Stage. Based on the outline generated at the former stage, the LLM expands each perspective to construct the
    final answer. Specifically, the model is then asked to generate an answer to the question, given the input containing
    the query x, the context z and the outline o generated in the first stage.


    The training of the outline-enhanced generator follows the standard supervised fine-tuning (SFT) procedure, which is widely
    adopted in previous works [4, 36].'
  citations:
  - marker: '[4]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Parameter-Accessible Generators (White-box)
  - marker: '[7]'
    intent_label: Prior Methods
    topic_label: Training-free Methods
  - marker: '[23]'
    intent_label: Prior Methods
    topic_label: Training-free Methods
  - marker: '[29]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
  - marker: '[34]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
  - marker: '[36]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Parameter-Accessible Generators (White-box)
  - marker: '[37]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
- block_id: 6
  content: 'As far as we know, there are only two open-sourced web-enhanced long-form QA datasets available for training web-enhanced
    RAG models. The English dataset, i.e. the WebGLM-QA [29], contains 44k samples, while the Chinese dataset, i.e. WebCPM
    [37], contains 5,500 samples. The queries in both datasets are sampled from ELI5 [12], where WebGLM-QA sample question
    from it, and WebCPM additionally uses human annotators to translate the question into Chinese. The Web search engine are
    used to collect relevant passages.


    We construct an outline-enhanced bilingual long-form QA dataset using the queries and relevant passages from these two
    datasets. We apply our outline-enhanced generation technique using GPT4 [1] to collect outline-enhanced answers. We design
    a prompt to instruct GPT4 to execute the outline stage and the expansion stage in a step-by-step manner. The detailed
    statistics of the existing datasets and our outline-enhanced answers are reported in the paper. It is clear that our demonstration
    answers are much longer than that in existing works, due to the stronger logic structure. The outline-enhanced answers
    derived from WebCPM and WebGLM are publicly available.


    Note that the imbalance of the amount of training samples in English and Chinese datasets may pose difficulty to train
    a bilingual web-enhanced RAG model. To overcome this difficulty, we further collect 39k queries and relevant passages
    in Chinese from public sources, and follow the same process to generate outline-enhanced answers. These data will be released
    to the public after passing the censoring process of data release.'
  citations:
  - marker: '[1]'
    intent_label: Resource Utilization
    topic_label: Parameter-Inaccessible Generators (Black-box)
  - marker: '[12]'
    intent_label: Resource Utilization
    topic_label: NLP Applications
  - marker: '[29]'
    intent_label: Resource Utilization
    topic_label: NLP Applications
  - marker: '[37]'
    intent_label: Resource Utilization
    topic_label: NLP Applications
- block_id: 7
  content: In this section, we propose a novel factuality optimization method to address the aforementioned factuality issue
    in web-enhanced LFQA. Specifically, we first discuss the difficulty of directly applying the conventional RLHF method
    to factuality optimization, then develop a novel doubly fine-grained RLHF framework, which characterizes different granularities
    of automated evaluation and reward modeling, upon which our method is built.
  citations: []
- block_id: 8
  content: 'In LLM alignment, reinforcement learning with human feedback (RLHF) [10, 36] is a widely used technique to reduce
    undesirable generations, e.g., harmful responses in chat assistant tasks [4]. Viewing nonfactuality as a certain kind
    of undesirable behaviors, a natural way to promote factuality in web-enhanced RAG is to utilize RLHF to prevent the generator
    from producing nonfactual responses. To proceed, we first give a detailed description of RLHF.


    Conventionally, RLHF is conducted on manually annotated preference data. For example, given the query x and the retrieved
    context z, the factuality of an answer a_T (tokenized as (a1, . . . , a_T)) can be annotated as r ∼ R(a_T, x, z), where
    r ∈ [0, 1] reflects the underlying human preference. RLHF trains a reward model R̂ to estimate the factuality given any
    query x, reference z, and answer a, i.e., to learn the human preference function R. Then RL methods such as PPO are applied
    to optimize the generation model based on the trained reward model R̂. The optimization problem can be formulated as maximizing
    the expected reward with a KL divergence regularization term to prevent the generation model from deviating too far from
    a reference model π_ref.


    Directly applying the conventional RLHF method to factuality optimization in web-enhanced LFQA will encounter two intrinsic
    difficulties. First, the manually annotated factuality labels are typically expensive to collect, which involves comparing
    the factual details between a long answer and its corresponding lengthy reference. Second, as shown in the above equation,
    the standard RLHF uses the holistic reward, i.e., 1(t = T) R(a, x, z), which is not zero only for the last token of the
    whole response. This holistic reward can only provide a sparse signal for the training of the generation model π_θ. In
    web-enhanced LFQA where the answers are usually longer, the sparsity problem due to the use of the holistic reward will
    be even exaggerated.'
  citations:
  - marker: '[1]'
    intent_label: Prior Methods
    topic_label: Independent Training
  - marker: '[4]'
    intent_label: Prior Methods
    topic_label: Trustworthiness and Robustness
  - marker: '[10]'
    intent_label: Prior Methods
    topic_label: Independent Training
  - marker: '[36]'
    intent_label: Prior Methods
    topic_label: Independent Training
- block_id: 9
  content: 'In light of the above difficulties of conventional RLHF in factuality optimization for web-enhanced RAG, we propose
    a doubly fine-grained RLHF framework to conduct factuality optimization in a fine-grained manner. Our framework is inspired
    by recent study on fine-grained RLHF [50, 52]. Unlike these previous works that mainly focus on a single dimension, our
    framework incorporates fine-grained designs of two core steps, i.e., factuality evaluation and reward modeling.


    Before elaborating our framework in details, we first introduce necessary notations and definitions, which enables to
    characterize multiple rewards for an answer that constitute a denser reward signal R̂_φ for the RL process. Specifically,
    following [50], we first segment the output a into L text spans (a1, a2, . . . , a_L) corresponding to the evaluation
    granularity (which will be described later) of R̂_φ, where each segment a_j ends at the step T_j. The dense reward signal
    R̂_φ is an L-dimensional vector, whose j-th dimension represents the reward R̂_φ(a|x, z)[j] ∈ [0, 1] for each segment
    a_j given query x and retrieved context z as the input, which is assigned to the final token in a_j. Especially, when
    L = 1, our method degenerates to the standard RLHF with holistic reward.


    Fine-grained Evaluation. Recall that to perform high quality automatic factuality evaluation, recent methods have been
    proposed to first decompose a long answer into shorter pieces and then evaluate the factuality of each piece with respect
    to the given reference [21, 27]. Inspired by these methods, we consider three different levels of granularity in the answer
    decomposition and automated segment evaluation:

    - Holistic: It is the standard granularity to evaluate the answers [10]. Each generated answer is associated with a single
    factuality score r.

    - Sentence-level: As is suggested by previous research on automatic evaluation [24, 27], we can segment the answer into
    sentences, then evaluate each sentence individually. In this case, the evaluation result is denoted as r_i where i is
    the index for the sentence.

    - Subclaim-level: Following [9, 21, 33], we can further decompose each sentence into multiple subclaims via an LLM, each
    containing a single piece of factual information. After the decomposition, we evaluate each subclaim individually. Since
    the decomposition using LLM breaks the association between the subclaim and the original answer, we aggregate the scores
    of all subclaims into a single score to evaluate the factuality of the sentence. More specifically, assuming there are
    j subclaims for sentence i, then the evaluation score for the sentence is given as r_i = Agg_j(R_i^j), where R_i^j denotes
    the factuality score of the subclaim j of sentence i, and Agg is the aggregation function (in the form of average, minimum,
    or maximum).


    Fine-grained Reward Modeling. Recall that to build a reward model to estimate the factuality of a given answer, standard
    RLHF methods typically use a sequence-level reward model that produces a single factuality score for each answer. Recently,
    a token-level reward modeling method has been introduced to provide token-level feedback [52]. Enlightened by these methods,
    we can construct the reward model in two possible levels of granularity.

    - Sequence-level: A single reward R̂_φ(a|x, z) is learned for each sequence, whose actual form depends on the granularity
    of the evaluation. In this way, the associated reward reflects the factuality of the corresponding sequence, which is
    then assigned to the last token of each sequence.

    - Token-level: A reward r̂(s_t, a_t) learned for each token in the sequence. In this way, the reward of the sequence is
    calculated by aggregating the all token-level rewards i.e., R̂_φ(a|x, z) = Agg_t(r̂(s_t, a_t)).


    The training loss of each combination of automated evaluation and reward modeling in different levels of granularity is
    described in the paper. In most cases, the reward r is binary labelled, and the reward model is trained with the Logloss.
    When the aggregation step in subclaim-level evaluation yields continuous-valued reward r, we choose to use the MSE loss
    for reward model training instead.


    After the reward model R̂ is learned, we adopt PPO to optimize the generation model by maximizing a reward that assigns
    non-zero rewards to the final tokens of segments, alleviating the sparse feedback signal problem in conventional RLHF.


    Note that our proposed framework unifies the existing fine-grained RLHF works [50, 52] by containing these methods as
    special cases. Moreover, although our framework is motivated by optimization factuality for web-enhanced RAG, it can also
    be generalized to other RLHF tasks.'
  citations:
  - marker: '[1]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Independent Training
  - marker: '[9]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Evaluation Aspects and Tools
  - marker: '[10]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Evaluation Aspects and Tools
  - marker: '[21]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Evaluation Aspects and Tools
  - marker: '[24]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Evaluation Aspects and Tools
  - marker: '[27]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Evaluation Aspects and Tools
  - marker: '[33]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Evaluation Aspects and Tools
  - marker: '[50]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Independent Training
  - marker: '[52]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Independent Training
- block_id: 10
  content: In this section, we conduct extensive experiments to validate the effectiveness of our outline-enhanced generation
    technique and factuality optimization methods.
  citations: []
- block_id: 11
  content: 'Datasets. We conduct experiments on two commonly used datasets for web-enhanced long-form QA.


    The WebGPT’s dataset. Although the training dataset originally used for WebGPT is not publicly available, the 272 samples
    released on the WebGPT demo website can be used as a testbed for performance comparison [29]. In this dataset, each sample
    consists of a question from the ELI5 dataset [12], several Bing retrieved web pages, and extracted references. Note that
    it is a pure English dataset.


    The WebCPM’s dataset. This is a Chinese dataset constructed similarly to the WebGPT dataset. As there is no official train-test
    split, we randomly split 4,676 samples for training, 426 for validation, and 398 for testing.


    Compared Methods. We compare our method with three web-enhanced baseline methods.


    WebGPT [34] supports interactive web search for long-form QA. It has two versions, namely WebGPT-13B and WebGPT-175B,
    where the latter one is the currently state-of-the-art performing model for web-enhanced QA. Note that when comparing
    with WebGPT, we directly use the responses collected from its website.


    WebCPM [37] is an open source web-enhanced RAG involving interactive web search. It is the first work on Chinese web-enhanced
    RAG. It is trained on a dataset which contains 5,500 question-answer pairs in Chinese with references.


    WebGLM [29] is an open source web-enhanced QA system with human performance. It simplifies the interactive web search
    approach in WebGPT and WebCPM by a two-step retriever and generator framework. It is trained on the WebGLM-QA dataset,
    which focuses on English only.


    Metrics. We adopt three commonly used metrics for web-enhanced RAG, i.e., coherence, helpfulness, and factuality. As existing
    works show that GPT4’s evaluation is highly consistent to human annotations in both English [5, 13, 15, 19, 30] and Chinese
    [51], we use GPT4 to evaluate these metrics. For the completeness of our study, we also justify the consistency between
    GPT4 and human annotation in Chinese in ablation study. Following the framework of [30], we evaluates the coherence (Cohr.)
    and helpfulness (Help.) metrics. We count the scores greater than or equal to 4 as the judging criteria. For evaluation
    of factuality consistency, we adopt the method in [8] to achieve fine-grained evaluation. In addition, since the longer
    answers are more likely to have factuality mistakes, for the fairness of the evaluation, we report the scores at two granularities,
    i.e., query-level (Fact/q.) and sentence-level (Fact/s.). The prompts we used for the evaluations are given in the paper.


    Models and Training Configuration. Our experiments are conducted by fine-tuning on Llama2-7B-chat [46] and ChatGLM2-6B
    [11], which are widely used LLMs for question-answering in English and Chinese respectively. The prompt templates at fine-tuning
    and inference stages are provided in the paper. The maximum context length is set to 4096 for Llama2-7B-chat and 8192
    for ChatGLM2-6B. Both models are fine-tuned on 8 A100 GPUs for 5 epochs with an initial learning rate of 1e-5 and a cosine
    learning rate scheduler. Following the configuration of WebCPM [37], we adopt beam search for each inference on a single
    A100 GPU with the num_beams parameter set to 3. We use our outline-enhanced dataset to conduct supervised fine-tuning
    (SFT) [36], and our multi-granularity evaluation data to conduct corresponding factuality optimization. In order to decrease
    noise in the RLHF step, we normalize the reward. Specifically, for each prompt (x, z), we generate a response a_T using
    the SFT model, and estimates its reward score R̂(a_T, x, z) using the learned reward model. For any model generated answer
    a''_T'', we take R̃(a''_T'', x, z) = R̂(a''_T'', x, z) − R̂(a_T, x, z) as the estimated reward, and the same technique
    is applied to sentence-level and subclaim level factuality evaluations.'
  citations:
  - marker: '[5]'
    intent_label: Metrics Utilization
    topic_label: Evaluation Aspects and Tools
  - marker: '[8]'
    intent_label: Metrics Utilization
    topic_label: Evaluation Aspects and Tools
  - marker: '[11]'
    intent_label: Model/Architecture Adoption
    topic_label: Parameter-Accessible Generators (White-box)
  - marker: '[12]'
    intent_label: Benchmark Utilization
    topic_label: NLP Applications
  - marker: '[13]'
    intent_label: Metrics Utilization
    topic_label: Evaluation Aspects and Tools
  - marker: '[15]'
    intent_label: Metrics Utilization
    topic_label: Evaluation Aspects and Tools
  - marker: '[19]'
    intent_label: Metrics Utilization
    topic_label: Evaluation Aspects and Tools
  - marker: '[29]'
    intent_label: Result Comparison
    topic_label: NLP Applications
  - marker: '[30]'
    intent_label: Setting/Protocal Adoption
    topic_label: Evaluation Aspects and Tools
  - marker: '[34]'
    intent_label: Result Comparison
    topic_label: NLP Applications
  - marker: '[36]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Independent Training
  - marker: '[37]'
    intent_label: Result Comparison
    topic_label: NLP Applications
  - marker: '[46]'
    intent_label: Model/Architecture Adoption
    topic_label: Parameter-Accessible Generators (White-box)
  - marker: '[51]'
    intent_label: Metrics Utilization
    topic_label: Evaluation Aspects and Tools
- block_id: 12
  content: 'The main empirical results of our method trained on Llama2-7B-chat (FoRAG-L-7B) and ChatGLM2-6B (FoRAG-C-6B) are
    depicted in the paper. We here mainly report the results of FoRAG-L-7B, which attains the best performance among all possible
    combinations of the granularities of evaluation and reward model.


    Overall Performance. We compare the overall performance of FoRAG-L-7B and FoRAG-C-6B with existing methods on both datasets.
    From the results, we observe that on both English and Chinese datasets, FoRAG-C-6B surpasses all baselines on five out
    of six metrics, and FoRAG-L-7B performs the best on all metrics. Notably, FoRAG-L-7B substantially outperforms WebGPT-175B
    that contains 24 times more parameters, showing superiority of our method in bilingual web-enhanced RAG tasks.


    Evaluation of Outline-Enhanced Generator. We evaluate the effectiveness of outline-enhanced generator as a core design
    in dataset collection and RAG model design by showing that, without such technique, the reduced method will deteriorate
    severely. Specifically, for the reduced variant, we train the backbone models on a merged dataset from WebCPM and WebGLM-QA,
    which contains 4.7k samples in Chinese and 44k samples in English. Note that here we use the demonstration answers as
    provided in the original datasets, i.e., the answers in WebCPM are human written and answers in WebGLM-QA are GPT4 generated,
    which can be considered in high quality.


    The results show that applying our technique of outline-enhanced generator significantly boosts the performance in terms
    of coherence and helpfulness on both datasets. As for factuality, the sentence-level measurements of our methods are a
    little bit higher or comparable with the counterpart models without outline-enhanced techniques. In addition, applying
    our technique increases the length of model generations.


    Evaluation of Factuality Optimization. We then evaluate the effectiveness of the factuality optimization technique by
    comparing our method with the counterpart method without such a mechanism. Adding factuality optimization technique significantly
    raises the factuality consistency score in both query and sequence levels, without affecting the other two metrics or
    the generation length. The above results justify the introduction of factuality optimization technique to our proposed
    method.'
  citations: []
- block_id: 13
  content: 'In this subsection, we compare the performance of various implementations of our method on different evaluation
    and reward model granularities, with the following two commonly used alignment methods as baselines:


    MLE with Filtering (filter.): This method applies a filter to drop the samples with factual inconsistency errors and preserve
    the factually consistent ones. Then it follows the standard SFT procedure, i.e., fine-tuning the model by optimizing the
    maximum likelihood estimation loss on the positive samples.


    Unlikelihood: This method fine-tunes the model by maximizing the likelihood of positive (i.e., factually consistent) samples
    and minimizing the likelihood of negative (i.e., factually inconsistent) samples simultaneously.


    For a fair comparison, all the methods are fine-tuned from the same model, i.e., Llama2 after SFT on our outline-enhanced
    dataset. From the results, we observe that our proposed method attains better factual consistency than the baselines,
    regardless of the granularity of evaluation or reward modeling. In addition, among all the granularities of evaluation,
    subclaim-level evaluation performs the best. We also notice that token-level reward modeling performs worse than the conventional
    segment-level reward modeling, presumably because the length of our datasets may make token-level modeling over-fit.'
  citations: []
- block_id: 14
  content: 'We now conduct ablation study to justify the rational of some certain design choices in our proposed method.


    Effectiveness of the Outline-Expansion Two-Step Answer Generation. To illustrate the impact of our outline-enhanced generation
    technique, we train two baseline models that generate answers directly based on our dataset, which lack the outline stage.
    The outcomes clearly show that our outline-enhanced generation approach significantly augments the model’s capabilities
    by enhancing the coherence and helpfulness of the answers generated, with a particularly notable improvement observed
    in the Chinese language task.


    On GPT4 Evaluation Quality. To evaluate how well GPT4 correlates with human judgment in Chinese, we recruit 10 native
    Chinese-speaking annotators. Their task is to manually review coherence, helpfulness, and both query-level and sentence-level
    factuality on the Chinese generated results. A subset of 200 examples is selected, and we conduct two rounds of human
    evaluation on it. In each round, each sample is randomly assigned to one annotator. We report the agreement rate (the
    ratio of overlap) between two-round human labels and GPT4’s judgements. The results confirm a robust correlation between
    GPT4 and human ratings on Chinese QA evaluation. Except on the query-level factuality, human suffers from comparing two
    lengthy texts, a conclusion that is consistent with [50].


    Effects of Imbalance Dataset. To evaluate how the imbalance of the two languages in the dataset affects the training effect
    on the resulting bilingual LLMs, we perform further ablation study on the level of imbalance. We fix the number of training
    samples to be 40k. Then we tune the ratio of Chinese to English on five level, ranging from 1:10, 1:3, 1:1 to 3:1, 10:1.
    We then randomly sample the corresponding amount of samples from our dataset and train the models based on Llama2-7B using
    SFT. The evaluation results show that with an increasing amount of data, the model’s performance on the corresponding
    language increases on both coherency and helpfulness metrics. Meanwhile, the factuality metric is not affected by this
    ratio. Note that the performance of Llama2-7B is more sensitive to the number of training samples in Chinese. This may
    be because the pre-training of Llama2-7B contains more corpus in English, and therefore a few examples is enough to adapt
    to the new task.'
  citations:
  - marker: '[50]'
    intent_label: Background
    topic_label: Evaluation Aspects and Tools
- block_id: 15
  content: 'We finally evaluate the training efficiency of our proposed method. In the following, we will examine the additional
    computation cost of the two new modules of FoRAG, i.e., outline-enhanced generation and doubly fine-grained RLHF, respectively.


    The first step, i.e., outline-enhanced generation, has almost negligible effect on training time. During inference, it
    requires roughly 10% more tokens to be generated, and the extra time consumed at inference stage is roughly proportionally
    to this increase in tokens generated. Note that this extra inference time can be eliminated using context distillation
    techniques.


    The second step, i.e., doubly fine-grained RLHF, has no impact on inference time. To evaluate the additional computational
    expense during training, we consider a naive implementation of FoRAG that sequentially evaluates the reward for each sentence.
    The best performed version, the subclaim version of the doubly fine-grained RLHF framework, takes about 67.7% more time
    than standard RLHF. Note that the additional computational cost can be further reduced via implementation with a multi-head
    reward layer and carefully designed attention mask can use one forward pass to calculate the reward for all sentences,
    which will make the extra computational cost insignificant.


    In summary, FoRAG outperforms the baseline method with reasonable additional computational cost.'
  citations: []
- block_id: 16
  content: In this paper, we propose a novel answer generation method FoRAG for web-enhanced LFQA to tackle the factuality
    issue and lack of clear logical structure in existing methods. To this end, we first devise an outline-enhanced generator
    to fulfill clear logic in long-form answers and accordingly construct two datasets. Then we propose to optimize factuality
    in a carefully designed doubly fine-grained RLHF framework. Our developed framework contains automatic evaluation and
    reward modeling in different levels of granularity, and compasses traditional fine-grain RLHF methods as special cases.
    Empirically, FoRAG achieves state-of-the-art performance in terms of coherence, helpfulness, and factuality on both English
    and Chinese benchmarks. Notably, applying FoRAG to Llama2-7B-chat, we derive FoRAG-L-7B, which outperforms WebGPT-175B
    with only 1/24 in the number of parameters of WebGPT-175B.
  citations: []
