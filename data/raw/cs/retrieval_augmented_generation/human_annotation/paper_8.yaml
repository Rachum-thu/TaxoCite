title: Retrieval Augmented Generation with Collaborative Filtering for Personalized Text Generation
blocks:
- block_id: 0
  content: 'Recently, the personalization of Large Language Models (LLMs) to generate content that aligns with individual
    user preferences has garnered widespread attention. Personalized Retrieval-Augmented Generation (RAG), which retrieves
    relevant documents from the user’s history to reflect their preferences and enhance LLM generation, is one commonly used
    approach for personalization. However, existing personalized RAG methods do not consider that the histories of similar
    users can also assist in personalized generation for the current user, meaning that collaborative information between
    users can also benefit personalized generation. Inspired by the application of collaborative filtering in recommender
    systems, we propose a method called CFRAG, which adapts Collaborative Filtering to RAG for personalized text generation.
    However, this presents two challenges: (1) how to incorporate collaborative information without explicit user similarity
    labels? (2) how to retrieve documents that support personalized LLM generation? For Challenge 1, we use contrastive learning
    to train user embeddings to retrieve similar users and introduce collaborative information. For Challenge 2, we design
    a personalized retriever and reranker to retrieve the top-k documents from these users’ histories. We take into account
    the user’s preference during retrieval and reranking. Then we leverage feedback from the LLM to fine-tune the personalized
    retriever and reranker, enabling them to retrieve documents that meet the personalized generation needs of the LLM. Experimental
    results on the Language Model Personalization (LaMP) benchmark validate the effectiveness of CFRAG. Further analysis confirms
    the importance of incorporating collaborative information.'
  citations: []
- block_id: 1
  content: 'Personalizing Large Language Models (LLMs) [56] to generate personalized outputs tailored to individual user preferences
    has emerged as a significant and rapidly growing field [16, 23, 29, 31, 32, 37, 38, 58]. Personalized Retrieval-Augmented
    Generation (RAG) [8] has become a commonly used approach for personalizing LLMs [29, 31, 32, 58].


    The process of existing personalized RAG methods typically involves retrieving similar documents from the user’s historical
    behaviors based on the user’s input query, then concatenating these documents with the query as a prompt input to the
    LLM for generation. Although effective, this approach is limited to retrieving only the current user’s history, neglecting
    collaborative information. Users with similar histories tend to be more alike, and the information from these similar
    users can also aid in personalizing generation for the current user. As shown in the example in Figure 1, the upper part
    illustrates the results of the existing RAG method, which retrieves documents from the current user’s history. We can
    only infer from these results that “She” in the user’s input refers to “Hillary Clinton”. In contrast, the lower part
    demonstrates our method, which retrieves documents from the history of similar users. In this case, we can further infer
    that “his” in the user’s input refers to “Donald Trump”, leading to a better generation result. From this example, we
    can see that incorporating collaborative information allows the retrieval of more diverse documents, helping the LLM generate
    results that better meet the user’s needs.


    Inspired by the application of collaborative filtering in recommender systems [11, 41, 47], we propose to adapt collaborative
    information into RAG to personalize LLMs. However, adapting collaborative filtering to personalized RAG presents two challenges.


    Challenge 1: How to incorporate collaborative information. Without explicit labels indicating which users are similar,
    which users’ information should be selected to help personalize generation for the current user? Challenge 2: How to retrieve
    documents that support personalized LLM generation, rather than relying on traditional semantic relevance? Pre-trained
    dense retrieval models [55] only retrieve based on the semantic relevance between the query and document. Directly using
    these models for retrieval may not necessarily result in content that allows the LLM to generate outputs that meet the
    user’s needs [25, 36].


    To address the above challenges, this paper proposes a method named CFRAG which adapts Collaborative Filtering to personalized
    Retrieval Augmented Generation. Firstly, to address Challenge 1, since there are no explicit user similarity labels, we
    use contrastive learning [15, 45] to train user embeddings for retrieving similar users to introduce collaborative information.
    Specifically, we apply different data augmentation methods to the user’s history to obtain different views, and then treat
    different views of the same user’s history as positive samples for each other. Then we use contrastive learning on different
    views to train the user embeddings. Secondly, for Challenge 2, we designed a personalized retriever and reranker to retrieve
    the top-k documents from the histories of the retrieved users. In both retrieval and reranking, in addition to the semantic
    relevance between the query and documents, we also considered the user’s preferences for different documents to enable
    personalized retrieval. Additionally, we further fine-tune the retriever and reranker based on the feedback from the LLM
    to ensure that the retrieved documents better support the personalized LLM generation. Finally, the top-k documents are
    concatenated with the user’s input query to form a prompt, which is then fed into the LLM for personalized generation.


    The major contributions of the paper are summarized as follows:

    - We analyzed the necessity of introducing collaborative filtering into RAG for LLM personalization and identified the
    challenges: how to introduce collaborative information and how to retrieve documents that support personalized LLM generation.

    - We proposed a method called CFRAG, which uses contrastive learning to train user embeddings for retrieving similar users
    and incorporating collaborative information. It leverages LLM feedback to train the personalized retriever and reranker,
    enabling them to retrieve documents that support personalized LLM generation.

    - Experimental results on the Language Model Personalization (LaMP) [32] benchmark validate the effectiveness of CFRAG.
    The experimental analysis also demonstrates the importance of leveraging collaborative information.'
  citations:
  - marker: '[8]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
  - marker: '[11]'
    intent_label: Prior Methods
    topic_label: Downstream Tasks
  - marker: '[15]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Independent Training
  - marker: '[16]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[23]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[25]'
    intent_label: Research Gap
    topic_label: Retrieval Quality
  - marker: '[29]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
  - marker: '[31]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
  - marker: '[32]'
    intent_label: Benchmark Utilization
    topic_label: Evaluation Aspects and Tools
  - marker: '[36]'
    intent_label: Research Gap
    topic_label: Retrieval Quality
  - marker: '[37]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[38]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[41]'
    intent_label: Prior Methods
    topic_label: Downstream Tasks
  - marker: '[45]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Independent Training
  - marker: '[47]'
    intent_label: Prior Methods
    topic_label: Downstream Tasks
  - marker: '[55]'
    intent_label: Research Gap
    topic_label: Retriever Type
  - marker: '[56]'
    intent_label: Domain Overview
    topic_label: Parameter-Inaccessible Generators (Black-box)
  - marker: '[58]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
- block_id: 2
  content: 'Personalization of LLMs. Large Language Models (LLMs) [56] have demonstrated remarkable capabilities in various
    fields, such as text generation [22], information retrieval [57], recommender systems [5, 42], and so on. However, since
    LLMs are typically designed to serve all tasks with a single model and are trained on broad, domain-agnostic data, they
    face challenges in adapting to the personalized needs of individual users [4, 32]. Therefore, LLM personalization has
    attracted widespread attention [16, 31, 58].


    Existing works on LLM personalization mainly include the following types of methods: (1) Fine-tuning a personalized LLM
    for each user [37, 38, 43]; Tan et al. [38] fine-tuned the LLM using LoRA [12] to get personalized LoRA parameters for
    each user. (2) Aligning LLMs with user-specific preferences through Reinforcement Learning from Human Feedback (RLHF)
    [16, 23, 44]; Jang et al. [16] first trained different parameters for various objectives using RLHF, then merged these
    parameters based on users’ personalized needs. (3) Incorporating user-specific context into the prompt [21, 27, 29, 31,
    32, 58]. Richardson et al. [29] used instruction-tuned LLMs to summarize user history and then incorporated it into prompts
    for generation. Salemi et al. [31, 32] used RAG to retrieve relevant documents from user history based on the input query
    and incorporated them into the prompt.


    This paper further introduces collaborative filtering for personalization based on the RAG framework. Collaborative filtering
    has already been applied in fields such as recommender systems [33–35, 39, 49–53] and has been proven effective. It assumes
    that users who have interacted with similar items share similar preferences, and recommending items from similar users
    to the current user can meet their needs. Some works [11, 47] learn the collaborative information between users and items
    through matrix factorization [19], while others [10, 41] further explore higher-order collaborative information between
    users and items using graph neural networks. The application of collaborative filtering in LLM personalization remains
    under-explored.


    Retrieval Augmented Generation. Retrieval Augmented Generation [7, 8] introduces external knowledge through document retrieval,
    alleviating issues such as LLM hallucinations [54], and enhancing LLMs’ capabilities in knowledge-intensive tasks [17]
    such as open-domain question answering [14, 20]. Some works [3, 13] encode retrieved documents using separate encoders,
    and then fuse the results with the language model using cross-attention. A more common approach is to directly include
    the retrieved documents in the prompt of the LLM [2, 9, 20, 25, 36]. In recent years, this in-context RAG framework has
    also been applied to LLM personalization, which is personalized by retrieving documents from the user’s history [31, 32,
    58]. This paper introduces collaborative filtering by retrieving similar users’ histories for better personalization.'
  citations:
  - marker: '[2]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
  - marker: '[3]'
    intent_label: Prior Methods
    topic_label: Intermediate-Layer Integration
  - marker: '[4]'
    intent_label: Research Gap
    topic_label: NLP Applications
  - marker: '[5]'
    intent_label: Domain Overview
    topic_label: Downstream Tasks
  - marker: '[7]'
    intent_label: Prior Methods
    topic_label: Retrieval Integration for Generation
  - marker: '[8]'
    intent_label: Prior Methods
    topic_label: Retrieval Integration for Generation
  - marker: '[9]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
  - marker: '[10]'
    intent_label: Prior Methods
    topic_label: Downstream Tasks
  - marker: '[11]'
    intent_label: Prior Methods
    topic_label: Downstream Tasks
  - marker: '[12]'
    intent_label: Prior Methods
    topic_label: Parameter-Accessible Generators (White-box)
  - marker: '[13]'
    intent_label: Prior Methods
    topic_label: Intermediate-Layer Integration
  - marker: '[14]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[16]'
    intent_label: Prior Methods
    topic_label: Parameter-Accessible Generators (White-box)
  - marker: '[17]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[19]'
    intent_label: Prior Methods
    topic_label: Downstream Tasks
  - marker: '[20]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
  - marker: '[21]'
    intent_label: Prior Methods
    topic_label: Parameter-Inaccessible Generators (Black-box)
  - marker: '[22]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[23]'
    intent_label: Prior Methods
    topic_label: Parameter-Accessible Generators (White-box)
  - marker: '[25]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
  - marker: '[27]'
    intent_label: Prior Methods
    topic_label: Parameter-Inaccessible Generators (Black-box)
  - marker: '[29]'
    intent_label: Prior Methods
    topic_label: Context Curation and Compression
  - marker: '[31]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
  - marker: '[32]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
  - marker: '[36]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
  - marker: '[37]'
    intent_label: Prior Methods
    topic_label: Parameter-Accessible Generators (White-box)
  - marker: '[38]'
    intent_label: Prior Methods
    topic_label: Parameter-Accessible Generators (White-box)
  - marker: '[39]'
    intent_label: Prior Methods
    topic_label: Downstream Tasks
  - marker: '[41]'
    intent_label: Prior Methods
    topic_label: Downstream Tasks
  - marker: '[42]'
    intent_label: Domain Overview
    topic_label: Downstream Tasks
  - marker: '[43]'
    intent_label: Prior Methods
    topic_label: Parameter-Accessible Generators (White-box)
  - marker: '[44]'
    intent_label: Prior Methods
    topic_label: Parameter-Accessible Generators (White-box)
  - marker: '[47]'
    intent_label: Prior Methods
    topic_label: Downstream Tasks
  - marker: '[54]'
    intent_label: Domain Overview
    topic_label: Trustworthiness and Robustness
  - marker: '[56]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[57]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[58]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
- block_id: 3
  content: 'Let U = {u1, u2, . . . , uM} denotes the set of all users, where M is the number of users. Each user u ∈ U has
    a chronologically ordered history Hu = [d1, d2, . . . , dN] which includes all her historical documents, where N is the
    number of documents in the history. The personalized text generation dataset is D = {(u, q, y)i}|D|i=1. For each instance,
    q is the query input by the user u to the LLM, and y is the target output. Our goal is first to introduce collaborative
    information by retrieving the top-m most similar users for user u: Uretrieved = {u1, u2, . . . , um}.


    Then, we use a retriever to retrieve the top-k documents from each of the m users’ histories, resulting in a total of
    m × k documents. Dretrieved = {di,j | i ∈ {1, . . . , m}, j ∈ {1, . . . , k}}.


    Finally, we use a reranker to rerank these m × k documents and obtain the final top-k documents: Dreranked = {di | i ∈
    {1, . . . , k}}. These top-k documents will be concatenated with the user’s query q as a prompt and input into the LLM,
    enabling it to generate a response that aligns with the target output y.


    This paper primarily focuses on how to retrieve Uretrieved to introduce collaborative information, and how to train the
    retriever and reranker so that they can effectively retrieve documents that support the personalized LLM generation.'
  citations: []
- block_id: 4
  content: This section introduces our method CFRAG. CFRAG’s overall architecture is shown in Figure 2. As mentioned in Section
    1, to address Challenge 1, i.e., how to introduce collaborative information, we first train user embeddings using contrastive
    learning to retrieve the top-m most similar users (see Section 4.1). For Challenge 2, which involves retrieving documents
    that support personalized LLM generation, we fine-tune the personalized retriever and reranker using LLM feedback. The
    retriever first retrieves the top-k documents from the history of each of the m users, resulting in m × k documents (see
    Section 4.2). The reranker then reranks these documents to obtain the final top-k documents as input for the LLM (see
    Section 4.3).
  citations: []
- block_id: 5
  content: 'First, we perform user retrieval to get the top-m most similar users for user u to introduce collaborative information.
    However, we do not have labels indicating which users are similar to each other. To address this, we employ a contrastive
    learning [15, 45] approach. We apply different data augmentation methods to the user history Hu to obtain different views
    of the user’s history. We treat different views of the same user as positive samples and the histories of other users
    as negative samples, and then we use the InfoNCE [28] loss to train user embeddings for retrieval. Figure 3 illustrates
    the process of training user embeddings using contrastive learning.


    #### 4.1.1 User Encoder

    Specifically, we first use an embedding model (such as BERT [6], RoBERTa [26], BGE [46] etc.) Emb(·) to encode each document
    in the user’s history Hu to obtain Eu = [e1, e2, . . . , eN]⊺ ∈ RN×d, where ei = Emb(di) and d is the embedding dimension.
    To model the sequential relationships between different documents in the user’s history, we introduce positional embedding
    P ∈ RN×d. Afterward, the history Hu’s embedding becomes bEu = Eu + P. Then, we apply a transformer [40] as the user encoder
    to encode the user’s history bEu and average the transformer’s output to obtain the user’s embedding:


    eu = Encoderu(u) = MEAN(Trm(bEu)) ∈ Rd,


    where Encoderu(·) → Rd denotes the user encoder, Trm(·) denotes a transformer encoder. Next, we train the transformer
    encoder using contrastive learning.


    #### 4.1.2 Data Augmentation

    We generate different views of Hu using the following three data augmentation methods:


    Document Crop. We randomly select a continuous sub-sequence of length Lc = ⌊ηc N⌋ from Hu, where ηc is a hyper-parameter
    controlling the crop ratio. The history after cropping is as follows: Hcrop u = [dc, dc+1, . . . , dc+Lc−1].


    Document Mask. For the history Hu, we randomly mask out Lm = ⌊ηm N⌋ documents Imask = {i1, i2, . . . , iLm}, where Imask
    is the set of indices corresponding to the masked documents and ηm is a hyper-parameter that controls the mask ratio.
    The masked documents are replaced with a special token [mask]. The history after masking is as follows: Hmask u = [\^d1,
    \^d2, . . . , \^dN], \^di = (di, i ∉ Imask; [mask], i ∈ Imask).


    Document Reorder. We randomly select a sub-sequence [dr, dr+1, . . . , dr+Lr−1] of length Lr = ⌊ηr N⌋ from Hu, where ηr
    is a hyper-parameter controlling the reorder ratio, and then randomly shuffle the order of the documents within the sub-sequence
    to obtain [\^dr, \^dr+1, . . . , \^dr+Lr−1]. The history after reordering is as follows: Hreorder u = [d1, d2, . . . ,
    \^dr, . . . , \^dr+Lr−1, . . . , dN].


    #### 4.1.3 Contrastive Loss

    Each time, we randomly select two data augmentation methods A′ and A′′ to generate two different views of Hu, denoted
    as H′u and H′′u. Then, using the encoder described in Section 4.1.1, we obtain the user embeddings e′u and e′′u corresponding
    to the different views. Since e′u and e′′u are obtained through data augmentation of Hu, they are more similar to each
    other. Therefore, we treat them as positive samples for each other and use the views generated from the augmented histories
    of other users in the same batch as negative samples. We then perform contrastive learning using the InfoNCE [28] loss
    as follows:


    LCL = − [ log exp(cos(e′u, e′′u)/τ1) / Σu−∈Uneg exp(cos(e′u, e′′u−)/τ1) + log exp(cos(e′u, e′′u)/τ1) / Σu−∈Uneg exp(cos(e′u−,
    e′′u)/τ1) ],


    where τ1 is the temperature coefficient, Uneg are the set of randomly sampled in-batch negative samples, and cos(·) denotes
    the cosine similarity.


    #### 4.1.4 Top-m User Retrieval

    After training with contrastive learning, we can use the encoder from Section 4.1.1 to obtain the user embedding eu. We
    then calculate the cosine similarity between each pair of user embeddings and retrieve the top-m most similar users Uretrieved
    = {u1, u2, . . . , um} for user u. Subsequently, the histories of these m users will be used for further document retrieval.'
  citations:
  - marker: '[6]'
    intent_label: Model/Architecture Adoption
    topic_label: Retriever Type
  - marker: '[15]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Independent Training
  - marker: '[26]'
    intent_label: Model/Architecture Adoption
    topic_label: Retriever Type
  - marker: '[28]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Independent Training
  - marker: '[40]'
    intent_label: Model/Architecture Adoption
    topic_label: Retriever Type
  - marker: '[45]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Independent Training
  - marker: '[46]'
    intent_label: Model/Architecture Adoption
    topic_label: Retriever Type
- block_id: 6
  content: 'After retrieving the top-m users, we design a personalized retriever to retrieve the top-k documents from each
    user’s history, resulting in a total of m × k candidate documents Dretrieved = {di,j | i ∈ {1, . . . , m}, j ∈ {1, . .
    . , k}}. This section introduces how the retriever is designed and how it’s trained to retrieve documents that better
    align with the requirements of personalized LLM generation.


    #### 4.2.1 Retriever

    First, we use a pre-trained dense retrieval model (such as BGE retriever [46]) to compute the semantic relevance between
    the query and the candidate documents:


    Sretriever q,d = cos(Encoderq(q), Encoderd(d)),


    where Encoderq(·) → Rd and Encoderd(·) → Rd are the encoders for the query and the document in the retrieval model, respectively.
    Pre-trained retrieval models typically use Sretriever q,d directly for retrieval. However, Sretriever q,d only considers
    the semantic relevance between the query and the document. Since different users might input the same query but expect
    different outputs due to their varying preferences, we further account for user personalization by calculating the preference
    score of the user for the document as follows:


    Sretriever u,d = cos(MLP1(eu), Encoderd(d)),


    where MLP1 : Rd → Rd is a multi-layer perceptron that maps the user embedding to the space where the cosine similarity
    is computed. eu is the embedding obtained in Section 4.1.1. The total score for retrieval is computed as follows:


    Sretriever u,q,d = (1 − α)Sretriever q,d + α Sretriever u,d,


    where α is a hyper-parameter that controls the weight of personalization.


    #### 4.2.2 Training

    Since the pre-trained dense retrieval model is not fine-tuned for our specific task, the retrieved results may not necessarily
    lead to LLM responses that better match the target output y [25, 36]. However, there is no ground truth indicating which
    documents are better. Therefore, we evaluate the difference between the LLM’s output and the target output y, using this
    as a label to train the retrieval model. Figure 4 shows the process of training the retriever using LLM feedback.


    Specifically, we first use the pre-trained retrieval model to retrieve the top-k documents from each of the m users’ histories
    based on Sretriever q,d in Eq. (3), resulting in a total of m × k candidate documents. These documents are then concatenated
    with the query one by one and used as prompts for the LLM, producing m × k outputs: {Oq,di,j = LLM(q, di,j) | i ∈ {1,
    . . . , m}, j ∈ {1, . . . , k}}, where LLM(q, di,j) represents the output generated by inputting the concatenated query
    q and document di,j into the LLM. Then, based on the quality of these outputs, we can calculate the distribution of these
    candidate documents as follows:


    pLLM(di,j | q, y) = exp(eval(y, Oq,di,j)) / Σi=1..m Σj=1..k exp(eval(y, Oq,di,j)),


    where eval(·) measures the difference between the target output y and the LLM’s output, using metrics such as ROUGE [24]
    score. A larger value returned by eval(·) indicates a better-generated result. Similarly, we can also calculate the score
    distribution of the candidate documents by the retrieval model based on Sretriever u,q,d in Eq. (5):


    pretriever(di,j | q, u) = exp(Sretriever u,q,di,j) / Σi=1..m Σj=1..k exp(Sretriever u,q,di,j).


    We aim for the retrieval model to retrieve documents that lead to better LLM-generated results, which means making the
    distribution pretriever(d | q, u) in Eq. (7) closer to the distribution pLLM(d | q, y) in Eq (6). Therefore, we compute
    the KL divergence between the two distributions as the loss to optimize the retriever:


    Lretriever = KL(pretriever(d | q, u) || pLLM(d | q, y)).'
  citations:
  - marker: '[24]'
    intent_label: Metrics Utilization
    topic_label: Evaluation Aspects and Tools
  - marker: '[25]'
    intent_label: Research Gap
    topic_label: Generation Quality
  - marker: '[36]'
    intent_label: Research Gap
    topic_label: Generation Quality
  - marker: '[46]'
    intent_label: Resource Utilization
    topic_label: Retriever Type
- block_id: 7
  content: 'After retrieving Dretrieved through the retriever, in this section, we further refine the results by reranking
    Dretrieved to obtain the final top-k ranked results Dreranked = {di | i ∈ {1, . . . , k}}.


    #### 4.3.1 Reranker

    We use a pre-trained cross-encoder (such as the BGE reranker [46]) to encode the query and document, obtaining the hidden
    state corresponding to the [CLS] token from the last layer:


    hq,d = CrossEncoder(q, d),


    where hq,d ∈ Rd. Similarly, when reranking, in addition to considering the semantic relevance between query and document,
    we also take into account the user’s personalized preferences. However, since the cross-encoder does not encode documents
    separately, it cannot compute the cosine similarity between users and documents as shown in Eq. (4) to express the user
    preference score. Therefore, we directly concatenate the user embeddings to the output of the cross-encoder to account
    for the influence of user preferences. The overall score used for reranking is calculated as follows:


    Sreranker u,q,d = MLP3(CONCAT(hq,d, MLP2(eu))),


    where MLP2 : Rd → Rd and MLP3 : R2d → R are two multi-layer perceptions. CONCAT(·) denotes the concatenation operation.


    #### 4.3.2 Training

    Similar to the retriever’s training in Section 4.2.2, we also want the reranker to assign higher scores to the documents
    that lead to better LLM-generated results. Therefore, we train the reranker using a similar approach.


    We use the trained retrieval model from Section 4.2.2 to retrieve top-k documents from the history of each of the m users,
    resulting in a total of m × k candidate documents. These documents are concatenated with the query q and used as prompts
    for the LLM, producing m × k outputs. Similar to Eq.(6), we can obtain the distribution pLLM(d | q, y) of these candidate
    documents. Based on Sreranker u,q,d in Eq. (10), we can also get the score distribution of the candidate documents by
    the reranker:


    preranker(di,j | q, u) = exp(Sreranker u,q,di,j) / Σi=1..m Σj=1..k exp(Sreranker u,q,di,j).


    We compute the KL divergence between distributions preranker(d | q, u) and pLLM(d | q, y) as the loss to optimize the
    reranker:


    Lreranker = KL(preranker(d | q, u) || pLLM(d | q, y)).


    The loss allows the reranker to assign higher scores to documents that enable better personalized generation by the LLM.'
  citations:
  - marker: '[46]'
    intent_label: Model/Architecture Adoption
    topic_label: Context Curation and Compression
- block_id: 8
  content: Computational Efficiency. CFRAG comprises three modules. The User Encoder is a lightweight, single-layer Transformer
    with inputs derived from a frozen BGE embedding (dimension 768), resulting in minimal parameter overhead. The retriever
    and reranker are comparable in size to BERT (approximately 100M parameters). Overall, the training cost is low due to
    the modest parameter size. During inference, user and document embeddings can be precomputed, requiring only similarity
    calculations for retrieval, ensuring minimal computational cost. This efficiency enables our method to generalize quickly
    to new datasets.
  citations: []
- block_id: 9
  content: We conducted experiments to evaluate the performance of CFRAG. The source code is available.
  citations: []
- block_id: 10
  content: '#### 5.1.1 Dataset

    We conducted experiments on the Language Model Personalization (LaMP) [32] benchmark, which consists of seven personalized
    text generation tasks. We excluded LaMP-6 because its data is not publicly available. The remaining tasks include: LaMP-1
    (Personalized Citation Identification); LaMP-2 (Personalized Movie Tagging); LaMP-3 (Personalized Product Rating); LaMP-4
    (Personalized News Headline Generation); LaMP-5 (Personalized Scholarly Title Generation); LaMP-7 (Personalized Tweet
    Paraphrasing). We used the time-based split provided by LaMP to divide the data into training, validation, and test sets.


    #### 5.1.2 Evaluation Metrics

    Following previous works [31, 32], we evaluate Accuracy and F-1 score for LaMP-1 and LaMP-2, mean absolute error (MAE)
    and root mean squared error (RMSE) for LaMP-3, ROUGE-1 and ROUGE-L [24] for LaMP-4, LaMP-5 and LaMP-7.


    #### 5.1.3 Baselines

    In this work, we compare CFRAG with the following methods.


    No Personalization: We directly input the user’s query into the LLM without retrieving from user history, using this as
    the non-personalized baseline. We refer to this method as Zero Shot.


    Personalized Baselines: We compared CFRAG with methods that personalize by retrieving from user history using different
    retrieval models, including: (1) Random selects k items randomly from the user’s history; (2) Recency selects the most
    recent k items from the user’s history; (3) BM25 [30] retrieves top-k items from the user’s history using BM25; (4) BGE
    [46] retrieves top-k items from the user’s history using BGE retriever; (5) ROPG [31] optimizes the dense retrieval model
    based on the results generated by the LLM.


    #### 5.1.4 Implementation Details

    We conducted experiments on two LLMs: Llama3-8B-Instruct [1] and Qwen2-7B-Instruct [48]. In this paper, we do not fine-tune
    the LLM because fine-tuning is costly and could cause the LLM to retain user information, potentially compromising user
    privacy. To ensure a fair comparison, we use greedy search for text generation. The dense retrieval model used in all
    methods is bge-base-en-v1.5 [46]. The cross-encoder used for reranker in Section 4.3.1 is bge-reranker-base [46]. All
    hyper-parameters for the baselines are searched according to the settings in the original papers. The embedding dimension
    d is set to 768. The number of retrieved documents k is set to 5, and the number of retrieved users m is tuned among {2,
    3, 4, 5, 6}. The Trm(·) encoder in Eq. (1) has 1 layer and 2 heads. The hyper-parameters Lc, Lm, and Lr used for data
    augmentation in Section 4.1.2 are set to 0.7, 0.3, and 0.3, respectively. The temperature parameters τ1 in Eq. (2) is
    tuned among {0.01, 0.1, 1}. The weight α in Eq. (5) is tuned among [0.01, 1.0]. The learning rate is tuned among {1e-3,
    1e-4, 1e-5}. Adam [18] is used to conduct the optimization. The data input and output formats are provided in Appendix
    A.'
  citations:
  - marker: '[1]'
    intent_label: Resource Utilization
    topic_label: Parameter-Inaccessible Generators (Black-box)
  - marker: '[18]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Training Strategies
  - marker: '[24]'
    intent_label: Metrics Utilization
    topic_label: Generation Quality
  - marker: '[30]'
    intent_label: Result Comparison
    topic_label: Retriever Type
  - marker: '[31]'
    intent_label: Result Comparison
    topic_label: Sequential Training
  - marker: '[32]'
    intent_label: Benchmark Utilization
    topic_label: Evaluation Aspects and Tools
  - marker: '[46]'
    intent_label: Resource Utilization
    topic_label: Retriever Type
  - marker: '[48]'
    intent_label: Resource Utilization
    topic_label: Parameter-Inaccessible Generators (Black-box)
- block_id: 11
  content: 'Experimental results are shown in Table 2. From the results, we can find that:


    - Firstly, compared to existing methods, CFRAG achieved the best results across six datasets in the LaMP benchmark. This
    demonstrates the effectiveness of introducing collaborative information between users into RAG and using LLM feedback
    to tune the retriever and reranker to ensure that they can retrieve the documents that support the personalized LLM generation.

    - Secondly, we can observe that even randomly selecting user history outperforms the zero-shot method without any user
    history. This highlights the importance of incorporating user history to reflect user preferences for personalized generation.
    Additionally, we observe that retrieval methods perform better than simply selecting the most recent user history, underscoring
    the importance of retrieval.

    - Thirdly, we also observe that, in most cases, RAG and ROPG methods using dense retrieval models outperform BM25. Additionally,
    CFRAG, which fine-tunes the retriever based on LLM feedback, achieves better results. This shows, on the one hand, that
    the better the retriever, the better the generation results, and on the other hand, fine-tuning the retriever based on
    LLM feedback to ensure it can retrieve the documents that meet the personalized generation needs of LLM is crucial.'
  citations: []
- block_id: 12
  content: 'We conducted an ablation study to investigate the effectiveness of different modules in CFRAG. CFRAG consists
    of three modules: User Retrieval, Document Retrieval, and Document Rerank. We removed different modules from CFRAG one
    by one to verify the effectiveness of each module.


    #### 5.3.1 User Retrieval

    First, we validated the effectiveness of introducing collaborative information by retrieving similar users. It can be
    seen that without retrieving similar users and only retrieving from the current user’s history, the performance is worse
    than that of CFRAG, highlighting the importance of collaborative information.


    We also validated the effectiveness of training user embeddings using contrastive learning. For comparison, we directly
    averaged the document embeddings from the user’s history to create user embeddings for retrieval. It can be seen that
    CFRAG, which uses user embeddings trained with contrastive learning, achieves better results. This is because contrastive
    learning constructs user similarity labels through data augmentation and uses the InfoNCE loss to help the embeddings
    learn which users are similar. In contrast, using mean pooling directly cannot capture user similarity.


    #### 5.3.2 Document Retrieval

    We also validated the effectiveness of the personalized retriever we designed. First, we can see that without fine-tuning
    based on LLM feedback, using a pre-trained dense retrieval model leads to worse performance. This indicates that retrieval
    cannot be based solely on semantic relevance, ensuring that the retrieved documents support personalized LLM generation
    is crucial. Additionally, we analyzed the impact of removing Sretriever u,d from Eq. (5) and only using Sretriever q,d
    from Eq. (3) for retrieval. The results decreased, demonstrating that users’ personalized preferences should also be considered
    during retrieval, rather than solely focusing on the semantic relevance between the query and documents.


    #### 5.3.3 Document Rerank

    We also validated the effectiveness of the personalized reranker we designed. First, it can be seen that using a pre-trained
    reranker leads to worse results, highlighting the importance of fine-tuning based on LLM feedback. We also observed the
    effect of removing eu from Eq. (10) and only using hq,d to calculate Sreranker q,d for ranking. The results decreased
    in this case, highlighting the importance of considering users’ personalized preferences in the reranker.'
  citations: []
- block_id: 13
  content: 'As mentioned in Section 1, adapting collaborative filtering into personalized RAG faces two challenges. Challenge
    1: How to introduce collaborative information? Challenge 2: How to retrieve documents that support personalized LLM generation?
    In this section, we conduct experimental analysis to further demonstrate the effectiveness of our method in addressing
    these two challenges. Additionally, we provide further analysis of the results of CFRAG and the impact of hyper-parameters.
    Due to space limitations, we conducted experimental analysis on the LaMP-1 and LaMP-5 datasets.


    #### 5.4.1 Effectiveness of User Retrieval using Contrastive Learning (Challenge 1)

    As described in Section 1, to address Challenge 1, we train user embeddings using contrastive learning to retrieve the
    top-m most similar users for introducing collaborative information. To validate the effectiveness of this approach, we
    compared it with randomly selecting m users and selecting users from top-m to 2m. First, we can see that randomly selecting
    users yields the worst performance, indicating that collaborative information cannot be introduced indiscriminately. Secondly,
    the results show that retrieving users from the range of top-m to 2m performs worse than using the top-m users, suggesting
    that information from users who are more similar to the current user u is more important. These highlight the importance
    of retrieving the most similar top-m users.


    #### 5.4.2 Effectiveness of Document Retrieval using LLM Feedback (Challenge 2)

    As mentioned in Section 1, to address Challenge 2, we fine-tune the retriever and reranker using feedback from the content
    generated by the LLM, enabling them to retrieve documents that better meet personalized LLM generation needs. To validate
    its effectiveness, we compared the results with those using retrievers and rerankers without LLM feedback fine-tuning,
    as well as using BM25 as the retriever and reranker. It can be observed that CFRAG performs the best, highlighting the
    importance of fine-tuning with LLM feedback rather than relying solely on semantic relevance.


    #### 5.4.3 Impact of the Number of Documents from the Current User

    To further validate that CFRAG enhances personalization by incorporating collaborative information, we observed the impact
    of the number of documents from the current user in the final top-k documents on the results. We varied the number of
    documents retrieved from the current user’s history in the top-k documents from 0 to 5, with the remaining documents retrieved
    from similar users’ histories. The results indicate that retrieving only from the current user’s history leads to poor
    performance, while appropriately retrieving documents from similar users’ histories significantly improves the results.
    This verifies the importance of incorporating collaborative information.


    #### 5.4.4 Impact of the Number of Retrieved Users

    Since we enhance personalized text generation by introducing collaborative filtering, we further explored how much collaborative
    information to introduce, specifically the impact of the number of retrieved users on the results. In LaMP-1, retrieving
    too few or too many users leads to poorer performance, with the best results at 4 users. In LaMP-5, the performance improves
    as the number of users increases. This highlights the importance of introducing collaborative filtering, but it also indicates
    that excessive introduction can lead to decreased effectiveness.


    #### 5.4.5 Impact of the Number of Retrieved Documents

    We also analyzed the impact of the number of retrieved documents, k, on the results. It can be observed that as the number
    of retrieved documents increases, performance improves, indicating the importance of retrieving user history to reflect
    user preferences for enhancing LLM-generated results. Since more documents lead to longer prompts and slower LLM generation,
    we chose k = 5 for our experiments.'
  citations: []
- block_id: 14
  content: In this paper, we propose CFRAG, which adapts collaborative filtering into RAG to personalize LLMs. To introduce
    collaborative information without explicit user labels and retrieve documents that support personalized LLM generation,
    we first train user embeddings through contrastive learning to retrieve similar users. Then, we design the personalized
    retriever and reranker that considers user preferences during retrieval and fine-tune them using LLM feedback. The results
    on the Language Model Personalization (LaMP) benchmark validate the effectiveness of CFRAG. The experimental analysis
    also confirms the effectiveness of each module within CFRAG.
  citations: []
