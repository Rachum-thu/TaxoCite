title: 'CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models'
blocks:
- block_id: 0
  content: 'Retrieval-augmented generation (RAG) is a technique that enhances the capabilities of large language models (LLMs)
    by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information
    and the tendency to produce inaccurate “hallucinated” content. However, evaluating RAG systems is a challenge. Most benchmarks
    focus primarily on question-answering applications, neglecting other potential scenarios where RAG could be beneficial.
    Accordingly, in the experiments, these benchmarks often assess only the LLM components of the RAG pipeline or the retriever
    in knowledge-intensive scenarios, overlooking the impact of external knowledge base construction and the retrieval component
    on the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this article constructs a large-scale
    and more comprehensive benchmark and evaluates all the components of RAG systems in various RAG application scenarios.
    Specifically, we refer to the CRUD actions that describe interactions between users and knowledge bases and also categorize
    the range of RAG applications into four distinct types—create, read, update, and delete (CRUD). “Create” refers to scenarios
    requiring the generation of original, varied content. “Read” involves responding to intricate questions in knowledge-intensive
    situations. “Update” focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. “Delete”
    pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have
    developed different datasets to evaluate the performance of RAG systems. We also analyze the effects of various components
    of the RAG system, such as the retriever, context length, knowledge base construction, and LLM. Finally, we provide useful
    insights for optimizing the RAG technology for different scenarios. The source code is available at GitHub: https://github.com/IAAR-Shanghai/CRUD_RAG.'
  citations: []
- block_id: 1
  content: 'Retrieval-augmented generation (RAG) is an advanced technique that leverages external knowledge sources to enhance
    the text generation capabilities of large language models (LLMs). It retrieves relevant paragraphs from a corpus based
    on the input and feeds them to the LLMs along with the input. With the help of external knowledge, LLMs can generate more
    accurate and credible responses and effectively address challenges such as outdated knowledge [ 19], hallucinations [
    3, 9, 35, 65], and lack of domain expertise [ 30, 46]. Therefore, RAG technology is attracting increasing attention.


    Although the effectiveness of retrieval-augmented strategies has been proven through extensive practice, their implementation
    still requires a significant amount of tuning. The overall performance of the RAG system is affected by multiple factors,
    such as the retrieval model, construction of the external knowledge base, and language model. Therefore, automatic evaluation
    of RAG systems is crucial. Currently, there are only a few existing benchmarks for evaluating RAG performance, as creating
    high-quality datasets and experimenting with them entail significant costs. These benchmarks can be classified into two
    types: reference-required and reference-free evaluation.


    Reference-free evaluation frameworks, such as RAGAS [ 13] and ARES [ 44], use LLM-generated data to evaluate RAG systems
    on contextual relevance, faithfulness, and informativeness. These frameworks do not depend on ground truth references,
    but only assess the coherence of the generated text with the retrieved context. This approach may be unreliable if the
    retrieved external information is low-quality.


    Consequently, reference-required evaluations remain the predominant method for assessing RAG systems. Existing benchmarks
    for reference-required evaluations, such as Retrieval-Augmented Generation Benchmark (RGB) [8] and Natural Questions Benchmark
    (NQ) [26], do have their limitations. First, they all rely on question-answering (QA) tasks to measure the performance
    of RAG systems. QA is not the only RAG application scenario, and an optimization strategy that works well for QA may not
    be generalized to other scenarios. Thus, these benchmarks may not capture the full potential of RAG systems. Second, in
    the experiments, current evaluations usually focus on evaluating the LLM part of the RAG pipeline, or focus on retriever
    performance in the knowledge-intensive scenario [40], while ignoring the retrieval methods in non-knowledge-intensive
    scenarios and external knowledge base construction. These components are also crucial for RAG systems. Therefore, a comprehensive
    evaluation of the RAG system may not be obtained using any existing benchmarks.


    To evaluate the performance of RAG in different application scenarios, we need a comprehensive benchmark that covers more
    than just the QA task. Lewis et al. [ 28] argue that the core of RAG systems is their interactive way of combining LLMs
    with external knowledge sources. Following [25], we can group any interaction with external knowledge sources into four
    basic actions: create, read, update, and delete, which are also known as CRUD actions [ 48]. Therefore, we can use the
    create, read, update, and delete (CRUD) framework to classify the RAG systems’ application scenarios.


    Each CRUD category demonstrates different capabilities of the RAG system:

    — In “CREATE,” the system improves the input text by adding relevant information from external sources, making creative
    outputs such as poetry, stories, or code.

    — In “READ,” the system uses external knowledge retrieval to answer questions, solve problems in QA, dialogue, and reasoning,
    and increase understanding of the input text.

    — In “UPDATE,” the system fixes errors in the input text using retrieved content, correcting spelling, grammar, or factual
    errors to make the text better.

    — In “DELETE,” the system simplifies the input by improving retrieval results, removing unnecessary details, and doing
    tasks like text summarization or simplification.


    To evaluate the RAG system in these four scenarios, we introduce CRUD-RAG, a comprehensive, large-scale Chinese RAG benchmark.
    CRUD-RAG consists of four evaluation tasks: text continuation, QA (with single-document and multi-document questions),
    hallucination modification, and open-domain multi-document summarization, which respectively correspond to the CRUD-RAG
    classification of RAG application scenarios. We construct CRUD-RAG by crawling the latest high-quality news data from
    major news websites in China, which aims to minimize the likelihood of LLMs encountering these data during training. Then,
    we automatically create datasets using GPT-4 based on these news data. For the multi-document summarization task, we apply
    a reverse construction strategy. We first generate news events and their summaries using GPT-4. Then, we use these events
    as keywords to search for 10 related and non-duplicate reports from the web, which we add to our retrieval database. During
    evaluation, the RAG system will use the retrieval database to generate summaries for the events. For the text continuation
    task, we split the news text into a beginning and a continuation paragraph. We then use each sentence in the continuation
    paragraph as a keyword to search for 10 related reports on the Web. We remove any duplicate content and add the reports
    to the retrieval database. For the single-document QA task, we use the RGB [ 8] construction method. For the multi-document
    QA task, we use the chain-of-thought (CoT) technology to help the model identify common and different aspects among documents,
    and then generate questions based on these aspects with increasing difficulty. For the hallucination modification task,
    we use the annotations in the UHGEval dataset and correct hallucinations with GPT-4. We also include the real news in
    UHGEval in the retrieval database.


    In the experiments, we systematically evaluate the RAG system’s performance on our CRUD-RAG benchmark. We also investigate
    various factors that affect the RAG system, such as the context length, the chunk size, the embedding model, the retrieval
    algorithms, and the LLM. Based on our experimental results, we provide some valuable suggestions for building effective
    RAG systems. The contributions of this article are:

    — A comprehensive evaluation benchmark : Our benchmark covers not only QA, but also CRUD of RAG applications.

    — High-quality evaluation datasets : We constructed diverse datasets for different evaluation tasks, based on the application
    scenarios of RAG. These tasks include text continuation, multi-document summarization, QA, and hallucination modification.

    — Extensive experiments: we performed extensive experiments on our benchmark, using various metrics to measure the performance
    of RAG systems. Based on our experiments, we offered useful guidance for future researchers and RAG system developers.'
  citations:
  - marker: '[3]'
    intent_label: Domain Overview
    topic_label: Trustworthiness and Robustness
  - marker: '[8]'
    intent_label: Setting/Protocal Adoption
    topic_label: Evaluation Aspects and Tools
  - marker: '[9]'
    intent_label: Domain Overview
    topic_label: Trustworthiness and Robustness
  - marker: '[13]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[19]'
    intent_label: Domain Overview
    topic_label: Trustworthiness and Robustness
  - marker: '[25]'
    intent_label: Problem Formulation
    topic_label: Applications
  - marker: '[26]'
    intent_label: Research Gap
    topic_label: Evaluation Aspects and Tools
  - marker: '[28]'
    intent_label: Domain Overview
    topic_label: Retrieval Integration for Generation
  - marker: '[30]'
    intent_label: Domain Overview
    topic_label: Domain-specific Applications
  - marker: '[35]'
    intent_label: Domain Overview
    topic_label: Trustworthiness and Robustness
  - marker: '[40]'
    intent_label: Research Gap
    topic_label: Retrieval Quality
  - marker: '[44]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[46]'
    intent_label: Domain Overview
    topic_label: Domain-specific Applications
  - marker: '[48]'
    intent_label: Problem Formulation
    topic_label: Applications
  - marker: '[65]'
    intent_label: Domain Overview
    topic_label: Trustworthiness and Robustness
- block_id: 2
  content: ''
  citations: []
- block_id: 3
  content: 'LLMs excel in text generation, decision-making [ 52], information extraction [ 54], and personalized recommendations
    [62]; however, they also confront challenges such as outdated knowledge and the generation of hallucinatory content [
    6, 19, 43]. In response to these challenges, RAG, also referred to as Retrieval-Augmented Language Models, incorporates
    external knowledge to generate responses characterized by enhanced accuracy and realism [ 47]. This is particularly critical
    in domains that heavily depend on precision and reliability, including but not limited to the legal, medical, and financial
    sectors. Retrieval models have been promoting the development of language models [15, 33, 61].


    Conventional RAG systems adhere to a standardized workflow encompassing indexing, retrieval, and generation phases [ 28,
    36]. The indexing phase encompasses data cleansing, extraction, transformation into plain text, segmentation, and indexing,
    utilizing embedding models to transform text fragments into vector representations [ 2, 18]. In the retrieval phase, the
    system computes similarity scores based on the user’s query to select the most pertinent text fragments. In the generation
    phase, the query and selected documents are amalgamated into prompts, facilitating the LLMs in generating a response.
    While this method is straightforward, it encounters challenges related to retrieval quality, generation quality, and enhancement
    processes [ 21, 23].


    In response to these challenges, researchers concentrate on the enhancement of the retriever, a task that can be categorized
    into three key aspects: pre-retrieval processing, retrieval model optimization, and post-retrieval processing [ 20]. Pre-retrieval
    processing encompasses data transformer to enhance text standardization, ensuring factual accuracy, optimizing index structures,
    adjusting block sizes, and rewriting query [ 4, 16, 50, 53]. Retrieval model optimization entails the fine-tuning of domain-specific
    embedding models and the application of dynamic embedding techniques [11, 63]. Post-retrieval processing minimizes context
    length through reranking and compression operations, aiming to emphasize critical information, diminish noise interference,
    and enhance integration and utilization by the generator [ 37, 55, 57].


    Furthermore, to enhance the precision and efficiency of the generator when handling retrieval content, scholars have undertaken
    a series of optimization measures. As an illustration, researchers have devised methods such as chain-of-note (CON) for
    the generator [ 60]. CON generates continuous reading notes to comprehensively evaluate the relevance of retrieved documents
    to the posed question, integrating this information to produce precise final responses. This approach further enhances
    the capability of RAG in managing retrieval information, guaranteeing the production of responses that are simultaneously
    accurate and pertinent. In specific domains, such as medical and legal, models undergo fine-tuning to enhance the generator’s
    performance within those particular fields [ 10, 24, 58]. Through the implementation of these methods, the generator can
    more effectively process retrieved information and furnish responses that are more accurate and relevant.'
  citations:
  - marker: '[2]'
    intent_label: Prior Methods
    topic_label: Indexing and Query Optimization
  - marker: '[4]'
    intent_label: Prior Methods
    topic_label: Indexing and Query Optimization
  - marker: '[6]'
    intent_label: Domain Overview
    topic_label: Trustworthiness and Robustness
  - marker: '[10]'
    intent_label: Prior Methods
    topic_label: Parameter-Accessible Generators (White-box)
  - marker: '[11]'
    intent_label: Prior Methods
    topic_label: Independent Training
  - marker: '[15]'
    intent_label: Domain Overview
    topic_label: Retrieval
  - marker: '[16]'
    intent_label: Prior Methods
    topic_label: Indexing and Query Optimization
  - marker: '[18]'
    intent_label: Prior Methods
    topic_label: Indexing and Query Optimization
  - marker: '[19]'
    intent_label: Domain Overview
    topic_label: Trustworthiness and Robustness
  - marker: '[20]'
    intent_label: Prior Methods
    topic_label: Retrieval
  - marker: '[21]'
    intent_label: Research Gap
    topic_label: Retrieval Quality
  - marker: '[23]'
    intent_label: Research Gap
    topic_label: Generation Quality
  - marker: '[24]'
    intent_label: Prior Methods
    topic_label: Parameter-Accessible Generators (White-box)
  - marker: '[28]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
  - marker: '[33]'
    intent_label: Domain Overview
    topic_label: Retrieval
  - marker: '[36]'
    intent_label: Prior Methods
    topic_label: Input-Layer Integration
  - marker: '[37]'
    intent_label: Prior Methods
    topic_label: Context Curation and Compression
  - marker: '[43]'
    intent_label: Domain Overview
    topic_label: Trustworthiness and Robustness
  - marker: '[47]'
    intent_label: Domain Overview
    topic_label: Retrieval Integration for Generation
  - marker: '[50]'
    intent_label: Prior Methods
    topic_label: Indexing and Query Optimization
  - marker: '[52]'
    intent_label: Domain Overview
    topic_label: Downstream Tasks
  - marker: '[53]'
    intent_label: Prior Methods
    topic_label: Indexing and Query Optimization
  - marker: '[54]'
    intent_label: Domain Overview
    topic_label: NLP Applications
  - marker: '[55]'
    intent_label: Prior Methods
    topic_label: Context Curation and Compression
  - marker: '[57]'
    intent_label: Prior Methods
    topic_label: Context Curation and Compression
  - marker: '[58]'
    intent_label: Prior Methods
    topic_label: Parameter-Accessible Generators (White-box)
  - marker: '[60]'
    intent_label: Prior Methods
    topic_label: Training-free Methods
  - marker: '[61]'
    intent_label: Domain Overview
    topic_label: Retrieval
  - marker: '[62]'
    intent_label: Domain Overview
    topic_label: Downstream Tasks
  - marker: '[63]'
    intent_label: Prior Methods
    topic_label: Independent Training
- block_id: 4
  content: 'When investigating the development and optimization of RAG, the effective evaluation of their performance becomes
    a fundamental concern. LangChain provides benchmark tasks, such as LangChain Docs Q&A and Semi-Structured Reports [ 27],
    designed to assess various RAG architectures. These datasets are constructed from snapshots of Python documentation and
    PDFs containing tables and charts. They emphasize the model’s capability to handle structured and semi-structured data.
    Evaluation standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing large models
    for QA generation has emerged as a prevalent approach in building evaluation datasets. For instance, RGB [ 8] creates
    its evaluation dataset by gathering recent news reports and employing LLM to generate relevant events, questions, and
    answers. Conversely, ARES [ 44] relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
    These methods not only showcase the RAG system’s proficiency in handling real-time data but also illustrate the utility
    of automation and synthetic data in the evaluation process. For evaluating the capabilities of models across various professional
    domains, the Instruct-Benchmark-Tester dataset encompasses a range of question types, with a particular focus on financial
    services, legal, and intricate business scenarios [39].


    Depending on whether the evaluation phase incorporates ground truth, metrics of existing evaluation methods can be categorized
    into those necessitating reference and those not requiring it. Reference-required evaluation methods gauge the accuracy
    and robustness of the RAG by contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-Benchmark-Tester
    [39] employs accuracy score as an evaluation metric, a widely acknowledged measure of model performance that assesses
    the extent to which model-generated answers align with reference answers. The primary objective of RGB [ 8] is to evaluate
    whether large models can effectively utilize external documents to acquire knowledge and generate accurate answers. Its
    evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.


    Reference-free evaluation methods, including TruLens-Eval [ 14], RAGAS [13], and ARES [ 44], provide distinct viewpoints
    for evaluating the performance of RAG systems, particularly concerning context relevance, answer faithfulness, and answer
    relevance. TruLens-Eval [ 14] introduces the RAG Triad as an innovative approach to evaluate hallucination issues within
    the RAG architecture, encompassing context relevance, groundedness, and answer relevance. RAGAS [ 13], serving as a reference-free
    evaluation framework, concentrates on assessing the retrieval system’s capacity to identify pertinent and concentrated
    context passages, along with the LLMs’ proficiency in faithfully and accurately leveraging these passages. In contrast
    to RAGAS, which depends on a predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for
    each aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy when compared
    to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-powered inference to offer statistical assurances
    for its scoring, generating confidence intervals. ARES emphasizes three evaluation scores: context relevance, answer faithfulness,
    and answer relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts and producing
    both faithful and relevant answers. Regarding evaluation methods, Liu et al. [ 32] place an emphasis on assessing the
    credibility and accuracy of responses generated by generative search engines through manual inspection. Nonetheless, manual
    evaluation possesses drawbacks, including high costs and challenges in scalability. Hence, rule-based evaluation metrics
    such as accuracy, exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
    rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely approximates manual
    evaluation outcomes.'
  citations:
  - marker: '[8]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[13]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[14]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[27]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[32]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[39]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[44]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
- block_id: 5
  content: 'In traditional RAG methods, despite the rich information sources provided by retrieved contexts for text generation,
    these models often do not explicitly require responses to provide corresponding citations, making traceability difficult.
    Therefore, enhancing text verifiability by introducing citation links, i.e., explicit references, has become an important
    research direction in the RAG field [ 17, 56]. Providing citation indicators in the response text offers several clear
    benefits. First, users can easily verify the claims made by LLMs based on the provided citations, thus improving the transparency
    and credibility of the text. Second, if the text generated by LLMs adheres faithfully to the cited contexts, it can significantly
    improve its accuracy and reduce the phenomenon of “hallucinations” [17]. Given this, generating high-quality citations
    and evaluating the quality of citation generation have become crucial elements of assessing RAG performance. Constructing
    appropriate prompts directly through the retrieval context to guide the model in generating corresponding citations constitutes
    a direct and effective method of citation generation [ 22].


    In terms of evaluation, early research primarily focused on the fluency, accuracy, and basic citation quality of the text
    generated by LLMs [ 17, 29]. For example, Rashkin et al. proposed the “Attributable to Identified Sources” score [ 42],
    which serves as a valuable tool for measuring the degree to which generated text is faithful to its sources. As research
    progressed, scholars recognized the need for more detailed evaluation methods to differentiate between various levels
    of citation support. By creating specialized datasets such as SCIFI [ 7], researchers can more precisely evaluate fine-grained
    citations at the clause level in texts generated by LLMs. The ALiiCE framework [56], by analyzing the atomic structure
    of sentence claims, introduced fine-grained evaluation metrics, such as location citation recall and precision, and the
    coefficient of variation of citation locations, to more granularly evaluate the quality of citation generation in RAG
    [ 56]. In practical applications, Zhang et al. [ 64] found that RAG requires more complex evaluation frameworks to distinguish
    between various levels of citation support by comparing different fidelity metrics. These RAG evaluation methods not only
    consider the presence of citations but also their accuracy and relevance.


    While Citation-Enhanced RAG delves deeply into the specific domain of citation generation, aiming to improve the credibility
    and accuracy of text generated by RAG systems, our benchmark provides a comprehensive evaluation framework encompassing
    various aspects of RAG systems and multiple application scenarios.'
  citations:
  - marker: '[7]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[17]'
    intent_label: Prior Methods
    topic_label: Generation Quality
  - marker: '[22]'
    intent_label: Prior Methods
    topic_label: Training-free Methods
  - marker: '[29]'
    intent_label: Prior Methods
    topic_label: Generation Quality
  - marker: '[42]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[56]'
    intent_label: Prior Methods
    topic_label: Evaluation Aspects and Tools
  - marker: '[64]'
    intent_label: Research Gap
    topic_label: Evaluation Aspects and Tools
- block_id: 6
  content: 'As we discussed earlier, implementing RAG effectively requires careful tuning of multiple components, such as
    the retrieval model, the knowledge corpus, the language model, and the query formulation. Therefore, we need a framework
    that can evaluate the RAG system automatically. This framework would enable us to examine how these components affect
    the system’s performance and provide us with useful insights for improving and innovating the system.


    However, the current RAG benchmarks have several drawbacks: they only evaluate question-answering tasks [1, 41, 59], ignoring
    other diverse application of RAG. The optimization strategy for QA tasks may not suit other tasks; in the evaluation experiment,
    current RAG benchmarks only account for the LLM component in the RAG pipeline, or the retriever in the knowledge-intensive
    scenario, disregarding the vital roles of retrieval database construction and retrieval in non-knowledge-intensive scenarios.


    To address the shortcomings of previous benchmarks, we introduce CRUD-RAG, a comprehensive Chinese benchmark for RAG.
    It classifies the RAG application scenarios into four categories: create, read, update, and delete, then we construct
    appropriate evaluation tasks and datasets for each category. Besides, in the experiments, we will assess the impact of
    various components of RAG, such as chunk size, retrieval strategy, top-k, LLM, and so on, on all tasks.


    In the following section, we will describe the evaluation tasks and the datasets that we design for each RAG application
    scenario type. We select text continuation, QA (single and multi-document), hallucination modification, and multi-document
    summarization as representative tasks in the CRUD scenario and construct corresponding datasets. The summarization (D)
    and continuation (C) datasets were constructed simultaneously, since the construction of both datasets requires the use
    of a search engine. They will be discussed together in the following section. The construction of the QA (R) and hallucination
    modification (D) datasets is relatively independent. To maintain narrative coherence, we will introduce the dataset construction
    process in the order of DCRU. Table 2 presents the size and composition of our datasets, and Figure 3 illustrates an example
    of our datasets.'
  citations:
  - marker: '[1]'
    intent_label: Research Gap
    topic_label: Evaluation Aspects and Tools
  - marker: '[41]'
    intent_label: Research Gap
    topic_label: Evaluation Aspects and Tools
  - marker: '[59]'
    intent_label: Research Gap
    topic_label: Evaluation Aspects and Tools
- block_id: 7
  content: 'As mentioned above, the existing benchmarks for evaluating RAG systems are mainly constructed for QA tasks. Therefore,
    the datasets, such as NQ [ 26] and RGB [ 8], are also tailored for this type of task. Hence, we need to construct new
    datasets.


    We argue that the latest news data is the most suitable choice for creating an RAG evaluation dataset. Unlike other types
    of data, such as encyclopedias, questions, or conversations, the latest news minimizes the possibility that the model
    has been exposed to similar content during training. This dependency on external retrieval mechanisms allows for a comprehensive
    evaluation of the entire RAG process, not just the model’s generation ability. Additionally, news data is easy to collect,
    enabling us to maintain dataset timeliness. When the existing dataset loses its timeliness, we can quickly gather the
    latest news to rebuild a more challenging dataset. Moreover, the latest news data offer rich and diverse topics and content,
    which can test the model’s performance and adaptability in various domains and situations.


    Therefore, we select news as the base of our datasets. To ensure the authenticity and currency of the datasets, we collected
    nearly 300,000 of historical news articles from major Chinese news websites published after July 2023, which were not
    exposed to the LLMs during the training phase. We remove duplicate news documents from 300,000 news articles and filter
    out those that are too long or too short. We ended up with more than 80,000 news articles. Based on the news corpus we
    collected, we constructed our datasets for three tasks, namely open-domain multi-document summarization, text continuation,
    and QA.'
  citations:
  - marker: '[8]'
    intent_label: Research Gap
    topic_label: Evaluation Aspects and Tools
  - marker: '[26]'
    intent_label: Research Gap
    topic_label: Evaluation Aspects and Tools
- block_id: 8
  content: 'In one of the RAG’s application scenarios, “Delete,” the RAG system retrieves key information from external sources
    based on the input text, and eliminates redundancy and irrelevance, to generate concise summaries. A suitable task for
    evaluating this scenario is multi-document summarization, which aims to generate a brief and coherent summary from a set
    of related documents. For the news data we collect, this task involves retrieving major media reports on a news event,
    and summarizing the background, process, and results of the event.


    However, constructing such a dataset is extremely challenging. First, news articles retrieved based on events may not
    be fully relevant, requiring manual filtration to identify the correct and pertinent documents. Then, when generating
    summaries from these documents, it is essential to eliminate a significant amount of redundant information, retaining
    only the most important content. These tasks require manual annotation, which consumes substantial time and financial
    resources, and often results in too much redundant information.


    Fortunately, we can use an existing method, which constructs a multi-document summary dataset in reverse [ 34]. Specifically,
    our dataset construction process is as follows:

    — Instead of generating event summaries based on multiple related news content, we first acquire a news article from a
    high-quality corpus, and annotate its summary and events.

    — Then, we search for external reference materials related to the current news by using the event text, ensuring they
    are connected but not the same. We conduct extensive searches to gather sufficient information to reconstruct the summary
    of the selected news.

    — In this manner, the reference literature we collect, along with the summary of the current news, collectively form a
    dataset of multi-document summarization.


    Specifically, we first select 10,000 news articles from our high-quality news corpus, and then use GPT-4 to generate summaries
    and events for each article. Next, we use the events as keywords, and search for the most relevant 10 news articles on
    Baidu, excluding any data that is too similar to the original article. We repeat this process for all the articles, and
    add the expanded articles to our news corpus, removing the 10,000 articles simultaneously. The new news corpus serves
    as our retrieval corpus, and we expect the model to use the events and relevant information from the retrieval corpus
    to generate a summary of the articles.'
  citations:
  - marker: '[34]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Evaluation Aspects and Tools
- block_id: 9
  content: 'RAG is useful not only for “Delete,” where it retrieves and summarizes key information from massive texts, but
    also for “Create.” In this scenario, RAG systems show strong creativity by expanding existing texts, and we take the text
    continuation task as an evaluation. The text continuation task aims to automatically produce coherent and relevant subsequent
    content based on the beginning of the text, making the text more complete and vivid.


    To construct the continuation task dataset, we follow the same method as the summary task dataset. Specifically, we select
    a news article from a high-quality corpus and use a specialized Chinese word segmentation tool, to split it into sentences.
    Then, we divide the article into two equal parts: the first half serves as the input, and the second half as the output
    of the continuation dataset. We expect the model to use RAG technology to retrieve relevant information from the document
    library and generate a continuation that is coherent, informative, and consistent with the input and output.


    To ensure that the retrieval database covers the real continuation text, we use the Baidu search engine to find external
    documents and add them to the database. The continuation text differs from the event text in that it consists of multiple
    sentences. Therefore, we split the continuation text into paragraphs by sentences and retrieve relevant documents for
    each paragraph using the search engine. This way, we guarantee that the retrieval database contains most of the information
    to reconstruct the continuation text.'
  citations: []
- block_id: 10
  content: 'Another application scenario of RAG is to use external knowledge bases to enhance the question-answering capabilities
    of LLMs, which can be applied to various knowledge-intensive tasks. Currently, there are many evaluation benchmarks to
    measure the performance of RAG in this scenario, and multiple QA datasets have been created.


    However, the existing QA datasets also have some limitations. On the one hand, some datasets (such as NQ and WEBQA) are
    outdated, and may have been covered by LLMs in the pre-training stage, which reduces the advantage of RAG systems. On
    the other hand, some datasets (such as RGB) only contain some factual questions, which can be directly extracted from
    the retrieved texts, without requiring complex reasoning over multiple texts, which poses less challenge to RAG systems.
    The most recent LLMs capture enough knowledge to rival human performance across a wide variety of QA benchmarks [ 5].


    To overcome these limitations, we build a large-scale QA dataset, which is divided into two parts: single-document and
    multi-document QA. Single-document QA focuses on factual questions that ask for specific details in the news, such as
    the location or the main characters of an event. Multi-document QA, on the other hand, involves inferential and critical
    thinking questions that require readers to reason across multiple news paragraphs, such as comparing and contrasting two
    events or assessing their impact.


    For the single-document QA task, we follow the dataset construction process of the previous RGB benchmark [ 8]. We first
    select news articles from our collected high-quality corpus. Then we use prompts to make GPT-4 generate questions and
    answers for each article. For example, for a report on “The 2023 Nobel Prize,” GPT-4 will generate the question “Who was
    awarded the 2023 Nobel Prize for Physiology and Medicine?” and provide key information for answering it.


    For the multi-document QA task, constructing a reasoning question that requires the synthesis of multiple documents is
    not trivial. Simply using a prompt to force GPT-4 to generate the question is ineffective, because creating such a multi-document
    QA dataset is a complex reasoning task in itself. Therefore, we adopt CoT technology [ 51] to enhance GPT-4. We guide
    the model to build the dataset gradually through multiple reasoning steps. We will explain it in detail:

    (1) Retrieve multiple connected news, which should cover the same event, but offer different perspectives or information.

    (2) Use prompts to help GPT-4 identify the common elements between different reports, such as the event they report on,
    and ensure they are relevant.

    (3) Use prompts to help GPT-4 distinguish the differences between news articles. While keeping the connection between
    reports, we analyze the differences between each report. This step requires comprehensive understanding and analysis from
    multiple angles and avoids generating questions that can be answered from a single paragraph.

    (4) Generate the question based on different focus points, which should require integrating information from multiple
    sources to answer.

    (5) Reconstruct the question based on the contact point. Based on the connections in the reports, refine the questions,
    ensuring the inherent logical connection, and avoiding superficial combinations. The questions should be logically linked,
    rather than physically juxtaposed.


    For example, instead of simply asking “Describe the history of World War II and explain the basic principles of quantum
    physics,” a question like “How did the technological and political environment during World War II foster the development
    of quantum physics?” should be formulated, where the parts are interdependent or have causal relationships.


    We constructed two types of multi-document QA datasets with different levels of difficulty: one requires reasoning from
    two documents to answer the question, and the other is more challenging and requires reasoning from three documents to
    answer the question.


    To further ensure our dataset’s quality, we employed a manual refinement process for the data generated by GPT-4. Our
    annotation team comprises three native Chinese speakers, each with at least a bachelor’s degree. The annotation process
    is as follows:

    (1) The annotator evaluates the quality of the automatically generated query and chooses one of the following two options:

    — Reasonable: Conforms to natural language usage.

    — Needs refinement: Has issues with naturalness, accuracy, or grammar.

    (2) If “Reasonable” is selected, no further action is taken. If “Needs refinement” is chosen, the annotator manually improves
    the query’s naturalness and accuracy.


    In addition to their standard salary, annotators receive an extra 1 RMB per query evaluated or refined. The average annotation
    time per query is approximately 20 seconds. To ensure annotation quality, we randomly inspected 5% of the annotated data.


    Given the substantial cost of manual annotation and the large size of our dataset, we initially polished one-fifth of
    our dataset manually. We will continuously monitor dataset quality across various social media platforms and refine it
    manually as needed.


    Notably, only 5.8% of queries required refinement, indicating that the queries generated by GPT-4 are generally of high
    quality. This validates the effectiveness of using GPT-4 for initial data generation and underscores our commitment to
    ensuring dataset quality.'
  citations:
  - marker: '[5]'
    intent_label: Research Gap
    topic_label: Evaluation Aspects and Tools
  - marker: '[8]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Evaluation Aspects and Tools
  - marker: '[51]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Training-free Methods
- block_id: 11
  content: 'Besides the three scenarios mentioned above, the RAG framework can also be used to correct errors in the text.
    This involves using the RAG framework to access relevant information from external sources, identify and correct errors
    in the text, and maintain the accuracy of the text content.


    We construct a hallucination modification dataset using the open source large-scale dataset UHGEval [31]. UHGEval instructs
    the model to generate continuations that contain hallucinations for a given news text. It utilizes GPT-4 for automatic
    annotation and human evaluation to identify and mark segments in the text containing hallucinations. In our approach,
    we input the hallucination text along with the corresponding annotations from the dataset. Subsequently, GPT-4 is employed
    to rectify the hallucinations, resulting in the production of the text without any hallucinatory elements. Finally, real
    news continuations will be included in the document retrieval database.


    The RAG system’s experimental results on this dataset can confirm if the system can retrieve the real news information
    from the document database based on the input text, which consists of the beginning text and the hallucination continuation
    text, and then correct the hallucination text to generate the text without hallucination.'
  citations:
  - marker: '[31]'
    intent_label: Resource Utilization
    topic_label: Evaluation Aspects and Tools
- block_id: 12
  content: "The aim of this benchmark is to evaluate how well RAG systems can retrieve relevant documents, and use them to\
    \ generate sensible responses. Therefore, we adopt an end-to-end evaluation method, which directly compares the similarity\
    \ between the model output and the reference answers.\n\nEvaluating the performance of RAG systems requires choosing appropriate\
    \ evaluation metrics. We considered the previous evaluation metrics for text generation, ROUGE and BLEU, which are both\
    \ based on word overlap. ROUGE mainly counts the recall rate on the n-gram, while BLEU mainly counts the precision rate\
    \ on the n-gram. However, BLEU and ROUGE are word overlap-based accuracy metrics that depend on the overall expression\
    \ of the text, and do not capture the accuracy of the particular key information in the text. Therefore, they may not\
    \ reflect the factual consistency of a text well, especially for long texts. To alleviate this issue, recent work [ 12,\
    \ 45, 49] has proposed new evaluation metrics for abstractive summarization evaluation. These metrics are based on the\
    \ intuition that if you ask questions about the summary and the original document, you will get a similar answer if the\
    \ summary realistically matches the original document. They evaluate the accuracy of each local piece of key information\
    \ in the summary.\n\nWe also consider QA-based metrics to evaluate the factual accuracy of generation. In this article,\
    \ we examine QuestEval [45], a metric that improves the correlation with human judgments over previous metrics in their\
    \ extensive experiments. QuestEval evaluates the factual consistency between the generated text and the source document,\
    \ which is mainly used for text summarization tasks. Therefore, it does not require any ground truth reference. However,\
    \ for RAG systems, the retrieved texts may be irrelevant or incorrect, so consistency with them is not a valid criterion.\
    \ Instead, we use this metric to measure how well the generated text matches the ground-truth reference. We call this\
    \ metric RAGQuestEval.\n\nAs Figure 6 displayed, let \x1C) and \x1C\" be two sequences of tokens, where \x1C) denotes\
    \ the ground truth references and \x1C\" the corresponding evaluated generations. First, we generate a series of questions\
    \ from the ground truth references \x1C) using the QuestEval method, which extracts entities and noun phrases from the\
    \ text. The goal of RAGQuestEval is to check if the generated text includes and conveys correctly all the key information\
    \ from the ground truth reference.\n\nNext, we answer these questions using both real references and model-generated text.\
    \ If the question is unanswerable, the model returns “<Unanswerable>.”\n\nFinally, we calculate two scores to evaluate\
    \ the quality of the generated text: recall and precision.\n\nRecall. Recall is the ratio of answerable questions to all\
    \ questions. This score shows how much information in the ground truth reference is captured by the text generated by\
    \ the RAG system. A higher recall means that the generated text covers more information from the reference\n\nRecall(\x1C\
    ) , \x1C\") = 1\n| &\x1C (\x1C) )|\nÕ\n(@,A ) ∈&\x1C (\x1C) )\nI[&\x16 (\x1C\", @) ≠ <Unanswerable>].\n\nIn the above\
    \ equation, &\x1C is the question generator and &\x16 is the question answerer.\n\nPrecision. Precision is the average\
    \ answer similarity of all questions, excluding the unanswerable ones. We use the token level F1 score to measure the\
    \ answer similarity, which is a standard metric for evaluating factoid QA models. Higher precision means that the generated\
    \ text is more accurate and consistent with the reference\n\nPrec(\x1C) , \x1C\") = 1\n| &\x1C (\x1C) )|\nÕ\n(@,A ) ∈&\x1C\
    \ (\x1C) )\n\e 1 (&\x16 (\x1C\", @), A)."
  citations:
  - marker: '[12]'
    intent_label: Prior Methods
    topic_label: Generation Quality
  - marker: '[45]'
    intent_label: Metrics Utilization
    topic_label: Generation Quality
  - marker: '[49]'
    intent_label: Prior Methods
    topic_label: Generation Quality
- block_id: 13
  content: The current evaluation of RAG benchmark only focuses on the LLM component in the RAG pipeline and overlooks the
    importance of retrieval database construction and retriever. To address this gap, we examine how different aspects of
    RAG systems affect their performance in our benchmark. We also discuss some possible ways to improve existing RAG systems.
  citations: []
- block_id: 14
  content: 'In this section, we will introduce the components of the RAG system and describe how we conduct experiments to
    evaluate their impact on system performance. The RAG system consists of the following components:

    — Chunk size: The RAG system splits the external knowledge into chunks of a certain length and stores them in a vector
    database. The chunk size affects the retrieval accuracy and the completeness of the context.

    — Chunk overlap: Chunk overlap refers to the shared tokens between two consecutive text chunks and is used to ensure semantic
    coherence when chunking.

    — Embedding model: The RAG system converts the text chunks and the user’s query into vectors using an embedding model
    or other methods. The embedding model affects the quality and relevance of the context.

    — Retriever: The RAG system uses a retriever to find the top-k vectors most similar to the query vector in the vector
    database and retrieves the corresponding text chunks. The retriever affects the richness and diversity of the context.

    — Top-k: This is the number of text chunks that the RAG system retrieves for each query, which serves as the context part
    of the LLMs prompts. The top-k influences the size of the context that the model receives.

    — LLM: The RAG system inputs the context and the query to an LLM to generate the answer. The LLM affects the correctness
    and rationality of the answer.


    We use the following settings as the basic version of our RAG system: chunk size: 128, chunk overlap: 0%, embedding model:
    bge-base, retriever: dense retriever, top-k: 8, and LLM: GPT-3.5. In the experiments, we change one component at a time
    and evaluate the results on different tasks. We compare the following values for each component:

    — Chunk size: 64, 128, 256, 512.

    — Chunk overlap: 0%, 10%, 30%, 50%, 70%.

    — Embedding model: m3e-base, bge-base, stella-base, gte-base.

    — Retriever: dense, bm25, hybrid, hybrid+rerank.

    — Top-k: 2, 4, 6, 8, 10.

    — Base LLMs: GPT-3.5, GPT-4, ChatGLM2-6B, Baichuan2-13B, Qwen-7B, Qwen-14B.


    In the experiments, we use two types of evaluation metrics: The overall semantic similarity metrics (bleu, rouge-L, and
    bertScore) measure how closely the generated content matches the reference content in terms of meaning and fluency; and
    the key information metric (RAGQuestEval) measure how well the generated content captures and presents the key information
    from the reference content.


    Considering that we used gpt-3.5 as the baseline model for the experiments, to reduce the cost, we only conducted experiments
    on 1/5 of our dataset.'
  citations: []
- block_id: 15
  content: 'Chunking is the process of dividing a document into chunks of a fixed length, and then converting each chunk into
    a vector and storing it in an index. This creates an external knowledge index. Chunk size is a crucial parameter that
    varies depending on the corpus characteristics. If the chunk is too small or too large, it can reduce the search accuracy
    or omit important content. Hence, finding the optimal chunk size is vital for ensuring search accuracy and relevance,
    and enabling the LLMs to generate appropriate responses. Our experiments reveal that different RAG tasks correspond to
    different optimal chunk sizes.


    Text Continuation. The experimental results demonstrate that larger chunk size can improve the overall semantic similarity
    measures (bleu, rouge-L). Besides, the RAGQuestEval metrics, which reflect the precision and recall rate of key information,
    follow a consistent pattern. This indicates that larger blocks preserve the original document’s structure, which is crucial
    for creative tasks such as text continuation. Smaller chunks, on the other hand, result in fragmented and semantically
    incoherent content, which impairs the ability of large models to understand and generate engaging content.


    Open-Domain Multi-Document Summarization. We observe some intriguing patterns in the experimental results. Firstly, we
    discover that larger chunk size not only substantially increases the length of the generated text but also cause a notable
    drop in the bleu score, while the rouge-L and bertScore remain almost unchanged. This implies that larger chunks can preserve
    more original text information, but also introduce some semantic redundancy. Secondly, for the RAGQuestEval metric that
    evaluates key information, we found that a larger chunk size considerably enhances the recall of key information, but
    also lowers the precision of key information.


    We hypothesize that this is because larger chunks enable the retrieval of more relevant content, thus improving the recall
    of key information. However, larger chunks also make the summarization task more challenging, as more fine-grained selection
    is required from the more relevant information, leading to lower precision of key information, which may not be a good
    thing for the summary.


    QA. For single-document QA, too large chunks will reduce both recall and precision score of key information. The task
    only requires extracting information from a sub-paragraph of a single document, and the answer may be in a specific sentence.
    Therefore, smaller chunks are more suitable, as excessive content will make the extraction harder for the model.


    For multi-document QA, the results are different from those of single-document QA. Larger chunks can significantly improve
    the recall and precision of key information, as well as the semantic similarity of the generated and reference answers.
    This is because larger chunks retain the original structure of the article, which is crucial for reasoning and understanding
    tasks, and fragmented information is not conducive to reasoning.


    Hallucination Modification. For the hallucination modification task, the results are similar to those of the single-document
    QA task. Smaller chunks can significantly improve the semantic similarity metrics, such as the bleu score. This indicates
    that in the hallucination dataset created by UHGEval, the hallucination information often pertains to only one sentence,
    which is a mistake at the word or entity level, and does not require the comprehension of long text. Hence, there is no
    need to understand the whole document, only the relevant portions can be retrieved and modified.'
  citations: []
- block_id: 16
  content: 'Chunk overlap is the number of tokens that two adjacent chunks share. To keep the text semantics coherent, adjacent
    chunks have some overlap. Chunk overlap determines the size of this overlap. This splitting method meets the maximum length
    limit of LLMs and maintains the semantic connection between adjacent chunks. Suitable chunk size and overlap can enhance
    the fluency and coherence of LLMs for long texts. We will show how the chunk overlap rate affects the system performance
    for different tasks.


    Text Continuation. With an increase in chunk overlap, we observe a slight enhancement in the metrics that evaluate the
    alignment of generated text with a reference answer (bleu, rouge-L, and bertScore). The RAGQuestEval metric, which evaluates
    the accuracy and completeness of important information, improves more obviously. These results indicate that a greater
    chunk overlap is beneficial for preserving the flow of ideas in the text, which is essential for tasks that require generating
    new, creative content.


    Open-Domain Multi-Document Summarization. During summarization tasks, all evaluation metrics show a slight improvement
    as chunk overlap grows. Interestingly, despite assumptions that more overlap might reduce the variety of context information
    available, this does not result in a lower rate of recalling important information. In fact, the best performance in terms
    of recall occurs at a chunk overlap of 70%. This could mean that a larger overlap allows the model to focus more on the
    main points and ignore less relevant or redundant information.


    QA. In QA tasks, chunk overlap has minimal impact on overall semantic similarity metrics such as bleu, rouge-L, and bertScore.
    However, it significantly affects the accuracy and recall metrics for key information. The results indicate that as chunk
    overlap increases, the accuracy and recall of key information in single-document QA tasks improve substantially. Similar
    improvements are observed in two-document QA tasks. However, for three-document QA tasks, the improvement is less pronounced.
    This may be because three-document QA tasks require richer context, and larger chunk overlaps may reduce the available
    context.


    Hallucination Modification. Changes in chunk overlap have a minimal effect on the performance metrics for tasks that involve
    correcting hallucinations. This is likely due to the errors in these tasks typically being specific to individual entities
    or words, making the consistency of the chunks less impactful.'
  citations: []
- block_id: 17
  content: 'A retriever is a key component of the RAG pipeline, which finds relevant documents from a large database based
    on the user input, and provides contextual information for the large model. There are two main types of retrievers: keyword-based
    search-sparse retrieval algorithms, which use keywords and their frequencies to compute the relevance between documents
    and queries. Common sparse retrieval algorithms include TF-IDF and BM25. BM25 is an enhanced TF-IDF method, which accounts
    for factors such as the length and position of words in the document. Dense retrieval algorithms, which use deep learning
    models to encode documents and queries into low-dimensional vectors, and then measure the cosine similarity between them.
    This method can capture the semantic and contextual information of words, and improve the retrieval performance.


    In order to combine the advantages of both types of retrievers, we can fuse their retrieval results and randomly sample
    k from them as contexts for LLMs (Hybrid). Alternatively, we can also use a reranking model to rerank the fused retrieval
    results, and then select the top-k ones as the context of LLMs (Hybrid+Rerank). In our experiments, we employ the bge-rank
    as the rerank model.


    Text Continuation. The performance of the dense retriever is roughly equivalent to that of BM25, except for the key information
    recall rate. Compared to the keyword-based algorithm, the modern vector search can capture the semantic and contextual
    information of words, so that more content that does not match keywords but is obviously semantically related can be retrieved.
    However, the RAG system using BM25 also performs well. In terms of the precision of key information, BM25 even exceeds
    the dense retriever. This suggests that in the continuation task, which is a creative task, BM25 can retrieve content
    that is highly relevant to the user’s intention, but may overlook some details.


    Open-Domain Multi-Document Summarization. On the overall semantic similarity metric, the performance of the dense retriever
    is roughly equivalent to that of BM25. On the QuestEval metric, BM25 surpasses dense retriever in terms of key information
    precision, but slightly trails behind in key information recall. If the retrieved content contains a lot of irrelevant
    information, the model-generated summary may have errors or redundancies. BM25 retrieved content usually matches the user’s
    intention better, but sometimes may miss some important information. Therefore, BM25 is weaker than dense retriever in
    key information recall, but excels in key information accuracy. Besides, hybrid retrieval algorithms presumably combine
    the advantages of both, and the RAG system generates content with suitable precision and recall.


    QA. In QA, we find that dense retriever has a more obvious advantage over BM25, when dealing with reasoning questions
    that require synthesizing multiple documents. In QA tasks that require considering three documents, Dense retriever not
    only surpasses BM25 in all the overall semantic similarity metrics, but also achieves a significant improvement in key
    information precision and recall. This indicates that QA retrieval is more difficult than text continuation and other
    tasks, especially reasoning QA, which requires a higher level of semantic understanding, and simple key-word retrieval
    algorithms may not be sufficient. We also found that the Hybrid+Rerank algorithm, which combines and reranks the results
    of both algorithms, improves on all evaluation metrics. This suggests that this is a better retrieval algorithm for QA
    tasks.


    Hallucination Modification. Consistent with the conclusion of summarization, the BM25 retriever performs slightly better
    than or equal to the dense retriever. For RAG tasks such as hallucination modification, which require precise retrieval
    of highly relevant content, BM25 shows good performance. Moreover, BM25 requires less computational resources than dense
    retrievers. This indicates that different RAG tasks require different retrieval algorithms.


    Retrieval Accuracy Evaluation. To make a more comprehensive evaluation, we evaluated the retrieval accuracy on QA and
    hallucination modification tasks using mean reciprocal rank (MRR) as a separate metric. This separate evaluation allows
    for a more accurate assessment of the retriever’s capabilities. Notably, text continuation and open-domain summarization
    tasks were excluded due to their subjective and vague evaluation criteria, lacking clear ground truth. Additionally, both
    2-document and 3-document QA require multiple documents to address queries. Therefore, we calculate the MRR for each retrieved
    document individually and take the average as the final result.


    The hybrid+reranking method excels in most tasks, outperforming other methods. This demonstrates the effectiveness of
    combining multiple retrieval strategies with reranking. Notably, BM25 and dense retrievers perform comparably in many
    cases, highlighting the strengths of both traditional and neural network methods. In QA, performance for all methods declines
    as the number of documents increases, aligning with expectations since multi-document tasks are more challenging and require
    stronger information integration. These results are consistent with our previous end-to-end evaluations, confirming the
    reliability of the end-to-end evaluation method.'
  citations: []
- block_id: 18
  content: 'Most RAG systems use vector similarity-based algorithms as retrievers. Therefore, the embedding model that converts
    document blocks into vectors is crucial for the retrieval effect. We tested various embedding models optimized for retrieval
    tasks and compared their performance in the RAG system. We evaluated several embedding models with similar parameter sizes.
    According to [38], the embedding models’ performance on the retrieval task should follow the order of GTE > STELLA > BGE
    > M3E. Our results show some variations with this order.


    For creative tasks like continuation, the relevance of the retrieved content was often ambiguous. Thus, we noticed that
    the performance difference between the embedding models was small. For single-document QA tasks that required precise
    localization of relevant documents, we found that m3e-base performed much worse than others. This matched the finding
    of [ 38]. However, for the hallucination modification task, m3e-base, which ranked the lowest on the retrieval benchmark,
    outperformed the other models on all metrics. These results further show that the retrieval benchmark may not be fully
    appropriate for RAG.


    Retrieval Accuracy Evaluation. Similar to the experiments in the retriever evaluation, we use the MRR metric to evaluate
    four mainstream embedding methods: BGE, GTE, M3E, and STELLA. The results indicate that the performance of these methods
    is relatively close across different tasks, with no single method outperforming the others in all tasks. This underscores
    the importance of considering specific task requirements when selecting an embedding method.


    As the number of documents increases from 1 to 3, the MRR values for all methods show a downward trend. This trend aligns
    with our previous end-to-end experimental results, highlighting the challenges of multi-document understanding tasks.'
  citations:
  - marker: '[38]'
    intent_label: Result Comparison
    topic_label: Retrieval Quality
- block_id: 19
  content: 'The RAG system converts the user’s query into a vector using the same embedding model as the vector database.
    Then, it searches the index for the top-k most similar vectors to the query vector, and retrieves the corresponding text
    blocks from the database. These text blocks serve as the context for the LLM prompt. The amount of information that the
    model receives depends on the size of k.


    Text Continuation. Text continuation is a highly creative task. Increasing top-k improves both the overall semantic similarity
    metrics (bertScore, bleu, and rouge-L) and the RAGQuestEval metrics. The recall metric of RAGQuestEval shows how much
    key information from the reference is included in the generated text, while the precision metric shows how correct and
    relevant that information is. We found that higher top-k values lead to higher recall and precision scores, indicating
    that the generated text contains more and better key information. We attribute this to the increased diversity and accuracy
    of the generated text from more documents.


    Open-Domain Multi-Document Summarization. Increasing the top-k value leads to longer and lower-quality summaries. The
    rouge-L and bertScore metrics stay almost the same, but the bleu metric drops significantly, indicating less similarity
    between the summaries and the references. The top-k value also affects the key information metrics. Higher top-k values
    increase the recall scores, meaning more key information is included, but decrease the precision scores, meaning more
    errors or redundancies are present.


    QA. For single-document QA, increasing top-k has little impact on the semantic similarity metric, but improves the RAGQuestEval
    metrics, which measure the accuracy and recall of key information. When the top-k value is too small, increasing the value
    of the top-k can significantly increase the recall and precision scores. This is because when the retrieved content is
    small, it may not be helpful for answering.


    For multi-document QA (2-document and 3-document), increasing top-k significantly improves the recall and precision scores,
    as there are more chances to retrieve two relevant and complementary documents. More documents can also provide additional
    information, which helps to bridge the knowledge gap between documents and give more comprehensive answers. The results
    of 2-document and 3-document QA are similar.


    Hallucination Modification. The top-k value has little effect on the semantic similarity metrics (bleu, rouge and bertScore)
    and the key information metric (RAGQuestEval). They only drop sharply when the top-k is too large. This is because, in
    our hallucination modification dataset, correcting the wrong information only requires a small amount of context, and
    the model has a certain anti-interference ability in the hallucination modification task, so the top-k value is not a
    decisive factor.'
  citations: []
- block_id: 20
  content: 'The core of the RAG system is an LLM, which can generate accurate and fluent answers based on the user’s question
    and the retrieved information. In this article, we conducted experiments on several commonly used LLMs.


    Text Continuation. The experimental results show that the larger the model parameters, the better the performance. GPT-4
    surpassed other large models in all tasks, demonstrating its powerful generation ability.


    Open-Domain Multi-Document Summarization. GPT-4 also excelled in the summary generation task. It achieved higher scores
    than other models on the overall semantic accuracy metric, as well as the key information recall and precision metric.
    Moreover, the summary generated by GPT-4 was relatively concise, avoiding redundant information. GPT-4 is the most suitable
    model for this task.


    QA. For single-document QA, which only requires extracting relevant information from a sentence in the text, this task
    is relatively simple. Qwen and Baichuan2 even outperformed GPT series models. However, for multi-document QA that requires
    a comprehensive understanding of multiple documents, GPT-4 was far ahead of other models, showing its excellent knowledge
    fusion ability. The Baichuan2-13B model also performed better than GPT-3.5, indicating its potential.


    Hallucination Modification. We found that some models generated text that was too long, introducing redundant information.
    The hallucination modification task only requires modifying the hallucination information, retaining other information,
    and not introducing irrelevant information. Therefore, ChatGLM2, Qwen-7B, and Baichuan2 did not complete this task well.


    In summary, the GPT-4 model performed excellently on most tasks and evaluation metrics, proving that it is a powerful
    LLM. Qwen-7B and Qwen-14B models also performed well, especially in the text continuation and summary generation tasks.
    Baichuan2-13B model was very competitive with GPT-4 in the QA task, deserving more investigation.


    Latest LLM Evaluation. Our dataset was constructed in December 2023. To evaluate its challenge to the latest LLMs in 2024,
    we experimented with two newly released models: GPT-4o (Released in May 2024) and Qwen2-7b (Released in June 2024).


    The results show that GPT-4o performs similarly to its predecessor GPT-4, or with some slight improvements. In contrast,
    Qwen2-7b demonstrates improvements over its predecessor Qwen-7b in some tasks. These findings confirm that our benchmarks
    remain challenging for the latest LLMs. Additionally, it is encouraging to observe that the performance of many LLMs continues
    to improve with each new version.'
  citations: []
- block_id: 21
  content: 'Using the benchmark we constructed, we systematically evaluated the impact of each component in the RAG system
    in various application scenarios. Subsequently, we offer some suggestions for future researchers aiming to optimize the
    performance of the RAG system.


    The top-k value is a crucial parameter for the RAG system, as it determines how many documents are retrieved for each
    query. Depending on the scenario, the optimal top-k value may vary. For instance, in creative content generation tasks,
    such as text continuation, a larger top-k value is preferable. This allows the LLMs to access more diverse and relevant
    knowledge, resulting in richer and more accurate content. However, this also comes with a higher computational cost.


    In summary tasks, a moderate top-k value can strike a balance between precision and recall of information. For scenarios
    that require high precision, a smaller top-k value is recommended, while for scenarios that require high recall, a larger
    top-k value is recommended. In single-document QA, it is still recommended to use a large top-k value, which means that
    the answer can be determined multiple times. In QA tasks that involve reasoning across multiple documents, a larger top-k
    value can help to retrieve two related and complementary articles, thus enhancing the QA performance.


    The chunk size is also an important factor when building the vector index for external knowledge. For creative scenarios,
    such as content generation, we suggest using a larger chunk size to preserve the structure of the article and avoid affecting
    the performance of the RAG system. For summary scenarios, a smaller chunk size can be used if more information is desired
    to be recalled; however, if the precision of the generated content is more important, a larger chunk size is still recommended
    to avoid destroying the structure of the article. In factual QA scenarios, a smaller chunk size is beneficial for finding
    the answer in a short sentence. For reasoning tasks, a larger chunk size can ensure the article’s completeness and enhance
    the reasoning ability.


    The chunk overlap is the shared content between two adjacent chunks, and chunk overlap is key to maintaining the coherence
    of semantics in LLMs when dealing with long texts. Experiments show that for creative generation, summarization, and QA
    scenarios, the semantic consistency between chunks is very important, so a large chunk overlap value should be maintained.
    However, for error correction scenarios, the semantic consistency between chunks is relatively unimportant, and a smaller
    chunk overlap value can be considered.


    When choosing an embedding model, you can refer to the mteb leaderboard [ 38], which shows the performance of different
    embedding models on retrieval tasks. However, the actual performance of the RAG system may differ from the leaderboard,
    so you need to evaluate and adjust according to the specific scenario.


    When choosing a retrieval algorithm, BM25 has the advantage of saving computational resources compared to dense retrievers,
    and since it is a keyword-based algorithm, it can usually retrieve very relevant documents. However, keyword-based algorithms
    perform poorly in capturing semantics and may miss some relevant content. Therefore, we suggest using BM25 for tasks that
    require precise content generation, such as hallucination modification and summarization.


    However, BM25 may not be suitable for tasks that require semantic understanding, such as QA and creative generation, and
    we recommend using dense algorithms based on deep learning embeddings instead.


    Moreover, the hybrid algorithm that combines dense and BM25 retriever has very limited improvement on the overall quality
    of the generated results. However, by using a rerank model to reorder the retrieval results and then inputting them into
    LLMs, the performance of almost all tasks improved, especially reasoning tasks. Therefore, we suggest trying to use the
    hybrid algorithm+rerank retrieval mode when the conditions permit, which can achieve better performance in the RAG system.


    When choosing an LLM, GPT-4 model is undoubtedly the most advanced model at present. However, due to the high cost of
    invoking GPT-4, we may need to consider some open-source alternatives. According to our experimental results, Qwen-14B
    model has shown similar performance to GPT-4 in the two tasks of text continuation and summary generation, and can generate
    high-quality creative and summarizing texts. In the QA task, Baichuan2-13B model also showed a level close to GPT-4, and
    can generate accurate and fluent answers. Therefore, we can choose a suitable LLM according to different tasks and cost
    requirements.'
  citations:
  - marker: '[38]'
    intent_label: Domain Overview
    topic_label: Retrieval Quality
- block_id: 22
  content: 'In this article, we have introduced an innovative framework (CRUD-RAG) for evaluating RAG systems that is both
    comprehensive and scenario-specific. Our unique categorization of text generation tasks into the CRUD—Create, Read, Update,
    and Delete—types provides a structured approach to assess the capabilities and limitations of RAG systems in handling
    a variety of textual contexts. To facilitate this evaluation, we have meticulously constructed large-scale datasets for
    each CRUD category, which are tailored to challenge and reflect the performance of RAG systems under different operational
    conditions. Through rigorous experimental comparisons, we have demonstrated that RAG systems can significantly enhance
    the quality of generated content by effectively incorporating information from external knowledge sources.


    Our study delves into the intricate balance required in the fine-tuning process of RAG systems, highlighting the importance
    of optimizing the retrieval model, context length, construction of the knowledge base, and the deployment of the underlying
    LLM to achieve the best results. The insights provided by our findings offer a valuable roadmap for researchers and practitioners
    in the field, guiding them in the development and refinement of RAG systems. We believe that the methodologies and results
    presented in this article will spur further exploration and innovation in the realm of RAG technologies. Our work aims
    to catalyze advancements in text generation applications, pushing the envelope of what is possible with the integration
    of retrieval mechanisms and language models. We hope that this contribution will serve as a cornerstone for future research
    efforts, fostering the creation of more intelligent, adaptive, and context-aware generative systems.'
  citations: []
