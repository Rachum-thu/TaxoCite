title: 'Re3val: Reinforced and Reranked Generative Retrieval'
abstract: 'Generative retrieval models encode pointers to information in a corpus
  as an index within the model’s parameters. These models serve as part of a larger
  pipeline, where retrieved information conditions generation for knowledge-intensive
  NLP tasks. However, we identify two limitations: the generative retrieval does not
  account for contextual information. Secondly, the retrieval can’t be tuned for the
  downstream readers as decoding the page title is a non-differentiable operation.
  This paper introduces Re3val, trained with generative reranking and reinforcement
  learning using limited data. Re3val leverages context acquired via Dense Passage
  Retrieval to rerank the retrieved page titles and utilizes REINFORCE to maximize
  rewards generated by constrained decoding. Additionally, we generate questions from
  our pre-training dataset to mitigate epistemic uncertainty and bridge the domain
  gap between the pre-training and fine-tuning datasets. Subsequently, we extract
  and rerank contexts from the KILT database using the rerank page titles. Upon grounding
  the top five reranked contexts, Re3val demonstrates the Top 1 KILT scores compared
  to all other generative retrieval models across five KILT datasets.'
abstract_is_verbatim: true
segmented_markdown: '# Re3val: Reinforced and Reranked Generative Retrieval


  ## Abstract

  <block id="0">

  Generative retrieval models encode pointers to information in a corpus as an index
  within the model’s parameters. These models serve as part of a larger pipeline,
  where retrieved information conditions generation for knowledge-intensive NLP tasks.
  However, we identify two limitations: the generative retrieval does not account
  for contextual information. Secondly, the retrieval can’t be tuned for the downstream
  readers as decoding the page title is a non-differentiable operation. This paper
  introduces Re3val, trained with generative reranking and reinforcement learning
  using limited data. Re3val leverages context acquired via Dense Passage Retrieval
  to rerank the retrieved page titles and utilizes REINFORCE to maximize rewards generated
  by constrained decoding. Additionally, we generate questions from our pre-training
  dataset to mitigate epistemic uncertainty and bridge the domain gap between the
  pre-training and fine-tuning datasets. Subsequently, we extract and rerank contexts
  from the KILT database using the rerank page titles. Upon grounding the top five
  reranked contexts, Re3val demonstrates the Top 1 KILT scores compared to all other
  generative retrieval models across five KILT datasets.


  </block>

  ## 1 Introduction

  <block id="1">

  The primary objective of retrieval models is to enhance the accuracy of answers
  by selecting the most relevant documents retrieved for a given query, ensuring models
  have sufficient information to help the downstream reasoning process. For instance,
  DRQA (Chen et al., 2017) introduces a "retrieve and read" pipeline using TF-IDF
  to return documents for a question answering model to achieve this goal. More recently,
  NLP researchers have studied neural retrieval models like Dense Passage Retrieval
  (DPR) (Karpukhin et al., 2020) with a seq2seq model to build retrieval augmented
  language models.


  Rather than using inner-product-based retrieval, generative retrieval models such
  as GENRE (Cao et al., 2021) and CorpusBrain (Chen et al., 2022) generate page titles
  through constrained decoding, attaining higher R-Precision and Recall compared to
  DPR. In our work, we further evaluate how additional contextual information can
  benefit the generative retrieval models through reranking and how reinforcement
  learning can enhance relevance through reward signals.


  We introduce Re3val: Reinforced and Reranked Generative Retrieval, a novel framework
  specifically designed to address the challenges in neural information retrieval.
  Our approach utilizes 500k pre-training data and 48k task-specific data for training.
  Despite the reduced data used in distant supervision, Re3val achieves exceptional
  performance. Our contributions are described as below:

  - We minimize the entropy of the initially retrieved page titles with contexts obtained
  from DPR, facilitating the novel generative reranking process. Through this reranking
  procedure, Re3val outperforms other generative retrieval models, including GENRE,
  CorpusBrain, and SEAL (Bevilacqua et al., 2022) in terms of average R-Precision
  across five tasks, showcasing an average increase of 1.9%.

  - We incorporate REINFORCE (Williams, 1992) to integrate information during the
  decoding process of generative retrieval. Combined with question generation, REINFORCE
  enables Re3val to outperform CorpusBrain zero-shot retrieval with an average improvement
  of 8% in R-Precision across five tasks.

  - We suggest a new generative "retrieve and read" pipeline that extracts the contexts
  for the reranked page titles, applies our context reranker, and grounds answers
  with the reranked contexts. As a result, Re3val distinguishes itself by achieving
  the highest KILT scores among other generative retrieval models, with an average
  increase of 2.1%.


  In summary, Re3val uses DPR contexts for reranking page titles, leading to improved
  R-Precision. Re3val enhances performance by integrating generated questions in pre-training
  and utilizing REINFORCE during distant supervision. Moreover, Re3val achieves more
  accurate answers by reading reranked contexts retrieved with the reranked page titles.
  These advancements enable Re3val to achieve state-of-the-art performance while also
  offering cost savings by reducing training time and minimizing the need for extensive
  data labeling.


  </block>

  ## 2 Related Work

  <block id="2">


  </block>

  ### 2.1 Document Retrieval

  <block id="3">

  TF-IDF (Johns, 1972) and BM25 (Robertson et al., 2009) assign weight to terms in
  a document based on their term frequency and inverse document frequency. These methods
  cannot inherently consider semantic shift or distribution similarity while computing
  similarity metrics. In light of this limitation, Karpukhin et al. (2020) introduce
  the Dense Passage Retrieval (DPR), establishing a bi-encoder that creates dense
  embeddings of questions and related passages within a corpus. These embeddings are
  subsequently compared using a dot product operation. During inference, DPR retrieves
  the top-k relevant contexts employing either Nearest Neighbor Search or Maximum
  Inner Product Search on the FAISS index. Guu et al. (2020) and Lewis et al. (2020)
  retrieve knowledge from a corpus using DPR and generate an answer using a variant
  of the Transformer models. FiD (Fusion in Decoder) (Izacard and Grave, 2021) extends
  T5 (Wolf et al., 2020) by combining independently encoded queries and retrieved
  passages to decode an answer. However, these models do not rerank retrieved documents
  that allow a reader to perform better with fewer contexts utilized for a reader.


  </block>

  ### 2.2 Generative Retrieval

  <block id="4">

  Cao et al. (2021) introduce an Autoregressive Entity Retrieval model (GENRE). GENRE
  utilizes seq2seq language models for page title retrieval and employs a trie-based
  constrained decoding approach. This allows GENRE to assign a probability of 0 to
  non-existing page titles, ensuring accurate retrieval. Moreover, Chen et al. (2022)
  propose CorpusBrain, a generative retrieval model encoding the knowledge about the
  corpus through pre-training strategies. DEARDR (Thorne, 2022) proposes three distinct
  pre-training regimens and a data-efficient distant supervision method for generative
  retrieval. Moreover, SEAL (Bevilacqua et al., 2022) leverages an FM-Index to efficiently
  generate n-grams within the corpus for fast lookup speed without increasing the
  index size. The Differentiable Search Index (DSI) (Tay et al., 2022) employs a seq2seq
  model to map individual queries to atomic document identifiers, which in turn are
  associated with segmented chunks of the document. Similarly, the Neural Corpus Index
  (NCI) (Wang et al., 2022) utilizes hierarchical k-means for document representation,
  generates queries based on content, and trains a transformer model with a Prefix-Aware
  Weight-Adaptive Decoder using Consistency-based regularization. However, these models
  overlook the opportunity to minimize additional entropies in retrieved page titles
  or documents by incorporating contextual information. Leveraging such information
  reduces randomness and refines the ranking. Moreover, these models overlook the
  potential benefits of harnessing knowledge during decoding.


  </block>

  ### 2.3 Question Generation

  <block id="5">

  In the past, numerous endeavors (Labutov et al., 2015; Chali and Hasan, 2015; Serban
  et al., 2016; Duan et al., 2017) have been made to generate questions to enhance
  the task of Question Answering. Recently, studies analyzing questions have attempted
  to find the relationship with contexts. Mao et al. (2021) propose Generation-Augmented
  Retrieval (GAR) that generates query contexts. GAR employs a BM-25 retrieval model
  and achieves performance comparable to DPR. Sachan et al. (2022) create questions
  based on the retrieved contexts and rerank contexts based on the log-likelihood
  score over the generated questions. However, these studies overlook the fact that
  question generation can address the epistemic uncertainty arising from limited knowledge
  (Kendall and Gal, 2017) in question answering tasks by minimizing the domain gap
  between pre-training and fine-tuning data.


  </block>

  ### 2.4 Reranking Models

  <block id="6">

  Reranking in information retrieval involves refining the initial ranking of retrieved
  documents by utilizing scores from a more complex query, as exemplified by Apache
  Solr. Atlas (Izacard et al., 2022b) retrieves documents with Contriever (Izacard
  et al., 2022a), reranks the retrieved documents, and reasons with FiD. Re2G (Glass
  et al., 2022) employs a cross-encoder (Rosa et al., 2022; Nogueira and Cho, 2020)
  to rerank retrieved documents based on softmax probability using BM25(q) ∪ DPR(q),
  determining the relevance between a query and context. FiD-Light (Hofstatter et
  al., 2022) introduces a compression for encoded passages and reranks candidate lists
  using source pointers. These source pointers are textual indices that represent
  the relevant context, as initially introduced in FiD-Ex (Lakhotia et al., 2021).
  However, these reranking models do not perform reranking at the page title level
  and do not make use of a rerank query.


  </block>

  ### 2.5 Reinforcement Learning

  <block id="7">

  When framing text generation as a Reinforcement Learning (RL) problem, the state
  (st) represents the hidden states of the encoder and previously decoded outputs
  at time steps 1, 2,..., t−1. The action (at) encompasses the encoding and decoding
  behaviors, as well as the decoded word at time step t (Paulus et al., 2018). This
  formulation can incorporate non-differentiable feedback, such as common evaluation
  metrics as reward. Moreover, various RL methodologies such as REINFORCE (Williams,
  1992), Advantage Actor-Critic (A2C) (Mnih et al., 2016), and Proximal Policy Optimization
  (PPO) (Schulman et al., 2017) are being successfully applied in a multitude of scenarios.
  This study primarily utilizes REINFORCE, a simple yet effective method.


  </block>

  ## 3 Methodology

  <block id="8">

  The primary contribution of Re3val is its capability to generatively rerank page
  titles by incorporating contextual information and to apply REINFORCE during distant
  supervision of a generative retrieval. Additionally, Re3val utilizes question generation
  for pre-training. Furthermore, Re3val pioneers the reading of contexts retrieved
  using page titles obtained through a generative retrieval approach. The following
  elucidates the function of each component with respect to its task.


  </block>

  ### 3.1 Page Title Retrieval (Stage 1-4)

  <block id="9">

  Distant Supervision (Stage 1,3) Following DEARDR (Thorne, 2022), we pre-train the
  generative retrieval. To mitigate the domain shift problem during pre-training for
  question-answering and dialogue tasks, we generate questions for half of the pre-training
  passages. We utilize Flan-T5 base (Chung et al., 2022) to create questions given
  a prompt, "Generate a question related to the following Passage: ". Among generated
  questions, we employ Spacy’s Entity Recognizer of en_core_web_sm to filter out ambiguous
  questions such as "Where is he". Specifically, we remove questions that do not contain
  entities other than DATE, MONEY, CARDINAL, TIME, QUANTITY, ORDINAL, and PERCENT.


  During the pre-training and fine-tuning of Re3val, an instructive prompt - "rank
  document titles given a query: " - is introduced before each query on the t5-small,
  t5-base, and t5-large (Wolf et al., 2020). In Few-Shot training, we added labeled
  data to narrow the range of target candidates.


  REINFORCE (Stage 2,4) A policy (π) is parameterized by θ, where T denotes the sequence
  length. Additionally, R(τ) signifies the cumulative reward associated with a trajectory
  τ, characterized as a sequence of actions (a) and states (s). The formula for calculating
  the gradient of the REINFORCE objective function is:

  ∇J(θ) = Eπθ ( ∑_{t=1}^T ∇θ log πθ(at, st) R(τ) ) (1)


  The REINFORCE is employed during training to optimize the black box of zero-shot
  and few-shot retrieval in Re3val. The REINFORCE utilizes the R Precision of generated
  page titles as a reward.


  </block>

  ### 3.2 Page Title Reranker (Stage 5-7)

  <block id="10">

  Retrieved page titles are initially ranked based on their relevance score, computed
  by our retrieval model. Then, a reranking query can be introduced to refine the
  ranking further and increase the likelihood of obtaining the most relevant page
  titles. However, the KILT datasets do not provide a specific reranking query.


  To address the limitation above, our page title reranker leverages contexts retrieved
  via an auxiliary index, such as the Dense Passage Retrieval multi-set checkpoint,
  to serve as the reranking query. Unlike the prompt for ranking, which is "rank document
  titles given a query: ", the prompt for reranking is modified to "rerank document
  titles given a query and contexts: ".


  We have implemented a new training strategy to improve the refinement and reranking
  functions of our page title reranker. This strategy combines reinforced few-shot
  (Stage 4) and zero-shot (Stage 1) retrieved page titles during training. Additionally,
  we apply uniform shuffling to the page titles in the top half of the training sets
  generated by our zero-shot and few-shot retrieval.


  Mixing titles from different checkpoints and shuffling retrieved page titles introduces
  noise to the input data. This noise is beneficial as it enables the page title reranker
  to filter out inconsistencies, outliers, and misleading patterns in the test set,
  ultimately enhancing its performance.


  </block>

  ### 3.3 Context Retrieval (Stage 10-11)

  <block id="11">

  Preprocessing (Stage 10) To refine the data for context retrieval for a reader,
  we divide each context in the KILT Database into chunks, each consisting of 100
  words. To ensure data quality and relevance, we filter out sentences that only contain
  a page title, as well as sentences containing the specific patterns, "Section::::"
  or "BULLET::::".


  Extraction (Stage 10-11) After the page title reranking process, we acquire five
  reranked page titles. Subsequently, we retrieve the corresponding contexts for each
  page title. In situations where specific page titles are unavailable in the KILT
  database, we suggest using the BM-25 imputation method. This method employs the
  BM-25 algorithm to impute the most suitable page title from the KILT database.


  </block>

  ### 3.4 Context Reranker (Stage 8-11)

  <block id="12">

  To enhance the reader’s experience, we reduce memory and context usage through our
  Context Reranker. Specifically, we use a cross-encoder to assess the relevance of
  a query and context pair for reranking the contexts derived from the five page titles.
  The input structure for our context reranker is as follows: "[CLS] Query [SEP] Context
  [SEP]".


  We utilize gold passages as positive examples for training our Context Reranker
  on nboost/pt-bert-base-uncased-msmarco. We also include two types of hard negative
  examples retrieved with BM-25: the top 128 unlabeled context chunks mapped to labeled
  page titles and the top 128 unlabeled context chunks mapped to the unlabeled page
  titles retrieved by our Page Title Reranker.


  </block>

  ### 3.5 Reader (Stage 12-14)

  <block id="13">

  We employ the Fusion in Decoder (FiD) as our reader for the reading task. During
  the pre-training phase of FiD, we utilize gold passages and impute DPR contexts
  for queries with fewer than five available gold contexts. Subsequently, following
  the pre-training phase, we perform fine-tuning of the FiD model using the top five
  or ten contexts retrieved by our context reranker.


  </block>

  ## 4 Experiments

  <block id="14">


  </block>

  ### 4.1 Datasets

  <block id="15">

  We use datasets from the KILT (Petroni et al., 2021) benchmark. We study Natural
  Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and HotpotQA
  (Yang et al., 2018) for question answering tasks, FEVER (Thorne et al., 2018) for
  a fact-checking task, and WoW (Dinan et al., 2018) for a dialogue task, which are
  publicly available. Comprehensive details about the datasets are discussed in Appendix
  A.2.


  </block>

  ### 4.2 Evaluation

  <block id="16">

  KILT utilizes a page-level retrieval strategy, and the assessment of page-level
  retrieval tasks measures the capacity to present a collection of Wikipedia pages
  as supporting evidence for a prediction, assessed through R-Precision and Recall@k
  metrics. R-Precision quantifies the proportion of relevant documents retrieved out
  of the total retrieved documents. However, Recall@k quantifies the proportion of
  relevant documents retrieved out of the total number of actual documents, taking
  into account only the top-k retrieved documents. Downstream reading tasks utilize
  different evaluation metrics depending on the specific task. For example, question-answering
  tasks are evaluated using Exact Match (EM) and F1 scores. Dialogue tasks employ
  metrics such as ROUGE-L and F1 scores. Fact verification tasks, on the other hand,
  are evaluated based on Accuracy. However, KILT has recently introduced the KILT
  score as a ranking metric for evaluating downstream performance. The KILT score
  takes into account post-processed Accuracy, EM, ROUGE-L, and F1 scores, but only
  if the R-Precision for a given query is 1.


  </block>

  ### 4.3 Page Title Retrieval

  <block id="17">

  Training We utilize 250k uniformly sampled June 2017 and August 2019 Wikipedia dumps
  for the pre-training phase across all datasets. Additionally, we generate questions
  from an additional 250k uniformly sampled Wikipedia dumps and include them in the
  training process. For fine-tuning, we utilize 48k uniformly sampled task-specific
  datasets. Important, we reinforce the zero and few-shot retrieval stages by employing
  the same dataset for each retrieval stage.


  Evaluation We employ a multi-beam search approach with a beam size specified in
  our experiments to assess the performance on all development and test sets. In addition,
  we select the top five page titles from the list of multi-page titles generated
  per query for evaluation purposes.


  </block>

  ### 4.4 Page Title Reranker

  <block id="18">

  In our experimentation, we explore two types of initialization for our page title
  reranker. Firstly, we initialize the reranker using the plain t5-small, t5-base,
  and t5-large models. Secondly, considering the three different model sizes, we utilize
  the checkpoint from the reinforced few-shot retrieval process. To maintain input
  compatibility, we limit the query for the reranker’s input to the first 250 words.
  In addition, the input - consisting of a query, ten page titles, and five contexts
  - is truncated to a maximum of 512 tokens.


  </block>

  ### 4.5 Context Reranker

  <block id="19">

  We input the first 150 words of a query for question-answering and fact-verification
  tasks. In the case of a dialogue task, the last 300 words of the query are used,
  as the final sentence often serves as the closure to the conversation. The maximum
  sequence length of input is provided in our experimental setup.


  </block>

  ### 4.6 Reader

  <block id="20">

  Two types of inputs are used for pre-training our two versions of FiD. The first
  type includes only gold passages, while the second consists of gold passages and
  top-ranked Dense Passage Retrieval (DPR) contexts. For the Natural Questions (NQ)
  dataset, pre-training is conducted using the NQ FiD checkpoint, which has been pre-trained
  on 770 million parameters. For the remaining datasets, pre-training is performed
  using the TriviaQA FiD checkpoint, which has been pre-trained on 770 million parameters.
  Regarding the WoW dataset, we retain the last 385 words of the query for input.
  For other datasets, we use the first 125 words. An example of an input format is
  "question: query, title: page_title, context: retrieved_context". In this format,
  "question:", "title:", and "context:" are special tokens, while "query", "page_title",
  and "retrieved_context" represent variables denoting the respective components of
  the input.


  </block>

  ## 5 Result

  <block id="21">


  </block>

  ### 5.1 Page Title Retrieval

  <block id="22">

  Zero-shot Retrieval Based on the findings presented in our experiments, CorpusBrain
  exhibits an 8% lower R-Precision on average compared to Re3val, despite being trained
  on more than 500 times more data. We hypothesize that the question-generation process
  mitigates the epistemic uncertainty resulting from limited training data, thus minimizing
  the domain shift between the pre-training and task-specific fine-tuning data.


  Examining development results, we observe that REINFORCE yields a modest improvement
  in the performance of zero-shot retrieval, with a few exceptions. Specifically,
  REINFORCE effectively captures the variability introduced during the constrained
  beam search exploration, as it utilizes the search results as a reward signal, thereby
  reducing bias towards the pre-training data in our retrieval model.


  Few-shot Retrieval However, the effectiveness of REINFORCE diminishes when applied
  to the few-shot retrieval scenario. In some instances, REINFORCE results in performance
  degradation across specific datasets. We postulate that this phenomenon can be attributed
  to the inherent variance associated with Reinforcement Learning. Furthermore, the
  performance degradation may arise from the exploration-exploitation trade-off during
  the multi-beam search, where a broad range of solution spaces is explored, potentially
  leading to a decreased focus on exploitation. For instance, relative performance
  ranking can be reversed as the number of samples (K) increases.


  </block>

  ### 5.2 Page Title Reranker

  <block id="23">

  The validity of our reranker’s input concatenation is supported by the principles
  of Mutual Information theory (Shannon, 1948). Let X be the set of page titles and
  Y be the set of DPR contexts. By considering the joint probability of DPR contexts
  and page titles, I(X;Y) allows us to gain insights into the dependency between these
  two variables. Therefore, our page title reranker leverages this shared information
  to reduce uncertainty in the ranking of page titles, thus improving the reranking
  and refinement process.


  Development set results indicate that the page title reranker, fine-tuned from the
  reinforced few-shot retrieval, outperforms the reranker initialized from the T5
  pre-trained model when the number of parameters is small. However, the opposite
  trend is observed as the number of parameters increases. While the knowledge about
  ranking compensates for the limited capacity to learn complex reranking patterns
  when the number of parameters is small, prior knowledge about ranking interferes
  with the reranking function as the number of parameters grows. In essence, ranking
  and reranking serve distinct purposes. Ranking focuses on sorting relevant documents,
  while reranking involves permuting the initially ranked documents.


  The dialogue task requires more detailed reasoning over textual information than
  question-answering and fact-verification tasks. Reranking with a few parameters
  does not yield improvements in performance for the WoW test set. Furthermore, the
  inconsistency between the test set results and the dev set results for the reranking
  stage of the larger parameter configurations highlights the need for further investigation.


  </block>

  ### 5.3 Context Reranker

  <block id="24">

  The performance of our Context Reranker, evaluated using gold passages and hard
  negative passages retrieved with BM-25, shows that our Context Reranker exhibits
  a higher precision compared to recall. This characteristic shows that the Context
  Reranker effectively filters out irrelevant and low-quality results, prioritizing
  accuracy in retrieving relevant documents, even if they may miss some. The high
  precision score indicates that relevant documents are ranked at the top. However,
  further investigation is required to examine the trade-off between precision and
  recall in the Context Reranker for downstream reading tasks.


  </block>

  ### 5.4 Reader

  <block id="25">

  The slight performance difference observed between the reader with 5 and 10 contexts
  suggests that our context reranker excels in retrieving highly relevant documents
  at the top, showcasing its exceptional precision. Moreover, our context imputation
  pre-training strategy is effective, enabling Re3val to outperform SEAL, although
  SEAL utilizes 100 contexts for grounding with FiD. Finally, Re3val achieves superior
  results with only five passages, underscoring the advantages of our approach.


  </block>

  ## 6 Conclusion

  <block id="26">

  This paper presents Re3val, a novel reranking architecture for generative retrieval.
  Re3val achieves state-of-art performance with question generation, REINFORCE, and
  reranking. Succinctly, Re3val incorporates question generation to address epistemic
  uncertainty and domain shift. It utilizes REINFORCE on constrained beam search outputs
  to enhance exploration. Experimental results demonstrate Re3val’s superiority over
  the CorpusBrain zero-shot baseline, with an average 8% R-Precision improvement across
  five tasks using reduced pre-training data. Re3val also achieves an average 1.9%
  R-Precision increase compared to other generative models via page title reranking
  with limited task-specific data. Moreover, by employing a context reranker before
  grounding, Re3val achieves top-1 KILT scores among generative retrieval models,
  showing an average 2.1% improvement across five datasets. Re3val’s data-efficient
  approaches reduce training time and labeling costs, representing notable advancements
  in generative retrieval.


  </block>

  ## Limitations

  <block id="27">

  Given this project’s time and resource limitations, a comprehensive comparison of
  REINFORCE with other reinforcement learning algorithms, such as PPO and TRPO, which
  require more memory for their reference model, is not feasible. Furthermore, the
  observed disparity between the performance on the development and test sets for
  both the retrieval and reader components necessitates further investigation. Lastly,
  it is worth noting that specific labeled page titles in the FEVER dataset are not
  present in the KILT database, introducing a discrepancy that should be considered.


  </block>

  ## Ethics Statement

  <block id="28">

  In this study, we utilize datasets obtained from various sources, including Natural
  Questions, TriviaQA, HotpotQA, FEVER, and Wizard of Wikipedia. These datasets serve
  as integral components of the KILT benchmark and are derived from the KILT knowledge
  source, which is based on the August 1st, 2019, Wikipedia dump. In addition to the
  2019 Wikipedia dump, we incorporate the June 2017 Wikipedia dump into our pre-training.
  It is crucial to acknowledge that these datasets may contain instances of incorrect
  or misconstrued information, which could potentially result in the generation of
  biased, toxic, or fabricated content. Moreover, the utilization of language models,
  such as T5, during the training and preprocessing stages introduces the possibility
  of ethical risks that may be embedded within the internal parameters of these models.
  Consequently, it is imperative for researchers to exercise caution when employing
  our paper and the associated outputs and to establish suitable policies to mitigate
  any potential ethical risks that may arise from the use of these models in real-world
  production settings.

  </block>'
