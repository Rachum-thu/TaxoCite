title: Recommender Systems with Generative Retrieval
abstract: Modern recommender systems perform large-scale retrieval by embedding queries
  and item candidates in the same unified space, followed by approximate nearest neighbor
  search to select top candidates given a query embedding. In this paper, we propose
  a novel generative retrieval approach, where the retrieval model autoregressively
  decodes the identifiers of the target candidates. To that end, we create semantically
  meaningful tuple of codewords to serve as a Semantic ID for each item. Given Semantic
  IDs for items in a user session, a Transformer-based sequence-to-sequence model
  is trained to predict the Semantic ID of the next item that the user will interact
  with. We show that recommender systems trained with the proposed paradigm significantly
  outperform the current SOTA models on various datasets. In addition, we show that
  incorporating Semantic IDs into the sequence-to-sequence model enhances its ability
  to generalize, as evidenced by the improved retrieval performance observed for items
  with no prior interaction history.
abstract_is_verbatim: true
segmented_markdown: '# Recommender Systems with Generative Retrieval


  ## Abstract

  <block id="0">

  Modern recommender systems perform large-scale retrieval by embedding queries and
  item candidates in the same unified space, followed by approximate nearest neighbor
  search to select top candidates given a query embedding. In this paper, we propose
  a novel generative retrieval approach, where the retrieval model autoregressively
  decodes the identifiers of the target candidates. To that end, we create semantically
  meaningful tuple of codewords to serve as a Semantic ID for each item. Given Semantic
  IDs for items in a user session, a Transformer-based sequence-to-sequence model
  is trained to predict the Semantic ID of the next item that the user will interact
  with. We show that recommender systems trained with the proposed paradigm significantly
  outperform the current SOTA models on various datasets. In addition, we show that
  incorporating Semantic IDs into the sequence-to-sequence model enhances its ability
  to generalize, as evidenced by the improved retrieval performance observed for items
  with no prior interaction history.


  </block>

  ## 1 Introduction

  <block id="1">

  Recommender systems help users discover content of interest and are ubiquitous in
  various recommendation domains such as videos [4, 43, 9], apps [3], products [6,
  8], and music [18, 19]. Modern recommender systems adopt a retrieve-and-rank strategy,
  where a set of viable candidates are selected in the retrieval stage, which are
  then ranked using a ranker model. Since the ranker model works only on the candidates
  it receives, it is desired that the retrieval stage emits highly relevant candidates.


  There are standard and well-established methods for building retrieval models. Matrix
  factorization [19] learns query and candidate embeddings in the same space. In order
  to better capture the non-linearities in the data, dual-encoder architectures [39]
  (i.e., one tower for the query and another for the candidate) employing inner-product
  to embed the query and candidate embeddings in the same space have become popular
  in recent years. To use these models during inference, an index that stores the
  embeddings for all items is created using the candidate tower. For a given query,
  its embedding is obtained using the query tower, and an Approximate Nearest Neighbors
  (ANN) algorithm is used for retrieval. In recent years, the dual encoders architectures
  have also been extended for sequential recommendations [11, 24, 41, 17, 32, 6, 44]
  that explicitly take into account the order of user-item interactions.


  We propose a new paradigm of building generative retrieval models for sequential
  recommendation. Instead of traditional query-candidate matching approaches, our
  method uses an end-to-end generative model that predicts the candidate IDs directly.
  We propose to leverage the Transformer [36] memory (parameters) as an end-to-end
  index for retrieval in recommendation systems, reminiscent of Tay et al. [34] that
  used Transformer memory for document retrieval. We refer to our method as Transformer
  Index for GEnerative Recommenders (TIGER). A high-level overview of TIGER is shown
  in Figure 1.


  TIGER is uniquely characterized by a novel semantic representation of items called
  "Semantic ID" – a sequence of tokens derived from each item’s content information.
  Concretely, given an item’s text features, we use a pre-trained text encoder (e.g.,
  SentenceT5 [27]) to generate dense content embeddings. A quantization scheme is
  then applied on the embedding of an item to form a set of ordered tokens/codewords,
  which we refer to as the Semantic ID of the item. Ultimately, these Semantic IDs
  are used to train the Transformer model on the sequential recommendation task.


  Representing items as a sequence of semantic tokens has many advantages. Training
  the transformer memory on semantically meaningful data allows knowledge sharing
  across similar items. This allows us to dispense away with the atomic and random
  item Ids that have been previously used [33, 42, 11, 8] as item features in recommendation
  models. With semantic token representations for items, the model is less prone to
  the inherent feedback loop [1, 26, 39] in recommendation systems, allowing the model
  to generalize to newly added items to the corpus. Furthermore, using a sequence
  of tokens for item representation helps alleviate the challenges associated with
  the scale of the item corpus; the number of items that can be represented using
  tokens is the product of the cardinality of each token in the sequence. Typically,
  the item corpus size can be in the order of billions and learning a unique embedding
  for each item can be memory-intensive. While random hashing-based techniques [16]
  can be adopted to reduce the item representation space, in this work, we show that
  using semantically meaningful tokens for item representation is an appealing alternative.
  The main contributions of this work are summarized below:


  1. We propose TIGER, a novel generative retrieval-based recommendation framework
  that assigns Semantic IDs to each item, and trains a retrieval model to predict
  the Semantic ID of an item that a given user may engage with.

  2. We show that TIGER outperforms existing SOTA recommender systems on multiple
  datasets as measured by recall and NDCG metrics.

  3. We find that this new paradigm of generative retrieval leads to two additional
  capabilities in sequential recommender systems: 1. Ability to recommend new and
  infrequent items, thus improving cold-start recommendations, and 2. Ability to generate
  diverse recommendations using a tunable parameter.


  Paper Overview. In Section 2, we provide a brief literature survey of recommender
  systems, generative retrieval, and the Semantic ID generation techniques we use
  in this paper. In Section 3, we explain our proposed framework, and outline the
  various techniques we use for Semantic ID generation. We present the result of our
  experiments in Section 4, and conclude the paper in Section 5.


  </block>

  ## 2 Related Work

  <block id="2">

  Sequential Recommenders. Using deep sequential models in recommender systems has
  developed into a rich literature. GRU4REC [11] was the first to use GRU based RNNs
  for sequential recommendations. Li et al. [24] proposed Neural Attentive Session-based
  Recommendation (NARM), where an attention mechanism along with a GRU layer is used
  to track long term intent of the user.


  AttRec [41] proposed by Zhang et al. used self-attention mechanism to model the
  user’s intent in the current session, and personalization is ensured by modeling
  user-item affinity with metric learning. Concurrently, Kang et al. also proposed
  SASRec [17], which used self-attention similar to decoder-only transformer models.
  Inspired by the success of masked language modeling in language tasks, BERT4Rec
  [32] and Transformers4Rec [6] utilize transformer models with masking strategies
  for sequential recommendation tasks. S3-Rec [44] goes beyond just masking by pre-training
  on four self-supervised tasks to improve data representation. The models described
  above learn a high-dimensional embedding for each item and perform an ANN in a Maximum
  Inner Product Search (MIPS) space to predict the next item. In contrast, our proposed
  technique, TIGER, uses Generative Retrieval to directly predict the Semantic ID
  of the next item.


  P5 [8] fine-tunes a pre-trained large language models for multi-task recommender
  systems. The P5 model relies on the LLM tokenizer (SentencePiece tokenizer [29])
  to generate tokens from randomly-assigned item IDs. Whereas, we use Semantic ID
  representation of items thay are learned based on the content information of the
  items. In our experiments (Table 2), we demonstrate that recommendation systems
  based on Semantic ID representation of items yield much better results than using
  random codes.


  Semantic IDs. Hou et al. proposed VQ-Rec [12] to generate “codes” (analogous to
  Semantic IDs) using content information for item representation. However, their
  focus is on building transferable recommender systems, and do not use the codes
  in a generative manner for retrieval. While they also use product quantization [15]
  to generate the codes, we use RQ-VAE to generate Semantic IDs, which leads to hierarchical
  representation of items (Section 4.2). In a concurrent work to us, Singh et al.
  [31] show that hierarchical Semantic IDs can be used to replace item IDs for ranking
  models in large scale recommender systems improves model generalization.


  Generative Retrieval. While techniques for learning search indices have been proposed
  in the past [20], generative retrieval is a recently developed approach for document
  retrieval, where the task is to return a set of relevant documents from a database.
  Some examples include GENRE [5], DSI [34], NCI [37], and CGR [22]. A more detailed
  coverage of the related work is in Appendix A. To the best of our knowledge, we
  are the first to propose generative retrieval for recommendation systems using Semantic
  ID representation of items.


  </block>

  ## 3 Proposed Framework

  <block id="3">

  Our proposed framework consists of two stages:


  1. Semantic ID generation using content features. This involves encoding the item
  content features to embedding vectors and quantizing the embedding into a tuple
  of semantic codewords. The resulting tuple of codewords is referred to as the item’s
  Semantic ID.

  2. Training a generative recommender system on Semantic IDs. A Transformer model
  is trained on the sequential recommendation task using sequences of Semantic IDs.


  </block>

  ### 3.1 Semantic ID Generation

  <block id="4">

  In this section, we describe the Semantic ID generation process for the items in
  the recommendation corpus. We assume that each item has associated content features
  that capture useful semantic information (e.g. titles or descriptions or images).
  Moreover, we assume that we have access to a pre-trained content encoder to generate
  a semantic embedding x ∈ Rd. For example, general-purpose pre-trained text encoders
  such as Sentence-T5 [27] and BERT [7] can be used to convert an item’s text features
  to obtain a semantic embedding. The semantic embeddings are then quantized to generate
  a Semantic ID for each item. Figure 2a gives a high-level overview of the process.


  We define a Semantic ID to be a tuple of codewords of length m. Each codeword in
  the tuple comes from a different codebook. The number of items that the Semantic
  IDs can represent uniquely is thus equal to the product of the codebook sizes. While
  different techniques to generate Semantic IDs result in the IDs having different
  semantic properties, we want them to at least have the following property: Similar
  items (items with similar content features or whose semantic embeddings are close)
  should have overlapping Semantic IDs. For example, an item with Semantic ID (10,
  21, 35) should be more similar to one with Semantic ID (10, 21, 40), than an item
  with ID (10, 23, 32). Next, we discuss the quantization schemes which we use for
  Semantic ID generation.


  RQ-VAE for Semantic IDs. Residual-Quantized Variational AutoEncoder (RQ-VAE) [40]
  is a multi-level vector quantizer that applies quantization on residuals to generate
  a tuple of codewords (aka Semantic IDs). The Autoencoder is jointly trained by updating
  the quantization codebook and the DNN encoder-decoder parameters. Fig. 3 illustrates
  the process of generating Semantic IDs through residual quantization.


  RQ-VAE first encodes the input x via an encoder E to learn a latent representation
  z := E(x). At the zero-th level (d = 0), the initial residual is simply defined
  as r0 := z. At each level d, we have a codebook Cd := {ek}Kk=1, where K is the codebook
  size. Then, r0 is quantized by mapping it to the nearest embedding from that level’s
  codebook. The index of the closest embedding ecd at d = 0, i.e., c0 = arg mini ∥r0
  − ek∥, represents the zero-th codeword. For the next level d = 1, the residual is
  defined as r1 := r0 − ec0. Then, similar to the zero-th level, the code for the
  first level is computed by finding the embedding in the codebook for the first level
  which is nearest to r1. This process is repeated recursively m times to get a tuple
  of m codewords that represent the Semantic ID. This recursive approach approximates
  the input from a coarse-to-fine granularity. Note that we chose to use a separate
  codebook of size K for each of the m levels, instead of using a single, mK-sized
  codebook. This was done because the norm of residuals tends to decrease with increasing
  levels, hence allowing for different granularities for different levels.


  Once we have the Semantic ID (c0, . . . , cm−1), a quantized representation of z
  is computed as bz := ∑m−1 d=0 eci. Then bz is passed to the decoder, which tries
  to recreate the input x using bz. The RQ-VAE loss is defined as L(x) := Lrecon +
  Lrqvae, where Lrecon := ∥x − bx∥2, and Lrqvae := ∑m−1 d=0 ∥sg[ri] − eci ∥2 + β∥ri
  − sg[eci ]∥2. Here bx is the output of the decoder, and sg is the stop-gradient
  operation [35]. This loss jointly trains the encoder, decoder, and the codebook.


  As proposed in [40], to prevent RQ-VAE from a codebook collapse, where most of the
  input gets mapped to only a few codebook vectors, we use k-means clustering-based
  initialization for the codebook. Specifically, we apply the k-means algorithm on
  the first training batch and use the centroids as initialization.


  Other alternatives for quantization. A simple alternative to generating Semantic
  IDs is to use Locality Sensitive Hashing (LSH). We perform an ablation study in
  Subsection 4.2 where we find that RQ-VAE indeed works better than LSH. Another option
  is to use k-means clustering hierarchically [34], but it loses semantic meaning
  between different clusters [37]. We also tried VQ-VAE, and while it performs similarly
  to RQ-VAE for generating the candidates during retrieval, it loses the hierarchical
  nature of the IDs which confers many new capabilities that are discussed in Section
  4.3.


  Handling Collisions. Depending on the distribution of semantic embeddings, the choice
  of codebook size, and the length of codewords, semantic collisions can occur (i.e.,
  multiple items can map to the same Semantic ID). To remove the collisions, we append
  an extra token at the end of the ordered semantic codes to make them unique. For
  example, if two items share the Semantic ID (12, 24, 52), we append additional tokens
  to differentiate them, representing the two items as (12, 24, 52, 0) and (12, 24,
  52, 1). To detect collisions, we maintain a lookup table that maps Semantic IDs
  to corresponding items. Note that collision detection and fixing is done only once
  after the RQ-VAE model is trained. Furthermore, since Semantic IDs are integer tuples,
  the lookup table is efficient in terms of storage in comparison to high dimensional
  embeddings.


  </block>

  ### 3.2 Generative Retrieval with Semantic IDs

  <block id="5">

  We construct item sequences for every user by sorting chronologically the items
  they have interacted with. Then, given a sequence of the form (item1, . . . , itemn),
  the recommender system’s task is to predict the next item itemn+1. We propose a
  generative approach that directly predicts the Semantic ID of the next item. Formally,
  let (ci,0, . . . , ci,m−1) be the m-length Semantic ID for item i. Then, we convert
  the item sequence to the sequence (c1,0, . . . , c1,m−1, c2,0, . . . , c2,m−1, .
  . . , cn,0, . . . , cn,m−1). The sequence-to-sequence model is then trained to predict
  the Semantic ID of item n+1, which is (cn+1,0, . . . , cn+1,m−1). Given the generative
  nature of our framework, it is possible that a generated Semantic ID from the decoder
  does not match an item in the recommendation corpus. However, as we show in appendix
  (Fig. 6) the probability of such an event occurring is low. We further discuss how
  such events can be handled in appendix E.


  </block>

  ## 4 Experiments

  <block id="6">

  Datasets. We evaluate the proposed framework on three public real-world benchmarks
  from the Amazon Product Reviews dataset [10], containing user reviews and item metadata
  from May 1996 to July 2014. In particular, we use three categories of the Amazon
  Product Reviews dataset for the sequential recommendation task: “Beauty”, “Sports
  and Outdoors”, and “Toys and Games”. We discuss the dataset statistics and pre-processing
  in Appendix C.


  Evaluation Metrics. We use top-k Recall (Recall@K) and Normalized Discounted Cumulative
  Gain (NDCG@K) with K = 5, 10 to evaluate the recommendation performance.


  RQ-VAE Implementation Details. As discussed in section 3.1, RQ-VAE is used to quantize
  the semantic embedding of an item. We use the pre-trained Sentence-T5 [27] model
  to obtain the semantic embedding of each item in the dataset. In particular, we
  use item’s content features such as title, price, brand, and category to construct
  a sentence, which is then passed to the pre-trained Sentence-T5 model to obtain
  the item’s semantic embedding of 768 dimension.


  The RQ-VAE model consists of three components: a DNN encoder that encodes the input
  semantic embedding into a latent representation, residual quantizer which outputs
  a quantized representation, and a DNN decoder that decodes the quantized representation
  back to the semantic input embedding space. The encoder has three intermediate layers
  of size 512, 256 and 128 with ReLU activation, with a final latent representation
  dimension of 32. To quantize this representation, three levels of residual quantization
  is done. For each level, a codebook of cardinality 256 is maintained, where each
  vector in the codebook has a dimension of 32. When computing the total loss, we
  use β = 0.25. The RQ-VAE model is trained for 20k epochs to ensure high codebook
  usage ( ≥ 80%). We use Adagrad optimizer with a learning rate of 0.4 and a batch
  size of 1024. Upon training, we use the learned encoder and the quantization component
  to generate a 3-tuple Semantic ID for each item. To avoid multiple items being mapped
  to the same Semantic ID, we add a unique 4th code for items that share the same
  first three codewords, i.e. two items associated with a tuple (7, 1, 4) are assigned
  (7, 1, 4, 0) and (7, 1, 4, 1) respectively (if there are no collisions, we still
  assign 0 as the fourth codeword). This results in a unique Semantic ID of length
  4 for each item in the recommendation corpus.


  Sequence-to-Sequence Model Implementation Details. We use the open-sourced T5X framework
  [28] to implement our transformer based encoder-decoder architecture. To allow the
  model to process the input for the sequential recommendation task, the vocabulary
  of the sequence-to-sequence model contains the tokens for each semantic codeword.
  In particular, the vocabulary contains 1024 (256 × 4) tokens to represent items
  in the corpus. In addition to the semantic codewords for items, we add user-specific
  tokens to the vocabulary. To keep the vocabulary size limited, we only add 2000
  tokens for user IDs. We use the Hashing Trick [38] to map the raw user ID to one
  of the 2000 user ID tokens. We construct the input sequence as the user Id token
  followed by the sequence of Semantic ID tokens corresponding to a given user’s item
  interaction history. We found that adding user ID to the input, allows the model
  to personalize the items retrieved.


  We use 4 layers each for the transformer-based encoder and decoder models with 6
  self-attention heads of dimension 64 in each layer. We used the ReLU activation
  function for all the layers. The MLP and the input dimension was set as 1024 and
  128, respectively. We used a dropout of 0.1. Overall, the model has around 13 million
  parameters. We train this model for 200k steps for the “Beauty” and “Sports and
  Outdoors” dataset. Due to the smaller size of the “Toys and Games” dataset, it is
  trained only for 100k steps. We use a batch size of 256. The learning rate is 0.01
  for the first 10k steps and then follows an inverse square root decay schedule.


  </block>

  ### 4.1 Performance on Sequential Recommendation

  <block id="7">

  In this section, we compare our proposed framework for generative retrieval with
  the following sequential recommendation methods (which are described briefly in
  Appendix B): GRU4Rec [11], Caser [33], HGN [25], SASRec [17], BERT4Rec [32], FDSA
  [42], S3-Rec [44], and P5 [8]. Notably all the baselines (except P5), learn a high-dimensional
  vector space using dual encoder, where the user’s past item interactions and the
  candidate items are encoded as a high-dimensional representation and Maximum Inner
  Product Search (MIPS) is used to retrieve the next candidate item that the user
  will potentially interact with. In contrast, our novel generative retrieval framework
  directly predicts the item’s Semantic ID token-by-token using a sequence-to-sequence
  model.


  Recommendation Performance. We perform an extensive analysis of our proposed TIGER
  on the sequential recommendation task and compare against the baselines above. The
  results for all baselines, except P5, are taken from the publicly accessible results
  made available by Zhou et al. [44]. For P5, we use the source code made available
  by the authors. However, for a fair comparison, we updated the data pre-processing
  method to be consistent with the other baselines and our method. We provide further
  details related to our changes in Appendix D.


  The results are shown in Table 1. We observe that TIGER consistently outperforms
  the existing baselines. We see significant improvement across all the three benchmarks
  that we considered. In particular, TIGER performs considerably better on the Beauty
  benchmark compared to the second-best baseline with up to 29% improvement in NDCG@5
  compared to SASRec and 17.3% improvement in Recall@5 compared to S3-Rec. Similarly
  on the Toys and Games dataset, TIGER is 21% and 15% better in NDCG@5 and NDCG@10,
  respectively.


  </block>

  ### 4.2 Item Representation

  <block id="8">

  In this section, we analyze several important characteristics of RQ-VAE Semantic
  IDs. In particular, we first perform a qualitative analysis to observe the hierarchical
  nature of Semantic IDs. Next, we evaluate the importance of our design choice of
  using RQ-VAE for quantization by contrasting the performance with an alternative
  hashing-based quantization method. Finally, we perform an ablation to study the
  importance of using Semantic IDs by comparing TIGER with a sequence-to-sequence
  model that uses Random ID for item representation.


  Qualitative Analysis. We analyze the RQ-VAE Semantic IDs learned for the Amazon
  Beauty dataset. For exposition, we set the number of RQ-VAE levels as 3 with a codebook
  size of 4, 16, and 256 respectively, i.e. for a given Semantic ID (c1, c2, c3) of
  an item, 0 ≤ c1 ≤ 3, 0 ≤ c2 ≤ 15 and 0 ≤ c3 ≤ 255. We annotate each item’s category
  using c1 to visualize c1-specific categories in the overall category distribution
  of the dataset. As shown, c1 captures the high-level category of the item. For instance,
  c1 = 3 contains most of the products related to “Hair”. Similarly, majority of items
  with c1 = 1 are “Makeup” and “Skin” products for face, lips and eyes. We also visualize
  the hierarchical nature of RQ-VAE Semantic IDs by fixing c1 and visualizing the
  category distribution for all possible values of c2. We again found that the second
  codeword c2 further categorizes the high-level semantics captured with c1 into fine-grained
  categories. The hierarchical nature of Semantic IDs learned by RQ-VAE opens a wide-array
  of new capabilities which are discussed in Section 4.3. As opposed to existing recommendation
  systems that learn item embeddings based on random atomic IDs, TIGER uses Semantic
  IDs where semantically similar items have overlapping codewords, which allows the
  model to effectively share knowledge from semantically similar items in the dataset.


  Hashing vs. RQ-VAE Semantic IDs. We study the importance of RQ-VAE in our framework
  by comparing RQ-VAE against Locality Sensitive Hashing (LSH) [14, 13, 2] for Semantic
  ID generation. LSH is a popular hashing technique that can be easily adapted to
  work for our setting. To generate LSH Semantic IDs, we use h random hyperplanes
  w1, . . . , wh to perform a random projection of the embedding vector x and compute
  the following binary vector: (1w⊤1 x>0, . . . ,1w⊤h x>0). This vector is converted
  into an integer code as c0 = ∑h i=1 2i−11w⊤i x>0. This process is repeated m times
  using an independent set of random hyperplanes, resulting in m codewords (c0, c1,
  . . . , cm−1), which we refer to as the LSH Semantic ID.


  We compare the performance of LSH Semantic ID with our proposed RQ-VAE Semantic
  ID. In this experiment, for LSH Semantic IDs, we used h = 8 random hyperplanes and
  set m = 4 to ensure comparable cardinality with the RQ-VAE. The parameters for the
  hyperplanes are randomly sampled from a standard normal distribution, which ensures
  that the hyperplanes are spherically symmetric. Our results show that RQ-VAE consistently
  outperforms LSH. This illustrates that learning Semantic IDs via a non-linear, Deep
  Neural Network (DNN) architecture yields better quantization than using random projections,
  given the same content-based semantic embedding.


  Random ID vs. Semantic ID. We also compare the importance of Semantic IDs in our
  generative retrieval recommender system. In particular, we compare randomly generated
  IDs with the Semantic IDs. To generate the Random ID baseline, we assign m random
  codewords to each item. A Random ID of length m for an item is simply (c1, . . .
  , cm), where ci is sampled uniformly at random from {1, 2, . . . , K}. We set m
  = 4 , and K = 255 for the Random ID baseline to make the cardinality similar to
  RQ-VAE Semantic IDs. A comparison of Random ID against RQ-VAE and LSH Semantic IDs
  is shown in Table 2. We see that Semantic IDs consistently outperform Random ID
  baseline, highlighting the importance of leveraging content-based semantic information.


  </block>

  ### 4.3 New Capabilities

  <block id="9">

  We describe two new capabilities that directly follow from our proposed generative
  retrieval framework, namely cold-start recommendations and recommendation diversity.
  We refer to these capabilities as “new” since existing sequential recommendation
  models cannot be directly used to satisfy these real-world use cases. These capabilities
  result from a synergy between RQ-VAE based Semantic IDs and the generative retrieval
  approach of our framework.


  Cold-Start Recommendation. In this section, we study the cold-start recommendation
  capability of our proposed framework. Due to the fast-changing nature of the real-world
  recommendation corpus, new items are constantly introduced. Since newly added items
  lack user impressions in the training corpus, existing recommendation models that
  use a random atomic ID for item representation fail to retrieve new items as potential
  candidates. In contrast, the TIGER framework can easily perform cold-start recommendations
  since it leverages item semantics when predicting the next item.


  For this analysis, we consider the Beauty dataset from Amazon Reviews. To simulate
  newly added items, we remove 5% of test items from the training data split. We refer
  to these removed items as unseen items. Removing the items from the training split
  ensures there is no data leakage with respect to the unseen items. As before, we
  use Semantic ID of length 4 to represent the items, where the first 3 tokens are
  generated using RQ-VAE and the 4th token is used to ensure a unique ID exists for
  all the seen items. We train the RQ-VAE quantizer and the sequence-to-sequence model
  on the training split. Once trained, we use the RQ-VAE model to generate the Semantic
  IDs for all the items in the dataset, including any unseen items in the item corpus.


  Given a Semantic ID (c1, c2, c3, c4) predicted by the model, we retrieve the seen
  item having the same corresponding ID. Note that by definition, each Semantic ID
  predicted by the model can match at most one item in the training dataset. Additionally,
  unseen items having the same first three semantic tokens, i.e. (c1, c2, c3) are
  included to the list of retrieved candidates. Finally, when retrieving a set of
  top-K candidates, we introduce a hyperparameter ϵ which specifies the maximum proportion
  of unseen items chosen by our framework.


  We compare the performance of TIGER with the k-Nearest Neighbors (KNN) approach
  on the cold-start recommendation setting. For KNN, we use the semantic representation
  space to perform the nearest-neighbor search. We refer to the KNN-based baseline
  as Semantic_KNN. Our framework with ϵ = 0.1 consistently outperforms Semantic_KNN
  for all Recall@K metrics. For all settings of ϵ ≥ 0.1, our method outperforms the
  baseline.


  Recommendation diversity. While Recall and NDCG are the primary metrics used to
  evaluate a recommendation system, diversity of predictions is another critical objective
  of interest. A recommender system with poor diversity can be detrimental to the
  long-term engagement of users. Here, we discuss how our generative retrieval framework
  can be used to predict diverse items. We show that temperature-based sampling during
  the decoding process can be effectively used to control the diversity of model predictions.
  While temperature-based sampling can be applied to any existing recommendation model,
  TIGER allows sampling across various levels of hierarchy owing to the properties
  of RQ-VAE Semantic IDs. For instance, sampling the first token of the Semantic ID
  allows retrieving items from coarse-level categories, while sampling a token from
  second/third token allows sampling items within a category.


  We quantitatively measure the diversity of predictions using Entropy@K metric, where
  the entropy is calculated for the distribution of the ground-truth categories of
  the top-K items predicted by the model. We report the Entropy@K for various temperature
  values. We observe that temperature-sampling in the decoding stage can be effectively
  used to increase the diversity in the ground-truth categories of the items.


  </block>

  ### 4.4 Ablation Study

  <block id="10">

  We measure the effect of varying the number of layers in the sequence-to-sequence
  model. We see that the metrics improve slightly as we make the network bigger. We
  also measure the effect of providing user information, the results for which are
  provided in Appendix.


  </block>

  ### 4.5 Invalid IDs

  <block id="11">

  Since the model decodes the codewords of the target Semantic ID autoregressively,
  it is possible that the model may predict invalid IDs (i.e., IDs that do not map
  to any item in the recommendation dataset). In our experiments, we used semantic
  IDs of length 4 with each codeword having a cardinality of 256 (i.e., codebook size
  = 256 for each level). The number of possible IDs spanned by this combination =
  256^4, which is approximately 4 trillion. On the other hand, the number of items
  in the datasets we consider is 10K-20K. Even though the number of valid IDs is only
  a fraction of all complete ID space, we observe that the model almost always predicts
  valid IDs. For top-10 predictions, the fraction of invalid IDs varies from ∼ 0.1%
  − 1.6% for the three datasets. To counter the effect of invalid IDs and to always
  get top-10 valid IDs, we can increase the beam size and filter the invalid IDs.


  It is important to note that, despite generating invalid IDs, TIGER achieves state-of-the-art
  performance when compared to other popular methods used for sequential recommendations.
  One extension to handle invalid tokens could be to do prefix matching when invalid
  tokens are generated by the model. Prefix matching of Semantic IDs would allow retrieving
  items that have similar semantic meaning as the tokens generated by the model. Given
  the hierarchical nature of our RQ-VAE tokens, prefix matching can be thought of
  as model predicting item category as opposed to the item index. Note that such an
  extension could improve the recall/NDCG metrics even further. We leave such an extension
  as a future work.


  </block>

  ## 5 Conclusion

  <block id="12">

  This paper proposes a novel paradigm, called TIGER, to retrieve candidates in recommender
  systems using a generative model. Underpinning this method is a novel semantic ID
  representation for items, which uses a hierarchical quantizer (RQ-VAE) on content
  embeddings to generate tokens that form the semantic IDs. Our framework provides
  results in a model that can be used to train and serve without creating an index
  — the transformer memory acts as a semantic index for items. We note that the cardinality
  of our embedding table does not grow linearly with the cardinality of item space,
  which works in our favor compared to systems that need to create large embedding
  tables during training or generate an index for every single item. Through experiments
  on three datasets, we show that our model can achieve SOTA retrieval performance,
  while generalizing to new and unseen items.

  </block>'
