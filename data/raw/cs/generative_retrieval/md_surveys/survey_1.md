# From Matching to Generation: A Survey on Generative Information Retrieval

## Abstract
Information Retrieval (IR) systems are crucial tools for users to access information, which have long been dominated by traditional methods relying on similarity matching. With the advancement of pre-trained language models, generative information retrieval (GenIR) emerges as a novel paradigm, attracting increasing attention. Based on the form of information provided to users, current research in GenIR can be categorized into two aspects: (1) Generative Document Retrieval (GR) leverages the generative model’s parameters for memorizing documents, enabling retrieval by directly generating relevant document identifiers without explicit indexing. (2) Reliable Response Generation employs language models to directly generate information users seek, breaking the limitations of traditional IR in terms of document granularity and relevance matching while offering flexibility, efficiency, and creativity to meet practical needs. This paper aims to systematically review the latest research progress in GenIR. We will summarize the advancements in GR regarding model training and structure, document identifier, incremental learning, etc., as well as progress in reliable response generation in aspects of internal knowledge memorization, external knowledge augmentation, etc. We also review the evaluation, challenges and future developments in GenIR systems. This review aims to offer a comprehensive reference for researchers, encouraging further development in the GenIR field.

## 1 INTRODUCTION
Information retrieval (IR) systems are crucial for navigating the vast sea of online information in today’s digital landscape. From using search engines such as Google [76], Bing [196], and Baidu [209], to engaging with question-answering or dialogue systems like ChatGPT [209] and Bing Chat [197], and discovering content via recommendation platforms like Amazon [4] and YouTube [77], IR technologies are integral to our everyday online experiences. These systems are reliable and play a key role in spreading knowledge and ideas globally.

Traditional IR systems primarily rely on sparse retrieval methods based on word-level matching. These methods, which include Boolean Retrieval [242], BM25 [238], SPLADE [65], and UniCOIL [163], establish connections between vocabulary and documents, offering high retrieval efficiency and robust system performance. With the rise of deep learning, dense retrieval methods such as DPR [117] and ANCE [324], based on the bidirectional encoding representations from the BERT model [121], capture the deep semantic information of documents, significantly improving retrieval precision. Although these methods have achieved leaps in accuracy, they rely on large-scale document indices [57, 187] and cannot be optimized in an end-to-end way. Moreover, when people search for information, what they really need is a precise and reliable answer. This document ranking list-based IR approach still requires users to spend time summarizing their required answers, which is not ideal enough for information seeking [195].

With the development of Transformer-based pre-trained language models such as T5 [231], BART [138], and GPT [228], they have demonstrated their strong text generation capabilities. In recent years, large language models (LLMs) have brought about revolutionary changes in the field of AI-generated content (AIGC) [19, 359]. Based on large pre-training corpora and advanced training techniques like RLHF [36], LLMs [8, 105, 209, 286] have made significant progress in natural language tasks, such as dialogue [209, 282] and question answering [174, 225]. The rapid development of LLMs is transforming IR systems, giving rise to a new paradigm of generative information retrieval (GenIR), which achieves IR goals through generative approaches.

As envisioned by Metzler et al. [195], in order to build an IR system that can respond like a domain expert, the system should not only provide accurate responses but also include source citations to ensure the credibility of the results. To achieve this, GenIR models must possess both sufficient memorized knowledge and the ability to recall the associations between knowledge and source documents, which could be the final goal of GenIR systems. Currently, research in GenIR primarily focuses on two main patterns: (1) Generative Document Retrieval (GR), which involves retrieving documents by generating their identifiers; and (2) Reliable Response Generation, which entails directly generating user-centric responses with reliability enhancement strategies. Noting that although these two methods have not yet been integrated technically, they represent two primary forms by which IR systems present information to users in generative manners: either by generating lists of document identifiers or by generating reliable and user-centric responses. These strategies are essential to the next generation of information retrieval and constitute the central focus of this survey.

Generative document retrieval, a new retrieval paradigm based on generative models, is garnering increasing attention. This approach leverages the parametric memory of generative models to directly generate document identifiers (DocIDs) related to the documents [18, 281, 307, 371]. Specifically, GR assigns a unique identifier to each document, which can be numeric-based or text-based, and then trains a generative retrieval model to learn the mapping from queries to the relevant DocIDs. This allows the model to index documents using its internal parameters. During inference, GR models use constrained beam search to limit the generated DocIDs to be valid within the corpus, ranking them based on generation probability to produce a ranked list of DocIDs. This eliminates the need for large-scale document indices in traditional methods, enabling end-to-end training of the model.

Recent studies on generative retrieval have delved into model training and structure [6, 153, 281, 307, 365, 369, 372], document identifier design [18, 265, 281, 288, 330], continual learning on dynamic corpora [80, 124, 192], downstream task adaptation [27, 28, 152], multi-modal generative retrieval [157, 178, 357], and generative recommender systems [74, 233, 304]. The progress in GR is shifting retrieval systems from matching to generation. It has also led to the emergence of workshops [10] and tutorials [279]. However, there is currently no comprehensive review that systematically organizes the research, challenges, and prospects of this emerging field.

Reliable response generation is also a promising direction in the IR field, offering user-centric and accurate answers that directly meet users’ needs. Since LLMs are particularly adept at following instructions [359], capable of generating customized responses, and can even cite the knowledge sources [204, 223], making direct response generation a new and intuitive way to access information [54, 75, 241, 315, 367]. The generative approach marks a significant shift from traditional IR systems, which return a ranked list of documents. Instead, response generation methods offer a more dynamic form of information access by directly generating detailed, user-centric responses, thereby providing a richer and more immediate understanding of the information need behind the users’ queries.

However, the responses generated by language models may not always be reliable. They have the potential to generate irrelevant answers [85], contradict factual information [90, 104], provide outdated data [291], or generate toxic content [93, 263]. Consequently, these limitations render them unsuitable for many scenarios that require accurate and up-to-date information. To address these challenges, the academic community has developed strategies across four key aspects: enhancing internal knowledge [16, 37, 56, 119, 132, 193, 243, 267, 285]; augmenting external knowledge [5, 113, 139, 151, 204, 245, 333]; generating responses with citation [129, 142, 156, 204, 314]; and improving personal information assistance [149, 172, 295, 327]. Despite these efforts, there is still a lack of a systematic review that organizes the existing research under this new paradigm of generative information access.

This review will systematically review the latest research progress and future developments in the field of GenIR. We will introduce background knowledge in Section 2, generative document retrieval technologies in Section 3, direct information accessing with generative language models in Section 4, evaluation in Section 5, current challenges and future directions in Section 6, respectively. Section 7 will summarize the content of this review. This article is the first to systematically organize the research, evaluation, challenges and prospects of generative IR, while also looking forward to the potential and importance of GenIR’s future development. Through this review, readers will gain a deep understanding of the latest progress in developing GenIR systems and how it shapes the future of information access. The main contribution of this survey is summarized as follows:

- First comprehensive survey on generative information retrieval (GenIR): This survey is the first to comprehensively organize the techniques, evaluation, challenges, and prospects on the emerging field of GenIR, providing a deep understanding of the latest progress in developing GenIR systems and its future in shaping information access.
- Systematic categorization and in-depth analysis: The survey offers a systematic categorization of research related to GenIR systems, including generative document retrieval and reliable response generation. It provides an in-depth analysis of each category, covering model training and structure, document identifier, etc. in generative document retrieval; internal knowledge memorization, external knowledge enhancement, etc. for reliable response generation.
- Comprehensive review of evaluation metrics and benchmarks: The survey reviews a range of widely used evaluation metrics and benchmark datasets for accessing GenIR methods, alongside analysis on the effectiveness and weaknesses of existing GenIR methods.
- Discussions of current challenges and future directions: The survey identifies and discusses the current challenges faced in the GenIR field. We also provide potential solutions for each challenge and outline future research directions for building GenIR systems.

## 2 BACKGROUND AND PRELIMINARIES
Information retrieval techniques aim at efficiently obtaining, processing, and understanding information from massive data. Technological advancements have continuously driven the evolution of these methods: from early keyword-based sparse retrieval to deep learning-based dense retrieval, and more recently, to generative retrieval, large language models, and their augmentation techniques. Each advancement enhances retrieval accuracy and efficiency, catering to the complex and diverse query needs of users.

### 2.1 Traditional Information Retrieval
Sparse Retrieval. In the field of traditional information retrieval, sparse retrieval techniques implement fast and accurate document retrieval through the inverted index method. Inverted indexing technology maps each unique term to a list of all documents containing that term, providing an efficient means for information retrieval in large document collections. Among these methods, TF-IDF (Term Frequency-Inverse Document Frequency) [235] is a particularly important statistical tool used to assess the importance of a word in a document collection, thereby widely applied in various traditional retrieval systems.

The core of sparse retrieval technology lies in evaluating the relevance between documents and user queries. Specifically, given a document collection D and a user query q, traditional information retrieval systems identify and retrieve information by calculating the relevance R between document d and query q. This relevance evaluation typically relies on the similarity measure between document d and query q, as shown below:
R(q, d) = sum_{t in q ∩ d} tf-idf(t, d) · tf-idf(t, q),
where t represents the terms common to both query q and document d, and tf-idf(t, d) and tf-idf(t, q) represent the TF-IDF weights of term t in document d and query q, respectively. Although sparse retrieval methods like TF-IDF [235] and BM25 [238] excel at fast retrieval, it struggles with complex queries involving synonyms, specialized terms, or context, as term matching and TF-IDF may not fully meet users’ information needs [180].

Dense Retrieval. The advent of pre-trained language models like BERT [121] has revolutionized information retrieval, leading to the development of dense retrieval methods, like DPR [117], ANCE [324], E5 [298], SimLM [299]. Unlike traditional sparse retrieval, these methods leverage Transformer-based encoders to create dense vector representations for both queries and documents. This approach enhances the capability to grasp the underlying semantics, thereby improving retrieval accuracy.

The core of dense retrieval lies in converting documents and queries into vector representations. Given document d and query q and their vector representations vq, each document d is transformed into a dense vector vd through a pre-trained language model, similarly, query q is transformed into vector vq. Specifically, we can use encoder functions Ed(·) and Eq(·) to represent the encoding process for documents and queries, respectively:
vd = Ed(d), vq = Eq(q),
where Ed(·) and Eq(·) can be the same model or different models optimized for specific tasks. Dense retrieval methods evaluate relevance by calculating the similarity between the query vector and document vector, which can be measured by cosine similarity:
R(q, d) = cos(vq, vd) = vq · vd / (|vq| |vd|),
where vq · vd represents the dot product of query vector vq and document vector vd, and |vq| and |vd| respectively represent the magnitudes of the query and document vector. Finally, documents are ranked based on these similarity scores to identify the most relevant ones for the user.

### 2.2 Generative Retrieval
With the significant progress of language models, generative retrieval has emerged as a new direction in the field of information retrieval [195, 281, 328]. Unlike traditional index-based retrieval methods, generative retrieval relies on pre-trained generative language models, such as T5 [231] and BART [138], to directly generate document identifiers (DocIDs) relevant to the query, thereby achieving end-to-end retrieval without relying on large-scale pre-built document indices.

DocID Construction and Prefix Constraints. To facilitate generative retrieval, each document d in the corpus D = {d1, d2, ..., dN} is assigned a unique document identifier d′, forming the set D′ = {d′1, d′2, ..., d′N}. This mapping is typically established via a bijective function φ: D → D′, ensuring that:
φ(di) = d′i, ∀di ∈ D.
To enable the language model to generate only valid DocIDs during inference, we construct prefix constraints based on D′. This is typically implemented using a trie (prefix tree), where each path from the root to a leaf node corresponds to a valid DocID.

Constrained Beam Search. Given a query q, the generative retrieval model aims to generate the top-k DocIDs that are most relevant to q. The language model P(·|q; θ) generates DocIDs token by token, guided by the prefix constraints. At each decoding step i, only those tokens that extend the current partial sequence d′_{<i} into a valid prefix of some DocIDs in D′ are considered. Formally, the set of allowable next tokens is:
V(d′_{<i}) = {v | ∃ d′ ∈ D′ such that d′_{<i} v is a prefix of d′}.
By employing constrained beam search, the model efficiently explores the space of valid DocIDs, maintaining a beam of the most probable sequences at each decoding step while adhering to the DocID prefix constraints.

Document Relevance. The relevance between the query q and a document d is quantified by the probability of generating its corresponding DocID d′ given q. This is computed as:
R(q, d) = P(d′ | q; θ) = ∏_{i=1}^{T} P(d′_i | d′_{<i}, q; θ),
where T is the length of the DocID d′ in tokens, d′_i is the token at position i, and d′_{<i} denotes the sequence of tokens generated before position i. The constrained beam search produces a ranked list of top-k DocIDs {d′(1), d′(2), ..., d′(k)} based on their generation probabilities {R(q, d(1)), R(q, d(2)), ..., R(q, d(k))}. The corresponding documents {d(1), d(2), ..., d(k)} are then considered the most relevant to the query q.

Model Optimization. Generative retrieval models are typically optimized using cross-entropy loss, which measures the discrepancy between the generated DocID sequence and the ground truth DocID. Given a query q and its corresponding DocID d′, the cross-entropy loss is defined as:
L = −∑_{i=1}^{T} log P(d′_i | d′_{<i}, q; θ),
where T is the length of the DocID in tokens, d′_i is the token at position i, and d′_{<i} denotes the sequence of tokens generated before position i. This loss function encourages the model to learn the association between query and labeled DocID sequence.

This approach allows the generative retrieval model to produce a relevance-ordered list of documents without relying on traditional indexing structures. The core of this approach lies in leveraging the language model’s capability to generate DocID sequences within prefix constraints.

### 2.3 Large Language Models
The evolution of Large Language Models (LLMs) marks a significant leap in natural language processing (NLP), rooted from early statistical and neural network-based language models [374]. These models, through pre-training on vast text corpora, learned deep semantic features of language, greatly enriching the understanding of text. Subsequently, generative language models, most notably the GPT series [16, 228, 229], significantly improved text generation and understanding capabilities with improved model size and number of parameters.

LLMs can be mainly divided into two categories: encoder-decoder models and decoder-only models. Encoder-decoder models, like T5 [231] and BART [138], convert input text into vector representations through their encoder, then the decoder generates output text based on these representations. This model perspective treats various NLP tasks as text-to-text conversion problems, solving them through text generation. On the other hand, decoder-only models, like the GPT [228] and GPT-2 [229], rely entirely on the Transformer decoder, generating text step by step through the self-attention mechanism. The introduction of GPT-3 [16], with its 175 billion parameters, marked a significant milestone in this field and led to the creation of models like InstructGPT [210], Falcon [215], PaLM [34] and Llama series [59, 285, 286]. These models, all using a decoder-only architecture, trained on large-scale datasets, have shown astonishing language processing capabilities [359].

For information retrieval tasks, large language models (LLMs) play a crucial role in directly generating the exact information users seek [55, 173, 374]. This capability marks a significant step towards a new era of generative information retrieval. In this era, the retrieval process is not solely about locating existing information but also about creating new content that meets the specific needs of users. This feature is especially advantageous in situations where users might not know how to phrase their queries or when they are in search of complex and highly personalized information, scenarios where traditional matching-based methods fall short.

### 2.4 Augmented Language Models
Despite the advances of LLMs, they still face significant challenges such as hallucination, particularly in complex tasks or those requiring access to long-tail or real-time information [90, 359]. To address these issues, retrieval augmentation and tool augmentation have emerged as effective strategies. Retrieval augmentation involves integrating external knowledge sources into the language model’s workflow. This integration allows the model to access up-to-date and accurate information during the generation process, thereby grounding its responses in verified data and reducing the likelihood of hallucinations [139, 252, 271]. Tool augmentation, on the other hand, extends the capabilities of LLMs by incorporating specialized tools or APIs that can perform specific functions like mathematical computations, data retrieval, or executing predefined commands [226, 245, 276]. With retrieval and tool augmentations, language models can provide more precise and contextually relevant responses, thereby improving factuality and functionality in practical applications.

Moreover, due to the aforementioned issue of hallucinations, the responses generated by LLMs are often considered unreliable because users are unaware of the sources behind the generated content, making it difficult to verify its accuracy. To enhance the credibility of responses, some studies have focused on generating responses with citations [143, 204, 256]. This approach involves enabling language models to cite the source documents of their generated content, thereby increasing the trustworthiness of the responses. All these methods are effective strategies for improving both the quality and reliability of language model outputs and are essential technologies for building the next generation of generative information retrieval systems.

## 3 GENERATIVE DOCUMENT RETRIEVAL: FROM SIMILARITY MATCHING TO GENERATING DOCUMENT IDENTIFIERS
In recent advancements in AIGC, generative retrieval (GR) has emerged as a promising approach in the field of information retrieval, garnering increasing interest from the academic community. Initially, GENRE [18] proposed to retrieve entities by generating their unique names through constrained beam search via a pre-built entity prefix tree, achieving advanced entity retrieval performance. Subsequently, Metzler et al. [195] envisioned a model-based information retrieval framework aiming to combine the strengths of traditional document retrieval systems and pre-trained language models to create systems capable of providing expert-quality answers in various domains.

Following their lead, a diverse range of methods including DSI [281], DynamicRetriever [370], SEAL [13], NCI [307], etc., have been developed, with a continuously growing body of work. These methods explore various aspects such as model training and architectures, document identifiers, incremental learning, task-specific adaptation, and generative recommendations. We provide an in-depth discussion of each associated challenge in the following sections.

### 3.1 Model Training and Structure
One of the core components of GR is the model training and structure, aiming to enhance the model’s ability to memorize documents in the corpus.

#### 3.1.1 Model Training
To effectively train generative models for indexing documents, the standard approach is to train the mapping from queries to relevant DocIDs, based on standard sequence-to-sequence (seq2seq) training methods. This method has been widely used in numerous GR research works, such as DSI [281], NCI [307], SEAL [13], etc. Moreover, a series of works have proposed various model training methods tailored for GR tasks to further enhance GR retrieval performance, such as sampling documents or generating queries based on document content to serve as pseudo queries for data augmentation; or including training objectives for document ranking.

Specifically, DSI [281] proposed two training strategies: one is “indexing”, that is, training the model to associate document tokens with their corresponding DocIDs, where DocIDs are pre-built based on documents in corpus; the other is “retrieval”, using labeled query-DocID pairs to fine-tune the model. Notably, DSI was the first to realize a differentiable search index based on the Transformer [290] structure, showing good performance in web search [205] and question answering [126] scenarios. Next, a series of methods propose training methods for data augmentation and improving GR model ranking ability.

Sampling Document Pieces as Pseudo Queries. DynamicRetriever [370], also based on the encoder-decoder model, constructed a model-based IR system by initializing the encoder with a pre-trained BERT [121]. DynamicRetriever utilizes passages, sampled terms and N-grams to serve as pseudo queries to enhance the model’s memorization of DocIDs. Formally, sampled document d_si → DocID and labeled query q_i → DocID are used as training pairs.

Generating Pseudo Queries from Documents. Following DSI, the NCI [307] model was trained using a combination of labeled query-document pairs and augmented pseudo query-document pairs. Specifically, NCI proposed two strategies: one using the DocT5Query [208] model as a query generator, generating pseudo queries for each document in the corpus through beam search; the other strategy directly uses the document as a query. Similarly, DSI-QG [375] also proposed using a query generator to enhance training data, establishing a bridge between indexing and retrieval in DSI. This data augmentation method has been proven in subsequent works to be an effective method to enhance the model’s memorization for DocIDs.

Improving Ranking Capability. Additionally, a series of methods focus on further optimizing the ranking capability of GR models. Chen et al. [30] proposed a multi-task distillation method to improve retrieval quality without changing the model structure, thereby obtaining better indexing and ranking capabilities. Meanwhile, LTRGR [159] introduced a ranking loss to train the model in ranking paragraphs. Subsequently, [365] proposed GenRRL, which improves ranking quality through reinforcement learning with relevance feedback, aligning token-level DocID generation with document-level relevance estimation. Moreover, [161] introduced DGR, which enhances generative retrieval through knowledge distillation. Specifically, DGR uses a cross-encoder as a teacher model, providing fine-grained passage ranking supervision signals, and then optimizes the model with a distilled RankNet loss. ListGR [280] defined positional conditional probabilities, emphasizing the importance of the generation order of each DocID in the list. In addition, ListGR employs relevance calibration that adjusts the generated list of DocIDs to better align with the labeled ranking list.

#### 3.1.2 Model Structure
Basic generative retrieval models mostly use pre-trained encoder-decoder structured generative models, such as T5 [231] and BART [138], fine-tuned for the DocID generation task. To better adapt to the GR task, researchers have proposed a series of specifically designed model structures [130, 224, 237, 275, 307, 342, 346].

Model Decoding Methods. For the semantic structured DocID proposed by DSI [281], NCI [307] designed a Prefix-Aware Weight-Adaptive (PAWA) decoder. By adjusting the weights at different positions of DocIDs, this decoder can capture the semantic hierarchy of DocIDs. To allow the GR model to utilize both own parametric knowledge and external information, NP-Decoding [130] proposed using non-parametric contextualized word embeddings (as external memory) instead of traditional word embeddings as the input to the decoder. Additionally, PAG [345] proposed a planning-ahead generation approach, which first decodes the set-based DocID to approximate document-level scores, and then continues to decode the sequence-based DocID on this basis.

Combining Generative and Dense Retrieval Methods. Combining seq2seq generative models with dual-encoder retrieval models, MEVI [346] utilizes Residual Quantization (RQ) [189] to organize documents into hierarchical clusters, enabling efficient retrieval of candidate clusters and precise document retrieval within those clusters. Similarly, Generative Dense Retrieval (GDR) [342] proposed to first broadly match queries to document clusters, optimizing for interaction depth and memory efficiency, and then perform precise, cluster-specific document retrieval, boosting both recall and scalability.

Utilizing Multiple Models. TOME [237] proposed to decompose the GR task into two stages, first generating text paragraphs related to the query through an additional model, then using the GR model to generate the URL related to the paragraph. DiffusionRet [224] proposed to first use a diffusion model (SeqDiffuSeq [341]) to generate a pseudo-document from a query, where the generated pseudo-document is similar to real documents in length, format, and content, rich in semantic information; then, it employs another generative model to perform retrieval based on N-grams, similar to the process used by SEAL [13], leveraging an FM-Index [62] for generating N-grams found in the corpus. Self-Retrieval [275] fully integrated indexing, retrieval, and evaluation into a single large language model. It generates natural language indices and document segments, and performs self-evaluation to score and rank the generated documents.

### 3.2 Design of Document Identifiers
Another essential component of generative retrieval is document representation, also known as document identifiers (DocIDs), which act as the target outputs for the GR model. Accurate document representations are crucial as they enable the model to more effectively memorize document information, leading to enhanced retrieval performance. We explore the design of DocIDs from two categories: numeric-based identifiers and text-based identifiers.

#### 3.2.1 Numeric-based Identifiers
An intuitive method to represent documents is by using a single number or a series of numbers, referred to as DocIDs. Existing methods have designed both static and learnable DocIDs.

Static DocIDs. Initially, DSI [281] introduced three numeric DocIDs to represent documents: (1) Unstructured Atomic DocID: a unique integer identifier is randomly assigned to each document, containing no structure or semantic information. (2) Naively Structured String DocID: treating random integers as divisible strings, implementing character-level DocID decoding to replace large softmax output layers. (3) Semantically Structured DocID: introducing semantic structure through hierarchical k-means method, allowing semantically similar documents to share prefixes in their identifiers, effectively reducing the search space. Concurrently, DynamicRetriever [370] also built a model-based IR system based on unstructured atomic DocID. Subsequently, Ultron [371] encoded documents into a latent semantic space using BERT [121], and compressed vectors into a smaller semantic space via Product Quantization (PQ) [73, 102], preserving semantic information. Each document’s PQ code serves as its semantic identifier. MEVI [346] clusters documents using Residual Quantization (RQ) [189] and utilizes dual-tower and seq2seq model embeddings for a balanced performance in large-scale document retrieval.

Learnable DocIDs. Unlike previous static DocIDs, GenRet [265] proposed learnable document representations, transforming documents into DocIDs through an encoder, then reconstructs documents from DocIDs using a decoder, trained to minimize reconstruction error. Furthermore, it used progressive training and diversity clustering for optimization. To ensure that DocID embeddings can reflect document content, Tied-Atomic [206] proposed to link document text with token embeddings and employs contrastive loss for DocID generation. LMIndexer [112] and ASI [330] learned optimal DocIDs through semantic indexing, with LMIndexer using a reparameterization mechanism for unified optimization, facilitating efficient retrieval by aligning semantically similar documents under common DocIDs. ASI extends this by establishing an end-to-end retrieval framework, incorporating semantic loss functions and reparameterization to enable joint training.

Furthermore, RIPOR [344] treats the GR model as a dense encoder to encode document content. It then splits these representations into vectors via RQ [189], creating unique DocID sequences. RIPOR implements a prefix-guided ranking optimization, increasing relevance scores for prefixes of pertinent DocIDs through margin decomposed pairwise loss during decoding.

In summary, numeric-based document representations can utilize the embeddings of dense retrievers, obtaining semantically meaningful DocID sequences through methods such as k-means, PQ [102], and RQ [189]; they can also combine encoder-decoder GR models with bi-encoder DR models to achieve complementary advantages [206, 346].

#### 3.2.2 Text-based Identifiers
Text-based DocIDs have the inherent advantage of effectively leveraging the strong capabilities of pre-trained language models and offering better interpretability.

Document Titles. The most straightforward text-based identifier is the document title, which requires each title to uniquely represent a document in the corpus, otherwise, it would not be possible to accurately retrieve a specific document. The Wikipedia corpus used in the KILT [218] benchmark, due to its well-regulated manual annotation, has a unique title corresponding to each document. Thus, GENRE [18], based on the title as DocID and leveraging the generative model BART [138] and pre-built DocID prefix, achieved superior retrieval performance across 11 datasets in KILT. Following GENRE, GERE [27], CorpusBrain [28], Re3val [257], and CorpusBrain++ [80] also based their work on title DocIDs for Wikipedia-based tasks. LLM-URL [376] directly generated URLs using ChatGPT prompts, achieving commendable performance after removing invalid URLs. However, in the web search scenario [205], document titles in the corpus often have significant duplication and many meaningless titles, making it unfeasible to use titles alone as DocIDs. Thus, Ultron [371] effectively addressed this issue by combining URLs and titles as DocIDs, identifying documents through keywords in web page URLs and titles.

Sub-strings of Documents. To increase the flexibility of DocIDs, SEAL [13] proposed a sub-string identifier, representing documents with any N-grams within them. Using FM-Index (a compressed full-text sub-string index) [62], SEAL could generate N-grams present in the corpus to retrieve all documents containing those N-grams, scoring and ranking documents based on the frequency of N-grams in each document and the importance of N-grams. For a more comprehensive representation of documents, MINDER [160] proposed multi-view identifiers, including generated pseudo queries from document content via DocT5Query [208], titles, and sub-strings. This multi-view DocID was also used in LTRGR [159] and DGR [161].

Term Sets. Unlike the sequential DocIDs described earlier, AutoTSG [352] proposed a term set-based document representation, using keywords extracted from titles and content, rather than predefined sequences, allowing for retrieval of the target document as long as the generated term set is included in the extracted keywords. Recently, PAG [345] also constructed DocIDs based on sets of key terms, disregarding the order of terms, which is utilized for approximating document relevance in decoding.

Learnable DocIDs. Text-based identifiers can also be learnable. NOVO [311] proposed learnable continuous N-grams constituting term-set DocIDs. Through denoising query modeling, the model learned to generate queries from documents with noise, thereby implicitly learning to filter out document N-grams more relevant to queries. NOVO also improves the document’s semantic representation by updating N-gram embeddings. Later, GLEN [135] uses dynamic lexical DocIDs and follows a two-phase index learning strategy. First, it assigns DocIDs by extracting keywords from documents using self-supervised signals. Then, it refines DocIDs by integrating query-document relevance through two loss functions. During inference, GLEN ranks documents using DocID weights without additional overhead.

### 3.3 Incremental Learning on Dynamic Corpora
Prior studies have focused on generative retrieval from static document corpora. However, in reality, the documents available for retrieval are continuously updated and expanded. To address this challenge, researchers have developed a range of methods to optimize GR models for adapting to dynamic corpora.

Optimizer and Document Rehearsal. DSI++ [192] aims to address the incremental learning challenges encountered by DSI [281]. DSI++ modifies the training by optimizing flat loss basins through the Sharpness-Aware Minimization (SAM) optimizer, stabilizing the learning process of the model. It also employs DocT5Query [208] to generate pseudo queries for documents in the existing corpus as training data augmentation, mitigating the forgetting issue of GR models.

Constrained Optimization. IncDSI [124] views the addition of new documents as a constrained optimization problem to find optimal representations for the new documents. This approach aims to (1) ensure new documents can be correctly retrieved by their relevant queries, and (2) maintain the retrieval performance of existing documents unaffected.

Incremental Product Quantization. CLEVER [25], based on Product Quantization (PQ) [102], proposes Incremental Product Quantization (IPQ) for generating PQ codes as DocIDs for documents. Compared to traditional PQ methods, IPQ designs two adaptive thresholds to update only a subset of centroids instead of all, maintaining the indices of updated centroids constant. This method reduces computational costs and allows the system to adapt flexibly to new documents.

Fine-tuning Adapters for Specific Tasks. CorpusBrain++ [80] introduces the KILT++ benchmark for continuously updated KILT [218] tasks and designs a dynamic architecture paradigm with a backbone-adapter structure. By fixing a shared backbone model to provide basic retrieval capabilities while introducing task-specific adapters to incrementally learn new documents for each task, it effectively avoids catastrophic forgetting. During training, CorpusBrain++ generates pseudo queries for new document sets and continues to pre-train adapters for specific tasks.

### 3.4 Downstream Task Adaption
Generative retrieval methods, apart from addressing retrieval tasks individually, have been tailored to various downstream generative tasks. These include fact verification [284], entity linking [86], open-domain QA [126], dialogue [51], slot filling [137], among others, as well as knowledge-intensive tasks [218], code [179], conversational QA [3], and multi-modal retrieval scenarios [165], demonstrating superior performance and efficiency. These methods are discussed in terms of separate training, joint training, and multi-modal generative retrieval.

#### 3.4.1 Separate Training
For fact verification tasks [284], which involve determining the correctness of input claims, GERE [27] proposed using an encoder-decoder-based GR model to replace traditional indexing-based methods. Specifically, GERE first utilizes a claim encoder to encode input claims, and then generates document titles related to the claim through a title decoder to obtain candidate sentences for corresponding documents.

Knowledge-Intensive Language Tasks. For Knowledge-Intensive Language Tasks (KILT) [218], CorpusBrain [28] introduced three pre-training tasks to enhance the model’s understanding of query-document relationships at various granularities: Internal Sentence Selection, Leading Paragraph Selection, and Hyperlink Identifier Prediction. Similarly, UGR [26] proposed using different granularities of N-gram DocIDs to adapt to various downstream tasks, unifying different retrieval tasks into a single generative form. UGR achieves this by letting the GR model learn prompts specific to tasks, generating corresponding document, passage, sentence, or entity identifiers. DearDR [283] utilizes remote supervision and self-supervised learning techniques, using Wikipedia page titles and hyperlinks as training data. The model samples sentences from Wikipedia documents as input and trains a self-regressive model to decode page titles or hyperlinks, or both, without the need for manually labeled data. Re3val [257] proposes a retrieval framework combining generative reordering and reinforcement learning. It first reorders retrieved page titles using context information obtained from a dense retriever, then optimizes the reordering using the REINFORCE algorithm to maximize rewards generated by constrained decoding.

Multi-hop retrieval. In multi-hop retrieval tasks, which require iterative document retrievals to gather adequate evidence for answering a query, GMR [131] proposed to employ language model memory and multi-hop memory to train a generative retrieval model, enabling it to memorize the target corpus and simulate real retrieval scenarios through constructing pseudo multi-hop query data, achieving dynamic stopping and efficient performance in multi-hop retrieval tasks.

Code Retrieval. CodeDSI [203] is an end-to-end generative code search method that directly maps queries to pre-stored code samples’ DocIDs instead of generating new code. Similar to DSI [281], it includes indexing and retrieval stages, learning to map code samples and real queries to their respective DocIDs. CodeDSI explores different DocID representation strategies, including direct and clustered representation, as well as numerical and character representations.

Conversational Question Answering. GCoQA [158] is a generative retrieval method for conversational QA systems that directly generates DocIDs for passage retrieval. This method focuses on key information in the dialogue context at each decoding step, achieving more precise and efficient passage retrieval and answer generation, thereby improving retrieval performance and overall system efficiency.

#### 3.4.2 Joint Training
The methods in the previous section involve separately training generative retrievers and downstream task generators. However, due to the inherent nature of GR models as generative models, a natural advantage lies in their ability to be jointly trained with downstream generators to obtain a unified model for retrieval and generation tasks.

Multi-decoder Structure. UniGen [155] proposes a unified generation framework to integrate retrieval and question answering tasks, bridging the gap between query input and generation targets using connectors generated by large language models. UniGen employs shared encoders and task-specific decoders for retrieval and question answering, introducing iterative enhancement strategies to continuously improve the performance of both tasks.

Multi-task Training. CorpusLM [152] introduces a unified language model that integrates GR, closed-book generation, and retrieval-augmented generation to handle various knowledge-intensive tasks. The model adopts a multi-task learning approach and introduces ranking-guided DocID decoding strategies and continuous generation strategies to improve retrieval and generation performance. In addition, CorpusLM designs a series of auxiliary DocID understanding tasks to deepen the model’s understanding of DocID semantics.

#### 3.4.3 Multi-modal Generative Retrieval
Generative retrieval methods can also leverage multi-modal data such as text, images, etc., to achieve end-to-end multi-modal retrieval.

Tokenizing Images to DocID Sequences. IRGen [357] transforms image retrieval problems into generative problems, predicting relevant discrete visual tokens, i.e., image identifiers, through a seq2seq model given a query image. IRGen proposed a semantic image tokenizer, which converts global image features into short sequences capturing high-level semantic information.

Advanced Model Training and Structure. GeMKR [178] combines LLMs’ generation capabilities with visual-text features, designing a generative knowledge retrieval framework. It first guides multi-granularity visual learning using object-aware prefix tuning techniques to align visual features with LLMs’ text feature space, achieving cross-modal interaction. GeMKR then employs a two-step retrieval process: generating knowledge clues closely related to the query and then retrieving corresponding documents based on these clues. GRACE [178] achieves generative cross-modal retrieval method by assigning unique identifier strings to images and training multi-modal large language models (MLLMs) [7] to memorize the association between images and their identifiers. The training process includes (1) learning to memorize images and their corresponding identifiers, and (2) learning to generate the target image identifiers from textual queries. GRACE explores various types of image identifiers, including strings, numbers, semantic and atomic identifiers, to adapt to different memory and retrieval requirements.

#### 3.4.4 Generative Recommender Systems
Recommendation systems, as an integral part of the information retrieval, are currently undergoing a paradigm shift from discriminative models to generative models. Generative recommendation systems do not require the computation of ranking scores for each item followed by database indexing, but instead accomplish item recommendations through the direct generation of IDs. Several works, including P5 [74], GPT4Rec [146], TIGER [233], SEATER [254], IDGenRec [273], LC-Rec [360] and ColaRec [309], outline the development trends in generative recommendations.

P5 [74] transforms various recommendation tasks into different natural language sequences, designing a universal, shared framework for recommendation completion. TIGER [233] initially learns a residual quantized autoencoder to generate semantically informative indexing identifiers for different items. LC-Rec [360] engages in a series of alignment tasks, including sequential item prediction, explicit index-language alignment, and recommendation-oriented implicit alignment. IDGenRec [273] combines generative recommendation systems with large language models by using human language tokens to generate unique, concise, semantically rich and platform-agnostic textual identifiers for recommended items. SEATER [254] designs a balanced k-ary tree-structured indexes, using a constrained k-means clustering method to recursively cluster vectors encoded from item texts, obtaining equal-length identifiers. ColaRec [309] integrates collaborative filtering signals and content information by deriving generative item identifiers from a pretrained recommendation model and representing users via aggregated item content.

## 4 RELIABLE RESPONSE GENERATION: DIRECT INFORMATION ACCESSING WITH GENERATIVE LANGUAGE MODELS
The rapid advancement of large language models has positioned them as a novel form of IR system, capable of generating reliable responses directly aligned with users’ informational needs. This not only saves the time users would otherwise spend on collecting and integrating information but also provides personalized, user-centric answers tailored to individual users.

However, challenges remain in creating a grounded system that delivers faithful answers, such as hallucination, prolonged inference time, and high operational costs. This section outlines strategies for constructing a faithful GenIR system by: (1) Optimizing the GenIR model internally, (2) Enhancing the model with external knowledge, (3) Increasing accountability, and (4) Developing personalized information assistants.

### 4.1 Internal Knowledge Memorization
To develop a user-friendly and reliable IR system, the generative model should be equipped with comprehensive internal knowledge. Optimization of the backbone generative model can be categorized into three aspects: structural enhancements, training strategies, and inference techniques.

#### 4.1.1 Model Structure
With the advent of generative models, various methods have been introduced to improve model structure and enhance generative reliability.

(1) Model Scaling. Model parameter scaling is a pivotal factor influencing performance. Contemporary language models predominantly employ the Transformer architecture, and scaling both the model parameters and the training data enhances the model’s capacity to retain knowledge and capabilities [116]. Larger models tend to perform better on diverse downstream tasks, including few-shot learning, language understanding, and generation [34]. Additionally, scaling the model contributes to improved instruction-following capabilities [227].

(2) Model Integration. Model integration is an effective method to enhance the reliability of generated outputs by capitalizing on the diverse strengths of various models. The predominant approach is the Mixture of Experts (MoE) [96], which utilizes a gating mechanism to selectively activate sections of network parameters during inference, greatly increasing the effective parameters without inflating inference costs [58, 61, 106, 136]. Alternatively, the LLM-Blender framework [107] employs a ranker and a fuser to combine answers from various LLMs, including black-box models.

#### 4.1.2 Training and Inference
In the model training stage, methods to enhance the reliability of answers can be categorized into two aspects: training data optimization and training methods optimization.

(1) Training Data Optimization. The quality of training data substantially affects the reliability of model outputs. Noise, misinformation, and incomplete information can disrupt the learning process, leading to hallucinations and other issues. To address this, [79] used GPT-3.5 to artificially create textbooks filled with examples and language descriptions as training data, resulting in significant improvements on downstream tasks after minor fine-tuning. LIMA [363] used dialogues from community forums to construct a small-scale fine-tuning dataset, enhancing the model’s conversation capabilities during the alignment phase. To reduce redundancies in crawled internet data, Lee et al. [132] combined suffix arrays [188] and MinHash [15] to approximate matching and deduplicate the training dataset, reducing direct reproduction from the same source.

(2) Training Methods Optimization. Beyond conventional training methods, additional techniques have been proposed to improve the factuality of model outputs. MixCL [264] incorporates contrastive learning into the training objective, using an external knowledge base to identify correct snippets and reduce the probability of generating incorrect tokens, thus enhancing model reliability. CaliNet [56] utilizes a contrastive method to assess erroneous knowledge learned by the model and fine-tunes the parameters of the FFN layer to rectify these errors. FactTune [211] incorporates factuality assessment during the RLHF phase, using automatic evaluation methods like FactScore [198] to rank outputs and employing DPO [230] to teach the model factuality preference.

(3) Prompt Engineering. Prompting methods play a vital role in guiding the model. A well-designed prompt can better promote the model’s internal capabilities to provide more accurate answers. The Chain-of-Thought (CoT) [313] prompting method guides the model to explicitly decompose the question into a reasoning chain during decoding, improving response accuracy by grounding the final answer on accurate intermediate steps. Further, CoT-SC [306] samples multiple answers and chooses the most consistent one as the final answer. The Tree of Thoughts [332] expands CoT’s single reasoning path to multiple paths, synthesizing their outcomes to arrive at the final answer. The Chain-of-Verification (CoVE) [49] introduces a self-reflection mechanism where the LLM generates a draft response, then validates each statement for factual inaccuracies, correcting errors to enhance factual accuracy. Additionally, methods like RECITE [268] and GenRead [339] prompt the model to output relevant internal knowledge fragments, which are then used to bolster the question-answering process.

(4) Decoding Strategy. Decoding strategies are another critical factor influencing the reliability of model-generated responses. Nucleus Sampling [133] samples within a set probability range for tokens, ensuring better diversity while balancing variety and reliability. Factual-Nucleus Sampling [134] employs a dynamic, decaying threshold for token sampling, ensuring later tokens are not influenced by earlier less factual tokens. Wan et al. [292] proposed a faithfulness-aware decoding method to enhance the faithfulness of the beam-search approach by incorporating a Ranker to reorder generated sequences and a lookahead method to avoid unfaithful tokens.

Apart from directly modifying the decoding method, several studies influence the decoding distribution by leveraging hidden layer information. DoLa [37] uses distributional differences between hidden and output layers to prioritize newly learned factual knowledge or key terms, increasing their generation likelihood. Inference-Time Intervention (ITI) [147] identifies attention heads strongly correlated with response correctness, adjusts their orientations, and moderates their activation, achieving more truthful generation with minimal model interference. Shi et al. [251] proposed CAD, comparing output distributions before and after adding extra information, reducing reliance on the model’s own knowledge to avoid conflicts leading to inaccuracies.

#### 4.1.3 Knowledge Updating
In real-life scenarios, information is constantly evolving, and therefore, the GenIR system needs to continuously acquire the latest knowledge to meet users’ information needs. Knowledge updating methods include incremental learning and knowledge editing.

(1) Incremental Learning. Incremental learning refers to the ability of machine learning models to continuously learn new skills and tasks while retaining previously acquired knowledge. In the GenIR system, it is crucial to enable the language model to memorize the latest information while preventing the forgetting of previous knowledge. One approach is Incremental Pre-training, which does not rely on supervised data but continues pre-training on continuously updated corpora to alleviate catastrophic forgetting. For example, ERNIE 2.0 [267] enhances language understanding through continuous multi-task learning. Ke et al. [119] proposed Domain Adaptive Pre-training (DAP-training) to improve the model’s adaptability to new domains while preventing forgetting using techniques like soft masking and contrastive learning. Incremental Fine-tuning utilizes only labeled data for training. Progressive Prompts [236] appends new soft prompts for each new task, facilitating knowledge transfer and reducing forgetting. DynaInst [201] enhances lifelong learning in pre-trained language models through parameter regularization and experience replay, employing dynamic instance and task selection for efficient learning under resource constraints.

(2) Knowledge Editing. Knowledge editing refers to the process of modifying and updating existing knowledge within language models, distinct from incremental learning. There are primarily three paradigms for internal knowledge editing within language models: adding trainable parameters, locate-then-edit, and meta-learning. Adding trainable parameters can be achieved by integrating new single neurons (patches) in the final feed-forward neural network (FFN) layer, as in T-Patcher [94] and CaliNet [56]. Locate-then-Edit methods first identify the parameters corresponding to specific knowledge and then update these targeted parameters directly. Techniques like ROME [193] use causal mediation analysis to pinpoint areas needing editing, and MEMIT [194] builds on ROME to implement synchronized editing in various scenarios. Meta-Learning uses hyper-networks to generate the necessary updates for model editing. KE (Knowledge Editor) [17] predicts weight updates for each data point using a hyper-network. MEND [199], by taking low-order decomposition of gradients as input, learns to rapidly edit language models to enhance performance.

### 4.2 External Knowledge Augmentation
Although large language models have demonstrated significant effectiveness in response generation, issues such as susceptibility to hallucinations, difficulty handling in-domain knowledge, and challenges with knowledge updating persist. Augmenting the model’s generative process with external knowledge sources can serve as an effective way to tackle these issues. Based on the form of external knowledge employed, these approaches can be classified into retrieval augmentation and tool augmentation.

#### 4.2.1 Retrieval Augmentation
Retrieval-Augmented Generation (RAG) enhances the response of generative models by combining them with a retrieval mechanism [95, 139, 368]. By querying a large collection of documents, information that is relevant to the input query can be fetched and integrated into the input of the generative model. RAG enables generative models to be grounded in existing reliable knowledge, significantly improving the reliability of model generation. Based on the interaction flow between retriever and generator, RAG methods can be divided into four categories.

(1) Sequential RAG. Sequential RAG operates on a linear progression, where the retriever first retrieves relevant information and the generator utilizes this information to directly complete the response generation process. The basic form is a “Retrieve-Read” framework [183], where early works perform joint [14, 81, 139] or separate [95] training of retriever and generator but require costly pre-training. In-Context RALM [234] addresses this by directly using retrieved documents as input, leveraging the model’s in-context learning without additional training. With frozen generators, many works fine-tune retrievers to adapt to the preferences of the generative model, e.g., AAR [340], LLM-embedder [353], and ARL2 [350]. Pre-retrieval and post-retrieval processes have also been introduced to enhance efficiency and relevance, including query rewriting and information compressors [43, 114, 170, 325].

(2) Branching RAG. In the Branching RAG framework, the input query is processed across multiple pipelines, and each pipeline may involve the entire process in the sequential pipeline. The outputs from all pipelines are merged to form the final response, allowing for finer-grained handling of the query or retrieval results. Examples include TOC [123], BlendFilter [297], and REPLUG [252].

(3) Conditional RAG. The Conditional RAG framework adapts to various query types through distinct processes, improving the system’s flexibility. A decision-making module determines whether to engage the retrieval process for each query. SKR [308], Self-DC [296], and Rowen [52] are examples that decide when retrieval is necessary or beneficial.

(4) Loop RAG. Loop RAG involves deep interactions between the retriever and generator components, with multi-turn retrieval and generation processes. ITER-RETGEN [248], IR-COT [287], FLARE [111], COG [128], and Self-RAG [5] are examples that iteratively refine retrieved documents and generated outputs, allowing for improved handling of complex and multi-step queries.

#### 4.2.2 Tool Augmentation
Tool augmentation helps models invoke various tools that allow for timely acquisition and usage of the latest data, including finance, news, and more. Tool augmentation expands responses capabilities, such as language translation, image generation, and other tasks.

(1) Search Engine. Search engines like Google and Bing help answer frequent and time-sensitive queries. Systems such as Self-Ask [221], ReAct [333], Internet-Augmented Generation [125], LaMDA [282], and WebGPT [204] integrate search in reasoning or conversation workflows.

(2) Knowledge Graph (KG). KGs are useful for extracting structured, explicit knowledge. StructGPT [110], RoG [181], and ToG [262] are examples that allow models to query and reason over KGs.

(3) API-based Tools. APIs enable models to obtain information from specific data sources. Toolformer [245] trains models to call APIs, RestGPT [258] formulates a framework for invoking RESTful APIs, ToolLLM [226] builds datasets for fine-tuning on API usage, and Gorilla [214] provides reference API documentation for fine-tuning.

(4) Model-based Tools. With accessible AI models from communities like HuggingFace, models can act as tools. HuggingGPT [250] uses ChatGPT to orchestrate available models, and Visual ChatGPT [317] integrates visual foundation models to handle image-based requests efficiently.

### 4.3 Generating Response with Citation
Generating responses with citations is a promising approach for building a reliable GenIR system [88, 168, 195]. Citations allow users to understand the source of each piece of knowledge in the response, enhancing trust and facilitating verification. Existing methods can be divided into direct generation of responses with citations and retrieval-based approaches that fetch relevant documents to support citations.

#### 4.3.1 Direct Generating Response with Citation
This method uses the model’s intrinsic memory to generate source citations without relying on a retrieval module.

(1) Model Intrinsic Knowledge. According-to prompting [314] guides LLMs to more accurately cite information from pre-training data by adding phrases like “according to Wikipedia” in the prompts. Iterative Feedback Learning (IFL) [129] employs a critique model to assess and provide feedback on generated text, iteratively enhancing LLMs’ citation accuracy, content correctness, and fluency. Fierro et al. [63] introduce a plan-based approach using a series of questions as a blueprint for content generation, with abstract and extractive attribution models, showing that planning improves citation quality.

(2) Incorporating Generative Retrieval. Envisioned by Metzler et al. [195], allowing the model to directly generate responses with citations by generating DocIDs is promising. 1-PAGER [97] combines answer generation and evidence retrieval by generating N-gram DocIDs through constrained decoding using FM-Index [62], enabling step-by-step corpus partitioning, document selection, and response generation. Source-aware training [122] enables models to associate DocIDs with knowledge during pre-training and provide supporting citations during instruction tuning, enhancing verifiability.

#### 4.3.2 Retrieval-based Response with Citation
To enhance the accuracy of citations, several methods use retrieval techniques to fetch relevant documents and improve the quality of responses with embedded citations.

(1) Citation within Generation. Following retrieval, models directly generate responses that include citations. Systems like WebGPT [204], LaMDA [282], and WebBrain [223] taught models to generate responses with citations using web pages or Wikipedia. More advanced strategies include Search-in-the-Chain (SearChain) [326], LLatrieval [156], AGREE [335], VTG [261], HGOT [60], and reinforcement learning approaches [87] that fine-tune models to generate accurate citations.

(2) Citation after Generation. Models first generate a response, then add citations through verification models like NLI. RARR [70] improves attributability by automatically finding external evidence for the language model’s output and post-editing to correct content while preserving the original output. PURR [24] employs an unsupervised learning method where LLMs generate text noise themselves, then trains an editor to eliminate this noise, improving attribution performance. CEG [150] searches for supporting documents related to generated content and uses an NLI-based citation generation module to ensure each statement is supported. "Attribute First, then Generate" [256] decomposes the generation process, first selecting relevant source text details and then generating based on these details, achieving localized attributability with each sentence supported by a clear source.

### 4.4 Personal Information Assistant
The core of the GenIR system is the user, so understanding user intent is crucial. Personalized information assistants aim to better understand users’ personalities and preferences, generating personalized responses to better meet their information needs. This section reviews progress in personalized dialogue and domain-specific personalization.

#### 4.4.1 Personalized Dialogue System
Researchers have explored two main approaches: personalized prompt design and model fine-tuning.

(1) Personalized Prompt. Methods feed users’ interaction and rating history into LLMs for in-context learning, e.g., Liu et al. [169], Dai et al. [46]. LaMP [240] enhances personalized output by retrieving personalized history from user profiles. BookGPT [361] uses LLM prompts and interactive querying methods for personalized book recommendations. Prompt rewriting and supervised plus reinforcement learning methods have also been proposed to improve personalized generation.

(2) Personalized Fine-tuning. Large-scale persona datasets and fine-tuning have been used to train personalized dialogue models. Zhang et al. [354] introduced Persona-Chat dataset. P2Bot [172] generates personalized and consistent dialogues by simulating perception of personalities. DHAP [184] learns implicit user profiles from dialogue history without explicit personal information. Retrieval-enhanced methods like LAPDOG [91] and SAFARI [295] retrieve relevant information to enhance personal profiles and response generation. OPPU [274] uses personalized PEFT [53] to store user-specific behavioral patterns and preferences.

#### 4.4.2 Domain-specific Personalization
GenIR systems have broad applications across healthcare, academia, education, and other domains.

(1) Healthcare. Few-shot tuning and prompting methods have been used to process time-series physiological and behavioral data [175], medical diagnosis [347], and build domain-specific LLMs like Zhongjing [11]. Open-source conversational health agent frameworks and multidisciplinary agent collaborations have been explored.

(2) Academic. RevGAN [149] can generate controllable and personalized user reviews. Porsdam et al. [219] explore personalized enhancement of academic writing using LLMs trained on authors’ published works. PEARL [202] personalizes writing assistants using users’ historical documents.

(3) Education. Adaptive and personalized exercise generation methods and education-specific fine-tuning approaches (e.g., EduChat [48]) address knowledge update and expertise limitations in LLMs.

(4) Other Domains. Personalized recipe generation, headline generation, and robotic assistance are among other application areas explored using personalization techniques.

## 5 EVALUATION
This section provides evaluation metrics and benchmarks for generative information retrieval methods, along with analysis and discussions on their performance.

### 5.1 Evaluation for Generative Document Retrieval

#### 5.1.1 Metrics
Core metrics for evaluating Generative Retrieval (GR) methods include Recall, R-Precision, Mean Reciprocal Rank (MRR), Mean Average Precision (MAP), and Normalized Discounted Cumulative Gain (nDCG). Each metric captures unique aspects of retrieval performance.

- Recall measures the proportion of relevant documents retrieved by the search system.
- R-Precision evaluates the precision at a rank position corresponding to the number of relevant documents.
- Mean Reciprocal Rank (MRR) captures the average rank position of the first relevant document.
- Mean Average Precision (MAP) calculates the average precision across multiple queries, considering positions of all relevant documents.
- Normalized Discounted Cumulative Gain (nDCG) takes into account relevance and positions in the result list.

#### 5.1.2 Benchmarks
Evaluating GR methods relies on benchmark datasets such as MS MARCO [205], Natural Questions (NQ) [126], KILT [218], TREC Deep Learning Track 2019 & 2020 [41, 42], DynamicIR [337], and ExcluIR [356]. MS MARCO contains millions of documents and passages from real user queries. NQ uses Wikipedia as its corpus. KILT integrates multiple knowledge-intensive tasks using Wikipedia. DynamicIR addresses dynamic corpora. ExcluIR focuses on exclusionary retrieval tasks.

#### 5.1.3 Analysis
Several works analyze GR models’ behavior. Chen et al. [29] examined DSI [281] for uniqueness, completeness, and relevance ordering, finding strengths in memorization of query-to-DocID mappings but weaknesses in distinguishing relevant documents from random ones. Connections between generative and dense retrieval have been analyzed, showing common frameworks in computing document-query relevance [206, 319]. Pradeep et al. [220] conducted large-scale experiments on GR techniques over millions of passages, finding pseudo query augmentation effective and noting that scaling model size does not always improve GR performance. Liu et al. [176] investigated out-of-distribution robustness, showing sensitivity to query variations and the need for improved OOD robustness.

#### 5.1.4 Experiments
Empirical evaluations compare GR methods with sparse and dense retrieval baselines on benchmarks such as MS MARCO, NQ, and KILT. GR methods demonstrate competitive performance on smaller corpora (e.g., under 300K documents) but face scalability challenges for million-level corpora. Different DocID strategies (term set, multi-view, learnable IDs) show varied strengths. Title-based DocIDs work well on curated corpora like Wikipedia (KILT) but underperform in large-scale web corpora with noisy or duplicate titles (MS MARCO). Sub-string DocID methods and multi-view approaches often yield improvements on many tasks. Overall, experiments highlight the promise of GR methods while pointing out limitations in scalability and generalization.

### 5.2 Evaluation for Response Generation

#### 5.2.1 Metrics
Evaluating generated responses involves rule-based, model-based, and human evaluation metrics.

(1) Rule-based Metrics. Exact Match (EM), BLEU [213], METEOR [9], ROUGE [162], and Perplexity (PPL) are commonly used. PPL is defined as:
PPL(W) = exp(−(1/N) ∑_{i=1}^{N} log p(w_i | w_{<i})).

(2) Model-based Metrics. BERTScore [355], BLEURT [247], BARTScore [343], FActScore [198], and GPTScore [66] capture semantic similarity, factuality, and other nuanced aspects via neural scoring methods.

(3) Human Evaluation Metrics. Human evaluation assesses accuracy [255], relevance [362], fluency [289], and safety [103]. Human evaluation is costly and subjective but remains crucial for complex assessment.

#### 5.2.2 Benchmarks and Analysis
Benchmarks for evaluating general language capabilities include MMLU [84], BIG-bench [259], LLM-Eval [166], and C-Eval [92] for Chinese. Tool usage benchmarks include API-Bank [148] and ToolBench [226]. Factuality evaluation uses TruthfulQA [164], HaluEval [144], HalluQA [33], and ALCE [71]. Real-time evaluation datasets include RealTime QA [118] and FreshQA [291]. Safety and trustworthiness are evaluated by SafetyBench [358], TrustGPT [93], and TrustLLM [263]. These benchmarks reveal strengths and limitations of LLMs in factuality, timeliness, tool usage, ethics, and trustworthiness, guiding future research.

## 6 CHALLENGES AND PROSPECTS
This section discusses key challenges in generative document retrieval and reliable response generation, and potential research directions.

### 6.1 Challenges on Generative Document Retrieval

#### 6.1.1 Scalability Issues
Generative retrieval shows lower retrieval accuracy compared to dense retrieval when handling million-level document corpora. Simply increasing model size does not yield stable improvements. Challenges include training data mismatch (models not pre-trained on DocID-like targets), training methods, and model structure choice. A potential solution is large-scale pre-training tailored for generative retrieval with varied DocID types and instruction signals.

#### 6.1.2 Handling Dynamic Corpora
Real-world corpora are dynamic; incremental learning is essential. Existing methods include DSI++ [192], IncDSI [124], CLEVER [25], and CorpusBrain++ [80], but more universally applicable incremental learning strategies are needed.

#### 6.1.3 Document Identifier
Constructing high-quality DocIDs is crucial. While title-based DocIDs work well on Wikipedia, they fail on noisy web corpora. Research is needed on generating concise, representative DocIDs for general corpora. Choices between text or numeric DocIDs have trade-offs: text-based IDs leverage pre-trained language priors and interpretability, numeric IDs can leverage dense embedding techniques. Exploring alternatives such as sub-string DocIDs and efficient matching mechanisms is also valuable.

#### 6.1.4 Efficiency Concerns
Constrained beam search for generating multiple DocID sequences leads to high latency, especially when returning many documents. Designing concise DocIDs (e.g., ≤16 tokens) and more efficient decoding strategies is crucial.

#### 6.1.5 Multi-modal Generative Retrieval
Current multi-modal generative retrieval largely focuses on text and images. Challenges include image representation for DocIDs, end-to-end training of image encoding and identifier prediction, and extending to modalities like audio and video.

### 6.2 Challenges on Reliable Response Generation

#### 6.2.1 Improving Accuracy and Factuality
Improvements can target internal knowledge memorization (model scaling, novel architectures, training data and methods) and external knowledge enhancement (better ways to incorporate retrieved evidence, deciding when to retrieve, and integrating tools). Tool-augmented generation raises questions about cost-effectiveness versus performance gains.

#### 6.2.2 Real-time Properties of GenIR Systems
Timeliness is critical. Retrieval and tool augmentation help access new knowledge, but effective methods for real-time knowledge acquisition and knowledge updating (continual learning, knowledge editing) remain limited.

#### 6.2.3 Bias and Fairness
LLMs trained on large unfiltered datasets can propagate biases. Understanding mechanisms causing bias and designing mitigation strategies across training, generation, and rewriting phases is essential.

#### 6.2.4 Privacy and Security
GenIR systems risk plagiarism and unintended memorization of private training data. Research on reducing plagiarism, generating correct citations, defending against data leakage attacks, and detecting LLM-generated content is critical.

### 6.3 Unified Framework

#### 6.3.1 Unified Framework for Retrieval and Generation
Integrating generative retrieval and reliable response generation into a single model is a promising direction. Attempts like UniGen [155] and CorpusLM [152] show potential but face generalization challenges. Future work could aim to build a large search model (LSM) that enables an LLM to generate DocIDs, decide when to retrieve, and generate reliable responses in an integrated manner.

#### 6.3.2 Towards End-to-End Framework for Various IR Tasks
An expert-level corpus model understanding DocIDs and knowledge sources could generate texts with references pointing to source documents and perform diverse IR tasks, including generating main content given a DocID, finding related DocIDs, and supporting multi-lingual and multi-modal retrieval. Constructing high-quality training data aligning knowledge and DocIDs is key to training such end-to-end GenIR models.

## 7 CONCLUSION
In this survey, we explore the latest research developments, evaluations, current challenges, and future directions in generative information retrieval (GenIR). We discuss two main directions in the GenIR field: generative document retrieval (GR) and reliable response generation. Specifically, we systematically review the progress of GR covering model training, document identifier design, incremental learning, adaptability to downstream tasks, multi-modal GR, and generative recommendation systems; as well as advancements in reliable response generation in terms of internal knowledge memorization, external knowledge enhancement, generating responses with citations, and personal information assistance. Additionally, we review the existing evaluation methods and benchmarks for GR and response generation. We organize the current limitations and future directions of GR systems, addressing scalability, handling dynamic corpora, document representation, and efficiency challenges. Furthermore, we identify challenges in reliable response generation, such as accuracy, real-time capabilities, bias and fairness, privacy, and security. We propose potential solutions and future research directions to tackle these challenges. Finally, we also envision a unified framework, including unified retrieval and generation tasks, and even building an end-to-end framework capable of handling various information retrieval tasks. Through this review, we hope to provide a comprehensive reference for researchers in the GenIR field to further promote the development of this area.