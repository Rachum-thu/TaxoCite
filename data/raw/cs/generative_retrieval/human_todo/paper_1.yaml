title: 'CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks'
blocks:
- block_id: 0
  content: Knowledge-intensive language tasks (KILT) usually require a large body of information to provide correct answers.
    A popular paradigm to solve this problem is to combine a search system with a machine reader, where the former retrieves
    supporting evidences and the latter examines them to produce answers. Recently, the reader component has witnessed significant
    advances with the help of large-scale pre-trained generative models. Meanwhile most existing solutions in the search component
    rely on the traditional “index-retrieve-then-rank” pipeline, which suffers from large memory footprint and difficulty
    in end-to-end optimization. Inspired by recent efforts in constructing model-based IR models, we propose to replace the
    traditional multi-step search pipeline with a novel single-step generative model, which can dramatically simplify the
    search process and be optimized in an end-to-end manner. We show that a strong generative retrieval model can be learned
    with a set of adequately designed pre-training tasks, and be adopted to improve a variety of downstream KILT tasks with
    further fine-tuning. We name the pre-trained generative retrieval model as CorpusBrain as all information about the corpus
    is encoded in its parameters without the need of constructing additional index. Empirical results show that CorpusBrain
    can significantly outperform strong baselines for the retrieval task on the KILT benchmark and establish new state-of-the-art
    downstream performances. We also show that CorpusBrain works well under zero- and low-resource settings.
  citations: []
- block_id: 1
  content: 'Knowledge-intensive language tasks (KILT) such as fact checking [45] and open-domain question answering [48],
    have received much attention in recent years. Compared with traditional information processing tasks, they are usually
    more complex and require to surface knowledge from a large body of information. A popular paradigm to approach these tasks
    is to combine a search system with a machine reader [38]. The former retrieves a limited subset of supporting evidences
    from large corpora, and the latter then examines the retrieved information to produce final answers. Recently, the reader
    component has witnessed significant advances with the help of the large-scale pre-trained generative models (e.g., BART
    [28] and T5 [28]), and such models have become the de-facto implementation of the reader. However, the search component
    has yet to benefit from these tremendous advances in generative models.


    Most existing solutions in the search component follow the paradigm of “index-retrieve-then-rank” [7, 13, 21, 49, 51]
    which includes three sequential steps: (1) building an index for each document in the corpus; (2) retrieving an initial
    set of candidate documents for a query; and (3) determining the relevance degree of each candidate. Despite its wide usage,
    this paradigm has clear limitations. At the training stage, heterogeneous ranking components are usually difficult to
    be optimized in an end-to-end way towards the global objective. At the inference stage, a large document index is needed
    to search over the corpus, leading to significant memory consumption and computational overhead. Besides, errors would
    accumulate and propagate among the sequential components.


    Recently, Metzler et al. [37] envisioned a fundamentally different paradigm called model-based IR, to replace the long-standing
    “index-retrieve-then-rank” paradigm. Specifically, with model-based IR, the indexing, retrieval, and ranking components
    of traditional IR systems are collapsed into a single consolidated model. This enables us to mitigate the aforementioned
    technical issues. Firstly, the knowledge of all documents in the corpus is encoded into the model parameters, which can
    be optimized directly in an end-to-end manner. Secondly, the memory and computational cost is greatly reduced because
    the document index is eliminated. Documents are returned using model parameters only, which can dramatically simplify
    the heavy search process. Inspired by this blueprint, researchers [6, 9, 44] have proposed to generate identifier strings
    for documents via generative language models. A typical way is to directly fine-tune the off-the-shelf pre-trained generative
    models (e.g., BART [28]) for specific KILT task, e.g., entity linking [9] and fact checking [6]. Unfortunately, these
    methods depend on large-scale application-specific supervision, which are often not available for many real-world applications.


    Therefore, in this paper, we propose to pre-train a novel single-step generative model, called CorpusBrain, to encode
    all information of the corpus within its parameters in a general way. In this work, we assume that the pre-training data
    is defined as positive pairs of query and document identifier. To resemble the relevance relationship between query and
    document in downstream KILT tasks, the pre-training task should be designed to meet the following requirements: (R1) It
    should capture different granularities of semantics between the query and document. For example, the input queries of
    different KILT tasks range from sentence-level [12, 20, 24, 45, 48] to paragraph-level [10, 17, 18]. (R2) It should be
    flexible to predict dynamic numbers of relevant documents for different queries. For example, the question answering usually
    requires multiple documents to support the answer, while the slot filling task [11, 27] usually needs one. (R3) It should
    capture inter-document semantic relation since multiple documents may share similar characteristics with respect to a
    query.


    In light of the above requirements, we carefully devise three pre-training tasks to capture the query-document relevance
    in different views, namely Inner Sentence Selection (ISS), Lead Paragraph Selection (LPS), and Hyperlink Identifier Prediction
    (HIP). The key idea of ISS and LPS is to sample sentences or paragraphs from documents and adopt them as pseudo queries,
    while that of HIP is to utilize hyperlinks and anchor texts to approximate the relationship between two documents. Based
    on the three proposed tasks, we can build large-scale pseudo pairs of query and document identifiers without additional
    human supervision. Then, initialized with BART [28], we pre-train our model via a standard seq2seq objective, i.e., maximizing
    the likelihood of the output sequence with teacher forcing [43]. Once such a strong generative retrieval model is learned,
    we expect it can be seamlessly adapted to a wide variety of downstream KILT tasks without the need of additional index.


    We pre-train CorpusBrain on English Wikipedia, which contains tens of millions of well-formed Wiki articles. We fine-tune
    CorpusBrain on the comprehensive KILT benchmark [38] which consists of eleven datasets spanning five distinct KILT tasks.
    Empirical experimental results demonstrated that CorpusBrain can achieve significant improvements over strong baseline
    solutions for the retrieval task and push forward the SOTA performance on a number of downstream tasks. We simulate both
    zero- or low-resource settings and show that CorpusBrain works well even when they were fine-tuned with very little supervision.'
  citations:
  - marker: '[6]'
    intent_label: Prior Methods
    topic_label: 'Document identifiers: text-based'
  - marker: '[7]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[9]'
    intent_label: Prior Methods
    topic_label: 'Document identifiers: text-based'
  - marker: '[10]'
    intent_label: Domain Overview
    topic_label: Downstream task adaptation
  - marker: '[11]'
    intent_label: Domain Overview
    topic_label: Downstream task adaptation
  - marker: '[12]'
    intent_label: Domain Overview
    topic_label: Downstream task adaptation
  - marker: '[13]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[17]'
    intent_label: Domain Overview
    topic_label: Downstream task adaptation
  - marker: '[18]'
    intent_label: Domain Overview
    topic_label: Downstream task adaptation
  - marker: '[20]'
    intent_label: Domain Overview
    topic_label: Downstream task adaptation
  - marker: '[21]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[24]'
    intent_label: Domain Overview
    topic_label: Downstream task adaptation
  - marker: '[27]'
    intent_label: Domain Overview
    topic_label: Downstream task adaptation
  - marker: '[28]'
    intent_label: Model/Architecture Adoption
    topic_label: Model structure enhancements
  - marker: '[37]'
    intent_label: Prior Methods
    topic_label: Unified retrieval-generation frameworks
  - marker: '[38]'
    intent_label: Benchmark Utilization
    topic_label: Evaluation of generative document retrieval
  - marker: '[43]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Training and inference optimization
  - marker: '[44]'
    intent_label: Prior Methods
    topic_label: 'Document identifiers: text-based'
  - marker: '[45]'
    intent_label: Domain Overview
    topic_label: Downstream task adaptation
  - marker: '[48]'
    intent_label: Domain Overview
    topic_label: Downstream task adaptation
  - marker: '[49]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[51]'
    intent_label: Prior Methods
    topic_label: Other Topics
- block_id: 2
  content: In this section, we briefly review three lines of the related works, including traditional pipeline IR framework,
    model-based IR approaches and knowledge-intensive language tasks.
  citations: []
- block_id: 3
  content: 'Most existing IR methods follow a common three-step pipeline framework, i.e., “index-retrieve-then-rank”. For
    the indexing and retrieval stage, existing approaches can be divided into two categories [15] from the view of representation
    type and index mode, including sparse retrieval and dense retrieval models. For sparse retrieval, they generally build
    the inverted index based on the corpus, which encode term-based features like term frequencies and term position. The
    typical methods in this category are TF-IDF and BM25 [40]. Besides, several works [13, 51] employed word embedding to
    enhance the semantic matching. With the development of pre-training techniques, researchers explore to utilize pre-trained
    models to estimate term weights. For example, DeepCT [7] used BERT [22] to obtain term weights for the inverted index.
    For dense retrieval, they usually project documents into dense representations to build index and turn to approximate
    nearest neighbor search algorithms for fast retrieval. Karpukhin et al. [21] demonstrated dense retrieval models could
    outperform BM25 by utilizing in-batch negatives. Recently, various fine-tuning techniques [19, 23, 32, 47, 49, 49] are
    explored to enhance dense retrieval.


    For the ranking stage, many different ranking models have been proposed, including vector space models [42], probabilistic
    models [41], learning to rank models [3, 30, 31] and neural ranking models [8, 16]. Recently, researcher have shown that
    developed pre-training objectives tailed for IR could further enhance the performance on downstream ranking tasks [4,
    25, 33–35]. For example, Lee et al. [25] introduced Inverse Cloze Task (ICT) for passage retrieval by randomly sampling
    a sentence from passage as pseudo query and taking the rest sentences as the document. Ma et al. [33] presented Representative
    Words Prediction (ROP) task for pre-training, which sampled representation words from the document according to a unigram
    document language model. Ma et al. [35] proposed HARP by leveraging the large-scale hyperlinks and anchor texts to pre-train
    the language model for ad-hoc retrieval. Despite their success, however, such pipeline framework has drawbacks on the
    end-to-end optimization and memory resources.'
  citations:
  - marker: '[3]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[4]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[7]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[8]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[13]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[15]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[16]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[19]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[21]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[22]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[23]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[25]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[30]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[31]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[32]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[33]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[35]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[40]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[41]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[42]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[47]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[49]'
    intent_label: Prior Methods
    topic_label: Other Topics
  - marker: '[51]'
    intent_label: Prior Methods
    topic_label: Other Topics
- block_id: 4
  content: To replace the long-lived “index-retrieve-then-rank” paradigm, the model-based IR paradigm is proposed to collapse
    the indexing, retrieval, and ranking components of traditional IR systems into a single consolidate model [37]. There
    have been some preliminary explorations in model-based IR [2, 6, 9, 44, 52] over the past year. For example, De Cao et
    al. [9] presented to generate entity names in an autoregressive fashion for entity linking task. Chen et al. [6] proposed
    GERE for fact checking task, which generates the document titles as well as evidence sentence identifiers. Tay et al.
    [44] explored different ways to obtain document identifiers, including unstructured atomic identifiers and semantically
    structured identifiers. However, these approaches are generally designed for specific task, resulting low flexibility
    for other tasks. In addition, a large number of annotated data is needed to learn a well behaved model. In this work,
    we propose to pre-train a general-purpose generative model which can serve a wide range of downstream KILT tasks.
  citations:
  - marker: '[2]'
    intent_label: Prior Methods
    topic_label: Generative document retrieval
  - marker: '[6]'
    intent_label: Prior Methods
    topic_label: 'Document identifiers: text-based'
  - marker: '[9]'
    intent_label: Prior Methods
    topic_label: 'Document identifiers: text-based'
  - marker: '[37]'
    intent_label: Prior Methods
    topic_label: Generative document retrieval
  - marker: '[44]'
    intent_label: Prior Methods
    topic_label: 'Document identifiers: numeric-based'
  - marker: '[52]'
    intent_label: Prior Methods
    topic_label: Generative document retrieval
- block_id: 5
  content: 'Knowledge-intensive language tasks (KILT) require access to large and external knowledge sources. For example,
    fact checking requires to find trustworthy evidences to determine the veracity of a claim [45]. Open-domain question answering
    needs to reason over a knowledge source to produce the correct answer for a question [12, 20, 24, 48]. Practical solutions
    to these tasks usually apply a two-step pipeline framework [5, 45]. First, an efficient retrieval component is employed
    to retrieve relevant information from a large knowledge source. Then, dedicated downstream models are adopted to produce
    the final results by capturing the relationship between the input and the context of the retrieved information.


    Up to now, numerous datasets for KILT tasks [10, 24, 45, 48] have been proposed to facilitate the research. Generally,
    these datasets have different formats, and are processed or evaluated with different assumptions. Besides, their knowledge
    sources vary from different versions of Wikipedia to entirely different corpora. To facilitate the task-to-task comparisons,
    a benchmark named KILT [38] was introduced, which consists of eleven datasets spanning five distinct tasks (i.e., fact
    checking, open domain question answering, slot filling, entity linking and dialogue). Crucially, all tasks in KILT are
    formulated into a common interface and grounded in the same snapshot of Wikipedia.'
  citations:
  - marker: '[5]'
    intent_label: Prior Methods
    topic_label: Retrieval augmentation
  - marker: '[10]'
    intent_label: Domain Overview
    topic_label: Downstream task adaptation
  - marker: '[12]'
    intent_label: Domain Overview
    topic_label: Downstream task adaptation
  - marker: '[20]'
    intent_label: Domain Overview
    topic_label: Downstream task adaptation
  - marker: '[24]'
    intent_label: Domain Overview
    topic_label: Downstream task adaptation
  - marker: '[38]'
    intent_label: Domain Overview
    topic_label: Evaluation
  - marker: '[45]'
    intent_label: Prior Methods
    topic_label: Downstream task adaptation
  - marker: '[48]'
    intent_label: Domain Overview
    topic_label: Downstream task adaptation
- block_id: 6
  content: In this section, we present the novel pre-trained generative retrieval model CorpusBrain in detail. We first introduce
    our motivation on the design of our method. We then describe the model architecture as well as the pre-training tasks.
  citations: []
- block_id: 7
  content: 'KILT tasks are usually approached by combining a search component with a reader component [38]. Recently, thanks
    to the popularity of pre-trained models and their strong natural language understanding capabilities, the large-scale
    pre-trained generative models have been the de-facto implementation of the reader component. Unfortunately, these tremendous
    advances in pre-trained generative models has yet to bring similar transformational changes in how the search component
    is approached. Most existing solutions in the search component follow the traditional “index-retrieve-then-rank” paradigm.
    To fully parameterize the traditional pipeline framework, Metzler et al. [37] outlined a high-level vision of the next
    generation of IR systems, called model-based IR, to replace the long-lived pipeline framework with a single consolidated
    model. In this way, we can not only optimize the entire model directly in an end-to-end way, but also consume largely
    reduced memory resources and computation cost.


    Motivated by this blueprint, there have been some preliminary works [6, 9, 44, 52] dedicated to mapping a query to a document
    identifier for the search component. Typically, these solutions directly fine-tune the off-the-shelf pre-trained generative
    models (e.g., BART) on specific downstream KILT tasks. However, they are designed to suit the need of specific tasks.
    It may not be practical to deploy separate specialised models for each application due to considerable memory resources
    or computation overhead. Besides, a large amount of application-specific annotated data is required to learn the model,
    which is unsuitable for low data regimes, e.g., zero and few-shot settings.


    Therefore, in this work, we propose to pre-train a general-purpose generative retrieval model, named CorpusBrain, by encoding
    all information about the corpus in the parameters. Namely, we target a strong generative retrieval model learned with
    several pre-training tasks, that can be used for a diverse range of KILT tasks without the need of additional index. Specifically,
    we carefully design three self-supervised pre-training objectives to satisfy the following three properties: (R1) The
    semantic granularities between the query and document vary greatly in different tasks; (R2) Various tasks usually require
    different numbers of retrieved supporting documents; (R3) Different documents could share similar characteristics with
    the queries. Then, we pre-train an encoder-decoder architecture to generate document identifiers via a standard seq2seq
    objective.'
  citations:
  - marker: '[6]'
    intent_label: Research Gap
    topic_label: Generative document retrieval
  - marker: '[9]'
    intent_label: Research Gap
    topic_label: Generative document retrieval
  - marker: '[37]'
    intent_label: Prior Methods
    topic_label: Unified retrieval-generation frameworks
  - marker: '[38]'
    intent_label: Domain Overview
    topic_label: Retrieval augmentation
  - marker: '[44]'
    intent_label: Research Gap
    topic_label: Generative document retrieval
  - marker: '[52]'
    intent_label: Research Gap
    topic_label: Generative document retrieval
- block_id: 8
  content: 'To tackle the generative retrieval problem, we leverage a Transformer-based encoder-decoder architecture, which
    contains the following two dependent components: (1) Query Encoder, a bidirectional encoder to achieve the query representation;
    (2) Identifier Decoder, a sequential generation process to yield document identifiers.


    #### 3.2.1 Query Encoder

    The query encoder is to map the input query q = {w1, w2, . . . , w|q|} into a compact vector that can capture its essential
    topics. Specifically, the encoder represents the query q as a series of hidden vectors, i.e., Hq = Encoder(w1, w2, . .
    . , w|q|), where Hq denotes the query representation.


    #### 3.2.2 Identifier Decoder

    The decoder is to generate a sequence of document identifiers of the relevant documents for each query. Note the document
    identifier can be defined in different ways, such as the unique title and url of the document. Specifically, the probability
    of generating the n-th token wm,n in the m-th document identifier tm is defined as, p(wm,n | w≤m,<n, q) = Decoder(w≤m,<n,
    Hq). At the inference time, we apply the constrained beam search strategy [9] to limit each generated identifier to be
    in the valid pre-defined candidate set, i.e., the identifiers of all the documents in the given corpus. Concretely, we
    define our constrain in terms of a prefix tree where nodes are annotated with tokens from the predefined candidate set.
    For each node in the tree, its children indicate all the valid continued token list from the prefix defined traversing
    the tree from the root to it.'
  citations:
  - marker: '[9]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Architectural and decoding innovations
- block_id: 9
  content: 'Formally, suppose C = {p0, p1, . . . } denotes a large-scale corpus where pi denotes an individual document. Each
    document pi = {ti, si, ai} consists of a unique document identifier ti, a sequence of sentences si, and an anchor set
    ai. Specifically, si = {s0i, s1i, . . . , sni} contains n sentences where sji denotes the j-th sentence in pi. ai = {a0i,
    a1i, . . . , ami} contains m anchors where aki is the k-th anchor in pi. taa i = {t0ai, t1ai, . . . , tmai} are the document
    identifiers of destination pages linked by each anchor text in ai, where tla i is linked by ali.


    What pre-training tasks are useful for improving Transformer-based encoder-decoder models is a crucial step in large-scale
    generative retrieval. It is generally hypothesized that using a pre-training task that more closely resembles the downstream
    task contributes to better fine-tuning performance [50]. To satisfy three requirements discussed above, we first assume
    that the pre-training data is defined as positive pairs of query and document identifier. Specifically, the document identifier
    can be defined in different ways, such as the unique title or url of the document. Then, we carefully design three pre-training
    tasks, i.e., Inner Sentence Selection (ISS), Lead Paragraph Selection (LPS), and Hyperlink Identifier Prediction (HIP).
    These three tasks target to learn the query-document relevance in different views, to generate pseudo pairs of query and
    document identifier to simulate the downstream retrieval task. The training data for these tasks can be freely obtained
    from the given corpus without an additional manual labeling process. In the following, we will present the proposed three
    pre-training tasks in detail.


    #### 3.3.1 Inner Sentence Selection (ISS)

    Given a document pi, to satisfy the requirement (R1) where the query for many KILT tasks (e.g., fact checking and question
    answering) is in the sentence level, the ISS treats the inner sentence sji randomly drawn from pi as the pseudo query
    q. This task contributes to capturing the semantic context of a sentence. To satisfy the requirement (R2), the ISS treats
    the pi and destination pages linked by o (o < m) anchor texts {asi i, as2 i, . . . , asoi} randomly sampled from ai as
    the relevant documents. ISS uses the concatenated document identifiers as the generation target [ti, tsa1 i, tsa2 i, .
    . . , tsao i]. In this way, it is feasible to achieve dynamic predictions of relevant documents for different downstream
    tasks.


    #### 3.3.2 Lead Paragraph Selection (LPS)

    Different from ISS, LPS samples a paragraph from the document pi and adopts it as the pseudo query q. With such pre-training
    task, we can fit the requirement (R1) where the query of entity linking, dialog and other KILT tasks is in the paragraph
    level. Generally, the top paragraphs can be a surrogate summary of better quality than the remains. Inspired by this,
    we regard the leading l paragraphs as the pseudo queries. To satisfy the requirement (R2), the output [ti, tsa1 i, tsa2
    i, . . . , tsao i] of the LPS is consistent with that of the ISS.


    #### 3.3.3 Hyperlink Identifier Prediction (HIP)

    Based on the classical anchor intuition [35], the hyperlink information could indicate the inter-document semantic relation
    to some extent. To satisfy the requirement (R3), we first view the anchor text as a pseudo query. Unfortunately, the anchor
    texts are usually too short to carry enough semantics. Therefore, we resort to the anchor context, i.e., the surrounding
    contextual information in the anchor’s corresponding sentence. Specifically, given a page pi, we randomly select an anchor
    aki from the anchor set ai and locate the sentence sq containing aki. Based on sq, we get the anchor context by looking
    at its previous and successive sentence, i.e., [sq−1, sq, sq+1] as the pseudo query q. Then, the generation target is
    the document identifier tka i of the destination page linked by the anchor aki.


    #### 3.3.4 Learning Objective

    Based on the three pre-training tasks, we can build a large number of pseudo pairs of query and document identifiers.
    All the tasks are formulated by a standard seq2seq objective, i.e., cross entropy loss, for the pre-training, L = arg
    max_θ ∑_D ∑_m ∑_n log p(wm,n | w≤m,<n, q; θ), where θ denotes the model parameters and D denotes the pre-training dataset.
    All parameters are optimized by the loss L, and the whole model is trained in an end-to-end fashion.'
  citations:
  - marker: '[35]'
    intent_label: Domain Overview
    topic_label: Training strategies and data augmentation
  - marker: '[50]'
    intent_label: Domain Overview
    topic_label: Training strategies and data augmentation
- block_id: 10
  content: In this section, we introduce our experimental settings.
  citations: []
- block_id: 11
  content: '#### 4.1.1 Pre-training Corpus

    We use the English Wikipedia as the pre-training corpus, since (1) Wikipedia is publicly available and easy to collect;
    and (2) A large collection of documents could well support our pre-training method. English Wikipedia contains tens of
    millions of documents which has been widely used in many pre-training methods. Following [9], we use 2019/08/01 Wikipedia
    dump pre-processed by Petroni et al. [38].


    #### 4.1.2 Downstream Tasks

    To verify the effectiveness of our method, we conduct experiments on the KILT benchmark [38] with eleven datasets spanning
    five distinct knowledge-intensive language tasks. In this work, we consider the retrieval task on the KILT benchmark,
    in which the model should provide a set of Wikipedia pages as evidences for final prediction with respect to the input
    query. Table 1 shows the overall statistics of the retrieval tasks in KILT.'
  citations:
  - marker: '[9]'
    intent_label: Resource Utilization
    topic_label: Training strategies and data augmentation
  - marker: '[38]'
    intent_label: Benchmark Utilization
    topic_label: Evaluation of generative document retrieval
- block_id: 12
  content: 'We adopt two types of baseline methods for comparison, including traditional IR models and model-based IR models.


    #### 4.2.1 Traditional IR Models

    We take several representative models that are widely used for KILT tasks as the baselines, including the sparse retrieval
    and dense retrieval methods.


    - BM25 [40] is a highly effective retrieval model that represents the classical probabilistic retrieval model.

    - TF-IDF [5] is a traditional sparse vector space retrieval model that combines bigram hashing and TF-IDF matching to
    return relevant documents.

    - DPR [21] is a BERT-based dual-encoder model trained with in-batch negatives and a few hard negatives selected with BM25.

    - DPR+BERT [38] combines a BERT-base classifier with passages returned from DPR where the query and retrieved passages
    are the input.

    - DPR+BART [38] incorporates an explicit retrieval step in addition to the generative pre-training together with DPR and
    BART.

    - RAG [29] combines pre-trained parametric and non-parametric memory for generation.

    - MT-DPR [36] jointly trains a DPR model on an extensive selection of retrieval tasks.

    - BLINK+flair [38] combines BLINK [46] and flair [1] retrieval solution that ranks pages according to entities in the
    input.


    #### 4.2.2 Generative Retrieval Models

    Furthermore, we consider several advanced generative retrieval baselines.


    - BART [28] is a denoising autoencoder built with a Seq2Seq model that is applicable to sequence generation tasks. Following
    [6, 9], we extract the query-title pairs from each downstream task and directly fine-tune the BART for generative retrieval.
    Specifically, we denote BART fine-tuned under three settings, i.e., zero-shot, specific-task fine-tuning and multi-task
    fine-tuning, as BART_zs, BART_ft and BART_mt, respectively. Note we report the official performance of BART_ft on KILT
    test data.

    - T5 [39] is a pre-trained encoder-decoder model on a multi-task mixture of unsupervised and supervised tasks. We report
    the official performance of fine-tuned T5 (T5_ft) on test and dev data.

    - SEAL [2] combines an autoregressive language model with a compressed full-text substring index. SEAL fine-tunes BART
    via multi-task training on all the datasets in the KILT benchmark.

    - GENRE [9] retrieves entities by generating their unique names. GENRE takes advantage of fine-tuning BART via multi-task
    training on the supervised BLINK data (i.e., 9M unique triples document-mention-entity from Wikipedia) and all the data
    in the KILT benchmark.'
  citations:
  - marker: '[1]'
    intent_label: Result Comparison
    topic_label: Other Topics
  - marker: '[2]'
    intent_label: Result Comparison
    topic_label: Architectural and decoding innovations
  - marker: '[5]'
    intent_label: Result Comparison
    topic_label: Other Topics
  - marker: '[6]'
    intent_label: Setting/Protocal Adoption
    topic_label: 'Document identifiers: text-based'
  - marker: '[9]'
    intent_label: Result Comparison
    topic_label: 'Document identifiers: text-based'
  - marker: '[21]'
    intent_label: Result Comparison
    topic_label: Other Topics
  - marker: '[28]'
    intent_label: Result Comparison
    topic_label: 'Document identifiers: text-based'
  - marker: '[29]'
    intent_label: Result Comparison
    topic_label: Retrieval augmentation
  - marker: '[36]'
    intent_label: Result Comparison
    topic_label: Other Topics
  - marker: '[38]'
    intent_label: Result Comparison
    topic_label: Other Topics
  - marker: '[39]'
    intent_label: Result Comparison
    topic_label: 'Document identifiers: text-based'
  - marker: '[40]'
    intent_label: Result Comparison
    topic_label: Other Topics
  - marker: '[46]'
    intent_label: Result Comparison
    topic_label: Other Topics
- block_id: 13
  content: To measure the retrieval performance on the downstream KILT dataset, we use R-precision (%) as the evaluation metric,
    which is suggested in the official instructions and widely used in previous works on KILT [2, 9, 29, 36, 38]. R-precision
    is calculated as r/R, where R is the number of Wikipedia pages inside each provenance set and r is the number of relevant
    pages among the top-R retrieved pages. Besides, we also report the average performance of all the eleven datasets. For
    the zero-shot and full fine-tuning setting, we report the performance results on both the test and dev sets. For the zero-
    and low-resource settings, we report the performance results on the dev sets since the KILT leaderboard limits the frequency
    of the submission for test performance.
  citations:
  - marker: '[2]'
    intent_label: Metrics Utilization
    topic_label: Evaluation of generative document retrieval
  - marker: '[9]'
    intent_label: Metrics Utilization
    topic_label: Evaluation of generative document retrieval
  - marker: '[29]'
    intent_label: Metrics Utilization
    topic_label: Evaluation of generative document retrieval
  - marker: '[36]'
    intent_label: Metrics Utilization
    topic_label: Evaluation of generative document retrieval
  - marker: '[38]'
    intent_label: Metrics Utilization
    topic_label: Evaluation of generative document retrieval
- block_id: 14
  content: '#### 4.4.1 Model Architecture

    We use the Transformer-based encoder-decoder architecture similar to BART_large version, where the number of Transformer
    layers is 12, the hidden size is 1024, the feed-forward layer size is 4096 and the number of self-attention heads is 16,
    for both the encoder and decoder. The total parameters is 406M. For a fair comparison, we leverage the same architecture
    in the experiments for our CorpusBrain and the baseline BART. Besides, we use sequence modeling toolkit fairseq for the
    implementation of CorpusBrain.


    #### 4.4.2 Pre-training Process

    In this work, we leverage the Wikipedia article title as the document identifier and leave other identifiers (e.g., url
    and hashcode) in the future work. Given an article, for the ISS task, we first select the lead 2 sentences, and then randomly
    sample up to 10 from the rest sentences set as the pseudo queries (i.e., 12 sentences in total). For the LPS task, we
    select the top 3 paragraphs (i.e., l=3) as the pseudo queries. For the output of ISS and RPS, we use the current article
    title joint with titles of destination Wikipedia pages linked by o anchors. Specifically, o is in [0, 1, 2, 3, 4] with
    the probability of [70%, 20%, 5%, 3%, 2%], respectively. For the HIP task, we randomly select 3 anchors from the anchor
    set, and then denote the anchor context as input. The output is the title of the destination Wikipedia page. In total,
    for each article, we construct 18 (i.e., 12+3+3) pseudo pairs.


    Considering the large cost of training from scratch, we initialize the parameters of the encoder-decoder architecture
    from the official BART’s checkpoint. We use a learning rate of 3e-5 and Adam optimizer with the warmup technique, where
    the learning rate increases over the first 10% of batches, and then decays linearly to zero. The label smoothing is 0.1,
    the weight decay is 0.01, and the gradient norm clipping is 0.1. We train in batches of 8192 tokens on two NVIDIA Tesla
    V100 32GB GPUs.


    #### 4.4.3 Fine-tuning Process

    For the retrieval tasks in the downstream KILT benchmark, we first process the original data into a Seq2Seq pair format.
    Specifically, the original input (e.g., claim, text chunk and question) remains unchanged. For the output, we use the
    [SEP] token to concatenate the titles of several ground-truth relevant pages. Then, we fine-tune CorpusBrain with the
    processed data, with the learning rate as 3e-5. For each task, we train in batches of 4096 tokens on one NVIDIA Tesla
    V100 32GB GPU. At the inference time, we use constrained beam search with 10 beams, and maximum decoding steps of 15.
    For the entity linking sub-task, we restrict the input sequence to be at most 384 tokens cutting the left, right, or both
    parts of the context around a mention. We normalize the log-probabilities by sequence length.


    In this work, we fine-tune CorpusBrain via three different strategies: (1) We fine-tune CorpusBrain on specific downstream
    KILT task, denoted as CorpusBrain_ft. (2) We fine-tune CorpusBrain via multi-task training on all KILT retrieval data
    following [2, 9, 36], denoted as CorpusBrain_mt. Note that not all dataset available in KILT have a training set as shown
    in Table 1. To address all tasks, multi-task training has been a common approach for the KILT benchmark. (3) We follow
    the fine-tuning setting used in GENRE [9] to train CorpusBrain on BLINK [46] and all KILT data simultaneously, denoted
    as CorpusBrain_mt+BLINK.'
  citations:
  - marker: '[1]'
    intent_label: Model/Architecture Adoption
    topic_label: Other Topics
  - marker: '[2]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Downstream task adaptation
  - marker: '[3]'
    intent_label: Resource Utilization
    topic_label: Other Topics
  - marker: '[4]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Other Topics
  - marker: '[9]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Downstream task adaptation
  - marker: '[36]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Downstream task adaptation
  - marker: '[46]'
    intent_label: Resource Utilization
    topic_label: Downstream task adaptation
- block_id: 15
  content: 'Our experiments mainly target the following research questions:

    - RQ1: How does CorpusBrain perform compared with strong retrieval baselines across both the unsupervised and supervised
    evaluations?

    - RQ2: What is the performance difference of CorpusBrain under specific-task fine-tuning and multi-task fine-tuning?

    - RQ3: How do the three pre-training tasks of CorpusBrain affect the retrieval performance?

    - RQ4: How does CorpusBrain perform under the low-resource setting?

    - RQ5: How does CorpusBrain perform compared with traditional baselines in terms of memory footprint and inference time?

    - RQ6: Can we better understand how different models perform via some case studies?'
  citations: []
- block_id: 16
  content: 'To answer RQ1, we compare CorpusBrain with various strong baselines on the KILT benchmark. We can observe that:
    (1) Among the traditional IR models, the TF-IDF model performs pretty well on the test set and even outperforms the strong
    dense retrieval method DPR with enough supervised data. (2) The generative retrieval models can outperform traditional
    IR models significantly across all the datasets, indicating the effectiveness of integrating all the components in traditional
    pipelines into a single unified model. (3) Our CorpusBrain_zs under zero-shot setting already achieves comparable results
    to most traditional baselines via full fine-tuning. For example, the R-precision of CorpusBrain_zs and DPR is 94.84% and
    28.9%, respectively, on the zsRE dataset.


    When we look at the generative retrieval baselines, we find that: (1) GENRE performs the best among these baselines in
    terms of most datasets, which benefits from multi-task training on supervised BLINK and entire KILT dataset at the same
    time using 128 GPUs. (2) CorpusBrain_ft only fine-tuned on specific tasks can obtain comparable results over GENRE, which
    could better serve the practical use of search systems. CorpusBrain_mt+BLINK outperforms GENRE on all the dev sets significantly
    (p-value < 0.05) and 10 out of 11 test sets. These results suggest that the pre-training tasks do help obtain a better
    understanding of the corpus for document retrieval. (3) CorpusBrain_mt+BLINK is able to achieve the best performance among
    all the baselines for all 11 dev sets, while for 8 of 11 test sets and the second place for other 3 test sets. Compared
    with baselines which are applicable to all 11 tasks, CorpusBrain_mt+BLINK performs the best on both the dev and test set.


    Finally, the observations from the KILT leaderboard with 11 KILT tasks are as follows: (1) As a universal retrieval model,
    our CorpusBrain_mt+BLINK wins the 1st place for 3 tasks (AY2, WnWi and WoW), the 2nd place for 3 tasks (WnCw, zsRE and
    ELI5), and the 3rd place for 4 tasks (FEV, T-REx, HoPo and TQA). These results indicate the good generalization ability
    of our method, which is able to memorize the knowledge about the corpus. (2) For the leaderboard top methods, generally
    speaking, they are specially optimized for each task, or incorporate extra information to enhance the retrieval performance.
    For example, Re2G leverages the information in the reader component to train a separate model for each task, while the
    top-1 model TABi [26] for T-REx leverages the knowledge graph, which is quite effective for the entity-related task. (3)
    CorpusBrain under-performs others especially for QA datasets (e.g., NQ, HoPo, and TQA). The reason might be that the pseudo
    queries in our pre-training tasks are mainly declarative sentences, which are quite different from the questions in QA.
    For ELI5, since there is no training data, all leaderboard methods perform poorly but our CorpusBrain still achieves the
    2nd. We will investigate to further enhance the generalization of our method.'
  citations:
  - marker: '[26]'
    intent_label: Result Comparison
    topic_label: Tool augmentation
- block_id: 17
  content: 'To answer RQ2, we further analyze the effect of different fine-tuning strategies, i.e., specific-task fine-tuning
    and multi-task fine-tuning, on the KILT dev set. We conduct a comparison of the retrieval performance between CorpusBrain
    and BART.


    We can see that: (1) CorpusBrain under multi-task fine-tuning achieves better results than under specific-task fine-tuning.
    This reason may be that all tasks share a common objective to generate document identifiers and boost each other in the
    training process. (2) Under both the specific-task and multi-task fine-tuning, CorpusBrain outperforms BART over all the
    KILT tasks. This indicates that the designed three objective in CorpusBrain which resembles the relevance relationship
    between the query and document identifier could contribute to the retrieval task. (3) CorpusBrain converges faster than
    BART at both fine-tuning strategies, indicating that with the encoded corpus information, CorpusBrain could easily adapt
    to the downstream KILT tasks via the same learning objective as in the pre-training stage.'
  citations: []
- block_id: 18
  content: 'To answer RQ3, we conduct a thorough ablation study on different pre-training tasks in CorpusBrain. Specifically,
    we pre-train the encoder-decoder model with ISS, LPS and HIP, respectively and evaluate the retrieval performance under
    the zero-shot setting on the dev set. For fair comparison, we set the number of pseudo (query, document identifiers) pairs
    extracted from each Wikipedia article for each pre-training task as 18.


    We can see that: (1) In general, ISS has the best performance, followed by LPS, and then HIP. One possible reason is that
    the input of 7 downstream KILT tasks is sentence-level, the (query, document identifier) pairs defined by the sampled
    sentences are suitable for these generative retrieval task. (2) HIP outperforms ISS and LPS on the entity linking tasks,
    i.e., AY2, WnWi and WnCw. This is because that the HIP task can capture the inter-page relation, which is similar to the
    entity linking formulation. Note CorpusBrain without fine-tuning performs poorly on entity linking under all the pre-training
    tasks. The reason is that we do not add any tokens (e.g., [START_ENT] and [END_ENT]) to denote the mention of entities
    at pre-training stage and thus CorpusBrain easily generates the same titles for different entities. (3) With all the pre-training
    tasks, CorpusBrain achieves the best performance. These results demonstrate the effectiveness of all the three pre-training
    tasks and the importance to measure different granularities of semantics between the query and document.'
  citations: []
- block_id: 19
  content: 'In real-world practice, it is often time-consuming and difficult to collect a large number of relevance labels
    to train or fine-tune a retrieval model for various KILT tasks. To answer RQ4, we simulate the low-resource retrieval
    setting for five datasets, i.e., FEV, AY2, zsRE, TQA and WoW, spanning five varied classes of KILT tasks. Following [14],
    we randomly select different numbers of instances from the original training set for fine-tuning CorpusBrain. Specifically,
    we randomly pick 20, 40, 60, 80 and 100 instances for the FEV, zsRE, TQA and WoW dataset, and pick 200, 400, 600, 800
    and 1000 instances for AY2. We fine-tune CorpusBrain and BART on each downstream dataset, with batches of 4096 tokens,
    learning rate as 3e-5, and pick the last checkpoint to evaluate the performance on the original dev set.


    We observe that: (1) CorpusBrain outperforms BART on all the five datasets by fine-tuning on the same limited supervised
    data, demonstrating that CorpusBrain is able to encode the relevance information about a given corpus through the pre-training.
    Furthermore, CorpusBrain achieves much better zero-shot performance than BART. (2) For the five datasets, CorpusBrain
    fine-tuned on limited supervised data can achieve competitive results with RAG fine-tuned on the full supervised datasets.
    For example, CorpusBrain fine-tuned with only 20 examples has outperformed RAG on TQA and WoW datasets. The results demonstrate
    that by fine-tuning with small numbers of supervised pairs, CorpusBrain is able to adapt to the target task quickly. (3)
    Under the zero resource setting, for example, CorpusBrain can outperform RAG significantly for the FEV (71.69% vs. 63.50%)
    and zsRS dataset (90.95% vs. 65.36%). (4) CorpusBrain also beats previous state-of-the-art baseline, i.e., GENRE, with
    limited fine-tuning examples. For the zsRE dataset, with just 100 examples, CorpusBrain could be fine-tuned to retrieval
    documents at comparable quality (i.e., R-precision = 98.28%) to GENRE (i.e., R-precision = 94.84%). It is worth noting
    that GENRE was trained on the full supervised datasets in the KILT benchmark plus 9M BLINK data. These results further
    validate that the pre-training stage encodes all the information about the corpus into the model parameters and CorpusBrain
    does work like an expert with a knowledgeable brain.'
  citations:
  - marker: '[14]'
    intent_label: Setting/Protocal Adoption
    topic_label: Evaluation of generative document retrieval
- block_id: 20
  content: 'To answer RQ5, we compare CorpusBrain with two representative traditional IR models, i.e., DPR and RAG, in terms
    of the memory footprint and inference time. We compare the memory footprint (disk space) required by different models.
    Here, we evaluate the end-to-end inference time of the retrieval phase in the fact checking task (i.e., FEVER dataset).


    We can see that: (1) CorpusBrain requires the least memory footprint and model parameter regardless of the size of the
    corpus. For example, CorpusBrain occupied 34 times less memory than DPR and 20 times less memory than RAG. It is because
    that CorpusBrain uses its model parameters to store all information of the corpus while traditional IR methods store dense
    representations for the whole corpus increased along with the increase of corpus. (2) CorpusBrain has a significant reduction
    of the inference time of document retrieval. As we can see, dense retrieval models like DPR need to compute relevance
    over all the document dense representations, while the inference process of CorpusBrain is relatively quite simple, where
    the inference time is directly proportional to the beam size with a limited overhead by constrained decoding. These results
    show that CorpusBrain can be well deployed in resource-limited platforms due to the memory-efficiency and time-efficiency.'
  citations: []
- block_id: 21
  content: 'To answer RQ6, we conduct some case studies to better understand how different models perform. We take one query
    from the ELI5 dev set (open-domain QA task) as an example, and show the generated Wikipedia article titles from our CorpusBrain_mt+BLINK
    model as well as that from the strong baselines GENRE and BART_mt.


    We have the following observations: (1) GENRE only predicts one title at each beam, but can obtain multiple titles via
    a post-processing way, i.e., selecting a fixed number of top-ranked beams. In this case, although GENRE could successfully
    predict one relevant document “False advertising”, it fails to predict another relevant document in other beams. The reason
    may be that using beams to return multiple documents can not well model the dependency information between documents.
    (2) BART_mt has the ability to model the document dependency for generating multiple titles. Nonetheless, without adequate
    pre-training tasks used for encoding the knowledge about the corpus, BART_mt may not be able to make totally correct ground-truth
    documents. (3) CorpusBrain_mt+BLINK can generate right document titles. Note in the other beams, CorpusBrain_mt+BLINK
    can still generate the right title (i.e., “False advertising”) or potential right titles (e.g., “Legal liability”). As
    we look at the “Legal liability” article, we found that it is actually a very proper supporting article. These results
    again demonstrate the effectiveness of the proposed pre-training tasks.'
  citations: []
- block_id: 22
  content: 'In this paper, we have proposed CorpusBrain, a novel pre-trained generative retrieval model to encode all information
    about the corpus into its parameters. To train such a strong generative model, we delicately devised a set of pre-training
    tasks to emphasize different aspects of semantics between queries and documents. The key idea is to sample a context from
    one document as a pseudo query and generate the document identifiers of source or destination documents based on hyperlinks.
    CorpusBrain just needs to pre-train one model and could be then adapted to improve a diversity of downstream KILT tasks
    without the need of constructing additional index. Through experiments on the KILT benchmark in terms of the retrieval
    task, CorpusBrain achieved significant improvements over strong baseline approaches. We also showed that CorpusBrain can
    achieve strong performance under both the zero- and low-resource settings.


    In future work, we would like to explore other document identifiers, e.g., page url and HashCode, and investigate new
    ways to further enhance the pre-training tailored for generative retrieval. Besides, it is worthwhile to go beyond the
    search component in the KILT tasks. We would try to test the ability of CorpusBrain over other types of downstream IR
    tasks, such as ad-hoc retrieval, passage retrieval in QA or response retrieval in dialog systems. Furthermore, it is valuable
    to design an end-to-end KILT system in a fully generative way.'
  citations: []
