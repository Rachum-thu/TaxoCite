title: 'Long-Range Distillation: Distilling 10,000 Years of Simulated Climate into Long Timestep AI Weather Models'
blocks:
- block_id: 0
  content: Accurate long-range weather forecasting remains a major challenge for AI models, both because errors accumulate
    over autoregressive rollouts and because reanalysis datasets used for training offer a limited sample of the slow modes
    of climate variability underpinning predictability. Most AI weather models are autoregressive, producing short lead forecasts
    that must be repeatedly applied to reach subseasonal-to-seasonal (S2S) or seasonal lead times, often resulting in instability
    and calibration issues. Long-timestep probabilistic models that generate long-range forecasts in a single step offer an
    attractive alternative, but training on the 40-year reanalysis record leads to overfitting, suggesting orders of magnitude
    more training data are required. We introduce long-range distillation, a method that trains a long-timestep probabilistic
    “student” model to forecast directly at long-range using a huge synthetic training dataset generated by a short-timestep
    autoregressive “teacher” model. Using the Deep Learning Earth System Model (DLESyM) as the teacher, we generate over 10,000
    years of simulated climate to train distilled student models for forecasting across a range of timescales. In perfect-model
    experiments, the distilled models outperform climatology and approach the skill of their autoregressive teacher while
    replacing hundreds of autoregressive steps with a single timestep. In the real world, they achieve S2S forecast skill
    comparable to the ECMWF ensemble forecast after ERA5 fine-tuning. The skill of our distilled models scales with increasing
    synthetic training data, even when that data is orders of magnitude larger than ERA5. This represents the first demonstration
    that AI-generated synthetic training data can be used to scale long-range forecast skill.
  citations: []
- block_id: 1
  content: 'Predicting weather weeks to months ahead is extremely challenging. Most current AI weather models extend short
    forecasts by repeatedly stepping forward in time, causing errors to accumulate and long-range predictions to become unstable.
    In addition, observational records span only a few decades, limiting the ability to learn slow climate processes that
    control long-range predictability.


    We introduce long-range distillation, an approach that trains an AI model to make long-range forecasts in a single step.
    We first use an existing AI weather model to generate over 10,000 years of synthetic climate data, enabling the distilled
    model to learn patterns not well represented in the limited observational record.


    The resulting model produces stable long-range forecasts using one step instead of hundreds. In controlled experiments,
    it nearly matches the performance of the model that generated the synthetic data, and when adapted to real-world conditions,
    it achieves skill comparable to the ECMWF subseasonal-to-seasonal system. Forecast skill improves with increasing synthetic
    training data, demonstrating that AI-generated climate simulations can be used to scale long-range prediction skill.'
  citations: []
- block_id: 2
  content: 'In recent years, there has been rapid progress in the development of AI weather models which now match or exceed
    the forecast skill of state-of-the-art dynamical models for medium-range weather forecasting (Pathak et al., 2022; Keisler,
    2022; Bi et al., 2023; K. Chen et al., 2023; L. Chen et al., 2023; Lam et al., 2023; Price et al., 2025; Alet et al.,
    2025). Although a wide diversity of model architectures, loss functions, and fine-tuning strategies have been explored,
    research in AI weather forecasting has predominantly focused on autoregressive models. Autoregressive models are trained
    to predict the atmospheric state at the next time step, typically a few hours forward, conditioned on some history of
    past states. Thus, the training objective is typically short in its time horizon, with the model effectively learning
    a timestepper for atmospheric dynamics (Dueben & Bauer, 2018; Weyn et al., 2019). Forecasting at long lead times, including
    beyond those optimized during training, is achieved by repeatedly calling the trained model for multiple iterative time
    steps. The repeated model calls lead to an accumulation of model errors when used for longer lead times, though this can
    be partly alleviated by fine-tuning with a multi-timestep objective (Weyn et al., 2020; Pathak et al., 2022; Lam et al.,
    2023; Nguyen et al., 2024; Price et al., 2025; Bonev et al., 2025) or by coupling to an ocean model and fine-tuning the
    full coupled system (Duncan et al., 2025). Autoregressive models are typically trained on reanalysis data from ERA5 (Hersbach
    et al., 2020). Training autoregressive models on reanalysis has proven successful for medium-range weather forecasting
    since ERA5 provides a large and diverse library of short-range atmospheric trajectories, allowing the training of expressive
    neural network architectures which generalize well to unseen weather states.


    Despite the impressive successes of autoregressive models, they suffer a number of key limitations when applied to long-range
    weather forecasting, for example at subseasonal-to-seasonal (S2S) and seasonal lead times. Autoregressive models must
    be iteratively rolled out over many steps for long-range forecasting, typically far beyond the horizon over which they
    were trained, often leading to rollout instability when errors accumulate. At S2S and seasonal lead times, predictability
    from initial conditions diminishes and the skill of deterministic models saturates to climatology, necessitating a probabilistic
    approach. To produce probabilistic forecasts at long lead times, autoregressive models can be combined with ensembling
    methods from numerical weather prediction such as perturbed or lagged initial condition ensembles (Brenowitz, Cohen, et
    al., 2025), or AI-specific approaches like model checkpoint ensembles (Weyn et al., 2021; Mahesh et al., 2025, 2025).
    Recent studies show that ensembles generated using autoregressive models can achieve similar skill at S2S time-scales
    to the ECMWF ensemble forecasting system (Weyn et al., 2024; L. Chen et al., 2024). However, such ensembling methods have
    thus far yielded only modest S2S skill gains compared to ECMWF and calibrating the spread of the ensemble at long lead
    times is an involved and delicate procedure. Alternatively, probabilistic predictions can be achieved using probabilistic
    deep learning models, for example diffusion models (Mardani et al., 2025; Price et al., 2025; Brenowitz, Ge, et al., 2025)
    or by training an ensemble of perturbed models with probabilistic scoring objectives (Lang et al., 2024; Alet et al.,
    2025; Bonev et al., 2025). However, past studies applying probabilistic models still work mostly within an autoregressive
    framework and are thus still limited in their long-range forecast skill by error accumulation and are hard to calibrate
    at long time horizons. Fundamentally, there is a mismatch between the autoregressive training objective, learning a short
    lead timestepper, and the intended inference task in long-range weather forecasting: a well calibrated probabilistic model
    of the slow modes of climate variability that underpin S2S and seasonal predictability.


    While autoregressive modeling has become common practice in AI weather forecasting, using a single, large model timestep
    is another viable avenue explored in some early studies (Sønderby et al., 2020; Rasp & Thuerey, 2021). Rasp and Thuerey
    (2021) outline two alternative approaches to autoregression: ‘direct’ modeling, where a separate neural network is trained
    for each lead time, and ‘continuous’ modeling, where a single network predicts a variety of lead times with the lead time
    provided as input conditioning (Sønderby et al., 2020). While these past studies used deterministic training objectives
    and targeted short-range lead times of just a few days (Sønderby et al., 2020; Rasp & Thuerey, 2021), it is plausible
    that single timestep methods could be adapted for probabilistic long-range weather forecasting. Training single timestep
    probabilistic models to forecast well calibrated ensembles at long lead times appears a promising approach since it removes
    the risk of rollout instability and calibrating the predictions of a single-step model would likely be less challenging
    than for an ensemble of autoregressive rollouts.


    Despite their appealing properties for long-range forecasting, training single timestep models for S2S and seasonal time-scales
    on ERA5 reanalysis data presents a data sparsity problem not encountered for short- and medium-range forecasting. At long
    lead times, the number of independent training examples within the ERA5 record diminishes by orders of magnitude due to
    the significant autocorrelation at long time-scales. ERA5 contains relatively few independent realizations of S2S variability,
    and even fewer of seasonal variability. This poses a challenge for training expressive neural network architectures without
    overfitting to the limited range of dynamics sampled in the ERA5 record.


    In this study, we propose a novel approach to probabilistic long-range weather modeling: long-range distillation. Our
    approach is inspired by the fact that short timestep autoregressive models are good generators of realistic atmospheric
    variability, but that controlling an ensemble of such models for well calibrated long-range forecasting is challenging.
    We propose to use a short timestep autoregressive model as a ‘teacher’ model to enable the training of a probabilistic
    long timestep ‘student’ model through synthetic training data. Autoregressive models are fast and cheap to run and, provided
    they can be rolled out stably over long time-horizons, can thus simulate orders of magnitude more years of atmospheric
    variability than is available in ERA5. By generating a large volume of simulated weather states using the autoregressive
    teacher model, we aim to construct a sufficiently large and diverse training dataset to train a probabilistic model to
    produce well calibrated long-range forecasts in a single model timestep. The trained student model can then be thought
    of as a distilled representation of the autoregressive teacher simulations. By using a single model timestep, the distilled
    model would provide a controllable interface for long-range modeling which sidesteps the need for a large number of autoregressive
    time steps and for extensive tuning of initial condition or model weight perturbations for well calibrated ensemble forecasts.


    The performance of large language models has been shown to scale as both the training dataset size and model capacity
    are increased by orders of magnitude, with optimal results obtained when both are scaled simultaneously (Kaplan et al.,
    2020). AI weather models have thus far explored scaling model capacity while keeping training dataset size fixed, restricted
    by the limited duration of the ERA5 record. By using computationally cheap autoregressive models to generate large volumes
    of synthetic weather data, we aim to unlock the dataset size scaling axis for our distilled model training. Our departure
    away from autoregressive modeling and toward compressing a large volume of climate data into a conditional generative
    model is also inspired by recent efforts to develop a generative foundation model of the global atmosphere (Brenowitz,
    Ge, et al., 2025).


    Training distilled models for long-range prediction requires an autoregressive teacher model capable of generating a large
    volume of synthetic training data with physically plausible S2S and seasonal variability. Here, we use the Deep Learning
    Earth System Model (DLESyM) (Cresswell-Clay et al., 2025) as the autoregressive teacher model for our distilled, long
    timestep student models. DLESyM is a coupled ocean-atmosphere autoregressive model trained on reanalysis data, with the
    ocean and atmosphere models trained separately before being coupled at inference. Despite only being optimized over short
    time horizons of a few days, DLESyM remains stable when rolled out for 100’s-1000’s of years and produces emergent modes
    of coupled ocean-atmosphere variability such as the El-Nino Southern Oscillation (ENSO), with realistic climatological
    statistics for a wide range of processes such as tropical cyclogenesis, the Indian Summer Monsoon, and mid-latitude blocking
    events (Cresswell-Clay et al., 2025). While we use DLESyM here, we emphasize that our long-range distillation framework
    could be applied in future to any autoregressive model capable of realistic long-range rollouts, for example SamudrACE
    (Duncan et al., 2025; Watt-Meyer et al., 2025; Dheeshjith et al., 2025) or NeuralGCM (Kochkov et al., 2024). Another advantage
    of DLESyM as the autoregressive teacher is that it is trained exclusively on reanalysis data, ensuring that our distilled
    models learn dynamics grounded in observations rather than inheriting the inherent biases of physics-based climate models,
    which have complementary advantages for forced multi-decadal prediction tasks.


    While DLESyM is a promising autoregressive teacher model for long-range distillation due to its long-term stability and
    coupled ocean–atmosphere variability, it has limitations relevant to this study. First, DLESyM simulates a limited set
    of state variables, just 8 atmospheric variables and a single ocean variable, compared with roughly 100 atmospheric variables
    in state-of-the-art medium-range AI weather models (Pathak et al., 2022; Keisler, 2022; Bi et al., 2023; K. Chen et al.,
    2023; L. Chen et al., 2023; Lam et al., 2023; Price et al., 2025; Alet et al., 2025), where the same variable at different
    pressure levels is counted separately. While this parsimonious state vector promotes long-term stability, it potentially
    restricts the range of dynamics the model can represent and could limit forecast skill relative to larger models. Second,
    recent studies have shown that autoregressive AI models struggle to capture the rapid initial growth of small-amplitude
    perturbations in the initial conditions (Selz & Craig, 2023). This has implications for ensemble forecasting using perturbed
    initial conditions, as considered in Section 3.1, because it suggests that perturbation strengths may need to be artificially
    increased beyond the true uncertainty in initial conditions to compensate for suppressed error growth. Selz and Craig
    (2023) found that the Pangu model (Bi et al., 2023), while failing to reproduce the initial fast growth of small perturbations
    compared with a physics-based model ground truth, largely captured the true error growth for larger initial condition
    perturbation sizes comparable to the uncertainty of real-world analysis. In contrast, our experiments (Section 3.1) suggest
    that DLESyM exhibits suppressed error growth even for large initial perturbations, likely due to its limited state vector,
    a point warranting further investigation beyond the scope of this study. The key implication for this work is that the
    predictability of the DLESyM-simulated atmosphere differs from the real world. This should be kept in mind when interpreting
    the results of the perfect-model experiment in Section 3.1, though we expect the difference to be most pronounced at short-to
    medium-range lead times, where sensitivity to initial conditions is strongest, and less so at S2S or seasonal scales.


    In this study, we explore the feasibility of long-range distillation for probabilistic forecasting across a range of time-scales
    through controlled perfect-model experiments, before applying our framework to real-world S2S forecasting. Section 2 outlines
    our proposed long-range distillation method. Section 3 describes the setup and results of our perfect-model experiment.
    Section 4 describes how we fine-tune and apply our distilled models for real-world forecasting and benchmarks our method
    against the ECMWF ensemble forecast system. We draw conclusions and discuss future research directions in Section 5.'
  citations: []
- block_id: 3
  content: ''
  citations: []
- block_id: 4
  content: 'Long-range distillation assumes access to a short time-step autoregressive model trained on reanalysis data capable
    of stable long rollouts, which here will be DLESyM (Section 2.2). This autoregressive ‘teacher’ model is used to train
    a distilled ‘student’ model to do long-range forecasting in a single model timestep.


    First, a large simulation is created using the teacher model, generating a diverse dataset of synthetic input-output pairs
    for forecasting at long time-scales. This dataset contains orders of magnitude more long-range training examples than
    the original reanalysis dataset on which the autoregressive teacher was trained.


    Given a long rollout from the autoregressive teacher, (x1, x2, ...), we first define our long-range target, xN, which
    is the teacher-simulated state at some large number, N, of autoregressive steps forward and averaged over a time window
    of M autoregressive steps, xN = (1/M) sum_{i=-M/2}^{M/2} x_{N+i}. Our long-range distilled model then seeks to directly
    model the conditional probability of this long-range target p(xN | x1), conditioned on the initial state. We thus replace
    a long autoregressive rollout of potentially hundreds of steps with a single model timestep.


    Concretely, at S2S time-scales, given an autoregressive model which takes 6 hr timesteps, we could choose N = 112 and
    M = 28 to target a weekly average forecast at 4-week lead time. In practice, to reduce the data volume of the large autoregressive
    teacher simulation we save the teacher simulation with degraded temporal resolution, averaging the 6-hourly timesteps
    to daily frequency. In practice, we then condition our distilled model forecasts on a short history of four daily averages
    rather than a single day. Although the synthetic training data are generated using a Markov model (i.e., DLESyM), and
    a history of states should therefore carry no extra information, we include it to compensate for information lost through
    daily averaging.'
  citations: []
- block_id: 5
  content: 'We use DLESyM (Cresswell-Clay et al., 2025) to generate O(10,000 years) of synthetic climate for training our
    long-range distilled models. To parallelize data generation, we choose to generate an ensemble of simulations initialized
    on different dates rather than running a single long-running simulation. We initialized DLESyM simulations on 200 dates
    equally spaced between 2008-01-01 and 2016-12-31, running each simulation for 90 years, yielding 18,000 years of simulated
    climate. We used the version of DLESyM distributed through the NVIDIA Earth2Studio package (v0.8.0), with each ensemble
    member using a randomly selected checkpoint from the four checkpoints provided through the NGC catalog (NVIDIA, 2025c)
    to boost ensemble variability. The Earth2Studio implementation of DLESyM closely follows that outlined in Cresswell-Clay
    et al. (2025), except that the atmospheric model omits outgoing longwave radiation from the set of prognostic variables
    predicted, with the model checkpoints being trained again from scratch following the procedure outlined in Cresswell-Clay
    et al. (2025). DLESyM is initialized using reanalysis data from ERA5 regridded onto the HEALPix64 grid. The prognostic
    variables predicted by this version of DLESyM are: sea surface temperature (SST); 2 m air temperature (T2m); temperature
    at 850hPa (T850); total column water vapor (TCWV); wind speed at 10m (w10m); geopotential height at 1000hPa (z1000); geopotential
    height at 500hPa (z500); geopotential height at 250hPa (z250); and difference between geopotential heights at 700hPa and
    300hPa (τ300−700).


    Generating this large ensemble of DLESyM simulations took 4 hours when run in parallel on 96 NVIDIA H100 GPUs, with a
    throughput of O(1000) simulated years per day per H100 GPU. This emphasizes the substantial speedup offered by AI climate
    emulators compared to physics-based climate models. We observed that ∼14% of the DLESyM ensemble members went unstable
    before finishing the 90 year rollout. This is in contrast to the 1,000 year stable rollouts demonstrated by Cresswell-Clay
    et al. (2025) and this infrequent instability appears to be specific to the Earth2Studio DLESyM checkpoints as running
    the checkpoints from Cresswell-Clay et al. (2025) for the initial conditions that went unstable here did not show signs
    of instability in the first 100 years of simulation (Cresswell-Clay, 2025). Regardless, it is easily identified and pruned
    in post-processing, i.e. we simply omit all ensemble members that went unstable during the 90 year simulation, leaving
    ∼15,000 years of simulated climate for training and validation of our distilled models.'
  citations: []
- block_id: 6
  content: 'We implement our distilled student model as a conditional diffusion model, enabling probabilistic forecasting.
    As input conditioning, c, we provide a short history of daily average weather states for the 4 preceding days. We then
    seek to model the conditional distribution, p(x|c), of future global weather states, x, at some long-range target lead
    time. All data are represented on the HEALPix64 grid as it provides a satisfying discretization of the sphere and is consistent
    with the DLESyM autoregressive teacher model.


    The neural network architecture we use to perform global conditional diffusion on the HEALPix64 grid follows that used
    in the recently-developed global foundation model, cBottle (Brenowitz, Ge, et al., 2025). The reader is referred to Brenowitz,
    Ge, et al. (2025) for implementation details. Briefly, this architecture adapts the UNet backbone used widely in diffusion
    models (Song et al., 2020; Karras et al., 2022) to the HEALPix grid by using a HEALPix-specific padding scheme proposed
    by Karlbauer et al. (2024), by adding a learned spatial embedding to learn non-stationary spatial structures, and by adding
    a periodic temporal embedding to represent the day of year and time of day (Brenowitz, Ge, et al., 2025). Since in this
    study all forecast and conditioning states are averaged over at least one day, we drop the time of day embedding used
    in cBottle but retain the day of year embedding. We train using a log uniform σ distribution and for sampling we use σmin
    = 0.002, σmax = 200, and 18 sampler steps. Note these settings differ from those in the original EDM paper (Karras et
    al., 2022) and were chosen following the rationale laid out in Brenowitz, Ge, et al. (2025) to ensure that the noise range
    fully covers all scales in our data and to ensure the training σ distribution matches the sampler scheme used at inference.'
  citations: []
- block_id: 7
  content: Calibrating forecasts requires a mechanism to control the ensemble spread of the forecast. For an initial condition
    ensemble of autoregressive models, this is typically achieved by varying the strength of the initial condition perturbations.
    To control the ensemble spread from our long timestep conditional diffusion models we here exploit classifier-free guidance
    (Ho & Salimans, 2022). Classifier-free guidance provides a way to achieve a trade-off between the diversity of generated
    samples and the strength of the conditioning. Concretely, the conditional score, ∇x log(p(x(t)|c)), used during sampling,
    is replaced with a linear combination of the conditional and unconditional scores, ∇x log(p(x(t))) + w [∇x log(p(x(t)|c))
    − ∇x log(p(x(t)))], where w is the classifier-free guidance weight that controls the strength of the guidance. This is
    equivalent to sampling from the probability density function p(x(t)|c)^w p(x(t))^{1−w}. When w = 0, the model generates
    samples unconditionally. When w = 1, the model generates samples conditionally with no guidance. When w > 1 the model
    is guided strongly towards the conditioning, and when 0 < w < 1 the conditioning strength is reduced. In practice, we
    jointly learn the conditional and unconditional models with the same network by randomly setting the input conditioning
    tensor to zero during training for 10% of the samples. While classifier-free guidance is common practice in computer vision,
    this is to our knowledge the first study to propose using it to control ensemble spread in weather forecasting. Unlike
    computer vision, where classifier guidance is used to increase fidelity to conditioning by choosing w > 1, we typically
    use the guidance to increase ensemble spread by choosing w < 1.
  citations: []
- block_id: 8
  content: ''
  citations: []
- block_id: 9
  content: 'In Section 4.1 we turn to the problem of real-world forecasting and account for the domain shift needed to apply
    the DLESyM-trained student model to real-world ERA5 data. First, we evaluate the ability of our long-range distilled models
    to produce skillful ensemble forecasts across a range of lead times in the controlled setting of a perfect-model experiment.
    All evaluation is done using a withheld simulation from the autoregressive teacher model (DLESyM). We assess the extent
    to which both the distilled long timestep models and the autoregressive DLESyM teacher model can forecast an unseen DLESyM
    simulation given imperfect knowledge of initial conditions.


    Concretely, we apply the following experiment protocol:

    1. Perform a single, multi-year deterministic simulation with DLESyM referred to hereafter as the Nature Run. This simulation
    serves as the ground truth reference and is assumed to represent the unknown, true state of the atmosphere.

    2. Select a series of starting-point dates from the Nature Run. Apply a different set of initial condition perturbations
    to each of these starting points to create the suite of imperfect initial conditions for each ensemble forecast.

    3. For the distilled long-timestep student model, construct ensemble forecasts using the conditional diffusion model applied
    to the imperfect initial conditions from 2. The spread of the ensemble forecast can be controlled using classifier-free
    guidance (Section 2.4).

    4. For the DLESyM autoregressive teacher model, construct either deterministic or ensemble forecasts from the same imperfect
    initial conditions from 2 as used by the distilled model. To construct an ensemble forecast from DLESyM, we use initial
    condition perturbations (perturbing around the already imperfect initial conditions from 2) to generate ensemble spread.
    The spread of the ensemble forecast can be controlled by varying the strength of the extra initial condition perturbations
    applied on top of those from 2.

    5. Score all forecasts against the unseen Nature Run from 1.


    The aim of this experiment is to test the extent to which our distilled models produce a well-calibrated ensemble forecast,
    and we compare its performance against an ensemble forecast made using the DLESyM autoregressive teacher model through
    initial condition perturbations for reference. The size of the initial condition perturbation made to the Nature Run when
    constructing the imperfect initial conditions is an important design choice of our experiment. Since DLESyM is a deterministic
    autoregressive model, in the limit where we provide a perfect initial condition it will provide a perfect forecast. Thus
    we seek to provide a large enough perturbation to ensure the experiment reflects the practical real-world forecasting
    ability of DLESyM given imperfect knowledge of initial conditions. To achieve this, we tune the size of the initial condition
    perturbation until the RMSE in Z500 at 7-day lead time of a DLESyM ensemble member matches that of IFS HRES (521 m2/s2)
    (Rasp et al., 2024). This corresponds to an initial condition perturbation size of 65 m2/s2 for Z500. We acknowledge this
    perturbation is larger than the likely real-world analysis error; IFS HRES has 22 m2/s2 RMSE at 6-hour lead time (Rasp
    et al., 2024), which we interpret as necessary to compensate for the known slower error growth of autoregressive AI weather
    models compared to dynamical models (Selz & Craig, 2023). We perturb all atmospheric variables by the same fraction of
    their global standard deviation which is the single parameter we tune to match day-7 IFS HRES RMSE. For SST we provide
    a perturbation of 0.1 K. The initial condition perturbations are sampled as Gaussian white noise with spatiotemporal correlation
    structure given by a Matern covariance function with de-correlation length-scale 500 km and time-scale 48 hours, using
    the implementation in Earth2Studio (NVIDIA, 2025b). This noise structure, which follows that used by Leutbecher and Palmer
    (2008), encourages the perturbed fields to retain physically realistic spatiotemporal structure, while being significantly
    simpler to implement than dynamical methods such as bred vectors (Toth & Kalnay, 1993) given that perturbed initial condition
    ensembling is not a primary focus of our study.


    All validation is done on a withheld DLESyM simulation not seen during training of the distilled model. In addition to
    deterministic metrics like ensemble RMSE, we score all models using continuous ranked probability score (CRPS) (Hersbach,
    2000). We test distilled long timestep models at three distinct lead times, representing three distinct predictability
    regimes:

    - Medium Range: We forecast a daily average at 7-day lead time (N = 28, M = 4). For evaluation, we use over 400 initialization
    dates spaced 2 days apart from 2017-01-01 to 2019-03-10 in simulation years.

    - S2S: We forecast a weekly average at 4-week lead time (N = 112, M = 28). For evaluation, we use over 400 initialization
    dates spaced 4 days apart from 2017-01-01 to 2021-05-16 in simulation years.

    - Seasonal: We forecast a monthly average at 12-week lead time (N = 336, M = 112). For evaluation, we use over 400 initialization
    dates spaced 8 days apart from 2017-01-01 to 2025-09-28 in simulation years.


    We split the 15,000 years of DLESyM simulation into training (75%) and validation (25%) datasets, splitting by ensemble
    member in our large ensemble to ensure separation between training and validation datasets. After applying our train-validation
    split, we are left with ∼11,000 years of DLESyM simulations for training. For each lead time, we train a separate distilled
    model.


    All forecasts are compared against two climatological forecast baselines calculated from a withheld 20 years of the DLESyM
    simulation. First, we refer to a deterministic climatology by retrieving the long-range forecast targets, with the requisite
    temporal averaging for the given M, for each of the 20 years and taking the mean. Second, we refer to a probabilistic
    climatology by using all forecast targets from the 20 year reference period together as an ensemble forecast.'
  citations: []
- block_id: 10
  content: '#### 3.2.1 Distilled Model Performance Scales with Size of Synthetic Training Data

    Our long-range distillation framework is based on the hypothesis that an increasing volume of synthetic training data
    from a long-duration autoregressive teacher simulation improves student skill, even when the teacher simulation duration
    far exceeds the reanalysis dataset used for training the teacher. For this to be true, the autoregressive teacher simulation
    must generate a diverse range of climate states beyond those seen during training, sampling internal variability not captured
    by the short reanalysis record. Analyses of long DLESyM rollouts suggest that it can indeed generate new samples of internal
    climate variability (Cresswell-Clay et al., 2025), but here we test whether this translates into improved performance
    of our distilled models with increasing amounts of synthetic training data.


    We test this hypothesis by evaluating distilled S2S forecast models trained over a range of autoregressive teacher simulation
    lengths. A distilled model trained on 40 years of DLESyM simulation, mimicking the availability of ERA5, shows clear signs
    of overfitting once trained beyond ∼1M samples, corresponding to ∼15,000 training steps. By contrast, the model trained
    on the full 11,000 year DLESyM simulation shows no signs of overfitting, with the training and validation losses matching
    closely throughout. The reduced overfitting can also be seen in the steadily improving minimum validation loss as the
    number of training years is increased by orders of magnitude. Crucially, the improved EDM denoising losses also translate
    to improvements in S2S forecast skill. Training on 11,000 years of synthetic data yields a 14% reduction in 2 m temperature
    CRPS at a 4-week lead time, compared to training on 40 years. Note, the scaling of the forecast skill with dataset size
    appears to be weaker than the power law scaling curves observed for natural language (Kaplan et al., 2020), with our CRPS
    values not following a power law with dataset size. This is either because beyond a certain point, the simulation becomes
    large enough to provide comprehensive sampling of the internal variability of the DLESyM model climate, or because the
    distilled student model capacity becomes the bottleneck as we used a fixed architecture for all dataset sizes whereas
    Kaplan et al. (2020) scaled both model capacity and dataset size jointly. Nonetheless, the improvements of week-4 CRPS
    with training data size far beyond the size of the original ERA5 training dataset confirms our hypothesis that generating
    a large synthetic corpus of training data using autoregressive models enables more skillful long-range models and is the
    first demonstration of scaling forecast performance with an increasing volume of synthetic training data.


    Hereafter, we use the shorthand DLESyM10K to refer to the distilled student model trained on the full O(10,000 year) DLESyM
    simulation and all distilled student model results use the model trained on the full dataset.


    #### 3.2.2 Classifier-Free Guidance Provides a Simple Recipe for Calibration of Long Timestep Forecasts

    A well calibrated forecast should have a spread-skill ratio near unity. One key motivation for our long-range distillation
    approach is that forecasting using a single model timestep should make it relatively easy to control the ensemble spread,
    and hence calibrate the forecasts, compared to controlling ensemble spread over a long autoregressive rollout. Here, we
    confirm our working hypothesis that classifier-free guidance (Section 2.4) provides a simple recipe for ensemble calibration.


    The ensemble spread of our medium-range (7-day lead time) DLESyM10K forecasts varies monotonically with classifier-free
    guidance strength, enabling simple calibration. When the guidance strength is set to zero, the model samples unconditionally
    and thus its spread is close to that of climatology. In this limit the model is conditioned only on the day of year and
    simply generates samples from the learned climatology of its training data. As the guidance is increased, the ensemble
    spread drops as the model generation depends more strongly on the input conditioning. When the guidance strength is close
    to unity, the predictions achieve an optimal tradeoff between forecast spread and accuracy, with a spread-skill ratio
    of one and the minimum CRPS. In hindsight, further exploration of guidance strengths between 0.5 and 1.0 would have helped
    to find precisely the optimal guidance strength. As the guidance is increased beyond one, the ensemble spread continues
    to decrease but the ensemble RMSE increases, leading to an increasingly overconfident but inaccurate forecast and thus
    a low spread-skill ratio and high CRPS. Interestingly, here the optimal CRPS was achieved using classifier-free guidance
    strength of one, suggesting that the training objective of the conditional model could implicitly encourage a good spread-skill
    ratio even with no further tuning of classifier-free guidance strength. Nonetheless, in cases where simple conditional
    sampling (guidance strength of one) doesn’t immediately yield a well calibrated forecast, we have shown here that adjusting
    the ensemble spread of the forecast can be achieved by simply changing the classifier-free guidance strength used at inference,
    providing a convenient method for calibrating ensemble forecasts post-training.


    #### 3.2.3 Distilled Models Are Skillful Across Lead Times

    Medium range. We first assess medium-range (7-day) forecast skill under a realistic setting where both models are initialized
    from an imperfect estimate of the atmospheric state that contains non-negligible error. In this setting, the autoregressive
    teacher model (DLESyM) exhibits substantial ensemble RMSE because initial condition errors grow during the autoregressive
    rollout. Increasing ensemble spread by further perturbing around the already-imperfect initial condition does not meaningfully
    reduce forecast error, as the 7-day evolution remains tightly constrained by the biased starting point. In contrast, DLESyM10K
    shows much greater robustness to this initialization error: its day 7 Z500 ensemble RMSE increases by only 25 m2/s2 due
    to the addition of initial condition error, and its RMSE is lower than that of the autoregressive teacher for moderate
    classifier-free guidance strengths. This improved performance stems from the student model’s probabilistic formulation,
    which represents a distribution of plausible future states and thereby expresses aleatoric model uncertainty that can
    hedge over multiple potential trajectories consistent with an imperfect initial condition. Equivalent benefits for the
    teacher model would require explicitly injecting model uncertainty, for example through a checkpoint ensemble (Weyn et
    al., 2021) or probabilistic model design (Lang et al., 2024; Alet et al., 2025; Bonev et al., 2025). By varying the classifier-free
    guidance strength, the distilled student model is able to explore spread-skill space. When the guidance strength is ∼1
    the distilled student model achieves a well-calibrated forecast by intersecting the 1-1 line, whereas the autoregressive
    teacher model is under-dispersive across the full range of initial condition perturbation strengths explored here.


    It is instructive to also examine forecast behavior in the idealized regime of perfect initial conditions. When initialized
    exactly from the true atmospheric state and run deterministically, the autoregressive teacher produces a perfect forecast
    with zero RMSE since it is a perfect model of the DLESyM atmosphere. As perturbations are added around the true state,
    its ensemble RMSE increases smoothly with perturbation amplitude. DLESyM10K behaves differently in this idealized setting:
    even when initialized from the true state, it yields a non-zero ensemble RMSE due to its probabilistic nature. Increasing
    perturbation amplitude does not significantly change its RMSE, reflecting the fact that uncertainty in DLESyM10K arises
    predominantly from its learned distribution over future states rather than from sensitivity to initial conditions.


    S2S and seasonal. Moving to long-range forecasting, we now evaluate the performance of DLESyM10K at the more challenging
    S2S and seasonal time-scales in the setting where we have an imperfect initial condition. At both S2S and seasonal lead
    times, DLESyM10K shows substantially more skill than a deterministic climatology, achieving global mean skill within 3-7%
    of an ensemble forecast using the autoregressive teacher model, constructed by further perturbing around the already imperfect
    initial conditions provided to DLESyM10K. Comparing both DLESyM10K and the autoregressive teacher to a probabilistic climatology
    highlights the significant challenge of long-range forecasting as they both only marginally improve upon probabilistic
    climatology in terms of global mean CRPS. Nonetheless, DLESyM10K shows statistically significant skill relative to climatology
    both in the global mean and in many regions, with the model showing most skill in the tropics and over oceans where the
    relatively slow evolution of sea surface temperature increases the long-range predictability of surface air temperature.
    At both S2S and seasonal lead times DLESyM10K appear to struggle primarily over the continental Northern Hemisphere and
    over Antarctica where there appear to be significant errors that warrant closer scrutiny in future work. Even with these
    limitations, DLESyM10K exceeds climatology and closely matches the skill of its autoregressive teacher in a single model
    timestep, which corresponds to 126 and 392 teacher steps at S2S and seasonal timescales respectively.'
  citations: []
- block_id: 11
  content: ''
  citations: []
- block_id: 12
  content: '#### 4.1.1 Fine-Tuning Distilled Models on ERA5

    Ultimately, our goal is to apply DLESyM10K to real-world long-range forecasting taking ERA5 reanalysis as initial condition.
    While DLESyM was trained on ERA5, the model climate that emerges when it is run for long periods does not exactly match
    that of the real-world (Cresswell-Clay et al., 2025), as is the case for any climate model. If we are to apply DLESyM10K,
    which was trained on DLESyM simulations, for real-world forecasting there is thus a domain shift to overcome.


    We use two strategies to alleviate this domain shift. First, we calculate the climatological bias of the DLESyM simulations
    compared to ERA5 and use this to shift both the ERA5 initial conditions and forecast targets seen by the distilled model,
    ensuring the large-scale mean and seasonal cycle of our target distribution matches that seen during training. Second,
    we fine-tune DLESyM10K on ERA5 data. During fine-tuning, we reduce the learning rate by a factor of ten compared to the
    original training and freeze all weights except for: the first convolutional layer in the UNet encoder, the final UNet
    block in the decoder, all UNet blocks with attention (which are near the middle of the UNet), and all GroupNorm weights
    and biases. During early testing, we found this strategy to achieve a good balance, providing enough capacity to adapt
    to ERA5 data while freezing enough of the pre-trained weights to avoid overfitting to the short ERA5 record.


    #### 4.1.2 Bias Correcting Long-Range Forecasts Using Lead Time-Dependent Model Climatology

    For both physics-based and autoregressive weather models it is common practice to perform bias correction when forecasting
    at S2S and seasonal time-scales to correct for model drift caused by accumulating model errors (Vitart, 2004; Weigel et
    al., 2008; Vitart et al., 2017; Weyn et al., 2021, 2024). In this study we use a lead time-dependent model climatology
    to bias correct all real-world long-range forecasts. In this approach, we take reforecasts for the same day of the year
    for the past 20 years and find the mean bias of the reforecasts compared to ERA5 ground truth. This mean bias is then
    subtracted from the forecast fields for the target year before scoring. Note while DLESyM10K does not in theory suffer
    an accumulation of errors over long autoregressive rollouts, we found in practice the same bias correction procedure to
    be helpful for our distilled models when applied to real-world data. This is likely a result of the DLESyM-ERA5 domain
    shift as bias correction was not found to be beneficial in our perfect-model experiment.


    #### 4.1.3 Benchmarking Distilled Models Against Operational S2S Forecasts

    We evaluate DLESyM10K’s ability to do S2S forecasting in the real world. As in Section 3.1, we seek to predict a weekly
    average at 4-week lead time. To benchmark our approach against existing operational forecasts, we use the ECMWF S2S ensemble
    forecasts which contain 50 ensemble members (Vitart et al., 2017). Specifically, we evaluate the ECMWF real-time forecasts
    for the period 2018-2022 taking ERA5 to be the target. We bias correct all ECMWF forecasts using the reforecasts provided
    for the preceding twenty years using the same model version as the real-time forecasts and we bias correct DLESyM10K using
    the same procedure. DLESyM10K is fine-tuned on ERA5 data from 1980-2016. To test the value of distilling the large DLESyM
    simulation, we also evaluate a long timestep model with the same architecture trained from scratch on ERA5. All forecasts
    are evaluated using CRPS focusing on 2 m air temperature. As in Section 3.1, all forecasts are benchmarked against a deterministic
    and probabilistic climatological forecast taken here from the 20 preceding years of ERA5.'
  citations: []
- block_id: 13
  content: 'As outlined in Section 4.1.1, we fine-tune DLESyM10K on real-world data from ERA5 to allow it to adapt to distribution
    shifts between the DLESyM simulated climate and the real world. Comparing the learning curves of our fine-tuned model
    to one with identical architecture but trained from scratch on ERA5 data reveals the benefit of pre-training on DLESyM
    simulations. Training from scratch on ERA5 leads to overfitting, whereas fine-tuning only a subset of pre-trained parameters
    on ERA5 acts as a strong regularization, preventing overfitting. Importantly, the fine-tuned model is able to achieve
    a lower validation loss than training from scratch on ERA5, suggesting transferability of the skill learned from the DLESyM
    simulations to real-world data.


    To benchmark DLESyM10K against an operational baseline, we compare the week-4 2 m temperature forecast skill to that of
    the ECMWF ensemble forecast system from 2018-2022. First, the challenge of forecasting at S2S time-scales is highlighted
    by the fact that a probabilistic climatology, taking the preceding 20 years as ensemble members, achieves comparable global
    mean CRPS to all forecast models. The bias-corrected ECMWF forecasts have the lowest CRPS, although our fine-tuned DLESyM10K
    model achieves almost the same skill, with both achieving a statistically significant skill improvement over probabilistic
    climatology. Based on the fact that DLESyM10K is competitive with an operational physics-based ensemble forecasting system,
    it is logical to expect that, with further refinements, long-range distillation is a viable path to state-of-the-art long-range
    forecasting. DLESyM10K outperforms the long-timestep model trained from scratch on ERA5, further highlighting the value
    of pre-training on a large DLESyM simulation.


    Looking beyond the global means, both ECMWF and DLESyM10K consistently outperform probabilistic climatology primarily
    over oceans, with predictability likely attributable to the relatively slow evolution of sea surface temperature. In contrast,
    over land both models show limited skill compared to climatology, with skill mostly confined to the tropics, highlighting
    that skillful S2S forecasting remains a challenging frontier for both physics-based and AI weather forecasting systems.
    Comparing DLESyM10K to ECMWF, DLESyM10K shows statistically significant improvements compared to ECMWF over the Americas
    and parts of central Africa, while ECMWF outperforms in the tropics, and parts of Eurasia and Africa.'
  citations: []
- block_id: 14
  content: 'Achieving skillful long-range forecasts, from S2S to seasonal time-scales, remains one of the most challenging
    and socioeconomically consequential goals in AI-driven weather prediction. It remains to be seen whether the current paradigm
    of training autoregressive models on reanalysis data can achieve breakthrough skill in forecasting the slower modes of
    climate variability that drive S2S and seasonal predictability.


    In this study, we proposed a novel approach to long-range weather modeling. Rather than relying on ensemble autoregressive
    rollouts, we repurpose a reanalysis-trained autoregressive model as a generator of large volumes of synthetic data to
    train a separate distilled model that predicts long-range forecasts in a single model timestep. This design leverages
    the ability of autoregressive models to produce realistic weather trajectories while mitigating their limitations for
    ensemble forecasting at extended lead times. To our knowledge, this is the first work to train an AI weather model on
    synthetic data generated by another AI model, although prior studies have trained on physics-based simulations (Rasp &
    Thuerey, 2021). Our results show that replacing long autoregressive rollouts with a single extended timestep and optimizing
    for a probabilistic long-range objective yields well-calibrated forecasts across multiple time scales. We further demonstrate
    that classifier-free guidance, commonly used in image generation, offers a simple recipe for controlling ensemble spread
    in conditional diffusion models. Training with such long timesteps (equivalent to up to 392 autoregressive steps) without
    overfitting required a massive synthetic dataset, and we observed that S2S forecast skill scaled with the number of synthetic
    years, even when the dataset far exceeded the size of ERA5. Finally, fine-tuning on ERA5 reanalysis allowed our distilled
    models to achieve real-world forecast skill comparable to the operational ECMWF S2S ensemble forecast system.


    Although our findings highlight the potential of distillation techniques for improving long-range weather prediction,
    several limitations remain that warrant attention in future research. Firstly, while it is impressive that our model achieves
    competitive S2S forecast skill compared to ECMWF, our model still slightly trails ECMWF in terms of global mean CRPS and
    thus does not yet represent a breakthrough in real-world S2S forecast skill. Note that S2S time-scales are notoriously
    challenging for both physics-based and data-driven weather models, and the current state of the art for AI S2S weather
    models is parity with, or at best marginal improvements upon, the ECMWF ensemble forecasts (L. Chen et al., 2024; Weyn
    et al., 2024; Lang et al., 2024). We hope that future refinements to the long-range distillation approach outlined here
    could eventually yield more substantial improvements in S2S forecasting.


    The primary opportunity we see for improving the performance of long-range distilled models is by leveraging the rapid
    recent advances in coupled autoregressive Earth System Models. One benefit of our distillation approach, is that it can
    be readily applied to any autoregressive model, allowing us to bootstrap future refinements to the autoregressive teacher
    model into improved distilled models. While the version of DLESyM used in this study generates realistic interannual and
    intraseasonal variability (Cresswell-Clay et al., 2025), it simulates only 8 atmospheric variables and only a single ocean
    variable. It is thus likely that the synthetic climate our distilled models were trained on here does not fully capture
    important modes of S2S and seasonal variability in the real world such as the Madden-Julian Oscillation and ENSO, indeed
    the ENSO amplitude was observed to be weak compared to observations (Cresswell-Clay et al., 2025). As increasingly sophisticated
    autoregressive Earth System Models emerge, we anticipate that applying our framework to models such as SamudrACE (Duncan
    et al., 2025; Watt-Meyer et al., 2025; Dheeshjith et al., 2025), NeuralGCM (Kochkov et al., 2024), and forthcoming enhancements
    to DLESyM could further improve the forecast skill of long-range distilled models. By proposing training a separate model
    for long-range forecasting, we highlight that autoregressive climate emulators can be useful as generators of huge synthetic
    training datasets for long-range forecasting even if they are not themselves directly amenable to ensemble forecasting
    from reanalysis. Finally, the ERA5 fine-tuning method for distilled models presented here provides a crucial tool for
    ensuring the resulting forecast models are well calibrated against real-world observations while retaining the benefits
    of pre-training on multi-centennial simulations.


    This study provides the first demonstration that forecast performance can improve with increasing volumes of AI-generated
    synthetic training data and that these improvements transfer to downstream real-world forecasts after ERA5 fine-tuning.
    This suggests a new scaling axis for AI weather models, which have to date only been scaled in terms of model capacity,
    whereas results from large language modeling indicate that model capacity and training dataset size should be scaled in
    tandem (Kaplan et al., 2020). Here, S2S forecast skill scaled with diminishing returns as dataset size increased, rather
    than following a power law, but dataset size was varied at fixed model capacity. Future work could explore whether jointly
    scaling model capacity and dataset size, as is done for large language models, yields stronger gains, potentially benefiting
    from transformer-based architectures that scale more effectively with large datasets than the convolutional UNet backbone
    used here (Kaplan et al., 2020). While this study focused on long-range forecasting, training on huge synthetic datasets
    could also improve short- and medium-range forecasts, either by leveraging existing large ensemble simulations or by exploiting
    the low inference cost of AI models to generate training data from any teacher model of interest.


    By an initial exploration of distilled AI models trained on large volumes of synthetic data from autoregressive AI weather
    models, we hope to expand the design space of AI weather modeling and inspire further exploration of synthetic training
    data approaches. Departing from the autoregressive modeling framework opens up a broader range of climate informatics
    and scientific exploration applications beyond forecasting. Brenowitz, Ge, et al. (2025) showed that training conditional
    generative models on ERA5 and ICON simulations outside of an autoregressive framework enabled a diverse range of use cases
    including downscaling, channel in-filling, and steerable sampling. While in this study, our distilled models were tasked
    with long-range forecasting, one could envisage training similar conditional models on huge autoregressive AI simulations
    for other tasks such as exploring the global drivers of local weather extremes or for conditioning sample generation on
    large-scale climate indices or other quantities of interest.'
  citations: []
