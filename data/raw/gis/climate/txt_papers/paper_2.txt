FuXi: A cascade machine learning forecasting
system for 15-day global weather forecast
Lei Chen 1†, Xiaohui Zhong 1†,F e n gZ h a n g2,3,Y u a n
Cheng1, Yinghui Xu 1,Y u a nQ i1* and Hao Li 1*
1Artiﬁcial Intelligence Innovation and Incubation Institute,
Fudan University, Shanghai, 200433, China.
2Key Laboratory of Polar Atmosphere-ocean-ice System for
Weather and Climate, Ministry of Education, Department of
Atmospheric and Oceanic Sciences, Fudan University, Shanghai,
200433, China.
3Shanghai Qi Zhi Institute, Shanghai, 200232, China.
*Corresponding author(s). E-mail(s): qiyuan@fudan.edu.cn;
lihao lh@fudan.edu.cn;
Contributing authors: cltpys@163.com; x7zhong@gmail.com;
fengzhang@fudan.edu.cn; cheng yuan@fudan.edu.cn;
renji.xyh@vip.163.com;
†These authors contributed equally to this work.
Abstract
Over the past few years, the rapid development of machine learning (ML)
models for weather forecasting has led to state-of-the-art ML models
that have superior performance compared to the European Centre for
Medium-Range Weather Forecasts (ECMWF)’s high-resolution forecast
(HRES), which is widely considered as the world’s best physics-based
weather forecasting system. Speciﬁcally, ML models have outperformed
HRES in 10-day forecasts with a spatial resolution of 0.25 °. However,
the challenge remains in mitigating accumulation of forecast errors for
longer e!ective forecasts, such as achieving comparable performance to
the ECMWF ensemble in 15-day forecasts. Despite various e!orts to
reduce accumulation errors, such as implementing autoregressive multi-
time step loss, relying on a single model has been found to be insu”cient
for achieving optimal performance in both short and long lead times.
Therefore, we present FuXi, a cascaded ML weather forecasting system
1
arXiv:2306.12873v3  [physics.ao-ph]  20 Oct 2023
2 FuXi: 15-day global weather forecasts
that provides 15-day global forecasts at a temporal resolution of 6 hours
and a spatial resolution of 0.25 °. FuXi is developed using 39 years of the
ECMWF ERA5 reanalysis dataset. The performance evaluation demon-
strates that FuXi has forecast performance comparable to ECMWF
ensemble mean (EM) in 15-day forecasts. FuXi surpasses the skillful fore-
cast lead time achieved by ECMWF HRES by extending the lead time for
Z500 from 9.25 to 10.5 days and for T 2M from 10 to 14.5 days. More-
over, the FuXi ensemble is created by perturbing initial conditions and
model parameters, enabling it to provide forecast uncertainty and demon-
strating promising results when compared to the ECMWF ensemble.
Keywords: weather forecast, machine learning, accumulation error, cascade,
FuXi, transformer
1 Introduction
Accurate weather forecasts play an important role in many aspects of human
society. Currently, national weather centers around the world generate weather
forecasts using numerical weather prediction (NWP) models, which simulate
the future state of the atmosphere. Nevertheless, running NWP models often
requires high-performance computing systems, with some simulations taking
several hours using thousands of nodes. The Integrated Forecast Systems (IFS)
of the European Centre for Medium-range Weather Forecast (ECMWF) is
widely regarded as the most accurate global weather forecast model [ 1]. The
ECMWF’s high-resolution forecast (HRES) runs at a horizontal resolution
of 0.1 °with 137 vertical levels for 10-day forecasts. However, uncertainty in
weather forecasts is inevitable due to the limited resolution, approximation
of physical processes in parameterizations, errors in initial conditions (and
boundary conditions for regional models), and the chaotic nature of the atmo-
sphere. Additionally, the degree of uncertainty and the magnitude of errors
in weather forecasts increases as forecast lead time. One way to address this
uncertainty is to run an ensemble of forecasts by incorporating perturbations
in initial conditions and physical parameterizations in the NWP model. The
ECMWF ensemble prediction system (EPS) [ 2] provides forecasts up to 15
days and is comprised of one control member and 50 perturbed members. The
IFS Cycle 48r1, which was introduced in June 2023, upgrade the spatial and
vertical resolution and vertical resolution of the EPS same as the HRES [ 3],
which was made possible by a new supercomputer with enhanced capacity.
Prior to the upgrade, the EPS ran at a lower spatial resolution of 18 km and
had fewer vertical levels of 91, due to the substantial computational demands
of running 51 members with limited computing resources.
In recent years, there have been increasing e!orts to replace the traditional
NWP models with machine learning (ML) models for weather forecasting [ 4].
ML-based weather forecasting systems have several advantages over NWP
models, including faster speeds and the potential to provide higher accuracy
FuXi: 15-day global weather forecasts 3
than uncalibrated NWP models due to training with reanalysis data [ 4]. To
facilitate intercomparison between di!erent ML models, the WeatherBench
benchmark was introduced to evaluate medium-range weather forecasting (i.e.,
3-5 days) [ 5, 6]. WeatherBench was created by regridding ERA5 reanalysis
data [7] from 0.25→ resolution to three di!erent resolutions (5 .625→,2 .8125→ and
1.40625→). Several studies have aimed to improve forecast performance on this
dataset [ 8–10]. For example, Rasp et al. [ 8] used a deep residual convolutional
neural network (CNN) known as ResNet [ 11] to predict 500 hPa geopoten-
tial ( Z500), 850 hPa temperature ( T 850), 2-meter temperature ( T 2M ), and
total precipitation ( TP ) at a spatial resolution of 5 .625→ for up to 5 days.
They found that the ResNet model has similar performance compared to phys-
ical baseline models, such as IFS T42 and T63, with a comparable resolution.
Meanwhile, Hu et al. [ 10] proposed the SwinVRNN model, which utilizes a
Swin Transformer-based recurrent neural network (RNN) (SwinRNN) model
coupled with a perturbation module to learn multivariate Gaussian distribu-
tions based on the Variational Auto-Encoder framework. They demonstrated
the SwinVRNN model’s potential as a powerful ML-based ensemble weather
forecasting system, with good ensemble spread and better accuracy compared
to IFS in terms of T 2M and 6-hourly TP in 5-day forecasts with a 5 .625→
resolution.
While ML models have shown good performance in weather forecasting,
their practical values are limited because of their forecasts’ low resolution (e.g.,
5.625→). As a remarkable breakthrough, the FourCastNet model [ 12]i st h eﬁ r s t
of its kind to provide high-resolution global weather forecasts of 0 .25→ for a time
period of 7 days. It integrates the Adaptive Fourier neural operator (AFNO)
[13] with a Vision Transformer (ViT) [ 14]. However, FourCastNet’s forecast
accuracy is still worse than HRES’s. SwinRDM [ 15] distinguishes itself as the
ﬁrst ML-based weather forecasting system to outperform ECMWF HRES in
5-day forecasts at a spatial resolution of 0 .25→. SwinRDM integrates Swin-
RNN+, an improved version of SwinRNN that surpasses ECMWF HRES at
a spatial resolution of 1 .40625→, with a di!usion-based super-resolution model
that increases the resolution to 0 .25→. Pangu-Weather [ 16] shows its superior
performance compared to ECMWF HRES in 7-days forecasts at a resolution
of 0 .25→. Additionally, GraphCast [ 17], an autoregressive model that imple-
ments a graph neural network (GNN), outperforms HRES in 90% of the 2760
variable and lead time combinations in 10-day forecasts.
Although ML models have shown promising results in generating weather
forecasts for 10 days, long-term forecasting remains challenging due to cumu-
lative errors. The iterative forecasting method, which uses the model outputs
as inputs for subsequent predictions, is a commonly used approach in devel-
oping ML-based weather forecasting systems. This approach is similar to the
time-stepping methods used in conventional NWP models [ 18]. However, as the
number of iterations increases, errors in the model outputs accumulate, which
may lead to signiﬁcant discrepancies with the training data and unrealistic
values in long-term forecasts. Many research has been conducted to enhance
4 FuXi: 15-day global weather forecasts
the stability and accuracy of long-term forecasts. Weyn et al. [ 9] proposed a
multi-time-step loss function to minimize errors over multiple iterated time
steps. Rasp et al. [ 5] compared iterative forecasts with direct forecasts that
predict speciﬁc lead times and found the latter to be more accurate. However,
one limitation of direct forecasts is that separate models need to be trained for
each lead time. The FourCastNet [ 12] model underwent two training phases:
pre-training, in which the model is optimized to map one time step to the next
with a 6-hour interval, and ﬁne-tuning to minimize errors in two-step predic-
tion, similar to the multi-time-step loss function proposed by Weyn et al. On
the other hand, Bi et al. proposed a hierarchical temporal aggregation strat-
egy for Pangu-Weather’s forecasts, training four separate models for 1-hour,
3-hour, 6-hour, and 24-hour forecasts [ 16]. They demonstrated that running
the 24-hour model 7 times is better than running the 1-hour model 168 times
as it signiﬁcantly reduces the accumulation errors for 7-day forecasts. How-
ever, they acknowledged that training a model directly predicting the lead time
beyond 24 hours is challenging with their current model. Meanwhile, Lam et
al. employed a curriculum training schedule following pre-training to improve
GraphCast’s ability to make accurate forecasts for multiple steps [ 17]. Increas-
ing autoregressive steps results in excessive memory and computational costs,
thereby limiting the maximum feasible number of steps. Chen et al. [ 19] pro-
posed a reply bu!er mechanism to mimic the long-lead autoregressive forecasts
with improved computational e”ciency and reduced memory costs. The study
by Lam et al. [ 17] revealed that GraphCast’s performance decreases in short
lead times and improves at longer lead times as the number of autoregressive
steps increases. Thus, using a single model is insu”cient for achieving the best
performance for both short and long lead times.
To conclude, signiﬁcant progress have been achieved in ML-based weather
forecasting, particularly in 10-day forecasts where the ML models have out-
performed ECMWF HRES. However, further breakthroughs are necessary to
address the issues related to iterative accumulated errors and enhance the
accuracy of forecasts for longer lead times. The next signiﬁcant goals are to
achieve comparable performance to ECMWF ensemble, of which the ensemble
mean (EM) often has greater skill than the deterministic forecasts for longer
lead times, and to increase the forecast lead time beyond 10 days. The objec-
tive of this study is to reduce the accumulation error and generate ML-based
weather forecasts for 15 days that have performance comparable to ECMWF
EM. However, since a single model has been shown to be incapable of achieving
optimal forecast performance across various forecast lead times, we propose
a novel cascade ML model architecture for weather forecasting based on pre-
trained models, each optimized for speciﬁc forecast time windows. As a result,
we present FuXi 1 weather forecasting system that generates 15-day forecasts
at the spatial resolution of 0 .25→. FuXi is a cascade of models optimized for
three sequential forecast time periods of 0-5 days, 5-10 days, and 10-15 days,
1 FuXi, (Chinese: 伡缡), the ﬁrst of ancient China’s mythological emperors, is said to be the ﬁrst
weather forecaster of China. He created bagua ( 儡匡), eight diagrams, which were used to explain
the constitution of the universe and predict weather.
FuXi: 15-day global weather forecasts 5
respectively. The base FuXi model is an autoregressive model designed to e”-
ciently extract complex features and learn relationships from a large volume
of high-dimensional weather data. Speciﬁcally, 39 years of 6-hourly ECMWF
ERA5 reanalysis data at a spatial resolution of 0 .25→ are used for developing
the FuXi system. The evaluation shows that FuXi signiﬁcantly outperforms
ECMWF HRES and achieves comparable performance to ECMWF EM for
the ﬁrst time. FuXi extends the skillful forecast lead time, as indicated by
whether anomaly correlation coe”cient (ACC) being greater than 0.6, to 10.5
and 14.5 days for Z500 and T 2M , respectively. Moreover, ensemble forecasts
provide greater values beyond EM by o!ering estimates of forecast uncertainty
and enabling skillful predictions for longer lead times. Therefore, we developed
the FuXi ensemble forecast by introducing perturbations to initial conditions
and model parameters in order to generate ensemble forecasts. The evaluation
based on the continuous ranked probability score (CRPS) demonstrates that
the FuXi ensemble performs comparably to the ECMWF ensemble within a
forecast lead time of 9 days for Z500, T 850, mean sea-level pressure ( MSL ),
and T 2M .
Overall, our contribution to this work can be summarized as follows:
• We propose a novel cascade ML model architecture for weather forecasting,
which aims to reduce accumulation errors.
• FuXi achieves comparable performance to ECMWF EM and extends the
skillful forecast lead time (ACC >0.6) to 10.5 and 14.5 days for Z500 and
T 2M ,r e s p e c t i v e l y .
2D a t a s e t
2.1 ERA5
ERA5 is the ﬁfth generation of the ECMWF reanalysis dataset, providing
hourly data of surface and upper-air parameters at a horizontal resolution of
approximately 31 km and 137 model levels from January 1940 to the present
day [ 7]. The dataset is generated by assimilating high-quality and abun-
dant global observations using ECMWF’s IFS model. Given its coverage and
accuracy, the ERA5 data is widely regarded as the most comprehensive and
accurate reanalysis archive. Therefore, we use the ERA5 reanalysis dataset as
the ground truth for the model training.
We use a subset of the ERA5 dataset spanning 39 years, which has a
spatial resolution of 0 .25→ (721 → 1440 latitude-longitude grid points) and a
temporal resolution of 6 hours. In this work, we focus on predicting 5 upper-air
atmospheric variables at 13 pressure levels (50, 100, 150, 200, 250, 300, 400,
500, 600, 700, 850, 925, and 1000 hPa), and 5 surface variables. The 5 upper-air
atmospheric variables are geopotential ( Z), temperature ( T ), u component of
wind ( U ), v component of wind ( V ), and relative humidity ( R). Additionally,
5 surface variables are T 2M , 10-meter u wind component ( U 10), 10-meter v
6 FuXi: 15-day global weather forecasts
T able 1: A summary of variable deﬁnitions used in this paper.
Va r i a b l e s D e ﬁ n i t i o n s
C, H, W Channel dimensions, and spatial dimensions in latitude and
longitude directions, respectively.
D As e tc o n t a i n i n ga l lt h ef o r e c a s ti n i t i a l i z a t i o nt i m e si nt h e
testing dataset.
c, i, j, t0, ω Indices for variables, latitude coordinates, and longitude
coordinates, as well as forecast initialization time and fore-
cast lead time steps added to t0,r e s p e c t i v e l y .
Xt, ˆXt, M Ground truth and model predicted weather parameters at
time step t,a n dc l i m a t o l o g i c a lm e a nc o m p u t e du s i n gE R A
reanalysis data between 1993 and 2016.
Z, R, T , U , V , TP , MSL They represent geopotential, relative humidity, tempera-
ture, u component of wind, v component of wind, 6-hourly
total precipitation, and mean sea-level pressure, respec-
tively.
wind component ( V 10), MSL , and TP 2. In total, 70 variables are predicted
and evaluated.
Following previous studies in splitting the data into training, validation,
and testing set [ 12, 17], the training set consists of 54020 3 samples spanning
from 1979 to 2015. The validation set contains 2920 samples corresponding to
the years 2016 and 2017, while out-of-sample testing is performed using 1460
samples from 2018.
2.2 HRES-fc0 and ENS-fc0 dataset
In this study, we evaluate our model against the ERA5 reanalysis data. Besides,
we also created two reference datasets, HRES-fc0 and ENS-fc0, which consist
of the ﬁrst time step of each HRES and ensemble control forecast, respectively.
We use these datasets to assess the performance of ECMWF HRES and EM.
This approach aligns with that used by Haiden et al. [ 1] and Lam et al. [ 17]
in evaluating ECMWF forecasts.
3 Methodology
3.1 Generating 15-day forecasts using FuXi
The FuXi model is an autoregressive model that leverages weather parameters
(Xt↑1,X t) from two previous time steps as input to forecast weather parame-
ters at the upcoming time step ( Xt+1). t, t ↑ 1, and t +1r e p r e s e n tt h ec u r r e n t ,
the prior, and upcoming time steps, respectively. The time step considered in
this model is 6 hours. By utilizing the model’s outputs as inputs, the system
can generate forecasts with di!erent lead times.
Generating 15-day forecasts using a single FuXi model requires 60 iterative
runs. Pure data-driven ML models, unlike physics-based NWP models, lack
2 As u m m a r yo fv a r i a b l ed e ﬁ n i t i o n sc a nb er e f e r r e dt oi nT a b l e1
3 54020 = 365 ↓ 4 ↓ 37, similarly, 2920 = 365 ↓ 4 ↓ 2, and 1460 = 365 ↓ 4
FuXi: 15-day global weather forecasts 7
physical constraints, which can result in signiﬁcantly growing errors and unre-
alistic predictions for long-term forecasts. Using an autoregressive, multi-step
loss e!ectively minimizes accumulation error for long lead times [ 17]. This loss
is similar to the cost function applied in the four-dimensional variational data
assimilation (4D-Var) method, which aims to identify the initial weather con-
ditions that optimally ﬁt observations distributed over an assimilation time
window. Although increasing the autoregressive steps leads to more accurate
forecasts for longer lead times, it also results in less accurate results for shorter
lead times. Besides, increasing autoregressive steps require more memory and
computing resources for handling gradients during the training process, similar
to increasing the assimilation time window of 4D-Var.
When making iterative forecasts, error accumulation is inevitable as lead
times increase. Also, previous studies indicate that a single model can not
perform optimally across all lead times. To optimize performance for both
short and long lead times, we propose a cascade [ 20, 21] model architecture
using pre-trained FuXi models, ﬁne-tuned for optimal performance in speciﬁc
5-day forecast time windows. These windows are referred to as FuXi-Short (0-
5 days), FuXi Medium (5-10 days), and FuXi-Long (10-15 days). As shown in
Figure 1, FuXi-Short and FuXi Medium outputs from the 20th and 40th steps
are used as inputs to FuXi-Medium and FuXi-Long, respectively. Unlike the
greedy hierarchical temporal aggregation strategy employed in Pangu-Weather
[16], which utilizes 4 models with forecast lead times of 1 h, 3 h, 6 h, and 24
h to minimize the number of steps, the cascaded FuXi model does not su!er
from temporal inconsistency. The cascaded FuXi model performs comparably
to ECMWF EM in 15-day forecasts, as shown in Figure 3.
3.2 FuXi model architecture
The model architecture of the base FuXi model consists of three main compo-
nents, which are illustrated in Figure 1: cube embedding, U-Transformer, and
a fully connected (FC) layer. The input data combines both upper-air and sur-
face variables and creates a data cube with dimensions of 2 → 70 → 721 → 1440,
where 2, 70, 721, and 1440 represent the two preceding time steps ( t ↑ 1 and
t), the total number of input variables, latitude ( H) and longitude ( W ) grid
points, respectively.
Firstly, the high-dimensional input data undergoes dimension reduction to
C →180→360 through joint space-time cube embedding, where C is the number
of channels, and is set to be 1536). The primary purpose of cube embedding
is to reduce the temporal and spatial dimensions of input data, making it less
redundant. Subsequently, the U-Transformer processes the embedded data,
and prediction follows using a simple FC layer. The output is initially reshaped
to 70 → 720 → 1440, then restored to the original input shape of 70 → 721 → 1440
by bilinear interpolation. The following subsections provide details for each
component in the base FuXi model.
8 FuXi: 15-day global weather forecasts
Fig. 1: Overall architecture of FuXi model. a) The FuXi model consists of three
components: cube embedding, U-Transformer, and fully connected (FC) layer;
b) FuXi-Short, FuXi-Medium, and FuXi-Long models cascade and produce 15-
day forecasts, with each model generating 5 days forecasts.
3.2.1 Cube embedding
To reduce the spatial and temporal dimensions of input and accelerate the
training process, the space-time cube-embedding [ 22] is applied. A similar
approach, patch embedding, which divides an image into N → N patches
with each patch being transformed into a feature vector, was used in the
Pangu-Weather model [ 16]. The cube embedding applies a 3-dimensional (3D)
convolution layer, with a kernel and stride of 2 →4→4 (equivalent to T
2 → H
4 → W
4 ),
and output channels numbering C. Following cube embedding, a layer normal-
ization (LayerNorm) [ 23] is utilized to improve training stability. The result is
a data cube with dimensions of C → 180 → 360.
3.2.2 U-T ransformer
This subsection presents the design of the U-Transformer, which is visually
represented in Figure 1 through its schematic diagram.
Recently, the ViT [ 14] and its variants have demonstrated remarkable
performance in various computer vision tasks by using the multi-head self-
attention, which enables the simultaneous processing of sequential input data.
Nevertheless, global self-attention is infeasible for processing high-resolution
inputs due to its quadratic computational and memory complexity with
FuXi: 15-day global weather forecasts 9
respect to the input size. Swin Transformer was proposed as a solution [ 24]
to improve computational e”ciency by limiting computation of self-attention
only within the non-overlapping local windows. Besides, the shifted-window
mechanism allows for cross-connections between windows. As a result, the Swin
Transformer has shown superior performance on various benchmarks and is
frequently used as a backbone architecture in many vision tasks. Additionally,
many researchers have developed ML-based weather forecasting models using
Swin Transformer blocks [ 10, 12, 15, 16].
However, training and applying a large-scale Swin Transformer model
for high-resolution inputs reveals several issues, including training instabil-
ity. To address these issues, Swin Transformer V2 [ 25] was proposed, which
upgrades the original Swin-Transformer (V1) [ 24] by using the residual post-
normalization instead of pre-normalization 4, scaled cosine attention instead
of the original dot product self-attention 5, and log-spaced coordinates instead
of the previous linear-spaced coordinates. As a result, Swin Transformer V2
has 3 billion parameters and advances state-of-the-art performance on multiple
vision task benchmarks.
The U-Transformer is constructed using 48 repeated Swin Transformer V2
blocks and calculates the scaled cosine attention as follows:
Attention(Q, K, V) = (cos (Q, K) /ω + B)V (1)
where B represents the relative position bias and ω is a learnable scalar,
which is not shared across heads and layers. The cosine function is naturally
normalized, which leads to smaller attention values.
The U-Transformer, as the name implies, also includes a downsampling
and upsampling block from the U-Net model [ 26]. The downsampling block,
referred to as the Down Block in Figure 1, reduces the data dimension to
C → 90 → 180, thereby minimizing computational and memory requirements for
self-attention calculation. The Down Block consists of a 3 → 3 2-dimensional
(2D) convolution layer with a stride of 2, and a residual [ 27] block that has two
3 → 3 convolution layers followed by a group normalization (GN) layer [ 28] and
a sigmoid-weighted linear unit (SiLU) activation [ 29, 30]. The SiLU activation
is calculated by multiplying the sigmoid function with its input ( ε(x)→x). The
upsampling block, known as Up Block in Figure 1, has the same residual block
as used in the Down Block, along with a 2D transposed convolution [ 31]w i t h
a kernel of 2 and a stride of 2. The Up Block scales the data size back up to
C → 180 → 360. Furthermore, a skip connection is included that concatenates
the outputs from the Down Block with those of the transformer blocks before
being fed into the Up Block.
4 The LN layer is moved from the beginning of each residual unit to the end, producing much
milder activation values.
5 This makes the computation irrelevant to amplitudes of block inputs so that attention values
are less likely to fall into extremes.
10 FuXi: 15-day global weather forecasts
3.3 FuXi model training
This section outlines the training process for FuXi models. The training proce-
dure involves two steps: pre-training and ﬁne-tuning, similar to the approach
used for training GraphCast [ 17].
3.3.1 One-step Pre-training
The pre-training step involves supervised training and optimizing the FuXi
model to predict a single time step using the training dataset. The loss function
used is the latitude-weighted L1 loss, which is deﬁned as follows:
L1= 1
C → H → W
C∑
c=1
H∑
i=1
W∑
j=1
ai | ˆXt+1
c,i,j ↑ Xt+1
c,i,j | (2)
where C, H, and W are the number of channels and the number of grid points
in latitude and longitude direction, respectively. c, i, and j are the indices
for variables, latitude and longitude coordinates, respectively. ˆXt+1
c,i,j and Xt+1
c,i,j
are predicted and ground truth for some variable and locations (latitude and
longitude coordinates) at time step of t + 1. ai represents the weight at latitude
i and the value of ai decreases as latitude increases. The L1 loss is averaged
over all the grid points and variables.
The FuXi model is developed using the Pytorch framework [ 32]. Pre-
training of the model requires approximately 30 hours on a cluster of 8 Nvidia
A100 GPUs. The model is trained with 40000 iterations using a batch size of 1
on each GPU. The AdamW [ 33, 34] optimizer is used with parameters ϑ1=0.9
and ϑ2=0.95, an initial learning rate of 2.5 →10↑4, and a weight decay coe”-
cient of 0.1. Scheduled DropPath [ 35] with a dropping ratio of 0.2 is employed
to prevent overﬁtting. In addition, Fully-Sharded Data Parallel (FSDP) [ 36],
bﬂoat16 ﬂoating point precision, and gradient check-pointing [ 37] are applied
to reduce memory costs during model training.
3.3.2 Fine-tuning cascaded models
After pre-training, the base FuXi model is ﬁrst ﬁne-tuned for optimal perfor-
mance for 6-hourly forecasts spanning from 0 to 5 days (0-20 time steps). This
ﬁne-tuning process is performed using an autoregressive training regime and
curriculum training schedule to increase the number of autoregressive steps
from 2 to 12, following the ﬁne-tuning approach of the GraphCast model [ 17].
This ﬁne-tuned model is referred to as FuXi-Short in Figure 1. With weights
from FuXi-Short, the FuXi-Medium model is initialized and then ﬁne-tuned
for optimal forecast performance for 5 to 10 days (21-40 time steps). Imple-
menting the online inference of FuXi-Short to get output at the 20th time step
(5th day), which is required for input to the FuXi-Medium model during its
ﬁne-tuning process, is inappropriate due to signiﬁcant memory consumption
and the slowdown of the ﬁne-tuning process for FuXi-Medium. To address this
issue, the results of FuXi-Short for six years of data (2012-2017) are cached on
FuXi: 15-day global weather forecasts 11
a hard disk beforehand. The same procedure for ﬁne-tuning FuXi-Medium is
repeated for the ﬁne-tuning of FuXi-Long, optimized for generating forecasts
of 10-15 days. Finally, FuXi-Short, FuXi-Medium, and FuXi-Long are cascaded
to produce the complete 15-day forecasts. As detailed in Appendix B, cas-
cade helps to reduce accumulation errors and improve forecast performance
for longer lead times.
During the ﬁne-tuning process, the model was trained using a constant
learning rate of 1 →10↑7. It takes approximately two days to ﬁne-tune each of
the cascaded FuXi models on a cluster of 8 Nvidia A100 GPUs.
3.4 FuXi ensemble forecast
Weather forecasting is an inherently uncertain due to the chaotic nature of the
weather system [ 38]. To address this uncertainty, ensemble forecasting is nec-
essary, particularly for longer lead times. Additionally, since ML models can
generate forecasts at signiﬁcantly lower computational costs compared to con-
ventional NWP models, we generated a 50-member ensemble forecast using
the FuXi model. Following the approach used by ECMWF for ensemble runs,
which involves perturbing both initial conditions and model physics [ 39, 40],
we incorporated random Perlin noise [ 16] into the initial conditions and imple-
mented the Monte Carlo dropout (MC dropout, dropout rate is 0.2) [ 41]t o
perturb the model parameters. More speciﬁcally, each of the 49 perturbations
contains 4 octaves of Perlin noise, a scaling factor of 0.5, and the number of
periods of noise to generate along each axis (channel, latitude, and longitude)
being 1, 6 and 6, respectively.
3.5 Evaluation method
We follow [ 5] to evaluate forecast performance using latitude-weighted root
mean square error (RMSE) and ACC, which are calculated as follows:
RM SE(c, ω)= 1
| D |
∑
t0 ↔D
√ 1
H → W
H∑
i=1
W∑
j=1
ai( ˆXt0 +ω
c,i,j ↑ Xt0 +ω
c,i,j )
2
(3)
ACC (c, ω)= 1
| D |
∑
t0 ↔D
∑
i,j ai( ˆXt0 +ω
c,i,j ↑ M t0 +ω
c,i,j )( ˆXt0 +ω
c,i,j ↑ M t0 +ω
c,i,j )√∑
i,j ai( ˆXt0 +ω
c,i,j ↑ M t0 +ω
c,i,j )2 ∑
i,j ai( ˆXt0 +ω
c,i,j ↑ M t0 +ω
c,i,j )2
(4)
where t0 is the forecast initialization time in the testing set D, and ω is the
forecast lead time steps added to t0. M represents the climatological mean
calculated using ERA5 reanalysis data between 1993 and 2016. Additionally,
to improve the discrimination of the forecast performance among models with
small di!erences, we use the normalized RMSE di!erence between model A and
12 FuXi: 15-day global weather forecasts
baseline B calculated as (RM SEA ↑ RM SEB )/RM SEB , and the normalized
ACC di!erence represented by (ACCA ↑ ACCB )/(1 ↑ ACCB ). Negative val-
ues in normalized RMSE di!erence and positive values in normalized ACC
di!erence indicate that model A performs better than the baseline model B.
To evaluate the performance of ECMWF HRES and EM, the veriﬁca-
tion method implemented by ECMWF [ 1] is used where the model analysis,
namely HRES-fc0 and ENS-fc0, serve as the ground truth for HRES and EM,
respectively.
In addition, we assess the quality of ensemble forecasts by calculating two
metrics: the CRPS [ 42, 43] and the spread-skill ratio (SSR). The CRPS is
computed using the following equation:
CRPS =
∫ ↗
↑↗
[F ( ˆXt0 +ω
c,i,j ) ↑H (Xt0 +ω
c,i,j ↓ z)] dz (5)
where F represents the cumulative distribution function (CDF) of the fore-
casted variable ( ˆXt0 +ω
c,i,j ), and H is an indicator function. The indicator function
equals 1 if the statement Xt0 +ω
c,i,j ↓ z is true; otherwise takes the value of 0
[44]. For deterministic forecasts, the CRPS reduces to the mean absolute error
(MAE) [ 42]. The xskillscore Python package is used to calculate the CRPS
metric. And we assume that the distribution of ensemble members follows a
Gaussian distributions, and the CRPS is computed based on the ensemble
mean and the ensemble variance. On the other hand, the SSR measures the
consistency between the spread of the ensemble and the RMSE of the EM.
The ensemble spread is deﬁned as:
Spread(c, ω)= 1
| D |
∑
t0 ↔D
√ 1
H → W
H∑
i=1
W∑
j=1
aivar( ˆXt0 +ω
c,i,j ) (6)
where var( ˆXt0 +ω
c,i,j ) denotes the variance within the ensemble dimension. A reli-
able ensemble is indicated by a SSR of one [ 45]. Lower values suggest an
underdispersive ensemble forecast, while higher values indicate overdispersion.
4 Results
For evaluating FuXi’s performance, the study uses the 2018 data and selects
two daily initialization times (00:00 UTC and 12:00 UTC) to produce 6-hourly
forecasts for 15 days.
4.1 Deterministic forecast metrics comparison
This subsection compares the forecast performance of FuXi, ECMWF HRES,
GrahpCast (the state-of-the-art ML-based weather forecast), ECMWF EM,
and FuXi EM on deterministic metrics. Figure 2 shows the time series of the
globally-averaged latitude-weighted ACC and RMSE of FuXi, ECMWF HRES,
FuXi: 15-day global weather forecasts 13
and GraphCast for 4 surface variables ( MSL , T 2M , U 10, and V 10) and 4
upper-air variables ( Z500, T 500, U 500, and V 500) at 500 hPa pressure level.
The ﬁgure illustrates that both FuXi and GraphCast signiﬁcantly outperform
ECMWF HRES. FuXi and GraphCast have comparable performance within
forecasts of 7 days, beyond which FuXi shows superior performance, with the
lowest values of RMSE and the highest values of ACC across all the vari-
ables and forecast lead times. Moreover, FuXi’s superior performance becomes
increasingly signiﬁcant as lead times increase. Using an ACC value of 0.6 as
the threshold to measure a skillful weather forecast, we ﬁnd that FuXi extends
the skilful forecast lead time compared to ECMWF HRES, especially pushing
the lead time of Z500 and T 2M from 9.25 and 10 days to 10.5 and 14.5 days
(see Figure B.2 for comparison of skillful forecast lead time), respectively.
14 FuXi: 15-day global weather forecasts
Fig. 2 : Comparison of the globally-averaged latitude-weighted ACC (ﬁrst and
second rows) and RMSE (third and fourth rows) of the HRES (dark green
lines), GraphCast (organge lines), and FuXi (light blue lines) for 4 surface
variables, such as MSL , T 2M , U 10, and V 10, and 4 upper-air variables at
the pressure level of 500 hPa, including Z500, T 500, U 500, and V 500, using
testing data from 2018. FuXi and GraphCast are evaluated against the ERA5
reanalysis dataset, and ECMWF HRES is evaluated against HRES-fc0.
Figure 3 shows the time series of the globally-averaged latitude-weighted
ACC and RMSE of ECMWF EM, FuXi, and FuXi EM, as well as the cor-
responding normalized di!erences in ACC and RMSE for 4 variables. The 4
variables include 2 upper-air variables ( Z500 and T 850) and 2 surface variables
(i.e., MSL and T 2M ). Many combinations of variables and pressure levels are
not included in the comparisons as they are unavailable from the ECMWF
server. The normalized di!erences in ACC and RMSE are computed using
ECMWF EM as the reference, as shown in the 2nd and 4th rows of Figure 3.
FuXi superior performance to ECMWF EM in 0-9 day forecasts, with positive
FuXi: 15-day global weather forecasts 15
values in the normalized ACC di!erence and negative values in normalized
RMSE di!erence. However, for forecasts beyond 9 days, FuXi shows slightly
poorer performance compared to ECMWF EM. Overall, FuXi shows compa-
rable performance to ECMWF EM in 15-day forecasts, with higher ACC and
lower RMSE than ECMWF EM on 67.92% and 53.75% of the 240 combi-
nations of variables, levels, and lead times in the testing set, which includes
2 surface variable and 2 upper-air variables over 15 days, with 4 steps each
day. The higher percentage of ACC could potentially be attributed to the fact
that the climatological mean used in the computation of the ACC is based on
ERA5 data, which serves as the ground truth for training FuXi. FuXi EM is
slightly inferior to the Fuxi deterministic forecast within short lead times for
all variables shown in Figure 3. However, it performs better after the lead time
surpasses 3 days, which aligns with Pangu-Weather and FourCastNet.
16 FuXi: 15-day global weather forecasts
Fig. 3: Comparison of the globally-averaged latitude-weighted ACC (ﬁrst row)
and RMSE (third row) as well as normalized ACC (second row) and RMSE
di!erence (fourth row) of ECMWF EM (light purple lines), FuXi (light blue
lines), and FuXi EM (light red lines) for 2 upper-air variables, including Z500
(ﬁrst column) and T 850 (second column), and 2 surface variables, such as MSL
(third column) and T 2M (fourth column), in 15-day forecasts using testing
data from 2018. FuXi and FuXi EM is evaluated against the ERA5 reanalysis
dataset, and ECMWF ensemble is evaluated against ENS-fc0.
Figure 4 illustrates the spatial distributions of the average RMSE of FuXi,
the RMSE di!erence between ECMWF HRES and FuXi, and the RMSE dif-
ference between ECMWF EM and FuXi for forecasts of Z500 and T 2M at lead
times of 5 days, 10 days, and 15 days, respectively. All forecasts in the test-
ing data from 2018 were averaged to produce the data. The RMSE di!erence
is represented by red, blue, and white patterns indicating whether ECMWF
HRES or ECMWF EM performs worse than, better than, or equally compared
to FuXi. Overall, all three forecasts have similar spatial error distributions,
FuXi: 15-day global weather forecasts 17
Fig. 4 : Spatial map of average RMSE (not latitude-weighted) of FuXi (ﬁrst
and fourth rows), the di!erence in RMSE between ECMWF EM (second and
ﬁfth rows) and FuXi, and the di!erence in RMSE between ECMWF HRES
(third and sixth rows) and FuXi for Z500 (ﬁrst to third rows) and T 2M (fourth
to sixth rows) at forecast lead times of 5 days (ﬁrst column), 10 days (second
column), and 15 days (third column), using the 2018 testing data.
with the RMSE di!erence values much lower than the RMSE values. The high-
est RMSE values appear at high latitudes, while relatively small values are
found in middle and low latitudes. The values of RMSE are higher over the
land than over the ocean. The RMSE di!erence between ECMWF HRES and
FuXi shows that FuXi outperforms ECMWF HRES in most grid points, as
shown by the predominance of red color. In contrast, ECMWF EM shows com-
parable performance to FuXi in most areas, as indicated by the predominantly
white color.
18 FuXi: 15-day global weather forecasts
4.2 Ensemble forecast metrics comparison
Compared to deterministic forecasts, ensemble forecasts have several advan-
tages. They provide a more accurate EM than deterministic forecast in terms
of deterministic metrics and also represent forecast uncertainty through the
ensemble spread. This subsection focuses on comparing ensemble evaluation
metrics between the FuXi ensemble and the ECMWF ensemble. Figure 5 illus-
trates the time series of the CRPS, globally-averaged latitude-weighted Spread
and SSR for the same 4 variables as shown in Figure 3. The CRPS values
for the FuXi ensemble are comparable to those of the ECMWF ensemble and
slightly smaller before 9 days. However, beyond 9 days, the FuXi ensemble
demonstrates inferior CRPS compared to the ECMWF ensemble. The SSR
values for the FuXi ensemble are signiﬁcantly higher than 1 for the 3 variables
such as Z500, T 850, and MSL in early lead times, indicating overdisper-
sion. These values then decrease dramatically with increasing lead times, and
becomes lower than 1, indicating an underdispersive ensemble. Meanwhile, the
SSR of the ECMWF ensemble are very close to 1, except for T 2M . Both the
FuXi ensemble and the ECMWF ensemble show underdispersion for T 2 as
their SSR values remain smaller than 1 throughout the 15-day forecast. While
the ensemble spread of the ECMWF ensemble grows as the forecast lead time
increases, the ensemble spread of the FuXi ensemble initially increases as the
lead time increases, then decreases after 9 days. One plausible explanation is
that the initial conditions are perturbed by the addition of Perlin noise, which
is random and independent of the background ﬂow. As a result, only a small
fraction of the perturbations remains after 9 days of model integration, causing
the ensemble spread to decrease.
FuXi: 15-day global weather forecasts 19
Fig. 5 : Comparison of the CRPS (ﬁrst row), the globally-averaged latitude-
weighted Spread (second row) and SSR (third row) of the ECMWF ensemble
(light purple lines) and the FuXi ensemble (light red lines) for 2 upper-air
variables, including Z500 (ﬁrst column) and T 850 (second column), and 2
surface variables, such as MSL (third column) and T 2M (fourth column), in
15-day forecasts using testing data from 2018. The FuXi ensemble is evaluated
against the ERA5 reanalysis dataset, and the ECMWF ensemble is evaluated
against ENS-fc0.
5 Conclusion and Future Work
It has been challenging for data-driven methods to compete with conventional
physics-based numerical weather prediction models in weather forecasting due
to the di”culty in reducing accumulation error. Recently, ML-based weather
forecasting systems have witnessed signiﬁcant breakthroughs, outperforming
ECMWF HRES in 10-day forecasts with a temporal resolution of 6 hours and
a spatial resolution of 0 .25→ [16, 17]. However, employing a single model proves
insu”cient to obtain optimal performance across various lead times. In order
to generate skillful weather forecasts for longer lead times, such as 15 days, we
ﬁrst develop a powerful base ML model architecture, FuXi model. The FuXi
model is based on the U-Transformer and has the capability to e”ciently learn
complex relationships from vast amounts of high-dimensional weather data.
Moreover, we propose a novel cascade ML model architecture for weather fore-
casting that utilizes three pre-trained FuXi models. Each model is ﬁne-tuned
20 FuXi: 15-day global weather forecasts
for optimal forecast performance for one of the forecast time windows: 0-5
days, 5-10 days, and 10-15 days. These models are then cascaded to generate
comprehensive 15-day forecasts. By implementing the aforementioned method-
ologies, we created FuXi, an ML-based weather forecasting system that, for
the ﬁrst time, performs comparably to ECMWF EM in 15-day forecasts with
a temporal and spatial resolution of 6 hours and 0 .25→. Additionally, the FuXi
ensemble forecast exhibits promising potential, with a comparable CRPS to
ECMWF ensemble within 9 days for Z500, T 850, MSL , and T 2M .
In this study, we incorporate Perlin noise into the initial conditions to gen-
erate ensemble forecasts. The Perlin noise is random and independent of the
background ﬂow. Previous studies [ 46, 47] have shown that ﬂow-independent
initial perturbations decay over time during the model integration. Conse-
quently, to ensure an adequate ensemble spread in the medium range, we will
investigate ﬂow-dependent methods for initial condition perturbations in order
to maintain a reasonable spread throughout longer lead times for the FuXi
ensemble.
Furthermore, we plan to explore the potential of utilizing the cascade
ML model architecture for sub-seasonal forecasting. This will involve ﬁne-
tuning additional models for forecast lead times ranging from 14 to 28 days.
Sub-seasonal forecasting remains a challenge and is considered as a ”pre-
dictability desert” [ 48]. Unlike medium-range weather forecasting, which can
utilize deterministic methods, ensemble forecasts are necessary for sub-seasonal
forecasting. In addition, research has identiﬁed various processes in the atmo-
sphere, ocean, and land that contribute to sub-seasonal predictability, such as
the Madden-Julian Oscillation (MJO), soil moisture, snow cover, Stratosphere-
troposphere interaction, and ocean conditions [ 49]. Therefore, more research
is needed to develop an ML-based sub-seasonal forecasting system.
In addition, one limitation of current ML-based weather forecasting meth-
ods is that they are not yet completely end-to-end. They still rely on analysis
data generated by conventional NWP models for initial conditions. Thus, we
aim to develop a data-driven data assimilation method that uses observation
data to generate initial conditions for ML-based weather forecasting systems.
Looking to the future, we aim to build a truly end-to-end, systematically
unbiased, and computationally e”cient ML-based weather forecasting system.
Data Availability Statement
We downloaded a subset of the ERA5 dataset from the o”cial website
of Copernicus Climate Data (CDS) at https://cds.climate.copernicus.eu/.
The ECMWF HRES forecasts are available at https://apps.ecmwf.
int/archive-catalogue/?type=fc&class=od&stream=oper&expver=1 and
ECMWF EM are available at https://apps.ecmwf.int/archive-catalogue/
?type=em&class=od&stream=enfo&expver=1. The preprocessed sam-
ple data used for running FuXi models in this work are available
FuXi: 15-day global weather forecasts 21
in a Google Drive folder ( https://drive.google.com/drive/folders/
1NhrcpkWS6MHzEs3i lsIaZsADjBrICYV)[ 50].
Code Availability Statement
We used the code base of Swin transformer V2 as the backbone archi-
tecture, available at https://github.com/microsoft/Swin-Transformer. The
source code used for training and running FuXi models in this work is
available in a Google Drive folder ( https://drive.google.com/drive/folders/
1NhrcpkWS6MHzEs3i lsIaZsADjBrICYV)[ 50]. The aforementioned Google
Drive folder contains the FuXi model, code, and sample input data, which
can be accessed by individuals with the provided link. As the FuXi model
and code are essential resources for this study, we have implemented pass-
word protection for the Google Drive folder link through a Google Form.
To obtain the link to the Google Drive folder from the Zenodo link,
users are required to complete the designated Google Form ( https://docs.
google.com/forms/d/e/1F AIpQLSfjwZLf6PmxRvRhIPMQ1WRLJ98iLxOq
0dXb87N8CFNPyYAGg/viewform?usp=sharing).
The xskillscore Python package can be accessed from https://github.com/
xarray-contrib/xskillscore/. The implementation of Perlin noise is based on
publicly available from the GitHub repository: https://github.com/pvigier/
perlin-numpy.
Acknowledgements
We appreciate the researchers at ECMWF for their e!orts in collecting, archiv-
ing, disseminating, and maintaining the ERA5 reanalysis dataset, HRES, and
ensemble, without which this study would not have been feasible.
Competing interests
The authors declare no competing interests.
Author Contributions
H.L., Y.Q., and L.C. designed the project. H.L. and Y.Q. managed and
oversaw the project. L.C. performed the model training and evaluation, and
H.L. improved the model design. L.C., X.Z., and H.L. wrote and revised the
manuscript. F.Z, Y.X., and Y.C. established the model training environment.
References
[1] Haiden, T., Janousek, M., Vitart, F., Ben-Bouallegue, Z., Ferranti, L.,
Prates, F.: Evaluation of ECMWF forecasts, including the 2021 upgrade
(2021)
22 FuXi: 15-day global weather forecasts
[2] Magnusson, L., et al.: ECMWF activities for improved hurricane forecasts.
Bull. Am. Meteorol. Soc. 100(3), 445–458 (2019)
[3] Balsamo, G., et al.: Recent progress and outlook for the ECMWF
integrated forecasting system. EGU23 (EGU23-13110) (2023)
[4] Schultz, M.G., et al. : Can deep learning beat numerical weather pre-
diction? Philos. Trans. Royal Soc. A PHILOS T R SOC A 379(2194),
20200097 (2021)
[5] Rasp, S., et al. : Weatherbench: a benchmark data set for data-driven
weather forecasting. J. Adv. Model. Earth Syst. 12(11), 2020–002203
(2020)
[6] Garg, S., Rasp, S., Thuerey, N.: Weatherbench probability: A
benchmark dataset for probabilistic medium-range weather fore-
casting along with deep learning baseline models. Preprint at
https://arxiv.org/abs/2205.00865 (2022)
[7] Hersbach, H., et al. : The era5 global reanalysis. Q. J. R. Meteorol. Soc.
146(730), 1999–2049 (2020)
[8] Rasp, S., Thuerey, N.: Data-driven medium-range weather prediction
with a resnet pretrained on climate simulations: A new model for
weatherbench. J. Adv. Model. Earth Syst. 13(2), 2020–002405 (2021)
[9] Weyn, J.A., Durran, D.R., Caruana, R.: Improving data-driven global
weather prediction using deep convolutional neural networks on a cubed
sphere. J. Adv. Model. Earth Syst. 12(9), 2020–002109 (2020)
[10] Hu, Y., Chen, L., Wang, Z., Li, H.: SwinVRNN: A data-driven ensemble
forecasting model via learned distribution perturbation. J. Adv. Model.
Earth Syst. 15(2), 2022–003211 (2023)
[11] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 770–778 (2016)
[12] Pathak, J., et al.: Fourcastnet: A global data-driven high-resolution
weather model using adaptive fourier neural operators. Preprint at
https://arxiv.org/abs/2202.11214 (2022)
[13] Guibas, J., Mardani, M., Li, Z., Tao, A., Anandkumar, A., Catan-
zaro, B.: Adaptive Fourier Neural Operators: E”cient Token Mixers for
Transformers. Preprint at https://arxiv.org/abs/2111.13587 (2022)
[14] Dosovitskiy, A., et al. : An image is worth 16x16 words: Transformers
FuXi: 15-day global weather forecasts 23
for image recognition at scale. In: International Conference on Learning
Representations (2021)
[15] Chen, L., Du, F., Hu, Y., Wang, F., Wang, Z.: SwinRDM: Integrate Swin-
RNN with Di!usion Model Towards High-Resolution and High-Quality
Weather Forecasting. (2023). Preprint at https://doi.org/10.48448/zn7f-
fc64
[16] Bi, K., et al.: Accurate medium-range global weather forecasting with 3d
neural networks. Nature (2023)
[17] Lam, R., et al.: GraphCast: Learning skillful medium-range global weather
forecasting. Preprint at https://arxiv.org/abs/2212.12794 (2022)
[18] Dueben, P.D., Bauer, P.: Challenges and design choices for global weather
and climate models based on machine learning. Geosci. Model Dev.
11(10), 3999–4009 (2018)
[19] Chen, K., et al.: FengWu: Pushing the Skillful Global Medium-
range Weather Forecast beyond 10 Days Lead. Preprint at
https://arxiv.org/abs/2304.02948 (2023)
[20] Ho, J., et al.: Cascaded di!usion models for high ﬁdelity image generation.
J. Mach. Learn. Res. 23(47), 1–33 (2022)
[21] Li, H., Lin, Z., Shen, X., Brandt, J., Hua, G.: A convolutional neural net-
work cascade for face detection. In: Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pp. 5325–5334 (2015)
[22] Tong, Z., Song, Y., Wang, J., Wang, L.: Videomae: Masked autoen-
coders are data-e”cient learners for self-supervised video pre-training.
In: Advances in Neural Information Processing Systems, vol. 35, pp.
10078–10093 (2022)
[23] Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer Normalization. Preprint at
https://arxiv.org/abs/1607.06450 (2016)
[24] Liu, Z., et al. : Swin transformer: Hierarchical vision transformer using
shifted windows. In: Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision, pp. 10012–10022 (2021)
[25] Liu, Z., et al. : Swin transformer v2: Scaling up capacity and resolu-
tion. In: 2022 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 11999–12009 (2022)
[26] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks
24 FuXi: 15-day global weather forecasts
for biomedical image segmentation. In: International Conference on Med-
ical Image Computing and Computer-assisted Intervention, pp. 234–241
(2015). Springer
[27] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 770–778 (2016)
[28] Wu, Y., He, K.: Group Normalization. Preprint at
https://arxiv.org/abs/1803.08494 (2018)
[29] Elfwing, S., Uchibe, E., Doya, K.: Sigmoid-weighted linear units for neu-
ral network function approximation in reinforcement learning. Neural
Networks 107, 3–11 (2018). Special issue on deep reinforcement learning
[30] Ramachandran, P., Zoph, B., Le, Q.V.: Searching for Activation Func-
tions. Preprint at https://arxiv.org/abs/1710.05941 (2017)
[31] Zeiler, M.D., Krishnan, D., Taylor, G.W., Fergus, R.: Deconvolutional net-
works. In: 2010 IEEE Computer Society Conference on Computer Vision
and Pattern Recognition, pp. 2528–2535 (2010)
[32] Paszke, A., et al. : Automatic di!erentiation in pytorch. In: NIPS 2017
Workshop on Autodi! (2017)
[33] Kingma, D.P., Ba, J.: Adam: A Method for Stochastic Optimization.
Preprint at https://arxiv.org/abs/1412.6980 (2017)
[34] Loshchilov, I., Hutter, F.: Decoupled Weight Decay Regularization.
Preprint at https://arxiv.org/abs/1711.05101 (2017)
[35] Larsson, G., Maire, M., Shakhnarovich, G.: Fractalnet: Ultra-deep neu-
ral networks without residuals. In: International Conference on Learning
Representations (2017)
[36] Zhao, Y., et al.: PyTorch FSDP: Experiences on Scaling Fully Sharded
Data Parallel. Preprint at https://arxiv.org/abs/2304.11277 (2023)
[37] Chen, T., Xu, B., Zhang, C., Guestrin, C.: Training Deep Nets with
Sublinear Memory Cost. Preprint at https://arxiv.org/abs/1604.06174
(2016)
[38] Lorenz, E.N.: Deterministic Nonperiodic Flow. J. Atmos. Sci. 20(2), 130–
148 (1963)
[39] Buizza, R., Milleer, M., Palmer, T.N.: Stochastic representation of model
uncertainties in the ECMWF ensemble prediction system. Q. J. R.
Meteorol. Soc. 125(560), 2887–2908 (1999)
Springer Nature 2021 LATEX template
FuXi: 15-day global weather forecasts 25
[40] Leutbecher, M., Palmer, T.N.: Ensemble forecasting. J. Comput. Phys.
227(7), 3515–3539 (2008)
[41] Gal, Y., Ghahramani, Z.: Dropout as a bayesian approximation: Repre-
senting model uncertainty in deep learning. In: Proceedings of The 33rd
International Conference on Machine Learning. Proceedings of Machine
Learning Research, vol. 48, pp. 1050–1059. PMLR, New York, New York,
USA (2016)
[42] Hersbach, H.: Decomposition of the continuous ranked probability score
for ensemble prediction systems. Weather Forecast 15(5), 559–570 (2000)
[43] Sloughter, J.M., Gneiting, T., Raftery, A.E.: Probabilistic wind speed
forecasting using ensembles and bayesian model averaging. J. Am. Stat.
Assoc. 105(489), 25–35 (2010)
[44] Wilks, D.S.: Statistical Methods in the Atmospheric Sciences vol. 100, 3rd
edn. (2011)
[45] Fortin, V., Abaza, M., Anctil, F., Turcotte, R.: Why should ensemble
spread match the rmse of the ensemble mean? J. Hydrometeorol. 15(4),
1708–1713 (2014)
[46] Magnusson, L., Nycander, J., K¨ all´ en, E.: Flow-dependent versus ﬂow-
independent initial perturbations for ensemble prediction. Tellus A 61(2),
194–209 (2009)
[47] Du, J., Zheng, F., Zhang, H., Zhu, J.: A multivariate balanced ini-
tial ensemble generation approach for an atmospheric general circulation
model. Water 13(2), 122 (2021)
[48] Vitart, F., Robertson, A.W., Anderson, D.: Subseasonal to seasonal pre-
diction project: bridging the gap between weather and climate. npj Clim.
Atmos. Sci. 1(3) (2018)
[49] Robertson, A.W., Vitart, F., Camargo, S.J.: Subseasonal to seasonal pre-
diction of weather to climate with application to tropical cyclones. J.
Geophys. Res. Atmos. 125(6), 2018–029375 (2020)
[50] Chen, L., et al.: Fuxi: A cascade machine learning forecasting system for
15-day global weather forecast (Version 1.0) [Dataset] [Software]. Zenodo.
https://doi.org/10.5281/zenodo.8100201 (2023)