FourCastNet 3: A geometric approach to probabilistic
machine-learning weather forecasting at scale
Boris Bonev 1,*, Thorsten Kurth 1,*, Ankur Mahesh 2,3, Mauro Bisson 1, Jean Kossaiﬁ 1, Karthik
Kashinath1, Anima Anandkumar 4, William D. Collins 2,3, Michael S. Pritchard 1, and Alexander Keller 1
1NVIDIA Corporation, Santa Clara, CA 95051, United States
2Lawrence Berkeley National Laboratory, Berkeley, CA 94720, United States
3University of California, Berkeley, CA 94720, United States
4California Institute of Technology, Pasadena, CA 91125, United States
Abstract
FourCastNet 3 advances global weather modeling by implementing a scalable, geometric machine
learning (ML) approach to probabilistic ensemble forecasting. The approach is designed to respect
spherical geometry and to accurately model the spatially correlated probabilistic nature of the
problem, resulting in stable spectra and realistic dynamics across multiple scales. FourCastNet 3
delivers forecasting accuracy that surpasses leading conventional ensemble models and rivals the best
di!usion-based methods, while producing forecasts 8 to 60 times faster than these approaches. In
contrast to other ML approaches, FourCastNet 3 demonstrates excellent probabilistic calibration
and retains realistic spectra, even at extended lead times of up to 60 days. All of these advances
are realized using a purely convolutional neural network architecture tailored for spherical geometry.
Scalable and e”cient large-scale training on 1024 GPUs and more is enabled by a novel training
paradigm for combined model- and data-parallelism, inspired by domain decomposition methods in
classical numerical models. Additionally, FourCastNet 3 enables rapid inference on a single GPU,
producing a 60-day global forecast at 0.25 °, 6-hourly resolution in under 4 minutes. Its computational
e”ciency, medium-range probabilistic skill, spectral ﬁdelity, and rollout stability at subseasonal
timescales make it a strong candidate for improving meteorological forecasting and early warning
systems through large ensemble predictions.
1 Introduction
Numerical weather prediction (NWP) is central to modern meteorology, underpinning our ability to
accurately understand and forecast atmospheric phenomena [ 1]. Advances in mathematical modeling,
computational power, and data assimilation have made NWP essential for weather forecasting, hazard
mitigation, energy management, and climate studies.
Traditional NWP models, however, are computationally intensive, limiting their ability to deliver rapid,
large-scale probabilistic forecasts. Recently, machine learning (ML) approaches have surpassed traditional
NWP in forecast skill and speed, enabling rapid generation of large ensembles and opening new possibilities
for weather and climate prediction [ 2–6]. These advances support improved sampling of rare events and
longer-range forecasts [ 7–9]. Despite these beneﬁts, ML models face challenges: they may struggle with
out-of-distribution events, physical consistency, and long-term stability [ 10–14]. Furthermore, commonly
used evaluation metrics fail to fully capture the accuracy with which these models approximate the
underlying dynamical systems [ 15]. Hybrid models that combine ML and traditional NWP o!er partial
solutions [ 16], but su!er from the same computational bottlenecks due to the Courant-Friedrichs-Lewy
(CFL) condition, making them expensive to evaluate, especially at high resolution. Additionally,
deterministic ML models often exhibit excessive smoothing, which are closer to ensemble averages,
lacking the ﬁdelity of traditional deterministic forecasts [ 17–19].
Recently, probabilistic approaches have aimed to address the latter problem [ 6, 20–22]. GenCast [ 6],
the state-of-the-art probabilistic ML model, has proven that a denoising di!usion model approach [ 23]
is e!ective at modeling the probabilistic nature of atmospheric phenomena. However, this comes at a
signiﬁcant cost overhead during inference, due to the iterative nature of denoising. Lang et al. [22] use
a scoring rule based objective function instead [ 24], with the implied computational beneﬁts over the
*Equal contribution. Correspondence to {bbonev, tkurth }@nvidia.com.
Code available at: https://github.com/NVIDIA/makani
arXiv:2507.12144v2  [cs.LG]  18 Jul 2025
FourCastNet 3
Figure 1: Schematic of the FourCastNet 3 model. The model predicts the state of the atmosphere at the
next timestep, given the state at the previous timestep. Auxiliary variables such as the cosine zenith
angle are computed from analytical expressions for each timestep and appended to the input. A hidden
Markov model is obtained by conditioning FourCastNet 3 on a stochastic latent variable whose temporal
dynamics are governed by a di!usion process on the sphere. The model itself is formed by an encoder, a
decoder and 8 neural operator blocks. Each of these operations can be grouped into local, global and
pointwise operations and therefore be formulated on arbitrary grids and resolutions, making FourCastNet
3 discretization independent. Green boxes illustrate learnable operations.
di!usion approach. While e!ectively addressing blurring, both approaches lead to build-up of small-scale
noise, requiring an ad-hoc truncation strategy in the latter case to suppress it. This build-up can be a
precursor to blow-up in NWP models [ 25] and attaining stable spectra remains a key challenge.
Most of today’s leading ML weather models repurpose mature architectures such as transformers and graph-
neural-networks that were fundamentally developed for other scientiﬁc ML tasks [ 26, 27]. This pragmatic
approach enables competitive medium-range skill, as demonstrated by numerous models, with little to
disambiguate between them [ 17]. As going beyond medium-range forecasts requires additional properties
beyond medium-range skill, bespoke geometric approaches o!er a simple and elegant alternative. These
methods are faithful to the underlying geometry, its topology and the symmetries of the underlying physics.
However, bespoke methods come with signiﬁcant engineering challenges, requiring custom implementations
and engineering frameworks to achieve the necessary scale of model training that is in turn needed to
achieve competitive skill [ 28].
F ourCastNet 3 We introduce FourCastNet 3 (FCN3), a skillful, probabilistic ML weather forecasting
system built as a hidden Markov model based on spherical signal processing primitives and a probabilistic
loss function in the spectral domain. Our method is purely convolutional, leveraging both local and global
spherical convolution kernels to better model the physical processes at various scales involved in weather
phenomena, while respecting spherical geometry and its inherent symmetries, enabling realistic ensemble
members.
FCN3 is trained end-to-end as an ensemble forecasting model, at scale. This approach retains the large
speed-ups o!ered by ML models, facilitating one-step generation of ensemble members, making it both
computationally e”cient and accurate. To enable training of a large FCN3 model, on a large hourly dataset
with multiple ensemble members, and multiple timesteps, we develop a hybrid machine learning paradigm
for simultaneous model- and data-parallelism, inspired by traditional NWP methods. The computational
domain is decomposed to simultaneously distribute both the model and the data during training. This is
combined with distributed batch- and ensemble-parallelism, resulting in extremely e”cient and scalable
training, which enabled seamlessly scaling training to over 1000 GPUs.
2
FourCastNet 3
FCN3 outperforms the integrated forecasting system’s ensemble method (IFS-ENS) [ 29], the golden
standard for traditional NWP methods, and nearly matches the medium-range forecast skill of GenCast [ 6],
the leading probabilistic ML weather model, at double the temporal resolution. A single forecast of 15
days is computed in 60 seconds on a single NVIDIA H100 GPU - a speedup of 8x over GenCast and 60x
over IFS-ENS. Simultaneously, it o!ers the key beneﬁt of retaining stable predictions and accurate spectra
well into the subseasonal range with lead times of up to 60 days. This key achievement mitigates the
issue of blurring and addresses the issue of build-up of small-scale noise. The probabilistic skill, stability,
spectral ﬁdelity and low inference cost make FCN3 an interesting model with the potential of generating
large ensembles with potential applications spanning medium-range to subseasonal forecasting.
2 Probabilistic forecasts with hidden Markov models
FourCastNet 3 (FCN3) is formulated as a probabilistic model to address the chaotic nature of atmospheric
phenomena. Given the current atmospheric state un on a 0 .25→ grid at a time tn, it predicts the state
un+1 = Fω (un,t n,z n) at the next time step tn+1, 6 hours into the future. Stochasticity is introduced
through a hidden Markov model approach, where the model takes an extra conditioning input zn -a
random noise vector drawn from a number of spherical di!usion processes with di!erent length- and
timescales [ 30]. Figure 1 depicts this setup, and a detailed description is found in Appendix A.
The parameters ω of the model Fω are optimized with the aim of accurately approximating atmospheric
processes and matching the observed spatio-temporal distributions of physical variables. FCN3 uses an
end-to-end ensemble training approach, minimizing a composite probabilistic loss function (48) based
on the continuously ranked probability score (CRPS) (47). This objective compares the predictive
ensemble of marginals to ground-truth observations. Although training with the CRPS objective has been
shown to produce models with high predictive skill, these models have not generated ensemble members
with physically accurate spectra that correctly capture spatial correlations [ 16, 20, 22]. Although the
scalar-valued Continuous Ranked Probability Score (CRPS) (40) is a proper scoring rule – meaning it is
uniquely minimized when the predictive distribution matches the target distribution – this property does
not extend to summary scores that aggregate individual CRPS values across marginals, as is commonly
done when forecasting spatial or multivariate variables. This is particularly problematic for multi-variate
spatial processes, where the CRPS can be minimized in a point-wise manner by an unphysical ensemble.
To address this issue, we combine the spatial, point-wise CRPS loss term with a loss term in the spectral
domain. A similar approach using a low-pass ﬁltered spectral loss term has previously been adopted
by Kochkov et al. [16], but failed to accurately capture the high-frequency behavior of the underlying
processes. Our approach weights spectral coe”cients according to their multiplicity and enforces a good
match of the their distributions across all wavelengths. A detailed discussion of the objective function and
its motivation are provided in Appendix E.1.
3 Spherical neural operator architecture
Although a combined spectral and spatial probabilistic loss function encourages the learned operator to
be accurately represented across scales, the concrete parameterization is equally important in determining
the space of learnable operators and therefore their properties. As such, we choose a geometric approach
grounded in signal processing principles and symmetry considerations:
FCN3 is a spherical neural operator architecture and relies heavily on local and global spherical group
convolutions. More precisely, global convolution ﬁlters are parameterized in the spectral domain by
leveraging the convolution theorem on the sphere and the associated spherical harmonic transform (SHT)
[10]. This approach resembles classical pseudo-spectral methods such as IFS, which compute the PDE
operator in the spectral domain. Additionally, we employ spherical group convolutions 1 with learnable,
locally supported kernels. This is implemented using the framework for discrete-continuous (DISCO)
convolutions on the sphere [ 31, 32], which formulate the convolution in the continuous domain and
approximate the integral with a quadrature rule. This formulation enables anisotropic ﬁlters that are
better suited to approximate atmospheric phenomena such as adiabatic ﬂow conﬁned to vertically tilted
isentropes with characteristic morphology, or blocked ﬂow around topographic features. The localized
convolutional approach also resembles ﬁnite di!erencing - another building block encountered in most
classical NWP models.
Building on these convolutional principles, the overall FCN3 architecture is organized into three main
components: an encoder, a processor composed of several spherical neural operator blocks, and a decoder
(see Figure 1). These blocks adopt the structure of the popular ConvNeXt architecture [ 33], which
contain a convolution, a GeLU activation function [ 34], a point-wise multi-layer perceptron (MLP) and
1Group convolutions are convolutions formulated w.r.t. a symmetry group. For the two-dimensional sphere,
this is the rotation group of three-dimensional rotations SO(3).
3
FourCastNet 3
a skip connection. We deliberately omit layer normalization, motivated by the importance of absolute
magnitudes in physical processes. The convolution ﬁlters are either parameterized in the spectral domain
or as approximately spherically equivariant local convolutions [ 10, 32]. In the latter case, we choose
smooth localized ﬁlter functions, parameterized by linear combinations of Morlet wavelets on a disk.
Through experimentation, we ﬁnd that a ratio of four local blocks to one global block yields the best
forecast skill. The encoder layer is comprised of a single local spherical convolutions and down-samples
the 721 → 1440 input/output signals to a latent representation on a 360 → 720 Gaussian grid with an
embedding dimension of 641. The decoder uses a combination of bilinear spherical interpolation and local
spherical convolution to up-sample the latent signals to the native resolution while mitigating aliasing
errors. Both encoder and decoder encode do not perform any channel mixing and instead encode input
signals separately, to avoid the mixing of signals with vastly di!erent spectral properties. Finally, water
channels are passed through a smooth, spline-based output activation function which constrains them to
positive values, while reducing the amount of high-frequency noise introduced through the non-linearity.
In contrast to most ML weather models which predict tendencies, i.e. the di!erence between the prediction
and the input, FCN3 predicts the next state directly. Empirically, we ﬁnd that this approach works better
in avoiding the build-up of high-frequency artifacts. Moreover, predicting tendencies may be interpreted as
restricting the model to Euler time-stepping, which may adversely a!ect the space of learnable operators
[22]. A detailed account of signal-processing considerations on the sphere and our ﬁlter parameterizations
is provided in Appendix B. Furthermore, architectural choices and hyperparameters are discussed in detail
in Appendix C.
4 Scalable training through hybrid parallelism
Training models with large internal representations such as FCN3 requires more memory than what is
available on a single GPU for their forward and backward passes. This memory requirement is further
exacerbated by autoregressive rollouts, where multiple forward and backward passes need to be ﬁt into
GPU memory. These considerations limit the size of the model to the memory available per GPU, and thus
set the maximum scale for most models. While some models such as GraphCast use gradient checkpoint
to enable the memory-intensive training [ 4], this comes with the signiﬁcant downside of trading memory
for compute, increasing already long iteration times further in training.
By distributing models across multiple GPUs, model parallelism o!ers an alternative path for practitioners
to reduce the memory requirements and train much bigger models. This approach greatly improved the
ﬁdelity and performance of modern ML models [ 35–37] and is the foundation of the success of current large
language models (LLM) such as ChatGPT 4 [ 38], Llama 3 [ 39], and others. Neural networks generally
scale well with available data and oftentimes, training larger models comes with an increase in skill, as
long as more training data is available [ 28]. This creates a unique challenge for scientiﬁc ML methods,
where the training data is often high-dimensional, in comparison to language modeling or computer vision
tasks. In the case of FCN3, a typical sample at 0 .25→ resolution consists of 721 → 1440 ﬂoating points per
variable, and multiple tens of variables are normally used for skillful predictions. This renders ML driven
weather prediction considerably more data-intensive than many other ML tasks.
Model parallelism is inspired by classical numerical methods, where not only the model and weights are
split across ranks, but also the data which the model processes. Model parallelism is typically achieved
through feature-space parallelism, i.e. by splitting the feature maps across multiple GPU. This approach
is heavily used in modern distributed LLMs, alongside other parallelism paradigms such as pipeline- and
traditional batch-parallelism. To enable the training of FCN3, we implement spatial model parallelism
(also referred to as domain parallelism), where both the model and data are split across ranks by employing
a spatial domain decomposition. This approach is inspired by traditional distributed scientiﬁc computing
applications and requires the implementation of distributed variants of all spatial algorithms (see Figure 2).
Besides these two approaches as well as traditional batch parallelism, another approach is to split members
of the same forecasting ensemble across multiple GPU. This variant of data parallelism is highly e”cient
because di!erent ensemble members are computationally independent until the loss computation, which
usually requires some communication across the ensemble group of GPUs.
The training of FCN3 requires spatial model parallelism via domain decomposition as well as ensemble
and batch parallelism. We will refer to the former as model and the latter two as data parallelism. We
have implemented all of these features in Makani, a framework for large-scale distributed training of ML
based weather models. For a more detailed description of parallelization features, cf. section G.
This paradigm enables us to train large principled models by scaling training to thousands of GPUs and
more. FCN3 is trained on historic atmospheric ERA5 reanalysis data ranging from 1980 to 2016. ERA5
is a multi-decadal, self-consistent record and represents our best understanding of Earth’s atmospheric
system [ 40]. Training is split into stages, thereby forming a curriculum training approach. The initial
pre-training phase focuses on the model’s 6-hourly prediction skill, by utilizing all hourly samples from the
ERA5 training dataset, constructing 6 hour lead time input-target-pairs that start at each discrete UTC
4
FourCastNet 3
Figure 2: Illustration of model- and data-parallelism for training of FourCastNet 3. In the given example,
the input data is spatially distributed across four ranks (green boxes) by splitting it across the latitude.
This reduces the memory footprint of the input, prediction and activations within the network. The
training data is read in a sharded fashion from the distributed ﬁle system, simultaneously lowering the
required I/O per rank. This domain-decomposition requires the model and it’s weights to be distributed;
spherical harmonic transforms and discrete-continuous convolutions are distributed and split across the
four ranks. In addition to the spatial model-parallelism, data-parallelism is utilized to distribute individual
ensemble members and batch samples (grey boxes). Finally, the ensemble loss for a single sample is
computed by taking the entire ensemble information across ensemble parallel ranks and spatial parallel
ranks. On top of this, batch parallelism is utilized (not illustrated in this ﬁgure).
hour. The model is trained for 208,320 gradient descent steps on this dataset with a batch size of 16 and
an ensemble size of 16. This initial training stage was carried out on 1024 NVIDIA H100 on the NVIDIA
Eos Supercomputer for a total of 78 hours. In the second pre-training phase, the model is trained on
6-hourly initial conditions using 4 autoregressive rollout steps. This is performed for 5,040 steps while
lowering the learning rate every 840 steps. The second pre-training stage took 15 hours on 512 NVIDIA
A100 GPUs to complete and was carried out on the NERSC Perlmutter system. The ﬁnal is ﬁne-tuned on
6-hourly samples ranging from 2012 to 2016 to account for potential drifts in the distribution and improve
performance on data that lie in the near- to medium-term future. This ﬁnal stage is carried out on 256
NVIDIA H100 GPUs on the Eos system and took 8 hours to complete. As a single model instance does
not ﬁt on a 80Gb VRAM GPU, we leverage the previously described spatial parallelism, splitting the
data and the model. This ranges from a 4-fold split in pretraining to a 16-fold split during ﬁnetuning, due
to the increased memory requirements from autoregressive training. Details of the training methodology
and setup are outlined in Appendix E.
5R e s u l t s
Key performance scores of FCN3 such as continuously ranked probability score (CRPS) and ensemble-mean
RMSE are averaged over 12-hourly initial conditions in the out-of-sample year 2020 and reported in
Figure 3. FCN3 beats the gold-standard physics-based NWP model IFS-ENS by a margin that is virtually
indistinguishable from GenCast, the state-of-the-art data-driven weather model. Our approach enables
direct, one-step generation of ensemble members and can generate a single 15-day forecast at a temporal
resolution of 6 hours and a spatial resolution of 0 .25→ in a matter of 60 seconds on a single NVIDIA H100
GPU. In comparison, a 15-day forecast of GenCast takes 8 minutes on a Cloud TPU v5 instance (at half
the temporal resolution) [ 6], and an IFS forecast takes about one hour on 96 AMD Epyc Rome CPUs (at
5
FourCastNet 3
Figure 3: Probabilistic skill of FourCastNet 3 relative to the ERA5 ground truth. Continuously ranked
probabilistic scores (lower is better), ensemble mean RMSE (lower is better), spread-skill ratios (closer
to one is better) and rank-histograms (more uniform is better) are reported from top to bottom. The
scores are computed over 12-hourly initial conditions ranging from 2020-01-01 00:00:00 UTC to 2020-12-31
23:59:00 UTC.
9km operational resolution) [ 41]. Barring the di!erences in hardware and resolution, this constitutes a
speed-up of ↑8x over GenCast and a speed-up of ↑60x over IFS-ENS.
Crucially, the 50-member FCN3 ensemble forecast is well-calibrated with spread-skill ratios approaching
1, indicating interchangeability between observations and ensemble members in the forecast. This is
conﬁrmed via rank-histograms, which report the frequencies of the ordinal ranks of the observation within
the predictive ensemble. The temporal evolution of the rank histograms closely mirrors the spread-skill
ratios, indicating a slightly over-dispersive ensemble at short lead times of up to 24 hours, which then
becomes under-dispersive and then gradually relaxes to a ﬂat rank-histogram. These results are especially
encouraging, given that the evaluated 50 member ensemble is larger than the 16 ensemble members used
in training, indicating that even larger ensembles are justiﬁable to test during inference.
It is important to investigate case studies, since scores such as ensemble-mean RMSE and CRPS are
incomplete metrics that alone do not provide a comprehensive view of a probabilistic weather forecast.
For example, the CRPS score only evaluates the accuracy of the predictive distribution point-wise and
does not take tempo-spatial correlations into account. As such, a perfect forecast from the ground-truth
distribution, which is scrambled by shu#ing the ensemble members at each point will result in unphysical
predictions yet still retain the optimal CRPS score. Similarly, RMSE scores can be easily improved by
blurring forecasts, rendering them useless for all practical purposes. A key challenge in data-driven weather
models is to reproduce the physical ﬁdelity of traditional NWP models and reduce possible spurious
correlations that stem from the data-driven approach.
Figure 4 examines a case study showing wind intensities at 850hPa and geopotential height at 500hPa of a
FCN3 forecast initialized on 2020-02-11 at 00:00:00 UTC, 48 hours before the extra-tropical storm Dennis
made its landfall over Ireland and the British Isles. The close-up plots in Figure 4 indicate that FCN3 is
capable of faithfully simulating this event, reproducing both realistic wind intensities and appropriate
co-variation of ﬂow with the pressure ﬁeld. This is conﬁrmed in the angular power spectral density (PSD)
of the 500hPa geopotential height, reported in the bottom row. FCN3 retains perfectly the correct slopes
in the power spectra, a desirable property towards better ML weather models with high physical ﬁdelity.
Even at long lead times of 30 days, we observe no apparent degradation of the angular power spectra and
predictions retain their e!ective resolution, remaining sharp even at long lead times.
6
FourCastNet 3
Figure 4: FourCastNet 3 prediction of storm Dennis initialized on 2020-02-11 at 00:00:00 UTC. The
plot depicts wind-speeds at a pressure level of 850hPa and isohypses (height contours) of the 500hPa
geopotential height. FCN3 accurately predicts the storm and its landfall 5 days in advance, with di!erent
ensemble members depicting di!erent scenarios. FCN3 skillfully predicts global weather phenomena at a
spatial resolution of 0 .25→ and a temporal resolution of 6 hours. FCN3 exhibits exceptionally accurate
and stable spectra even after extended rollouts of 30 days (720 hours) and more.
The spectral ﬁdelity of FCN3 is also observed in Figure 5, which depicts power spectral densities averaged
over the entire evaluation year of 2020 and the respective relative error w.r.t. the angular power spectrum
of the ERA5 ground truth. Even at high wavenumbers, we observe that the relative error remains bounded
with deviations ranging from ↓0.2t o0 .2.
We postulate that the spectral properties are a result of our careful architectural design choices, which
reﬂect geometrical and signal-processing principles, and the combined CRPS loss function which enforces
the correct local and global distribution, thus encouraging the model to learn the correct spatial correlations.
Competing, deterministic ML weather models typically display a decay of high-frequency information,
which appears as blurring. Even the CRPS-trained hybrid weather model NeuralGCM shows signiﬁcant
blurring in high-frequency modes. Moreover, newer, probabilistic ML weather models such as GenCast
[6] and AIFS-CRPS [ 22] cannot faithfully retain the correct spectral signatures and show a build-up of
high-frequency modes, as illustrated in Figure 4. In traditional NWP models, such build-ups can be a
precursor to an imminent blow-up [ 25]. As such, this constitutes a major milestone towards physically
faithful data-driven, probabilistic weather models, which can be e”ciently evaluated even at longer lead
times.
Additional evaluation of FCN3, angular and zonal power spectral densities, alongside physical consistency
tests, are provided in Appendix F. The detailed evaluation conﬁrms that FCN3 is a probabilistically
skillful, computationally e”cient global weather model, with unprecedented spectral ﬁdelity and a high
degree of physical realism. Forecasts remain stable well into the subseasonal range of 60 days, thus paving
the way toward subseasonal forecasts and large ensembles at these lead times.
6 Conclusions
We present FourCastNet 3 (FCN3), a novel probabilistic weather forecasting model that leverages
spherical signal processing and a hidden-Markov ensemble formulation, trained end-to-end with a
probabilistic objective in both spectral and spatial domains. FCN3 achieves skillful and computationally
e”cient forecasts, outperforming traditional numerical weather prediction (NWP) methods and matching
7
FourCastNet 3
Figure 5: Comparison of angular power spectral densities of a single FourCastNet 3 ensemble member at
a lead time of 360 hours to the ERA5 ground truth. Power spectral densities are averaged over 12-hourly
initial conditions ranging from 2020-01-01 00:00:00 UTC to 2020-12-31 23:59:00 UTC.
the performance of state-of-the-art di!usion models at a fraction of the computational cost. This is
accomplished using a purely convolutional architecture based on spherical group convolutions, in contrast
to the prevailing transformer-based approaches. Notably, FCN3 generates physically realistic spectra across
all wavelengths up to the cuto! in the training data, avoiding the overly smooth or spurious high-frequency
artifacts that challenge other machine learning models. This ﬁdelity enables stable, sharp forecasts even
at extended lead times of up to 60 days, positioning FCN3 as a promising tool for subseasonal prediction
with large ensembles.
FCN3 introduces major computational and practical improvements that make large-scale, high-resolution
ensemble forecasting more accessible than ever. Its massively parallel training workﬂows, model and
ensemble parallelism, and low inference cost enable rapid, e”cient production of large ensemble forecasts.
In-situ diagnostics and scoring can be performed during model execution, eliminating the need to store
terabytes of data and removing storage and I/O bottlenecks that have historically limited ensemble
analysis. All key components, including training and inference code, are fully open-source, providing the
research community with transparent, reproducible tools for both operational and experimental weather
prediction.
As an ensemble model, FCN3 enables detailed exploration of multiple plausible future weather scenarios
from a single initialization, making it a powerful tool for studying atmospheric dynamics, predictability, and
the statistics of low-probability, high-impact events. Looking ahead, we plan to extend FCN3 to include
precipitation as a diagnostic output and to integrate data assimilation uncertainty, further broadening its
applicability and impact. Together, these innovations position FCN3 as a robust, e”cient, and extensible
foundation for next-generation probabilistic weather forecasting and atmospheric science research.
Data and materials availability
FourCastNet 3’s training code is available in Makani, a training framework used for scale training of
ML weather models to 1000s of GPUs. It is openly available at https://github.com/NVIDIA/makani
under the Apache License 2.0. The ERA5 training data is openly available at https://cds.climate.
copernicus.eu/datasets/reanalysis-era5-single-levels . Finally, torch-harmonics, our library
for machine-learning and di!erentiable signal processing on the sphere, is openly available at https:
//github.com/NVIDIA/torch-harmonics under the BSD-3-Clause license.
Acknowledgements
This research was supported by NVIDIA as well as the Director, O”ce of Science, O”ce of Biological and
Environmental Research of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231
and by the Regional and Global Model Analysis Program area within the Earth and Environmental
Systems Modeling Program. The research used resources of the National Energy Research Scientiﬁc
Computing Center (NERSC), also supported by the O”ce of Science of the U.S. Department of Energy,
under Contract No. DE-AC02-05CH11231. The computation for this paper was supported in part by
the DOE Advanced Scientiﬁc Computing Research (ASCR) Leadership Computing Challenge (ALCC)
8
FourCastNet 3
2024-2025 award ‘Huge Ensembles of Weather Extremes using the Fourier Forecasting Neural Network’ to
William Collins (LBNL).
We are grateful to our colleagues Dmitry Alexeev, Noah Brenowitz, Alberto Carpentieri, Dale Durran,
Dallas Foster, Peter Harrington, Jan Kautz, Marius Koch, Jussi Leinonen, Morteza Mardani, Steve
Marschner, Peter Messmer, Thomas M¨ uller, Merlin Nimier-David, Andrea Paris, Jaideep Pathak, Suman
Ravuri, Ira Shokar, Shashank Subramanian for helpful and encouraging discussions, as well as feedback
on our work.
We thank the European Centre of Medium-Range Weather Forecasting (ECMWF) for publishing the
ERA5 dataset and enabling this line of research.
References
[1] Peter Lynch. The origins of computer weather prediction and climate modeling.
Journal of Computational Physics , 227:3431–3444, 2008. ISSN 0021-9991.
doi:https://doi.org/10.1016/j.jcp.2007.02.034. URL https://www.sciencedirect.com/science/
article/pii/S0021999107000952. Predicting weather, climate and extreme events.
[2] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay,
Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, Pedram
Hassanzadeh, Karthik Kashinath, and Animashree Anandkumar. FourCastNet: A Global Data-driven
High-resolution Weather Model using Adaptive Fourier Neural Operators, 2 2022. URL http:
//arxiv.org/abs/2202.11214.
[3] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Pangu-Weather:
A 3D High-Resolution Model for Fast and Accurate Global Weather Forecast, 11 2022. URL
http://arxiv.org/abs/2211.02556.
[4] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato,
Alexander Pritzel, Suman Ravuri, Timo Ewalds, Ferran Alet, Zach Eaton-Rosen, Weihua Hu,
Alexander Merose, Stephan Hoyer, George Holland, Jacklynn Stott, et al. GraphCast: Learning
skillful medium-range global weather forecasting, 12 2022. URL http://arxiv.org/abs/2212.12794.
[5] Thorsten Kurth, Shashank Subramanian, Peter Harrington, Jaideep Pathak, Morteza Mardani, David
Hall, Andrea Miele, Karthik Kashinath, and Animashree Anandkumar. FourCastNet: Accelerating
Global High-Resolution Weather Forecasting using Adaptive Fourier Neural Operators, 2022. URL
https://arxiv.org/abs/2208.05419.
[6] Ilan Price, Alvaro Sanchez-Gonzalez, Ferran Alet, Tom R. Andersson, Andrew El-Kadi, Dominic
Masters, Timo Ewalds, Jacklynn Stott, Shakir Mohamed, Peter Battaglia, Remi Lam, and Matthew
Willson. Probabilistic weather forecasting with machine learning. Nature, 1 2024. ISSN 14764687.
doi:10.1038/s41586-024-08252-9.
[7] Ankur Mahesh, William Collins, Boris Bonev, Noah Brenowitz, Yair Cohen, Joshua Elms, Peter
Harrington, Karthik Kashinath, Thorsten Kurth, Joshua North, Travis OBrien, Michael Pritchard,
David Pruitt, Mark Risser, Shashank Subramanian, and Jared Willard. Huge Ensembles Part I:
Design of Ensemble Weather Forecasts using Spherical Fourier Neural Operators, 8 2024. URL
http://arxiv.org/abs/2408.03100.
[8] Ankur Mahesh, William Collins, Boris Bonev, Noah Brenowitz, Yair Cohen, Peter Harrington,
Karthik Kashinath, Thorsten Kurth, Joshua North, Travis OBrien, Michael Pritchard, David Pruitt,
Mark Risser, Shashank Subramanian, and Jared Willard. Huge Ensembles Part II: Properties of
a Huge Ensemble of Hindcasts Generated with Spherical Fourier Neural Operators, 8 2024. URL
http://arxiv.org/abs/2408.01581.
[9] Jonathan A. Weyn, Dale R. Durran, Rich Caruana, and Nathaniel Cresswell-Clay. Sub-seasonal
forecasting with a large ensemble of deep-learning weather prediction models, 2 2021. URL http:
//arxiv.org/abs/2102.05107http://dx.doi.org/10.1029/2021MS002502.
[10] Boris Bonev, Thorsten Kurth, Christian Hundt, Jaideep Pathak, Maximilian Baust, Karthik
Kashinath, and Anima Anandkumar. Spherical Fourier Neural Operators: Learning Stable Dynamics
on the Sphere. Proceedings of the 40th International Conference on Machine Learning , 202:2806–2823,
6 2023. URL http://arxiv.org/abs/2306.03838.
[11] Oliver Watt-Meyer, Gideon Dresdner, Jeremy McGibbon, Spencer K. Clark, Brian Henn, James
Duncan, Noah D. Brenowitz, Karthik Kashinath, Michael S. Pritchard, Boris Bonev, Matthew E.
Peters, and Christopher S. Bretherton. ACE: A fast, skillful learned global atmospheric model for
climate prediction. 10 2023. URL http://arxiv.org/abs/2310.02074.
[12] Matthias Karlbauer, Nathaniel Cresswell-Clay, Dale R. Durran, Raul A. Moreno, Thorsten Kurth,
Boris Bonev, Noah Brenowitz, and Martin V. Butz. Advancing Parsimonious Deep Learning Weather
9
FourCastNet 3
Prediction Using the HEALPix Mesh. Journal of Advances in Modeling Earth Systems , 16, 8 2024.
ISSN 19422466. doi: 10.1029/2023MS004021.
[13] Haiwen Guan, Troy Arcomano, Ashesh Chattopadhyay, and Romit Maulik. Lucie: A lightweight
uncoupled climate emulator with long-term stability and physical consistency for o(1000)-member
ensembles, 4 2025. URL http://arxiv.org/abs/2405.16297.
[14] Nathaniel Cresswell-Clay, Bowen Liu, Dale Durran, Zihui Liu, Zachary I. Espinosa, Raul Moreno,
and Matthias Karlbauer. A deep learning earth system model for e”cient simulation of the observed
climate, 2 2025. URL http://arxiv.org/abs/2409.16247.
[15] Zhou Fang and Gianmarco Mengaldo. Dynamical errors in machine learning forecasts, 4 2025. URL
http://arxiv.org/abs/2504.11074.
[16] Dmitrii Kochkov, Janni Yuval, Ian Langmore, Peter Norgaard, Jamie Smith, Gri”n Mooers,
Milan Kl¨ ower, James Lottes, Stephan Rasp, Peter D¨ uben, Sam Hatﬁeld, Peter Battaglia, Alvaro
Sanchez-Gonzalez, Matthew Willson, Michael P. Brenner, and Stephan Hoyer. Neural General
Circulation Models for Weather and Climate, 11 2023. URL http://arxiv.org/abs/2311.
07222http://dx.doi.org/10.1038/s41586-024-07744-y .
[17] Stephan Rasp, Stephan Hoyer, Alexander Merose, Ian Langmore, Peter Battaglia, Tyler Russel, Alvaro
Sanchez-Gonzalez, Vivian Yang, Rob Carver, Shreya Agrawal, Matthew Chantry, Zied Ben Bouallegue,
Peter Dueben, Carla Bromberg, Jared Sisk, et al. WeatherBench 2: A benchmark for the next
generation of data-driven global weather models, 8 2023. URL http://arxiv.org/abs/2308.15560.
[18] Noah D. Brenowitz, Yair Cohen, Jaideep Pathak, Ankur Mahesh, Boris Bonev, Thorsten Kurth,
Dale R. Durran, Peter Harrington, and Michael S. Pritchard. A Practical Probabilistic Benchmark
for AI Weather Models. 1 2024. URL http://arxiv.org/abs/2401.15305.
[19] Christopher Subich, Syed Zahid Husain, Leo Separovic, and Jing Yang. Fixing the Double Penalty in
Data-Driven Weather Forecasting Through a Modiﬁed Spherical Harmonic Loss Function, 1 2025.
URL http://arxiv.org/abs/2501.19374.
[20] Christian Lessig, Ilaria Luise, Bing Gong, Michael Langguth, Scarlet Stadtler, and Martin Schultz.
AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning, 8
2023. URL http://arxiv.org/abs/2308.13280.
[21] Martin Andrae, Tomas Landelius, Joel Oskarsson, and Fredrik Lindsten. Continuous Ensemble
Weather Forecasting with Di!usion models, 10 2024. URL http://arxiv.org/abs/2410.05431.
[22] Simon Lang, Mihai Alexe, Mariana C. A. Clare, Christopher Roberts, Rilwan Adewoyin, Zied Ben
Bouall` egue, Matthew Chantry, Jesper Dramsch, Peter D. Dueben, Sara Hahner, Pedro Maciel,
Ana Prieto-Nemesio, Cathal O’Brien, Florian Pinault, Jan Polster, et al. AIFS-CRPS: Ensemble
forecasting using a model trained with a loss function based on the Continuous Ranked Probability
Score, 12 2024. URL http://arxiv.org/abs/2412.15832.
[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Di!usion Probabilistic Models, 6 2020. URL
http://arxiv.org/abs/2006.11239.
[24] Tilmann Gneiting and Adrian E Raftery. Strictly Proper Scoring Rules, Prediction, and
Estimation. Journal of the American Statistical Association , 102:359–378, 3 2007. ISSN
0162-1459. doi: 10.1198/016214506000001437. URL http://www.tandfonline.com/doi/abs/10.
1198/016214506000001437.
[25] Peter Lauritzen, Christiane Jablonowski, Mark Taylor, and Ramachandran Nair. Numerical techniques
for global atmospheric models . Lecture notes in computational science and engineering ; 80. Springer,
Berlin ;, 2011. ISBN 9783642116391.
[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention Is All You Need. Advances in Neural Information Processing
Systems, 6 2017. URL http://arxiv.org/abs/1706.03762.
[27] Tobias Pfa!, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W. Battaglia. Learning mesh-based
simulation with graph networks. 6 2021. URL http://arxiv.org/abs/2010.03409.
[28] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Je!rey Wu, and Dario Amodei. Scaling laws for neural language models. 1
2020. URL http://arxiv.org/abs/2001.08361.
[29] Thomas Haiden, Martin Janousek, Fr´ ed´ eric Vitart, Maliko Tanguy, Fernando Prates, and Matthieu
Chevalier. Evaluation of ecmwf forecasts. ECMWF Newsletter , 2024. doi: 10.21957/52F2F31351. URL
https://www.ecmwf.int/en/elibrary/81582-evaluation-ecmwf-forecasts .
[30] T N Palmer, R Buizza, F Doblas-Reyes, T Jung, M Leutbecher, G J Shutts, M Steinheimer, and
A Weisheimer. Stochastic Parametrization and Model Uncertainty. Technical report, ECMWF, 2009.
URL http://www.ecmwf.int/publications/.
10
FourCastNet 3
[31] Jeremy Ocampo, Matthew A. Price, and Jason D. McEwen. Scalable and Equivariant Spherical CNNs
by Discrete-Continuous (DISCO) Convolutions, 9 2022. URL http://arxiv.org/abs/2209.13603.
[32] Miguel Liu-Schia”ni, Julius Berner, Boris Bonev, Thorsten Kurth, Kamyar Azizzadenesheli, and
Anima Anandkumar. Neural Operators with Localized Integral and Di!erential Kernels, 2 2024. URL
http://arxiv.org/abs/2402.16845.
[33] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
A ConvNet for the 2020s, 1 2022. URL http://arxiv.org/abs/2201.03545.
[34] Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). 6 2016. URL http:
//arxiv.org/abs/1606.08415.
[35] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Hongjun Choi, Blake A. Hechtman, and Shibo
Wang. Automatic Cross-Replica Sharding of Weight Update in Data-Parallel Training. CoRR,
abs/2004.13336, 2020. URL https://arxiv.org/abs/2004.13336.
[36] Je! Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. DeepSpeed: System
Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. In
Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining, KDD ’20, page 3505–3506, New York, NY, USA, 2020. Association for Computing Machinery.
ISBN 9781450379984. doi: 10.1145/3394486.3406703. URL https://doi.org/10.1145/3394486.
3406703.
[37] Samyam Rajbhandari, Je! Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO:
Memory Optimizations Toward Training Trillion Parameter Models. ArXiv,
May 2020. URL https://www.microsoft.com/en-us/research/publication/
zero-memory-optimizations-toward-training-trillion-parameter-models/ .
[38] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red A vila, Igor
Babuschkin, Suchir Balaji, Valerie Balcom, et al. GPT-4 Technical Report, 2024. URL https:
//arxiv.org/abs/2303.08774.
[39] Aaron Grattaﬁori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad
Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan,
Anirudh Goyal, Anthony Hartshorn, Aobo Yang, et al. The Llama 3 Herd of Models, 2024. URL
https://arxiv.org/abs/2407.21783.
[40] Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, Andr´ as Hor´ anyi, Joaqu ´ ın Mu˜ noz-Sabater,
Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, Adrian Simmons, Cornel Soci, Saleh
Abdalla, Xavier Abellan, Gianpaolo Balsamo, et al. The ERA5 global reanalysis. Quarterly Journal
of the Royal Meteorological Society , 146:1999–2049, 7 2020. ISSN 1477870X. doi: 10.1002/qj.3803.
[41] Mihai Alexe, Simon Lang, Mariana Clare, Martin, Leutbecher, Christopher Roberts, Linus Magnusson,
Matthew Chantry, Rilwan Adewoyin, Ana Prieto-Nemesio, Jesper Dramsch, Florian Pinault, and
Baudouin Raoult. Data-driven ensemble forecasting with the aifs. https://www.ecmwf.int/en/
newsletter/181/earth-system-science/data-driven-ensemble-forecasting-aifs , October
2024. Accessed: 2025-07-06.
[42] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency Models, 3 2023. URL
http://arxiv.org/abs/2303.01469.
[43] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks, 6 2014. URL http://arxiv.
org/abs/1406.2661.
[44] J.R. Driscoll and D.M. Healy. Computing Fourier Transforms and Convolutions on the 2-Sphere.
Advances in Applied Mathematics , 15:202–250, 6 1994. ISSN 01968858. doi: 10.1006/aama.1994.1008.
URL https://linkinghub.elsevier.com/retrieve/pii/S0196885884710086.
[45] Jason. D. McEwen and Yves Wiaux. A novel sampling theorem on the sphere, 10 2011. URL
http://arxiv.org/abs/1110.6298http://dx.doi.org/10.1109/TSP.2011.2166394.
[46] C W Clenshaw and A R Curtis. A method for numerical integration on an automatic computer.
Numerische Mathematik , 2:197–205, 1960. ISSN 0945-3245. doi: 10.1007/BF01386223. URL https:
//doi.org/10.1007/BF01386223.
[47] Gene H. Golub and John H. Welsch. Calculation of Gauss quadrature rules. Mathematics of
Computation, 23:221–230, 1969. ISSN 0025-5718. doi: 10.1090/S0025-5718-69-99647-1. URL https:
//www.ams.org/mcom/1969-23-106/S0025-5718-69-99647-1/ .
[48] Taco S. Cohen and Max Welling. Group Equivariant Convolutional Networks, 2 2016. URL
http://arxiv.org/abs/1602.07576.
11
FourCastNet 3
[49] Nathana¨ el Schae!er. E”cient spherical harmonic transforms aimed at pseudospectral numerical
simulations. Geochemistry, Geophysics, Geosystems , 14:751–758, 3 2013. ISSN 15252027.
doi:10.1002/ggge.20071.
[50] Oliver J. Cobb, Christopher G. R. Wallis, Augustine N. Mavor-Parker, Augustin Marignier, Matthew A.
Price, Mayeul d’A vezac, and Jason D. McEwen. E”cient Generalized Spherical CNNs, 10 2020. URL
http://arxiv.org/abs/2010.11661.
[51] F.X. Giraldo, J.S. Hesthaven, and T. Warburton. Nodal High-Order Discontinuous Galerkin Methods
for the Spherical Shallow Water Equations. Journal of Computational Physics , 181:499–525, 9 2002.
ISSN 00219991. doi: 10.1006/jcph.2002.7139. URL https://linkinghub.elsevier.com/retrieve/
pii/S0021999102971391.
[52] Randall J. LeVeque. Numerical Methods for Conservation Laws . Birkh¨ auser Basel, 1992. ISBN
978-3-7643-2723-1. doi: 10.1007/978-3-0348-8629-1. URL http://link.springer.com/10.1007/
978-3-0348-8629-1 .
[53] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical
Image Segmentation, 5 2015. URL http://arxiv.org/abs/1505.04597.
[54] F.J. Harris. On the use of windows for harmonic analysis with the discrete Fourier transform.
Proceedings of the IEEE , 66:51–83, 1978. ISSN 0018-9219. doi: 10.1109/PROC.1978.10837. URL
http://ieeexplore.ieee.org/document/1455106/.
[55] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K¨ opf, Edward
Yang, Zach DeVito, Martin Raison, et al. PyTorch: An Imperative Style, High-Performance Deep
Learning Library, 12 2019. URL http://arxiv.org/abs/1912.01703.
[56] Yingzhou Li, Haizhao Yang, Eileen R. Martin, Kenneth L. Ho, and Lexing Ying. Butterﬂy
Factorization. Multiscale Modeling & Simulation , 13:714–732, 1 2015. ISSN 1540-3459.
doi:10.1137/15M1007173. URL http://epubs.siam.org/doi/10.1137/15M1007173.
[57] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate medium-range
global weather forecasting with 3D neural networks. Nature, 619:533–538, 7 2023. ISSN 14764687.
doi:10.1038/s41586-023-06185-3.
[58] L. Chen, X. Zhong, F. Zhang, Y. Xu, Y. Chen, F. Zhu, H. Li, Y. Qian, and L. Chen. FuXi: a cascade
machine learning forecasting system for 15-day global weather forecast. npj Climate and Atmospheric
Science, 6:190, 2023. doi: 10.1038/s41612-023-00512-1.
[59] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv´ e J´ egou. Going
deeper with Image Transformers. CoRR, abs/2103.17239, 2021. URL https://arxiv.org/abs/
2103.17239.
[60] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving Deep into Rectiﬁers: Surpassing
Human-Level Performance on ImageNet Classiﬁcation, 2 2015. URL http://arxiv.org/abs/1502.
01852.
[61] Shoaib Ahmed Siddiqui, Jean Kossaiﬁ, Boris Bonev, Christopher Choy, Jan Kautz, David Krueger,
and Kamyar Azizzadenesheli. Exploring the design space of deep-learning-based weather forecasting
systems, 10 2024. URL http://arxiv.org/abs/2410.07472.
[62] Tero Karras, Miika Aittala, Samuli Laine, Erik H¨ ark¨ onen, Janne Hellsten, Jaakko Lehtinen, and Timo
Aila. Alias-Free Generative Adversarial Networks, 6 2021. URL http://arxiv.org/abs/2106.12423.
[63] Vincent Fortin, M. Abaza, F. Anctil, and R. Turcotte. Why should ensemble spread match the
RMSE of the ensemble mean? Journal of Hydrometeorology , 15:1708–1713, 8 2014. ISSN 15257541.
doi:10.1175/JHM-D-14-0008.1.
[64] Tilmann Gneiting and Adrian E Raftery. Strictly Proper Scoring Rules, Prediction, and Estimation,
2004. ISSN 0704-0188.
[65] Micha¨ el Zamo and Philippe Naveau. Estimation of the Continuous Ranked Probability Score with
Limited Information and Applications to Ensemble Weather Forecasts. Mathematical Geosciences,
50:209–234, 2 2018. ISSN 18748953. doi: 10.1007/s11004-017-9709-7.
[66] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization, 12 2014. URL
http://arxiv.org/abs/1412.6980.
[67] Alberto Carrassi, Marc Bocquet, Laurent Bertino, and Geir Evensen. Data Assimilation in the
Geosciences - An overview on methods, issues and perspectives, 9 2017. URL http://arxiv.org/
abs/1709.02798.
[68] Stephan Rasp and Sebastian Lerch. Neural Networks for Postprocessing Ensemble Weather Forecasts.
Monthly Weather Review , pages 3885–3900, 2018. doi: 10.1175/MWR-D-18. URL https://doi.org/
10.1175/MWR-D-18- .
12
FourCastNet 3
[69] Troy Arcomano, Istvan Szunyogh, Jaideep Pathak, Alexander Wikner, Brian R. Hunt, and Edward
Ott. A Machine Learning-Based Global Atmospheric Forecast Model. Geophysical Research Letters,
47, 5 2020. ISSN 0094-8276. doi: 10.1029/2020GL087776. URL https://agupubs.onlinelibrary.
wiley.com/doi/10.1029/2020GL087776.
[70] Thomas M. Hamill. Interpretation of Rank Histograms for Verifying Ensemble
Forecasts. Monthly Weather Review , 129:550–560, 3 2001. ISSN 0027-0644.
doi:10.1175/1520-0493(2001)129<0550:IORHFV>2.0.CO;2. URL http://journals.ametsoc.
org/doi/10.1175/1520-0493(2001)129<0550:IORHFV>2.0.CO;2 .
[71] R. Tulloch and K. S. Smith. A theory for the atmospheric energy spectrum: Depth-limited
temperature anomalies at the tropopause. Proceedings of the National Academy of Sciences , 103(40):
14690–14694, 2006. doi: 10.1073/pnas.0605494103. URL https://www.pnas.org/doi/abs/10.1073/
pnas.0605494103.
[72] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien Chin Huang, Min Xu, Less Wright, Hamid
Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard
Nguyen, Geeta Chauhan, et al. Pytorch fsdp: Experiences on scaling fully sharded data parallel.
In Proceedings of the VLDB Endowment , volume 16, pages 3848–3860. VLDB Endowment, 2023.
doi:10.14778/3611540.3611569.
13