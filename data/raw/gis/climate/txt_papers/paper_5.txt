Numerical models outperform AI weather forecasts of
record-breaking extremes
Zhongwei Zhang a,b,*, Erich Fischer c, Jakob Zscheischler d,e, and Sebastian Engelke a,*
aResearch Institute for Statistics and Information Science, University of Geneva, Geneva, Switzerland
bScientific Computing Center, Karlsruhe Institute of Technology, Karlsruhe, Germany
cInstitute for Atmospheric and Climate Science, Department of Environmental Systems Science, ETH
Zurich, Zurich, Switzerland
dDepartment of Compound Environmental Risks,
Helmholtz Centre for Environmental Research – UFZ, Leipzig, Germany
eDepartment of Hydro Sciences, TUD Dresden University of Technology, Dresden, Germany
*Corresponding authors: zhongwei.zhangss@gmail.com, sebastian.engelke@unige.ch
Abstract
Artificial intelligence (AI)-based models are revolutionizing weather forecasting and have
surpassed leading numerical weather prediction systems on various benchmark tasks. How-
ever, their ability to extrapolate and reliably forecast unprecedented extreme events remains
unclear. Here, we show that for record-breaking weather extremes, the numerical model
High RESolution forecast (HRES) from the European Centre for Medium-Range Weather
Forecasts still consistently outperforms state-of-the-art AI models GraphCast, GraphCast
operational, Pangu-Weather, Pangu-Weather operational, and Fuxi. We demonstrate that
forecast errors in AI models are consistently larger for record-breaking heat, cold, and wind
than in HRES across nearly all lead times. We further find that the examined AI models
tend to underestimate both the frequency and intensity of record-breaking events, and they
underpredict hot records and overestimate cold records with growing errors for larger record
exceedance. Our findings underscore the current limitations of AI weather models in ex-
trapolating beyond their training domain and in forecasting the potentially most impactful
record-breaking weather events that are particularly frequent in a rapidly warming climate.
Further rigorous verification and model development is needed before these models can be
solely relied upon for high-stakes applications such as early warning systems and disaster
management.
Keywords: AI weather forecasting, record-breaking extremes, early-warning systems, extrapo-
lation, neural networks
Introduction
Record-breaking weather extremes, such as the 2021 Pacific Northwest, 2010 Russian and 2003
European heatwaves, and winter storms Lothar in 1999 and Kyrill in 2007, have caused numer-
ous fatalities and severe impacts on society, the economy, and ecosystems [1–5]. The level of
1
arXiv:2508.15724v1  [physics.ao-ph]  21 Aug 2025
disaster preparedness and adaptation to extreme events is strongly influenced by events observed
in recent decades. Consequently, after extended periods without major events, or when events
substantially exceed previous record levels, socio-economic impacts tend to be particularly large.
In addition to long-term disaster preparedness [6], accurate numerical weather prediction
(NWP) is critical for early-warning systems to save lives and reduce the impacts of climate
extremes [7]. Recently, a new generation of AI weather models has reached and sometimes
exceeded forecast skills of state-of-the-art NWP systems [8–11]. These models offer considerable
advantages in speed and energy efficiency, raising important questions about their potential to
supplement or eventually replace traditional NWP systems [12].
Before warnings for population and critical infrastructure are routinely based on AI models,
their performance needs to be further evaluated. In particular, their reliability in forecasting
extreme events remains less well understood. Such events are, by definition, rare and contribute
little to aggregated overall skill metrics [13]. Nevertheless, recent studies suggest that AI models
perform well—and in some cases even better than numerical models—in forecasting extreme
weather events [14, 15], particularly for longer lead times [8, 9].
Current forecast evaluation approaches for extreme events typically focus on extreme events
exceeding a certain threshold for one or several given variables, such as extreme wind speeds
[15], tropical cyclones [8, 9, 14], and high and low temperatures [9, 14–16]. However, due to
small sample sizes, the thresholds are often set to, say, the 95th percentile of the test data,
thus capturing mostly moderate extremes. Much less is known about record-breaking events, a
subset of extreme events that are unprecedented in the observational record. Given the current
high rate of global warming, record-breaking events sometimes exceed previous record levels
by large margins and have been referred to as black or gray swans [17], or record-shattering
extremes [18, 19].
A number of case studies have shown mixed results on the ability of AI weather models to
extrapolate beyond the range of their training data. For instance, a seasonal AI forecasting
model [20] struggled to predict North Atlantic Oscillation values that extended outside its
training distribution [21]. While AI models appear to outperform traditional NWP models
on tracking tropical cyclones [8, 9, 23], they tend to underpredict the intensity of the most
extreme storms, as measured by mean sea-level pressure [17, 24, 25]. Similar limitations in
reaching unprecedented amplitudes have also been observed in other high-impact events such
as heatwaves, winter storms, or compound extremes [26]. On the other hand, the unprecedented
2024 rainfall in Dubai was well predicted by GraphCast, suggesting that generalization to new
events may be possible if they share dynamical similarity with past extremes from other regions
[27].
However, these insights primarily rely on isolated case studies of specific events, whose
conclusions are inherently difficult to generalize due to the unique features of the analyzed
events and models. To systematically evaluate extrapolation in state-of-the-art AI weather
models, we construct a benchmark dataset consisting of record-breaking events for heat, cold,
and wind extremes. This dataset includes all observations during the test years 2018 and 2020
that exceed the respective historical records from the training period 1979–2017 of all considered
AI models. For each variable, a record-breaking event is defined locally, per grid cell and per
2
calendar month, yielding a large sample size even in individual years (see Methods). For the year
2020, this yields 162 , 751 heat, 32 , 991 cold, and 53 , 345 wind records, which are spread across
different seasons and climatic zones from tropics to high latitudes (Fig. 1a,b, and Supplementary
Fig. 5a–d). The dataset includes many prominent record-breaking events, such as the Siberian
heatwave in early 2020 [28] and the U.S. heatwave of August 2020 [29]. Evaluating AI models
on this record dataset challenges them to forecast on out-of-distribution data, which is known
to be difficult for neural networks in the machine learning literature.
180° 120°W 60°W 0° 60°E 120°E 180°
60°S
30°S
0°
30°N
60°N
90°N
a Heat records occurrences
0-5
5-10
10-15
15-25
0 500 1000
60°S
30°S
0°
30°N
60°N
90°N
b Count
2 4 6 8 10
1.0
1.5
2.0
2.5
3.0
3.5
4.0RMSE (Kelvin)
c All events, t2m
GraphCast
Pangu-Weather
Fuxi
HRES
2 4 6 8 10
1
2
3
4
5
6
7
8
d Heat records
2 4 6 8 10
Lead time (days)
2
4
6
8
10
e Cold records
2 4 6 8 10
Lead time (days)
0.6
0.8
1.0
1.2
1.4
1.6
1.8RMSE (m/s)
f All events, w10
2 4 6 8 10
Lead time (days)
1
2
3
4
5
6
7
g Wind records
Figure 1: Model performance on all events and record-breaking events. a , Number
of heat records in 2020 in ERA5. b, Number of heat records per latitude. c–g, Root mean
square error (RMSE) of forecasted 2m temperature and 10m wind speed over land (excluding
the Antarctic region) of HRES, Pangu-Weather, GraphCast, and Fuxi for all events ( c, f ) and
only record-breaking events (d, e, g ) in 2020 for different lead times. The transparent shaded
areas indicate 95% confidence bands.
We assess the extrapolation performance on our benchmark dataset of record-breaking events
of three leading deterministic AI weather models: GraphCast [9], Pangu-Weather [8], Fuxi [30],
3
as well as the operational variants of GraphCast and Pangu-Weather. Their performance is com-
pared to HRES from the European Centre for Medium-Range Weather Forecasts (ECMWF),
which is widely considered as the leading NWP model.
Model comparison on records’ intensity
Consistent with previous studies [8, 9, 30, 31], we find that, on overall performance, all AI
models—except Pangu-Weather—outperform the ECMWF model HRES in forecasting 2-meter
temperature across most lead times (Fig. 1c). Forecast accuracy is quantified using root mean
square errors (RMSE), computed over all 00 and 12 UTC time steps in test year 2020 and over
all land grid points (excluding the Antarctic region; see Methods). For 10-meter wind speed,
all AI models consistently outperform HRES across nearly all lead times (Fig. 1f).
However, the predictive skill is drastically different for record-breaking temperature and
wind events in 2020. Restricting the RMSE to record-breaking events, the numerical HRES
model consistently outperforms all AI models for hot and cold temperature records as well as
wind speed records across almost all lead times (Fig. 1d,e,g). The performance gap is most
pronounced for short lead times. For lead times beyond 5 days HRES still generally performs
better but to a lesser extent. This aligns with previous findings that AI models tend to perform
relatively better at longer lead times [9].
While, due to limited data availability, the evaluation is shown for a single year only as in
most previous studies [8, 15], we observe the same pattern in 2018, the other year for which
forecasts are available, except for Fuxi (Supplementary Fig. 22). The years 2018 and 2020
are distinctly different in terms of ENSO conditions, with 2018 transitioning from La Nina
to El Nino and 2020 undergoing a strong El Nino to La Nina shift. Since ENSO strongly
influences the occurrence of temperature records [32], particularly in the tropics, the consistent
outperformance of HRES across both years shows the robustness of the results. The better skill
of HRES in predicting record-breaking events is further consistent across different seasons and a
wide range of different climate zones, including tropics, subtropics, mid-latitudes and northern
high latitudes (Fig. 1a, Supplementary Figs. 7 and 8).
While it is common to evaluate ERA5-trained AI models against ERA5 reanalysis, and
HRES against its own analysis at lead time 0 (HRES-fc0) [8, 9, 30] (see Methods), this approach
can complicate comparisons due to different horizontal resolution: ERA5 has a resolution of
0.25◦, whereas HRES operates at 0 .1◦. To assess the sensitivity of our findings to the choice
of different reference datasets, we also evaluate operational versions of GraphCast and Pangu-
Weather against HRES on a common test dataset of record-breaking events identified using
HRES-fc0 as observational ground truth. Also in this setting, HRES consistently outperforms
the AI models on the records (Supplementary Fig. 14).
Selecting a subset of extreme events based on observations can favor models that produce
too many extreme forecasts—a problem known as the forecaster’s dilemma [33] (see Methods
for discussion). Thus, we construct an alternative benchmark avoiding the forecaster’s dilemma,
based on events where the forecast itself, rather than the observation, exceeds the training record
[34]. Results from this forecast-conditioned evaluation (Supplementary Fig. 20) are consistent
4
with the previous conclusion that HRES outperforms current AI models in forecasting records.
180° 120°W 60°W 0° 60°E 120°E 180°
60°S
30°S
0°
30°N
60°N
90°N
a
Heat records bias (GraphCast)
Underprediction
Overprediction
0 1 2 3 4 5 6
2
3
4
5RMSE (lead time 2 days)
b Heat records (Kelvin)
GraphCast
Pangu-Weather
Fuxi
HRES
0 1 2 3 4 5 6
2
4
6
8
10
c Cold records (Kelvin)
0 1 2 3 4 5 6
2
4
6
8
10
12
14
d Wind records (m/s)
0 1 2 3 4 5 6
5
4
3
2
1
0
Forecast bias (lead time 2 days)
e
0 1 2 3 4 5 6
Record exceedance
0
2
4
6
8
10
f
0 1 2 3 4 5 6
12
10
8
6
4
2
0
2
g
Figure 2: Forecast bias against record exceedance. a , Forecast bias of the maximum heat
records (GraphCast). b–d, RMSE of 2m temperature for heat and cold records, and 10m wind
speed for wind records for events in 2020 that exceed the record by at least a certain margin
(x-axis). Only land pixels (excluding the Antarctic region) are considered. e–g, Forecast bias
of heat, cold, and wind records, for events that exceed the record by at least a certain margin.
The transparent shaded areas indicate 95% confidence bands.
AI models underestimate intensities of records
While we demonstrate that AI models underperform compared to HRES in forecasting record-
breaking events, their errors may arise from over- or underprediction of event intensity. When
considering all data of the test year 2020, all models have relatively small, unsystematic biases
5
(Supplementary Fig. 9a,d). To better understand model behavior beyond their training domain,
we compare forecast accuracy and bias against the record exceedance, that is, the margin by
which a record is exceeded. We find that AI models generally underpredict temperature during
high records and overpredict during low records. This pattern is shown for GraphCast and
heat records (Fig. 2a). The systematic underprediction is remarkably consistent across regions,
seasons, and location in tropics, subtropics and mid- to high-latitudes, despite the fact that the
physical drivers of heat records vary substantially across regions. This behavior is not limited
to a single model: other AI models show similar patterns of intensity underestimation, while
HRES demonstrates a more balanced distribution of over- and underpredictions (Supplementary
Fig. 6). These results strongly suggest that AI model forecast errors are at least partly due to
systematic extrapolation limitations.
For all record types, the errors of the three AI models seem to grow almost linearly with
respect to the degree of record exceedance (Fig. 2b–d for a lead time of 2 days; additional lead
times in Supplementary Fig. 10). This trend indicates that forecast bias is the primary driver
of error (Fig. 2e–g and Supplementary Fig. 9): the greater the record exceedance, the larger the
underestimation of event intensity. The models behave as if their predictions have an implicit
(soft) cap at a certain local value. In contrast, the physical HRES model is more robust to
extreme records exceedances. For temperature records, HRES exhibits a nearly constant error
across increasing exceedances. For wind records, it shows a mild tendency of underestimation,
though far less so than AI models. Overall, HRES exhibits lower forecast bias for all records
types, and bias is not the dominant source of error, particularly for cold and wind records.
Importantly, this behavior, shown here for the evaluation year 2020, is fully consistent
with results from both the operational forecasts in 2020 and non-operational forecasts in 2018
(Supplementary Figs. 15 and 23). The systematic, one-sided bias observed across event types,
lead times, regions and independent years provides strong evidence that current AI models have
a structural extrapolation problem when forecasting record-breaking events.
Model comparison on records’ occurrence
We further test the ability of AI models to predict not only the intensity but also the frequency of
record-breaking events. We find that, in addition to underestimating event intensity, AI models
systematically underpredict the number of records relative to their ERA5 ground truth (Fig. 3a–
c). This underestimation results in a high number of false negatives and consequently low recall
(defined as the ratio of true positives to the observed positives). In contrast, HRES forecasts a
number of records comparable to its HRES-fc0 ground truth, with a slight overestimation for
heat records at smaller lead times.
Correctly predicting the number of record-breaking events does not imply accurate tim-
ing. In risk management, the trade-off between false positives and false negatives is typically
evaluated using precision-recall curves (see Methods). Across all record types and lead times,
HRES’s precision-recall curves are consistently better than GraphCast’s, in the sense that they
are closer to the ideal point (precision = 1, recall = 1), indicating superior classification per-
formance for heat, cold and wind records (Fig. 3d–f). This is in contrast with earlier results
6
2 4 6 8 10
Lead time (days)
0
25
50
75
100
125
150
175
200Record count
a Heat records
2 4 6 8 10
Lead time (days)
0
20
40
60
80
100
120
b Cold records
2 4 6 8 10
Lead time (days)
0
50
100
150
200
250
300
350
c Wind records
HRES-fc0
HRES
HRES TP
ERA5
GraphCast
GraphCast TP
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0Precision
d
0.0 0.2 0.4 0.6 0.8 1.0
Recall
e
0.0 0.2 0.4 0.6 0.8 1.0
Recall
f
HRES 12h
HRES 2d
HRES 5d
GraphCast 12h
GraphCast 2d
GraphCast 5d
HRES-fc0 ERA5 GraphCast Pangu-Weather
HRESGraphCastPangu-WeatherFuxi
0.60
0.51
0.49 0.61
0.50 0.64 0.60
g
HRES-fc0 ERA5 GraphCast Pangu-Weather
0.65
0.41
0.36 0.52
0.43 0.60 0.54
h
HRES-fc0 ERA5 GraphCast Pangu-Weather
0.56
0.24
0.21 0.35
0.31 0.40 0.34
i
Figure 3: Prediction of occurrence of record-breaking events. a –c, Counts (in thou-
sands) of heat, cold, and wind records in the ground truth ERA5 and HRES-fc0 data, and
GraphCast and HRES forecast data for 2020, as well as counts of their true positives (TP)
over land (excluding the Antarctic region). d–f, Precision and recall curves of GraphCast and
HRES forecasts when the records are used as the threshold for different lead times. g–i, Corre-
lations between the indicator functions of whether the ground truth or 2-day forecasts exceed
the record.
that demonstrate that GraphCast outperforms numerical models for more moderate extreme
events [9]. Similar results are observed for Pangu-Weather and Fuxi, where HRES again shows
a better classification skill across all lead times (Supplementary Figs. 11 and 12).
As an additional evaluation, we convert both forecast and ground truth into binary variables
(1 if a record is exceeded and 0 otherwise) and compute the correlation between them (see
Methods). This metric complements the precision-recall analysis by incorporating true negatives
and measuring the degree of dependence between different models’ forecasts. HRES has a
higher correlation with its ground truth HRES-fc0 than the AI models with their ground truth
ERA5, reaffirming its superior performance in forecasting record-breaking events (Fig. 3g–i).
7
Interestingly, all AI models are positively correlated with each other, showing that they tend to
make errors on the same events. This may be due to shared biases learned from their common
training data.
Discussion
Our findings consistently show that current AI models underperform HRES in forecasting
record-breaking events. They tend to underpredict heat and wind speed records, and over-
predict cold records, with greater forecast biases the larger the record margin. This strongly
suggests a systematic extrapolation problem in these models.
All current state-of-the-art AI weather models are built on neural network architectures such
as transformers [8, 30] or graph neural networks [9, 10, 35]. In machine learning, extrapolation,
also referred to as out-of-distribution generalization, is a well-known fundamental challenge in
these models. It has been observed in a range of applications, including image classification [36],
protein fitness prediction [37], and large language models [38]. Our record benchmark dataset
is explicitly designed to test this out-of-distribution problem within AI weather models (see
Methods for discussion).
The AI models studied here do not use any knowledge of physical principles and do not
explicitly enforce energy balances or other physical constraints [39, 40]. They are purely data-
driven and essentially interpolate between observed historical weather patterns in the training
period 1979–2017 to produce forecasts for new initial conditions in the test period. This is
in stark contrast to physics-based numerical models like HRES that strongly rely on partial
differential equations describing the evolution of the atmosphere based on our understanding
of physics. This fundamental difference in modeling philosophy likely explains the discrepancy
in performance between AI and NWP models for record-breaking events (Fig. 1c–g). While AI
models excel when the test set closely resembles the training distribution, capturing complex
atmospheric patterns and improving skill on average conditions, they struggle when forecasting
unprecedented events outside the training domain, even at short lead times. The nearly linear
increase of the biases with record exceedance (Fig. 2e–g) suggests an implicit cap in AI forecasts
around the most extreme training observation. Physical models do not have such a bound since
physical principles allow them to extrapolate, and, consequently, they exhibit less bias across
record magnitudes. Moreover, deterministic AI weather forecasts often smooth out fine-scale
spatial features such as sharp wind peaks. By contrast, recent probabilistic AI weather models
[35, 41] aim to preserve variability and avoid such smoothing. Still, as our results suggest, even
these models likely face similar extrapolation challenges when forecasting out-of-distribution,
record-breaking events.
Several promising avenues exist to address this shortcoming in future generations of AI
weather models. One strategy is data augmentation, a widely used technique in machine learn-
ing to improve robustness to unseen scenarios by enriching the training data [42]. In weather
and climate modeling, a key advantage is that numerical climate models can produce very
large amounts of physically plausible extreme events outside the training domain. Augment-
ing training with simulations from different climate regimes [11] or record-breaking events from
8
ensemble boosting [19] could allow AI models to learn from more extreme events than in the orig-
inal training data. This approach has already shown promise: FourCastNet’s [23] performance
on tropical cyclones improves significantly when trained on datasets that include such events
[17]. Another promising direction involves hybrid modeling, where specific parameterizations
in physical climate models are replaced with AI components [43]. These models combine the
efficiency and learning capacity of AI models with the physical consistency and extrapolation
ability of physical models. Hybrid models such as NeuralGCM [44] remain fully differentiable
and thus allow for efficient optimization of initial conditions [45], for instance. Finally, the loss
functions used to train AI weather models are typically designed to predict the mean or bulk
of the distribution. To improve extrapolation performance on extremes, it may be possible to
adapt principles from statistical learning and extreme value theory [46–48].
Given the remarkably fast evolution of AI models in recent years, there are promising ways to
further improve these models even for forecasting record-breaking extremes that will continue
to frequently occur in a rapidly warming climate. Nevertheless, the current generation still
underperforms HRES exactly during the potentially most impactful weather events, including
record-breaking heat and cold events as well as wind storms. Thus, it remains vital to fund and
run NWP and AI weather models in parallel and to rigorously evaluate their performance for
the most impactful type of weather events.
References
1. White, R. H. et al. The unprecedented Pacific Northwest heatwave of June 2021. Nature
Communications 14 (2023).
2. Barriopedro, D., Fischer, E. M., Luterbacher, J., Trigo, R. M. & Garc´ ıa-Herrera, R. The
Hot Summer of 2010: Redrawing the Temperature Record Map of Europe. Science 332,
220–224 (2011).
3. Garc´ ıa-Herrera, R., D´ ıaz, J., Trigo, R. M., Luterbacher, J. & Fischer, E. M. A Review of
the European Summer Heat Wave of 2003. Critical Reviews in Environmental Science and
Technology 40, 267–306 (2010).
4. Wernli, H., Dirren, S., Liniger, M. A. & Zillig, M. Dynamical aspects of the life cycle of
the winter storm ‘Lothar’ (24–26 December 1999). Quarterly Journal of the Royal Meteo-
rological Society 128, 405–429 (2002).
5. Fink, A. H., Br¨ ucher, T., Ermert, V., Kr¨ uger, A. & Pinto, J. G. The European storm Kyrill
in January 2007: synoptic evolution, meteorological impacts and some considerations with
respect to climate change. Natural Hazards and Earth System Sciences 9, 405–423 (2009).
6. Kelder, T. et al. How to stop being surprised by unprecedented weather. Nature Commu-
nications 16 (2025).
7. Bauer, P., Thorpe, A. & Brunet, G. The quiet revolution of numerical weather prediction.
Nature 525, 47–55 (2015).
8. Bi, K. et al. Accurate medium-range global weather forecasting with 3D neural networks.
Nature 619, 533–538 (2023).
9
9. Lam, R. et al. Learning skillful medium-range global weather forecasting. Science 382,
1416–1421 (2023).
10. Lang, S. et al. AIFS – ECMWF’s data-driven forecasting systemPreprint at arXiv:2406.01465.
2024.
11. Bodnar, C. et al. A foundation model for the Earth system. Nature 641, 1180–1187 (2025).
12. Schultz, M. G. et al. Can deep learning beat numerical weather prediction? Philosophical
Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences
379, 20200097 (2021).
13. Watson, P. A. G. Machine learning applications for weather and climate need greater focus
on extremes. Environmental Research Letters 17, 111004 (2022).
14. Ben Bouall` egue, Z. et al. The Rise of Data-Driven Weather Forecasting: A First Statis-
tical Assessment of Machine Learning–Based Weather Forecasts in an Operational-Like
Context. Bulletin of the American Meteorological Society 105, E864–E883 (2024).
15. Olivetti, L. & Messori, G. Do data-driven models beat numerical models in forecasting
weather extremes? A comparison of IFS HRES, Pangu-Weather, and GraphCast. Geosci-
entific Model Development 17, 7915–7962 (2024).
16. Meng, Z., Hakim, G. J., Yang, W. & Vecchi, G. A. Deep Learning Atmospheric Mod-
els Reliably Simulate Out-of-Sample Land Heat and Cold Wave Frequencies Preprint at
arXiv:2507.03176. 2025.
17. Sun, Y. Q. et al. Can AI weather models predict out-of-distribution gray swan tropical
cyclones? Proceedings of the National Academy of Sciences 122, e2420914122 (2025).
18. Fischer, E. M., Sippel, S. & Knutti, R. Increasing probability of record-shattering climate
extremes. Nature Climate Change 11, 689–695 (2021).
19. Fischer, E. M. et al. Storylines for unprecedented heatwaves based on ensemble boosting.
Nature Communications 14, 4643 (2023).
20. Watt-Meyer, O. et al. ACE2: Accurately learning subseasonal to decadal atmospheric vari-
ability and forced responses Preprint at arXiv:2411.11268. 2024.
21. Kent, C. et al. Skilful global seasonal predictions from a machine learning weather model
trained on reanalysis data Preprint at arXiv:2503.23953. 2025.
22. Rasp, S., Pritchard, M. S. & Gentine, P. Deep learning to represent subgrid processes in
climate models. Proceedings of the National Academy of Sciences 115, 9684–9689 (2018).
23. Pathak, J. et al. FourCastNet: A Global Data-driven High-resolution Weather Model using
Adaptive Fourier Neural Operators Preprint at arXiv:2202.11214. 2022.
24. DeMaria, M. et al. Evaluation of Tropical Cyclone Track and Intensity Forecasts from
Artificial Intelligence Weather Prediction (AIWP) Models Preprint at arXiv:2409.06735.
2024.
25. Charlton-Perez, A. J. et al. Do AI models produce better weather forecasts than physics-
based models? A quantitative evaluation case study of Storm Ciar´ an. npj Climate and
Atmospheric Science 7, 93 (2024).
10
26. Pasche, O. C., Wider, J., Zhang, Z., Zscheischler, J. & Engelke, S. Validating Deep Learning
Weather Forecast Models on Recent High-Impact Extreme Events. Artificial Intelligence
for the Earth Systems 4, e240033 (2025).
27. Sun, Y. Q., Hassanzadeh, P., Shaw, T. & Pahlavan, H. A. Predicting Beyond Training Data
via Extrapolation versus Translocation: AI Weather Models and Dubai’s Unprecedented
2024 Rainfall Preprint at arXiv:2505.10241. 2025.
28. Overland, J. E. & Wang, M. The 2020 Siberian heat wave. International Journal of Cli-
matology 41 (2020).
29. Li, X. et al. New-generation geostationary satellite reveals widespread midday depression
in dryland photosynthesis during 2020 western U.S. heatwave. Science Advances 9 (2023).
30. Chen, L. et al. FuXi: a cascade machine learning forecasting system for 15-day global
weather forecast. npj Climate and Atmospheric Science 6, 190 (2023).
31. Rasp, S. et al. WeatherBench 2: A Benchmark for the Next Generation of Data-Driven
Global Weather Models.Journal of Advances in Modeling Earth Systems 16, e2023MS004019
(2024).
32. Fischer, E. M. et al. Record-breaking extremes in a warming climate.Nature Reviews Earth
and Environment 6, 456–470 (2025).
33. Lerch, S., Thorarinsdottir, T. L., Ravazzolo, F. & Gneiting, T. Forecaster’s Dilemma:
Extreme Events and Forecast Evaluation. Statistical Science 32, 106–127 (2017).
34. Holzmann, H. & Eulert, M. The role of the information set for forecasting—with applica-
tions to risk management. The Annals of Applied Statistics 8 (2014).
35. Price, I. et al. Probabilistic weather forecasting with machine learning. Nature 637, 84–90
(2024).
36. Ganin, Y. et al. Domain-Adversarial Training of Neural Networks. Journal of Machine
Learning Research 17, 1–35 (2016).
37. Freschlin, C. R., Fahlberg, S. A., Heinzelman, P. & Romero, P. A. Neural network ex-
trapolation to distant regions of the protein fitness landscape. Nature Communications 15
(2024).
38. Hupkes, D., Giulianelli, M., Dankers, V., et al. A taxonomy and review of generalization
research in NLP. Nat. Mach. Intell. 5, 1161–1174 (2023).
39. Selz, T. & Craig, G. C. Can Artificial Intelligence-Based Weather Prediction Models Sim-
ulate the Butterfly Effect? Geophysical Research Letters 50, e2023GL105747 (2023).
40. Bonavita, M. On Some Limitations of Current Machine Learning Weather Prediction Mod-
els. Geophysical Research Letters 51, e2023GL107377 (2024).
41. Lang, S. et al. AIFS-CRPS: Ensemble forecasting using a model trained with a loss function
based on the Continuous Ranked Probability Score Preprint at arXiv:2412.15832. 2024.
42. Shorten, C. & Khoshgoftaar, T. M. A survey on image data augmentation for deep learning.
Journal of Big Data 6, 1–48 (2019).
11
43. Shaw, T. A. & Stevens, B. The other climate crisis. Nature 639, 877–887 (2025).
44. Kochkov, D. et al. Neural general circulation models for weather and climate. Nature 632,
1060–1066 (2024).
45. Whittaker, T. & Luca, A. D. Pushing the Limits of Extreme Weather: Constructing Ex-
treme Heatwave Storylines with Differentiable Climate ModelsPreprint at arXiv:2506.10660.
2025.
46. Shen, X. & Meinshausen, N. Engression: extrapolation through the lens of distributional
regression. Journal of the Royal Statistical Society Series B: Statistical Methodology 87,
653–677 (2024).
47. Buritic´ a, G. & Engelke, S. Progression: an extrapolation principle for regression Preprint
at arXiv:2410.23246. 2024.
48. Boulaguiem, Y., Zscheischler, J., Vignotto, E., van der Wiel, K. & Engelke, S. Modeling and
simulating spatial extremes by combining extreme value theory with generative adversarial
networks. Environmental Data Science 1, e5 (2022).
12
Methods
Models and data
For the definition of records we use the ECMWF’s ERA5 reanalysis data [1] from 1979–2017
with daily observations at 00, 06, 12, and 18 UTC time. This dataset coincides with the training
data of almost all AI models considered in this paper. The time points in this training data
are denoted by Ttrain. The ERA5 data is available on a 0 .25◦ × 0.25◦ latitude-longitude grid.
Throughout the paper we only consider data over land. We use the land-sea mask from the
ERA5 and follow ECMWF [2] by defining a grid cell as land if more than 50% of the cell is
covered by land; otherwise it is considered as sea. We exclude the Antarctic region (grid cells
with latitude in the range ( −60◦, −90◦]) due to aberrant behavior exhibited by some AI models
in this region, and denote the remaining set of land grid cells (244 , 450 grid cells in total) from
the ERA5 dataset by G0.25◦.
We use forecasts from the state-of-the-art AI models GraphCast [3], Pangu-Weather [4], and
Fuxi [5] from a test period Ttest, which is either of the years 2018 or 2020 in our analyses. For the
same period, we use forecasts from the High RESolution model (HRES) of ECMWF for com-
parison. All the forecast data are publicly available from WeatherBench 2 [6]. Pangu-Weather
and Fuxi are trained and validated on ERA5 data from 1979–2017; the GraphCast forecast data
for years 2018 and 2020 are produced by two slightly different versions of GraphCast, i.e., the
2018 data are generated by the GraphCast model trained on ERA5 data from 1979–2017, whilst
the 2020 data are generated by the GraphCast model trained with ERA5 data from a slightly
extended period 1979–2019. In addition, we also employ the operational versions of GraphCast
and Pangu-Weather. The former has been fine-tuned on the HRES-fc0 data from 2016–2021,
while the latter was used in an operational setting without fine-tuning.
As ground truth for the AI models we use ERA5 data with locations in G0.25◦ in the test
period. For HRES and the operational AI models we use HRES-fc0 as ground truth. Using
these two different datasets to evaluate the forecasts against is the standard approach in the
literature of AI weather models to avoid unfair comparisons [3–5].
A benchmark dataset of record-breaking events
To define a dataset of record-breaking events in a given year (e.g., 2020) for a variable x of
interest (e.g., 2-meter temperature), we first compute the corresponding record in the ERA5
data Ttrain in the training period of the AI models from 1979–2017. We specify whether we
consider records in the positive direction (e.g., heat records) or the negative direction (e.g.,
cold records) by superscripts max or min, respectively. A record rx,max
s,m of variable x is defined
locally per grid cell s ∈ G0.25◦ and per month m ∈ { January, . . . ,December}. More precisely,
we define
rx,max
s,m = max
t∈Ttrain;t∈m
xs,t, (1)
where xs,t is the value of variable x at location s and time t, and t ∈ m indicates that only time
points in month m are considered.
13
We define the set Rx,max ⊆ G0.25◦ × Ttest of record-breaking events of variable x consisting of
location-time pairs encoding where and when the event occurred. The test period Ttest contains
all time points at 00 and 12 UTC in the test year, i.e., the year 2018 or 2020 in our analyses.
We denote by m(t) the month corresponding to a time t ∈ Ttest, so that xs,t > r x,max
s,m(t) means
that observation xs,t exceeds its respective monthly historical record. With this we have
Rx,max = {(s, t) ∈ G0.25◦ × Ttest : xs,t > r x,max
s,m(t)}. (2)
We do not evaluate forecasts initiated at 06 and 18 UTC since the HRES forecasts with
these initializations are only available for 3.75 days at ECMWF, and all AI-based forecasts are
only available for initializations at 00 and 12 UTC on WeatherBench 2 [6]. In addition, we only
consider lead times that are multiples of 12 hours to ensure that the subsets of ERA5 data used
as input and ground truth for AI models have the same +9h lookahead [3] (ERA5 and HRES-fc0
have different data assimilation windows: ERA5 has +3h lookahead at 06 and 18 UTC and +9h
loohahead at 00 and 12 UTC, while HRES-fc0 have +3h lookahead for all four time points).
Consequently, this comparison setup disadvantages HRES due to the mismatch between a +9h
lookahead of ERA5 input and +3h lookahead of HRES-fc0 input, thereby strengthening our
main result that HRES outperforms AI models on record-breaking events.
Note that the notion of a record-breaking event is to be understood relative to the training
period. We do not update the record if a larger event has occurred after 2017 (the end of the
training period). The reason is that AI models are not retrained and a record-breaking event
in the test period will not inform or improve the model for later time steps.
Using as test data the ERA5 ground truth in 2020 we obtain 162,751 records for heat, 32,991
for cold and 53,345 for wind; see the geographical distribution of these records in the map in
Fig. 1a and Supplementary Fig. 5, respectively. For the analysis of operational models we define
the set of record-breaking events as those where HRES-fc0 exceeds the training record, yielding
170,136 records for heat, 109,155 for cold and 338,235 for wind (Supplementary Fig. 13).
Extrapolation in AI models
Extrapolation or out-of-distribution generalization in AI models refers to the situation where a
test predictor is far away from the distribution of the training predictors. In high-dimensional
predictor spaces, it is not trivial to mathematically describe such points. One way of framing
extrapolation is to require that the predictor is outside of the convex hull (blue line in Fig. 4)
formed by the training data. [7] argue that with this definition of training domain it is in
fact very likely that test points need extrapolation. However, convex hulls are computationally
prohibitive in high dimensions since the number of facets grows rapidly with the dimension. Our
record dataset therefore considers a stronger yet simpler definition, namely all points where at
least one test variable is beyond its univariate training range. In Fig. 4 this corresponds to all
test points outside of the green rectangle. All events in the record set Rx,max in Equation (2)
satisfy this strong definition of out-of-distribution samples.
14
2020-01 2020-03 2020-05 2020-07 2020-09 2020-11 2021-01
Time
250
260
270
280
290
300
310t2m (Kelvin)
a
2020-01 2020-03 2020-05 2020-07 2020-09 2020-11 2021-01
Time
0
2
4
6
8
10w10 (m/s)
b
0 1 2 3 4 5 6 7 8
w10 (m/s)
285
290
295
300
305
310t2m (Kelvin)
c
Figure 4: Illustration of our definitions of record and extrapolation . a, Daily time
series of 2m temperature at the location with latitude 34 .75 and longitude −112.25 in 2020
(black), and monthly max/min records (in green) at this location, where orange points indicate
the record-breaking events in August. b, Daily time series of 10m wind speed and monthly max
records at the same location. c, Scatter plots of 2m temperature and 10m wind speed in August
in the training period from 1979–2017 (in grey) and in the evaluation year 2020 (in black) at
this location. The blue line represents the convex hull formed by the training data, while the
green rectangle shows the max/min records in the training period. Orange points indicate the
record-breaking events in the evaluation period.
Root mean square error
We quantify the forecast error with the root mean square error (RMSE). For a target variable
x of interest (e.g., 2-meter temperature T2m) the RMSE on a subset of location-initialization
pairs for lead time τ is defined as
RMSEI(τ) =
vuut 1P
(s,t0)∈I ωs
X
(s,t0)∈I
ωs(ˆxτ
s,t0 − xs,t0+τ)2, (3)
where
• G0.25◦ is the set of locations/grid cells,
• I ⊆ G0.25◦ × Ttest is the set of location-initialization pairs of interest,
• ˆxτ
s,t0 is a forecast of variable x with lead time τ at location s ∈ G0.25◦ and initialization
time t0 ∈ T , and xs,t0+τ is the corresponding ground truth,
• ωs is the latitude-based weight chosen as the one used in [3]
ωs =



cos(θlat(s)) sin(θ0.25◦/2), if |θlat(s)| < π/2,
sin2(θ0.25◦/4), if |θlat(s)| = π/2,
with θa as the radian associated with degree a.
Our definition of RMSE is more general than the conventional one [6] in the sense that we
allow to focus on a subset I of location-initialization pairs ( s, t0). If we set I as the product
15
of the set of all grid cells over the globe and all time points in Ttest, we recover the traditional
(latitude-weighted) RMSE on all test locations and initialization times.
In the computation of RMSE on record-breaking events such as shown in Fig. 1, we choose
the set I in Equation (3) in the following way. Recall the set Rx,max ⊆ G0.25◦ × Ttest of
location-time pairs of all records in a given time period (e.g., the year 2020). We choose all
location-initialization pairs such that the target of the forecasting with lead time τ corresponds
to a record, i.e.,
Iτ = {(s, t0) ∈ G0.25◦ × Ttest : (s, t0 + τ) ∈ Rx,max} . (4)
The corresponding RMSEI(τ) is the error of a model made in forecasting records with lead time
τ.
To construct a confidence interval for the RMSE I(τ) in Equation (3), we assume that the
central limit theorem holds for the weighted square errors, i.e.,
p
|I|
"
1P
(s,t0)∈I ωs
X
(s,t0)∈I
ωs(ˆxτ
s,t0 − xs,t0+τ)2 − µτ
#
d− − → N(0, σ2
τ), |I| → ∞ ,
where µτ denotes the true mean of weighted square error and σ2
τ its asymptotic variance, and
d− − →means convergence in distribution. Then by the Delta method, we obtain the asymptotic
distribution of RMSE I(τ)
p
|I|
 
RMSEI(τ) − √µτ
 d− − → N
 
0, σ2
τ /(4µτ)

.
Hence an approximate α-level confidence interval for RMSE I(τ) can be constructed as [ µτ −
q(1+α)/2στ /
p
4µτ |I|, µτ + q(1+α)/2στ /
p
4µτ |I|], where q(1+α)/2 is the (1 + α)/2-quantile of a
standard normal distribution. Alternatively, bootstrap can be used to construct the confidence
bands [8]. We tried the non-parametric bootstrap with 1000 resampling, which yielded similar
confidence bands to the normal ones. For the sake of computational feasibility, we use the above
normal confidence levels throughout the paper.
Forecast bias
To complement RMSE and investigate whether a forecasting model under- or overpredicts the
ground truth, we consider the latitude-weighted forecast bias
FBI(τ) = 1P
(s,t0)∈I ωs
X
(s,t0)∈I
ωs(ˆxτ
s,t0 − xs,t0+τ).
where the notation is the same as in Equation (3). Confidence intervals for the forecast bias
are computed in the same way as for RMSE based on asymptotic normality.
Precision and recall curves
For early warning systems it is crucial that a weather forecasting model is able to predict
the occurrence of an extreme event accurately. We therefore consider record-breaking event
16
forecasting as a binary classification problem by assessing whether a forecasting model can
predict the exceedance of a variable over its previous record in the sense of (1). Since this
classification problem is strongly imbalanced, similar to previous studies [3], we use precision-
recall curves that are well-suited for such cases since they account for both false positives and
false negatives.
For a setI ⊆ G0.25◦ ×Ttest of location-initialization pairs of interest, we compute the precision
and recall for variable x at lead time τ as (we set rs,m = rx,max
s,m to simplify notation)
PrecisionI(τ) =
P
(s,t0)∈I 1{ˆxτ
s,t0 > r s,m(t0+τ )}1{xs,t0+τ > r s,m(t0+τ )}
P
(s,t0)∈I 1{ˆxτ
s,t0 > r s,m(t0+τ )} ,
and
RecallI(τ) =
P
(s,t0)∈I 1{ˆxτ
s,t0 > r s,m(t0+τ )}1{xs,t0+τ > r s,m(t0+τ )}
P
(s,t0)∈I 1{xs,t0+τ > r s,m(t0+τ )} .
where ˆxτ
s,t0 denotes a forecast of variable x at location s initialized at time t0 with lead time τ,
and xs,t0+τ is the corresponding ground truth. As above m(t0 + τ) is the month corresponding
to time t0 + τ.
In order to produce a precision-recall curve from a deterministic forecast, following [3], we
introduce a common “gain” parameter to define scaled forecasts by
scaled forecast = forecast + gain × forecast std. deviation . (5)
Using these scaled forecasts in the precision and recall formulae instead of only ˆxτ
s,t0 and varying
the gain parameter in a suitable range ([ −1.5, 1.5] in our case) yields a precision-recall curve.
The scaling allows the study of different trade-offs between false positives and false negatives,
and using a common gain parameter enables averaging over all spatial locations s ∈ G0.25◦.
Our parameterization of the scaled forecasts is slightly different from the one in [3], but it
is theoretically more justified. Indeed, for a probabilistic forecast from a location-scale family,
formula (5) corresponds to choosing the same quantile of the forecast distribution at all locations.
For each variablex, each location s ∈ G0.25◦, each month m and each lead time τ we estimate
the forecast standard deviations in (5) from forecasts in the year 2020 for the different models.
We assume that this standard deviation is constant for time points in the same month so that
we have enough data for the estimation.
Correlation between record forecasts
In order to compute the correlation between the different model forecasts and ground truths,
we define suitable functions indicating whether the corresponding record is exceeded. Fix a
lead time τ and, for instance, consider the variable x with max-records abbreviated by rs,m =
rx,max
s,m . For a forecasts ˆxτ
s,t from some model define for each time point t0 ∈ Ttest the indicator
1{ˆxτ
s,t0 > r s,m(t0+τ )} that takes value 1 if ˆxτ
s,t0 exceeds the record rs,m(t0+τ ) and 0 otherwise.
We can now compute the correlation between these indicators, indexed by all t0 + τ ∈ Ttest
and s ∈ G0.25◦, for two different forecast models. Similarly, for a ground truth (either ERA5
17
or HRES-fc0) we define for each time point t0 + τ ∈ Ttest the indicator 1{xs,t0+τ > r s,m(t0+τ )}.
We then compute correlations of these ground truths with the forecast indicators, and between
forecast indicators from different models. The resulting correlation matrix is shown for the
ERA5-trained AI models in Fig. 3g–i, and for the operational AI models in Supplementary
Fig. 17g–i.
Forecaster’s dilemma
In the theory of forecast evaluation, the forecaster’s dilemma [9] shows that computing an
evaluation score only on a subset of observations can incentivize sub-optimal forecasts. Such
conditioning appears in the RMSE defined in (3) if the set I depends on the observations, as, for
instance, in the case of record-breaking events defined in (4). This metric should therefore not
be used as the sole evaluation criterion, but rather in combination with others. We therefore
also report the overall RMSE on all events in Fig. 1, Supplementary Figs. 14 and 22, which
show that all methods yield errors on a comparable scale and do not appear to artificially
hedge forecasts of extreme events. In addition, we consider different evaluation criteria such as
precision-recall curves that take into account both false positives and false negatives (Fig. 3d–f
and Supplementary Fig. 17d–f).
Computing the RMSE on a subset of extreme observations is common in the literature of AI
weather forecasts [3, 8, 10]. Another approach that avoids the forecaster’s dilemma completely
is to condition on the forecasts instead of the observations [11]. We follow this approach to
compare the operational version of GraphCast with HRES. We choose as the set of record-
breaking events in (2) all location-initialization pairs such that forecasts with lead time τ from
both GraphCast operational and HRES exceed the training record. For max-records of variable
x, for instance, this yields an index set
Iτ =
n
(s, t0) ∈ G0.25◦ × Ttest : ˆxHRES,τ
s,t0 > r s,m(t0+τ ), ˆxGraphCast,τ
s,t0 > r s,m(t0+τ )
o
,
to be used in the RMSE in (3). The results (Supplementary Fig. 20) look qualitatively similar
to those from conditioning on the observations, except that we have a much smaller set of events
that are jointly forecasted to be record-breaking by both models compared to the original record
dataset.
References
1. Hersbach, H. et al. The ERA5 global reanalysis. Quarterly Journal of the Royal Meteoro-
logical Society 146, 1999–2049 (2020).
2. Owens, R. & Hewson, T. ECMWF Forecast User Guide 2018.
3. Lam, R. et al. Learning skillful medium-range global weather forecasting. Science 382,
1416–1421 (2023).
4. Bi, K. et al. Accurate medium-range global weather forecasting with 3D neural networks.
Nature 619, 533–538 (2023).
18
5. Chen, L. et al. FuXi: a cascade machine learning forecasting system for 15-day global
weather forecast. npj Climate and Atmospheric Science 6, 190 (2023).
6. Rasp, S. et al. WeatherBench 2: A Benchmark for the Next Generation of Data-Driven
Global Weather Models.Journal of Advances in Modeling Earth Systems 16, e2023MS004019
(2024).
7. Balestriero, R., Pesenti, J. & LeCun, Y. Learning in High Dimension Always Amounts to
Extrapolation Preprint at arXiv:2110.09485. 2021.
8. Bodnar, C. et al. A foundation model for the Earth system. Nature 641, 1180–1187 (2025).
9. Lerch, S., Thorarinsdottir, T. L., Ravazzolo, F. & Gneiting, T. Forecaster’s Dilemma:
Extreme Events and Forecast Evaluation. Statistical Science 32, 106–127 (2017).
10. Olivetti, L. & Messori, G. Do data-driven models beat numerical models in forecasting
weather extremes? A comparison of IFS HRES, Pangu-Weather, and GraphCast. Geosci-
entific Model Development 17, 7915–7962 (2024).
11. Holzmann, H. & Eulert, M. The role of the information set for forecasting—with applica-
tions to risk management. The Annals of Applied Statistics 8 (2014).
19
Supplementary materials
A Supplementary figures for the evaluation of non-operational forecasts in 2020 21
B Supplementary figures for the evaluation of operational forecasts in 2020 29
C Supplementary figures for the evaluation of non-operational forecasts in 2018 37
20
A Supplementary figures for the evaluation of non-operational
forecasts in 2020
60°S
30°S
0°
30°N
60°N
90°N
a Cold records
0-5
5-10
10-15
15-35
0 500
60°S
30°S
0°
30°N
60°N
90°N
b Count
180° 120°W 60°W 0° 60°E 120°E 180°
60°S
30°S
0°
30°N
60°N
90°N
c Wind records
0-3
3-6
6-10
0 200
60°S
30°S
0°
30°N
60°N
90°N
d
Supplementary Fig. 5: Number of record-breaking events over land (excluding the
Antarctic region) in 2020 in ERA5 . a and c, Number of cold and wind records. b and d,
Number of cold and wind records per latitude.
21
180° 120°W 60°W 0° 60°E 120°E 180°
60°S
30°S
0°
30°N
60°N
90°N
a HRES
Underprediction
Overprediction
180° 120°W 60°W 0° 60°E 120°E 180°
60°S
30°S
0°
30°N
60°N
90°N
b Fuxi
180° 120°W 60°W 0° 60°E 120°E 180°
60°S
30°S
0°
30°N
60°N
90°N
c Pangu-Weather
Supplementary Fig. 6: Forecast bias of the maximum heat records for different models
in 2020.. a, Forecast bias for the numerical model HRES. b, Forecast bias for AI model Fuxi.
c, Forecast bias for AI model Pangu-Weather.
22
2 4 6 8 10
2
4
6
8Global
a Heat records
GraphCast
Pangu-Weather
Fuxi
HRES
2 4 6 8 10
2
4
6
8
10
12
b Cold records
2 4 6 8 10
1
2
3
4
5
6
7
8
c Wind records
2 4 6 8 10
2
4
6
8NHem Extratropics
d
2 4 6 8 10
2
4
6
8
10
12
e
2 4 6 8 10
2
4
6
8
f
2 4 6 8 10
0
2
4
6
8
10
12SHem Extratropics
g
2 4 6 8 10
2
4
6
8
10
12
14
h
2 4 6 8 10
2
4
6
8
10
i
2 4 6 8 10
1
2
3
4Tropics
j
2 4 6 8 10
1
2
3
4
5
k
2 4 6 8 10
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
l
Lead time (days)
Supplementary Fig. 7: Regional RMSE of 2m temperature and 10m wind speed over
land in 2020 . a–c, RMSE for the whole globe. d–f, RMSE for Northern Hemisphere Extra-
tropics (NHem Extratropics, i.e., grid cells with latitude in [20 , 90]). g–i, RMSE for Southern
Hemisphere Extratropics (SHem Extratropics, i.e., grid cells with latitude in [ −90, −20]). j–l,
RMSE for Tropics (grid cells with latitude in [ −20, 20]). The transparent shaded areas indicate
95% confidence bands.
23
2 4 6 8 10
2
4
6
8
10DJF
a Heat records
GraphCast
Pangu-Weather
Fuxi
HRES
2 4 6 8 10
2
4
6
8
10
12
b Cold records
2 4 6 8 10
1
2
3
4
5
6
7
8
c Wind records
2 4 6 8 10
1
2
3
4
5
6
7MAM
d
2 4 6 8 10
2
4
6
8
10
12
e
2 4 6 8 10
2
4
6
8
f
2 4 6 8 10
1
2
3
4
5
6
7JJA
g
2 4 6 8 10
1
2
3
4
5
6
7
h
2 4 6 8 10
1
2
3
4
5
6
i
2 4 6 8 10
1
2
3
4
5
6SON
j
2 4 6 8 10
2
4
6
8
10
12
k
2 4 6 8 10
1
2
3
4
5
6
7
l
Lead time (days)
Supplementary Fig. 8: Seasonal RMSE of 2m temperature and 10m wind speed over
land (excluding the Antarctic region) in 2020 . a–c, RMSE for the months December –
February. d–f, RMSE for the months March – May. g–i, RMSE for the months June – August.
j–l, RMSE for the months September – November.
24
2.5 5.0 7.5 10.0
0.3
0.2
0.1
0.0
0.1
0.2
Forecast bias (Kelvin)
a All events, t2m
2.5 5.0 7.5 10.0
6
5
4
3
2
1
0
b Heat records
2.5 5.0 7.5 10.0
Lead time (days)
0
2
4
6
8
c Cold records
GraphCast
Pangu
Fuxi
HRES
2.5 5.0 7.5 10.0
Lead time (days)
0.8
0.6
0.4
0.2
0.0
Forecast bias (m/s)
d All events, w10
2.5 5.0 7.5 10.0
Lead time (days)
6
5
4
3
2
1
0
e Wind records
Supplementary Fig. 9: Forecast bias for all and record-breaking events in 2020. Forecast
bias of 2m temperature and 10m wind speed over land (excluding the Antarctic region) of HRES,
GraphCast, Pangu-Weather, and Fuxi for all events (a, d) and only record-breaking events ( b,
c, e) in 2020. The transparent shaded areas indicate 95% confidence bands.
25
0 2 4 6
1.0
1.5
2.0
2.5
3.0RMSE (lead time 12 hours)
a Heat records (Kelvin)
GraphCast
Pangu
Fuxi
HRES
0 2 4 6
1
2
3
4
5
6
7
8
b Cold records (Kelvin)
0 2 4 6
2
4
6
8
c Wind records (m/s)
0 2 4 6
4
6
8
10RMSE (lead time 5 days)
d
0 2 4 6
4
6
8
10
12
14
e
0 2 4 6
5.0
7.5
10.0
12.5
15.0
17.5
f
Record exceedance
Supplementary Fig. 10: RMSE against record exceedance for different lead times in
2020. RMSE of 2m temperature and 10m wind speed for lead time 12 hours ( a–c) and 5 days
(d–f) over land (excluding the Antarctic region) in 2020, for events that exceed the record
by at least a certain margin (in Kelvin or m/s). The transparent shaded areas indicate 95%
confidence bands.
26
2 4 6 8 10
Lead time (days)
0
25
50
75
100
125
150
175
200Record count
a Heat records
2 4 6 8 10
Lead time (days)
0
20
40
60
80
100
120
b Cold records
2 4 6 8 10
Lead time (days)
0
50
100
150
200
250
300
350
c Wind records
HRES-fc0
HRES
HRES TP
ERA5
Pangu-Weather
Pangu-Weather TP
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0Precision
d
0.0 0.2 0.4 0.6 0.8 1.0
Recall
e
0.0 0.2 0.4 0.6 0.8 1.0
Recall
f
HRES 12h
HRES 2d
HRES 5d
Pangu-Weather 12h
Pangu-Weather 2d
Pangu-Weather 5d
Supplementary Fig. 11: Pangu-Weather forecast of occurrence of record-breaking
events over land (excluding the Antarctic region) in 2020 . a-c, Counts (in thousands)
of heat, cold, and wind records in the ground truth ERA5 and HRES-fc0, and Pangu-Weather
and HRES forecast data, as well as counts of their true positives (TP). d-f, Precision and recall
curve of Pangu-Weather and HRES forecasts when using the record data as the threshold.
27
2 4 6 8 10
Lead time (days)
0
25
50
75
100
125
150
175
200Record count
a Heat records
2 4 6 8 10
Lead time (days)
0
20
40
60
80
100
120
b Cold records
2 4 6 8 10
Lead time (days)
0
50
100
150
200
250
300
350
c Wind records
HRES-fc0
HRES
HRES TP
ERA5
Fuxi
Fuxi TP
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0Precision
d
0.0 0.2 0.4 0.6 0.8 1.0
Recall
e
0.0 0.2 0.4 0.6 0.8 1.0
Recall
f
HRES 12h
HRES 2d
HRES 5d
Fuxi 12h
Fuxi 2d
Fuxi 5d
Supplementary Fig. 12: Fuxi forecast of occurrence of record-breaking events over
land (excluding the Antarctic region) in 2020 . a-c, Counts (in thousands) of heat, cold,
and wind records in the ground truth ERA5 and HRES-fc0, and Fuxi and HRES forecast data,
as well as counts of their true positives (TP). d-f, Precision and recall curve of Fuxi and HRES
forecasts when using the record data as the threshold.
28
B Supplementary figures for the evaluation of operational fore-
casts in 2020
60°S
30°S
0°
30°N
60°N
90°N
a Heat records
0-5
5-25
25-50
50-260
0 1000
60°S
30°S
0°
30°N
60°N
90°N
b Count
60°S
30°S
0°
30°N
60°N
90°N
c Cold records
0-5
5-25
25-50
50-750
0 500 1000
60°S
30°S
0°
30°N
60°N
90°N
d
180° 120°W 60°W 0° 60°E 120°E 180°
60°S
30°S
0°
30°N
60°N
90°N
e Wind records
0-5
5-25
25-50
50-700
0 1000 2000
60°S
30°S
0°
30°N
60°N
90°N
f
Supplementary Fig. 13: Number of records over land (excluding the Antarctic region)
in 2020 in HRES-fc0 . a, c, e, Number of heat, cold, and wind records. b, d, f, Number of
heat, cold, and wind records per latitude.
29
2 4 6 8 10
1.0
1.5
2.0
2.5
3.0
3.5
4.0RMSE (Kelvin)
a All events, t2m
GraphCast oper
Pangu-Weather oper
HRES
2 4 6 8 10
2
4
6
8
b Heat records
2 4 6 8 10
Lead time (days)
1
2
3
4
5
6
c Cold records
2 4 6 8 10
Lead time (days)
0.6
0.8
1.0
1.2
1.4
1.6
1.8RMSE (m/s)
d All events, w10
2 4 6 8 10
Lead time (days)
1
2
3
4
5
e Wind records
Supplementary Fig. 14: Model performance of operational forecasts on all events and
record-breaking events in 2020. RMSE of 2m temperature and 10m wind speed over land
(excluding the Antarctic region) of HRES, Pangu-Weather operational, and GraphCast opera-
tional for all events ( a, d) and only record-breaking events ( b, c, e) in 2020. The transparent
shaded areas indicate 95% confidence bands.
30
1
2
3
4
5
6Lead time 12 hours
a Heat records (Kelvin)
GraphCast oper
Pangu-Weather oper
HRES
2
4
6
8
10
b Cold records (Kelvin)
2
4
6
8
10
c Wind records (m/s)
2
3
4
5
6
7
8Lead time 2 days
d
2
4
6
8
10
e
2
4
6
8
10
f
0 2 4 6
4
6
8
10Lead time 5 days
g
0 2 4 6
2
4
6
8
10
12
14
h
0 2 4 6
4
6
8
10
12
i
Record exceedances
Supplementary Fig. 15: RMSE of operational forecasts against record exceedances
for different lead times in 2020 . RMSE of 2m temperature and 10m wind speed over
land (excluding the Antarctic region) of HRES, Pangu-Weather operational, and GraphCast
operational in 2020, for events that exceed the record at least by a certain margin, for lead
time 12 hours ( a–c), 2 days ( d–f), 5 days ( g–i). The transparent shaded areas indicate 95%
confidence bands.
31
7
6
5
4
3
2
1
0
Lead time 12 hours
a Heat records (Kelvin)
GraphCast oper
Pangu-Weather oper
HRES
0
2
4
6
8
b Cold records (Kelvin)
100
80
60
40
20
0
c Wind records (m/s)
8
6
4
2
0
Lead time 2 days
d
0
2
4
6
8
e
120
100
80
60
40
20
0
f
0 2 4 6
12
10
8
6
4
2
0
Lead time 5 days
g
0 2 4 6
0
2
4
6
8
10
12
h
0 2 4 6
140
120
100
80
60
40
20
0
i
Record exceedances
Supplementary Fig. 16: Forecast bias of operational forecasts against record ex-
ceedances for different lead times in 2020 . Forecast bias of 2m temperature and 10m
wind speed over land (excluding the Antarctic region) of HRES, Pangu-Weather operational,
and GraphCast operational in 2020, for events that exceed the record at least by a certain
margin, for lead time 12 hours ( a–c), 2 days ( d–f), forecast bias for lead time . g–i, forecast
bias for lead time 5 days. The transparent shaded areas indicate 95% confidence bands.
32
2 4 6 8 10
Lead time (days)
0
25
50
75
100
125
150
175
200Record count
a Heat records
2 4 6 8 10
Lead time (days)
0
20
40
60
80
100
120
b Cold records
2 4 6 8 10
Lead time (days)
0
50
100
150
200
250
300
350
c Wind records
HRES-fc0
HRES
HRES TP
GraphCast oper
GraphCast oper TP
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0Precision
d
0.0 0.2 0.4 0.6 0.8 1.0
Recall
e
0.0 0.2 0.4 0.6 0.8 1.0
Recall
f
HRES 12h
HRES 2d
HRES 5d
GraphCast oper 12h
GraphCast oper 2d
GraphCast oper 5d
HRES-fc0 HRES GraphCast oper
HRESGraphCast operPangu-Weather oper
0.60
0.52 0.47
0.39 0.37 0.49
g
HRES-fc0 HRES GraphCast oper
0.65
0.57 0.56
0.21 0.21 0.25
h
HRES-fc0 HRES GraphCast oper
0.56
0.25 0.25
0.14 0.14 0.28
i
Supplementary Fig. 17: Operational forecasts of the occurrence of record-breaking
events. a–c, counts (in thousands) of heat, cold, and wind records over land (excluding the
Antarctic region) in 2020 in the ground truth HRES-fc0 data, and GraphCast operational and
HRES forecast data, as well as counts of their true positives (TP). d–f, Precision and recall
curve of GraphCast operational and HRES forecasts when the record data are used as the
threshold. g–i, Correlations between the indicator functions of whether the ground truth or
2-day forecast data exceed the record.
33
2 4 6 8 10
Lead time (days)
0
25
50
75
100
125
150
175
200Record count
a Heat records
2 4 6 8 10
Lead time (days)
0
20
40
60
80
100
120
b Cold records
2 4 6 8 10
Lead time (days)
0
50
100
150
200
250
300
350
c Wind records
HRES-fc0
HRES
HRES TP
Pangu-Weather oper
Pangu-Weather oper TP
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0Precision
d
0.0 0.2 0.4 0.6 0.8 1.0
Recall
e
0.0 0.2 0.4 0.6 0.8 1.0
Recall
f
HRES 12h
HRES 2d
HRES 5d
Pangu-Weather oper 12h
Pangu-Weather oper 2d
Pangu-Weather oper 5d
Supplementary Fig. 18: Pangu-Weather operational forecast of the occurrence of
record-breaking events over land (excluding the Antarctic region) in 2020 . a–c,
Counts (in thousands) of heat, cold, and wind records in the ground truth HRES-fc0 data, and
Pangu operational and HRES forecast data, as well as counts of their true positives (TP). d–f,
Precision and recall curve of Pangu operational and HRES forecasts when the record data are
used as the threshold. g–i, Correlations between the indicator functions of whether the ground
truth or 2-day forecast data exceed the record.
34
2 4 6 8 10
2
4
6
8Global
a Heat records
GraphCast oper
Pangu-Weather oper
HRES
2 4 6 8 10
1
2
3
4
5
6
b Cold records
2 4 6 8 10
1
2
3
4
5
6
c Wind records
2 4 6 8 10
2
4
6
8
10NHem Extratropics
d
2 4 6 8 10
2
4
6
8
e
2 4 6 8 10
1
2
3
4
5
6
f
2 4 6 8 10
2
4
6
8
10
12SHem Extratropics
g
2 4 6 8 10
2
4
6
8
10
h
2 4 6 8 10
2
4
6
8
i
2 4 6 8 10
1
2
3
4
5Tropics
j
2 4 6 8 10
1
2
3
4
k
2 4 6 8 10
1.0
1.5
2.0
2.5
3.0
3.5
l
Lead time (days)
Supplementary Fig. 19: Regional RMSE of 2m temperature and 10m wind speed over
land for operational forecasts in 2020 . a–c, RMSE for the whole globe. d–f, RMSE
for Northern Hemisphere Extratropics (NHem Extratropics, i.e., grid cells with latitude in
[20, 90]). g–i, RMSE for Southern Hemisphere Extratropics (SHem Extratropics, i.e., grid cells
with latitude in [ −90, −20]). j–l, RMSE for Tropics (grid cells with latitude in [ −20, 20]). The
transparent shaded areas indicate 95% confidence bands.
35
2 4 6 8 10
1.0
1.5
2.0
2.5
3.0
3.5RMSE
a Heat records (Kelvin)
GraphCast oper
HRES
2 4 6 8 10
0.50
0.75
1.00
1.25
1.50
1.75
2.00
b Cold records (Kelvin)
2 4 6 8 10
1.0
1.5
2.0
2.5
3.0
c Wind records (m/s)
Lead time (days)
Supplementary Fig. 20: RMSE when conditioning on the forecasts rather than the
observational ground truth. RMSE of heat ( a), cold ( b), and wind records ( c) over land
(excluding the Antarctic region) in 2020, when the record-breaking events are selected using
the HRES and GraphCast operational forecasts (when both of them issue a record-breaking
forecast). The transparent shaded areas indicate 95% confidence bands.
36
C Supplementary figures for the evaluation of non-operational
forecasts in 2018
60°S
30°S
0°
30°N
60°N
90°N
a Heat records
0-5
5-10
10-15
15-20
0 500
60°S
30°S
0°
30°N
60°N
90°N
b Count
60°S
30°S
0°
30°N
60°N
90°N
c Cold records
0-5
5-10
10-15
0 200
60°S
30°S
0°
30°N
60°N
90°N
d
180° 120°W 60°W 0° 60°E 120°E 180°
60°S
30°S
0°
30°N
60°N
90°N
e Wind records
0-5
5-10
10-15
0 200
60°S
30°S
0°
30°N
60°N
90°N
f
Supplementary Fig. 21: Number of records over land (excluding the Antarctic region)
in 2018 in ERA5 . a, c, e, Number of heat, cold, and wind records. b, d, and f, Number of
heat, cold, and wind records per latitude.
37
2 4 6 8 10
1.0
1.5
2.0
2.5
3.0
3.5
4.0RMSE (Kelvin)
a All events, t2m
GraphCast
Pangu-Weather
HRES
2 4 6 8 10
1
2
3
4
5
6
b Heat records
2 4 6 8 10
Lead time (days)
2
4
6
8
10
c Cold records
2 4 6 8 10
Lead time (days)
0.6
0.8
1.0
1.2
1.4
1.6
1.8RMSE (m/s)
d All events, w10
2 4 6 8 10
Lead time (days)
1
2
3
4
5
6
e Wind records
Supplementary Fig. 22: Model performance on all events and record-breaking events
in 2018. a–e, RMSE of 2m temperature and 10m wind speed over land (excluding the Antarctic
region) of HRES, Pangu-Weather, and GraphCast for all events (a, d) and only record-breaking
events (b, c, e) in 2018. The transparent shaded areas indicate 95% confidence bands.
38
0.75
1.00
1.25
1.50
1.75
2.00
2.25Lead time 12 hours
a Heat records (Kelvin)
1
2
3
4
5
6
7
b Cold records (Kelvin)
GraphCast
Pangu-Weather
HRES
1
2
3
4
5
c Wind records (m/s)
1.2
1.4
1.6
1.8
2.0
2.2
2.4
2.6Lead time 2 days
d
2
3
4
5
6
7
8
e
2
3
4
5
6
7
8
f
0 1 2 3 4
2.5
3.0
3.5
4.0
4.5
5.0Lead time 5 days
g
0 1 2 3 4
4
6
8
10
h
0 1 2 3 4
4
6
8
10
12
i
Record exceedances
Supplementary Fig. 23: RMSE against record exceedances in 2018. RMSE of 2m temper-
ature and 10m wind speed over land (excluding the Antarctic region) of HRES, Pangu-Weather,
and GraphCast in 2018, for events that exceed the record by at least a certain margin, for lead
time 12 hours ( a–c), 2 days ( d–f), and 5 days ( g–i). The transparent shaded areas indicate
95% confidence bands.
39
10
8
6
4
2
0
Lead time 12 hours
a Heat records (Kelvin)
GraphCast
Pangu-Weather
HRES 0
1
2
3
4
5
b Cold records (Kelvin)
3
2
1
0
c Wind records (m/s)
12
10
8
6
4
2
0
Lead time 2 days
d
0
1
2
3
4
5
6
e
7
6
5
4
3
2
1
0
f
0 1 2 3 4
25
20
15
10
5
0
Lead time 5 days
g
0 1 2 3 4
0
2
4
6
8
10
h
0 1 2 3 4
10
8
6
4
2
0
i
Record exceedances
Supplementary Fig. 24: Forecast bias against record exceedances in 2018 . Forecast bias
of 2m temperature and 10m wind speed over land (excluding the Antarctic region) of HRES,
Pangu-Weather, and GraphCast in 2018, for events that exceed the record by at least a certain
margin, for lead time 12 hours ( a–c), 2 days (d–f), and 5 days ( g–i).
40
2 4 6 8 10
1
2
3
4
5
6
7Global
a Heat records
GraphCast
Pangu-Weather
HRES
2 4 6 8 10
2
4
6
8
10
b Cold records
2 4 6 8 10
1
2
3
4
5
6
7
c Wind records
2 4 6 8 10
1
2
3
4
5
6
7NHem Extratropics
d
2 4 6 8 10
2
4
6
8
10
12
e
2 4 6 8 10
1
2
3
4
5
6
7
f
2 4 6 8 10
2
4
6
8
10SHem Extratropics
g
2 4 6 8 10
2
4
6
8
10
h
2 4 6 8 10
2
4
6
8
10
i
2 4 6 8 10
1.0
1.5
2.0
2.5
3.0
3.5
4.0Tropics
j
2 4 6 8 10
1
2
3
4
k
2 4 6 8 10
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
l
Lead time (days)
Supplementary Fig. 25: Regional RMSE of 2m temperature and 10m wind speed over
land in 2018 . a–c, RMSE for the whole globe. d–f, RMSE for Northern Hemisphere Extra-
tropics (NHem Extratropics, i.e., grid cells with latitude in [20 , 90]). g–i, RMSE for Southern
Hemisphere Extratropics (SHem Extratropics, i.e., grid cells with latitude in [ −90, −20]). j–l,
RMSE for Tropics (grid cells with latitude in [ −20, 20]). The transparent shaded areas indicate
95% confidence bands.
41