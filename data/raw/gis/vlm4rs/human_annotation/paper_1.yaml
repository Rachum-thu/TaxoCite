title: Bi-Modal Transformer-Based Approach for Visual Question Answering in Remote Sensing Imagery
blocks:
- block_id: 0
  content: 'Remote sensing technology has undergone steady progress very recently. It powers a myriad of civilian and military
    applications, thanks to the deployment of quantitative and qualitative sensors, on the one hand, and the evolution of
    powerful hardware and software platforms, on the other hand.


    Remote sensing image analysis has been acquiring a particular interest, owing to the amount of data potentially present
    in an image, which enables many tasks such as land use/land cover classification [1], [2] and object detection [3], [4].
    In this regard, effective image analysis often remains dependent on the pinpointing of objects therein. Early systems
    relied on shallow representations which demonstrated rather limited performance. The appearance of deep learning strategies
    has brought several breakthroughs in advancing RS image understanding.


    To provide a comprehensive human-like interpretation of the RS information, it is crucial to integrate natural language
    processing (NLP) with visual understanding to go from merely recognizing objects present in the scene to describing the
    attributes of the objects and the relation between them. To this end, joint vision-language modeling has been introduced
    recently for the tasks of describing the content of images via captioning [5]–[8] and image retrieval using textual queries
    [9], [10].


    Beyond image captioning and text-to-image retrieval, recent literature suggests another query fashion, namely visual question
    answering (VQA) [11], which consists of providing feedback about an image-relevant question. Therefore, unlike prior image
    retrieval and captioning tasks, in VQA, both the image and a specific question are given as inputs (see Fig. 1). Yet,
    with respect to image captioning which normally generates a holistic natural language description of the image, VQA offers
    the advantage of retrieving finer details (e.g., the specifics of certain objects in the image such as the area they occupy
    within the image, the relative position of an object with respect to another object, and the density of certain objects
    across a particular area in the image, among others). Therefore, it is rational to regard VQA as a complementary component
    to captioning.


    A typical VQA pipeline involves four modules, namely: 1) a feature extractor to draw meaningful visual cues from the query
    image; 2) a textual feature extractor from the query question; 3) an embedding component to combine the earlier two feature
    modalities; and 4) a prediction head [12]. In the context of RS imagery, the few existing VQA models use long short-term
    memory networks (LSTMs) for question modeling. Yet in recent years, another type of model called transformer has revolutionized
    the field of NLP, pushing the state-of-the-art for several NLP tasks. Furthermore, the emergence of joint vision-language
    modeling based on transformers also showed to be a promising alternative for representation learning compared with standard
    paradigms that learn to map images to discrete labels. Instead, they learn to align images to raw text such as contrastive
    language image pretraining (CLIP) [13] and object-semantics aligned pretraining (OSCAR) [14]. This type of learning allows
    transferring knowledge to a broader range of downstream tasks such as classification, zero shot, and few-shot learning
    to name a few.


    In this work, we exploit these models for transferring knowledge to our VQA task. We use the CLIP network, based on vision
    and language transformer encoders, as feature extractor for embedding the image patches and question words into sequence
    of feature representations. Then, we learn attention mechanisms through two decoders to capture the intradependencies
    and interdependencies between these representations. In order to generate the final answer, we take the average prediction
    of two classiﬁers mounted on the top of the resulting contextual representations. The parameters of the decoders and classiﬁers
    are optimized using the well-known cross-entropy loss, while those of CLIP are kept fixed.


    The main contributions of this article can be summarized as follows.


    1) We propose a VQA approach based on visual-language transformers models;


    2) We capture the intradependencies and interdependencies within-and-between visual and textual representations using
    self-attention and co-attention mechanisms;


    3) We show experimentally that our approach can yield promising results on low-resolution and high-resolution datasets
    acquired by Sentinel-2 and Aerial sensors.


    In particular, we show that our approach can obtain competitive results using only 10% of the original training set.


    The rest of this article is organized as follows. Section II presents the related works. Section III describes the proposed
    VQA approach. Section IV presents the experimental results. Finally, Section V draws the conclusions and suggestions for
    future developments.'
  citations:
  - marker: '[1]'
    intent_label: Domain Overview
    topic_label: Image-Level Understanding Tasks
  - marker: '[2]'
    intent_label: Domain Overview
    topic_label: Image-Level Understanding Tasks
  - marker: '[3]'
    intent_label: Domain Overview
    topic_label: Image-Level Understanding Tasks
  - marker: '[4]'
    intent_label: Domain Overview
    topic_label: Image-Level Understanding Tasks
  - marker: '[5]–[8]'
    intent_label: Prior Methods
    topic_label: Image-Level Understanding Tasks
  - marker: '[9]'
    intent_label: Prior Methods
    topic_label: Image-Level Understanding Tasks
  - marker: '[10]'
    intent_label: Prior Methods
    topic_label: Image-Level Understanding Tasks
  - marker: '[11]'
    intent_label: Problem Formulation
    topic_label: Image-Level Understanding Tasks
  - marker: '[12]'
    intent_label: Prior Methods
    topic_label: Cross-Modal Fusion, Alignment, and Outputs
  - marker: '[13]'
    intent_label: Prior Methods
    topic_label: Contrastive Vision-Language Encoders
  - marker: '[14]'
    intent_label: Prior Methods
    topic_label: Contrastive Vision-Language Encoders
- block_id: 1
  content: In the following sections, we present a review of some notable works in VQA in the context of computer vision and
    remote sensing followed by the utilization of transformers for solving other remote sensing tasks.
  citations: []
- block_id: 2
  content: 'The literature suggests several VQA works in computer vision, thanks to the existing abundant large-scale datasets
    and rich object annotations. Respectively, only a few contributions have been developed so far in remote sensing, owing
    mainly to the scarcity of diverse and rich ground truths. For instance, among the notable works proposed in the general
    computer vision literature, one can find the work of [11], which poses the problem of open-ended VQA by prompting selective
    questions about the image at hand. To this end, a dataset that collects roughly 0.25-M images and 0.76-M respective questions,
    and 10-M answers was built. The questions ranged from a holistic style such as “how many bikes are there?” or “what kind
    of store is this?” to an action-associated style such as “what is the woman reaching for?” or “can you park here?” or
    “why is the boy on the right freaking out?” The work in [15] attempts to associate image regions to the query text by
    concatenating the question and answer representations, and combining them with weighted image region features. Such an
    approach has proven efficient to tell the network where to look in the image given a question. The work in [16] pays attention
    not only to the relevant image regions but also to the words that compose the posed question. Thus, a hierarchical visual-textual
    pipeline is presented by applying co-attention on both the image and the question, where word level, phrase level, and
    question level embeddings are modeled. In [31], VQA was addressed within an adversarial learning approach in order to
    augment training examples without altering the visual and semantic properties of ground-truth data.


    Both question and image cues are essential in VQA. The work in [17] supplicates this information pair with cues from the
    answer itself, where image and question features are fused and branched out into two heads which are availed to: 1) predict
    the answer and 2) reattend image objects under guidance of answer representations, respectively.


    Recent trends consider scenarios in which a question and a set of images are presented at the input, where the query regards
    the relations of objects or image portions within the image set, or about a topic/scene relevant to the image set [18].
    Video-based VQA is another promising trend [19].'
  citations:
  - marker: '[11]'
    intent_label: Problem Formulation
    topic_label: Task Scoping and Formulations
  - marker: '[15]'
    intent_label: Prior Methods
    topic_label: Attention-Based Fusion Blocks
  - marker: '[16]'
    intent_label: Prior Methods
    topic_label: Attention-Based Fusion Blocks
  - marker: '[31]'
    intent_label: Prior Methods
    topic_label: Training Signals and Objectives
  - marker: '[17]'
    intent_label: Prior Methods
    topic_label: Attention-Based Fusion Blocks
  - marker: '[18]'
    intent_label: Domain Overview
    topic_label: Image-Level Understanding Tasks
  - marker: '[19]'
    intent_label: Domain Overview
    topic_label: Multispectral and Temporal Modeling
- block_id: 3
  content: 'VQA in remote sensing images, however, is still at its debut and the amount of contributions is sharply scarce.
    For instance, in [20], low- and high-resolution datasets were constructed by leveraging OpenStreetMap (containing geo-localized
    information provided by volunteers) to release questions and answers relevant to the images. Convolutional neural networks
    (CNNs) and LSTM networks were adopted to model the visual and textual cues, respectively. The resulting textual and visual
    features are fused using point-wise multiplication. In [21], convolutional features and word vectors were used to capture
    the visual and natural language contents, respectively. Mutual attention is adopted to improve the alignment between the
    two feature modalities, topped with a fully connected (FC) layer to output the answer. In another work, a large VQA dataset
    was built and released [22]. In particular, question generation is based on image labels explicitly and proceeds by: 1)
    composing yes/no queries about the presence of one or more land cover classes and 2) composing which/what questions regarding
    land cover classes.'
  citations:
  - marker: '[20]'
    intent_label: Prior Methods
    topic_label: Rule-Driven QA and Grounding Labels
  - marker: '[21]'
    intent_label: Prior Methods
    topic_label: Attention-Based Fusion Blocks
  - marker: '[22]'
    intent_label: Prior Methods
    topic_label: Rule-Driven QA and Grounding Labels
- block_id: 4
  content: 'Transformers are now regarded as de facto standard in NLP tasks. Recently, they have been extended to vision tasks
    yielding the so-called ViT network [23]. In the context of remote sensing, some notable works have been developed based
    on these backbones. For example, Bashmal et al. [24] present a multilabel classification method for UAV images. Bazi et
    al. [25] proposed an extensive evaluation of ViT [25] for scene classification. In another work, He et al. [26] introduced
    a hyperspectral image classification model consisting of spectral and spatial transformer blocks. Chen et al. [27] introduced
    a transformer-based change detection method in order to model contexts within the bi-temporal image which enhances the
    recognition of objects of interest. In another work [28], the transformer model has been applied for crop circle detection.
    In the context of image captioning, Shen et al. [29], [30] used the transformer as a replacement to LSTM. Both works confirm
    its superiority compared with LSTM for sequences modeling.


    It can be seen that recent literature regards transformers as an efficient tool for image modeling thanks to the attention
    mechanism which allows them to model inter region relations.'
  citations:
  - marker: '[23]'
    intent_label: Prior Methods
    topic_label: Pretraining Backbones and Modal Encoders
  - marker: '[24]'
    intent_label: Prior Methods
    topic_label: Image-Level Understanding Tasks
  - marker: '[25]'
    intent_label: Prior Methods
    topic_label: Image-Level Understanding Tasks
  - marker: '[26]'
    intent_label: Prior Methods
    topic_label: Spectral/Temporal Encoding and Masking
  - marker: '[27]'
    intent_label: Prior Methods
    topic_label: Spectral/Temporal Encoding and Masking
  - marker: '[28]'
    intent_label: Prior Methods
    topic_label: Image-Level Understanding Tasks
  - marker: '[29]'
    intent_label: Prior Methods
    topic_label: Image-Level Understanding Tasks
  - marker: '[30]'
    intent_label: Prior Methods
    topic_label: Image-Level Understanding Tasks
- block_id: 5
  content: 'Let us consider a training set Dtr = {Xi, qi, yi}_i=1^Ntr composed of Ntr input image and question pairs (Xi,
    qi) associated with the desired categorical answer yi ∈ R^C where C represents the number of possible answers. In this
    work, we consider closed-ended VQA, where the answers are given in the form of “yes,” “no,” object counting, area covered
    by particular objects, urban, and rural. Our aim is to learn a model that can predict answers for a set of unseen N_ts
    test image and question pairs Dts = {Xj, qj}_j=1^Nts.


    Thus, this VQA setting can be regarded as a multiclass classification problem with bimodal heterogonous inputs coming
    from vision and NLP worlds. In the following sections, we provide detailed descriptions of our VQA model shown in Fig.
    2.'
  citations: []
- block_id: 6
  content: "A common practice for learning visual representations is to use vision models pretrained on large scale datasets\
    \ such as ImageNet dataset. Yet, our task involves learning from both images and texts modalities. For such purpose, we\
    \ propose to explore vision-language pretrained models as an alternative solution. In particular, we adopt the CLIP model\
    \ that is based on dual transformers pretrained on matching 400-M image-text pairs through contrastive loss optimization.\n\
    \nThe text encoder is a Bert-like transformer architecture [31]. The process starts by tokenizing the words of the question\
    \ qi appended with two special tokens [SOS] and [EOS] representing the start and end of the sequence into unique numeric\
    \ numbers. The vocabulary size is equal to 49 408. To facilitate batch processing, the text encoder uses a sequence with\
    \ a fixed length equal to 77 in addition to a word embedding layer to embed the sequence into features of dimension 512.\
    \ To supply the sequence with information about the order of each word, a learnable positional embedding is added. The\
    \ resulting initial representation is then fed as input through multiple identical layers (i.e., 12 layers) to generate\
    \ the final representation.\n\nEach layer of this encoder, as shown in Fig. 3, uses a multihead self-attention (MSA) block\
    \ followed by a small multilayer perceptron (MLP) block. Both blocks are connected with skip connections and use LayerNorm\
    \ (LN) [32]. Compared with batch normalization (BN), LN aims at estimating the normalization statistics from the summed\
    \ inputs to the neurons within a hidden layer. It has been applied in recurrent networks as a solution for improving the\
    \ generalization ability. Lately, it has been adopted in transformers. The MLP is a simple feed-forward network consisting\
    \ of two FC layers with Gaussian error linear unit (GELU) activation function applied in between [33]. The GELU activation\
    \ function is expressed as follows:\n\nGELU(x) = x\x02(x) (1)\n\nwhere \x02(x) is the standard Gaussian cumulative distribution\
    \ function. The choice of this function was made on the basis of prior NLP literature where it has been found that it\
    \ handles better the problem of vanishing gradient compared with other activations. The main goal of the MSA block is\
    \ to model the long-range dependencies between a specific token and all other tokens in the sequence. Each MSA block is\
    \ composed of several scaled dot-product attention mechanisms. The scaled dot-product attention function is performed\
    \ in parallel by multiple heads to enable the model to pay attention to more than one feature at a time. The results of\
    \ all heads are concatenated and linearly projected into the desired dimension using a weighted matrix.\n\nFinally, a\
    \ linear layer is used to map the features to the dimension 512. To meet the requirements of our VQA task, we remove the\
    \ pooling operation to keep the representation of each token in the sentence, thus resulting in a sequence of dimension\
    \ 77 × 512.\n\nOn the other side, the image encoder is based on the vision transformer proposed by Dosovitskiy et al.\
    \ [23] termed as ViT16. First, the image Xi is resized to the dimension 224 × 224 × 3 pixels. Then, it is divided into\
    \ 196 nonoverlapping patches of spatial dimension 16 × 16 pixels. These patches are flattened and mapped using a linear\
    \ embedding layer. Positional encoding is appended to the patch representations. The resulting embedded sequence of patches\
    \ is then fed to an encoder similar to the one used previously for NLP. As done for the NLP transformer, we discard the\
    \ pooling operation to keep the sequence of visual representations of dimension 196 × 512 where each token of dimension\
    \ 512 represents a particular region of the image of dimension 16 × 16 pixels."
  citations:
  - marker: '[31]'
    intent_label: Model/Architecture Adoption
    topic_label: Pretraining Backbones and Modal Encoders
  - marker: '[23]'
    intent_label: Model/Architecture Adoption
    topic_label: Pretraining Backbones and Modal Encoders
  - marker: '[32]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Other Topics
  - marker: '[33]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Other Topics
- block_id: 7
  content: 'Let us consider {FXi, Fqi}_i=1^Ntr as the visual and textual representations obtained from CLIP, with FXi ∈ R^{1×196×512}
    and Fqi ∈ R^{1×77×512}. As mentioned previously, 196 and 77 represent the length of the visual and textual sequences,
    while 512 represent the feature dimension. We propose to capture from these visual and textual representations the internal
    dependencies among each modality as well as the cross-modality correlation. For such purpose, we use two decoders with
    similar architecture for generating these contextual representations. For consistency, we use two decoders with the same
    architecture as the blocks adopted in CLIP yet our decoders contain additional layers for co-attention. For example, we
    show in Fig. 2 the decoder part related to the text branch.


    It is also composed of MSA for capturing the intradependencies among the textual representation followed by multihead
    co-attention (MCA) for capturing the correlation with the visual representations. Here also, both layers are connected
    with skip connections and use LN. The MSA layer receives Fqi and projects it using the weight matrices WQ, WK, and WV
    ∈ R^{512×512}, such that Q = WQ Fqi, K = WK Fqi, and V = WV Fqi.


    The MSA comprises eight independent self-attention heads operating in parallel, and each head computes a different attention
    score using the scaled dot-product similarity between Q, K, and V


    Attention(Q, K, V) = softmax( QK^T / sqrt(512) ) V. (2)


    Then, the outputs of all heads are concatenated and projected with another learnable weights matrix. The MCA block works
    in the same way as MSA except the key and values are now coming from the visual features: Q = WQ Fqi, K = WK FXi, and
    V = WV FXi. At the output of the text decoder, we obtain the conditional textual representations referred as F''_qi. On
    the other side, the image decoder yields the conditional visual representations F''_Xi by considering the visual representation
    as queries and the textual representations as keys and values.'
  citations: []
- block_id: 8
  content: 'For obtaining the answer, we use one classiﬁer per modality. For each modality, we use global average pooling
    (GAP) to convert the sequence into a single feature of dimension 512. Then, we feed this feature to an FC layer with Softmax
    activation function. Thus, the outputs of the text and image classiﬁers are given as follows:


    ˆy^q_i = Softmax( FC( GAP( F''_qi ) ) )

    ˆy^X_i = Softmax( FC( GAP( F''_Xi ) ) ). (3)


    Then, we train the network by optimizing the standard cross-entropy loss in a batch-wise manner over the training set.
    The total loss over the training set is given as follows:


    L = λ1 (1/Ntr) Σ_{i=1}^{Ntr} CrossEntropy( yi, ˆy^q_i ) + λ2 (1/Ntr) Σ_{i=1}^{Ntr} CrossEntropy( yi, ˆy^X_i ) (4)


    where λ1 and λ2 are the two regularization parameters controlling the contributions of both losses. In our experiments,
    we set them to 0.5. In the prediction phase, we take the average prediction of both classiﬁers


    ˆyi = 0.5 ( ˆy^q_i + ˆy^X_i ). (5)'
  citations: []
- block_id: 9
  content: ''
  citations: []
- block_id: 10
  content: '1) Low Resolution: This dataset consists of nine different tiles of 10 m of spatial resolution, covering 6.55
    km^2, which are based on Sentinel-2 images acquired over The Netherlands (RGB bands). These tiles were divided into 772
    images of size 256 × 256 pixels. Then, 77 232 question–answer pairs were generated. For each image, many questions–answers
    were given (around 100 question–answer per image). The questions are mainly related to object presence (answer: yes/no),
    comparisons between objects (answer: yes/no), rural or urban areas, and object counting. Regarding object counting, it
    was proposed in [20] to assign object counts in specific ranges (answer: equal to 0; greater than 0 and smaller or equal
    to 10; greater than 10 and smaller or equal to 100; greater than 100 and smaller or equal to 1000; and greater than 1000).
    Thus, the maximum number of possible answers for this dataset is C = 11. We follow the split proposed in [20] by taking
    77.8% of the original tiles, 11.1% and 11.1%, for training, validation, and test, respectively. In Fig. 4(a), we depict
    examples of images, questions, and answers. In contrast, in Fig. 5(a) and (b), we show the distribution of the answers
    for this dataset for both train and test sets.


    2) High Resolution: This dataset contains 161 different tiles of aerial images with 15 cm of spatial resolution. These
    tiles were divided into 10 659 images of size 512 × 512 pixels, where each image covers covering 5898 m^2. Similarly,
    1 066 316 question–answer pairs were generated (around 100 question–answer per image). The questions are related to object
    counting (answer: from 0 to 89 object counts), presence (answer: yes/no), comparison (answer: yes/no), and area covered
    by particular objects in specify ranges similar to object counting for the LR dataset (answer: equal to 0; greater than
    0 and smaller or equal to 10; greater than 10 and smaller or equal to 100; greater than 100 and smaller or equal to 1000;
    and greater than 1000). Based on this division, the number for classes in this dataset is C = 99. The training set holds
    61.5% of the tiles, 11.2% are allocated to the validation set, and the rest as test sets (20.5% for test set 1 and 6.8%
    for test set 2). Here, we notice that test set 2 is used for assessing the robustness of the model on image acquired at
    different locations. In Fig. 4(b), we show the examples of images, questions, and answers for this HR dataset. As done
    for the LR dataset, we show in Fig. 5(c) and (d), the distribution of the answers and examples of image–question pairs.
    For ease of interpretation, we present the number of counting answers as a single class yet in the experiments, we consider
    89 categorical counting classes as mentioned previously. It is worth recalling that the images of Fig. 4 were selected
    randomly so as to cover a variety of question types (count, presence, comparison, and urban/rural) for the LR dataset
    and (count, presence, comparison, and area) for the HR dataset.'
  citations:
  - marker: '[20]'
    intent_label: Setting/Protocal Adoption
    topic_label: Rule-Driven QA and Grounding Labels
- block_id: 11
  content: 'We implement our method using PyTorch library [34] on a PC with Core(TM) i9-7920X CPU, GeForce RTX 1080 Ti 11
    GB, and 64-GB RAM.


    We optimize our model using Adam optimizer [35] with a learning rate of 0.0001 and for 30 iterations. We set the minibatch
    size to 120. For data augmentation, we resize the images to 288 × 288 pixels and then we randomly crop 224 × 224 pixels.
    Then, we apply also random rotation by 20°.


    For the LR dataset, we present the accuracies per question type: count, presence, comparison, and urban/rural. In contrast,
    for the HR dataset, we group them into count, presence, comparison, and area.'
  citations:
  - marker: '[34]'
    intent_label: Resource Utilization
    topic_label: Other Topics
  - marker: '[35]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Other Topics
- block_id: 12
  content: We train the model on the training set for three different trials with different initializations. Then, we present
    the averaged classification accuracies for these three trials along with the standard deviation. From Table I, we see
    that the text classiﬁer (with the output ˆy^q_i) gives an average accuracy (AA) and overall accuracy (OA) of (85.96% and
    84.76%). While the image classiﬁer (with the output ˆy^X_i) performs slightly better, the respective scores are (86.14%
    and 84.73%). In contrast, fusing both classiﬁers (the output ˆyi) leads to (86.78% and 85.56%). We can observe that when
    the question type is relevant to presence, comparison, and urban/rural categories, the scores exceed 91%. Yet, for the
    counting class, it is only 72.22%, which indicates that this class is more challenging as we have to assign objects in
    specific ranges. Comparing our results to the work presented in [20], we observe significant improvements for all classes
    with 2.66% up to 9.66% for urban/rural and count classes, respectively. The increase in (OA and AA) accuracies is (5.29%
    and 6.48%).
  citations:
  - marker: '[20]'
    intent_label: Result Comparison
    topic_label: Task-Specific Metrics and Protocols
- block_id: 13
  content: Table II reports the scores obtained for the first HR dataset. Here, both text and image classiﬁers yield close
    results and their fusion yields slight improvement scoring an (AA and OA) equal to (84.98% and 85.30%). The counting class
    is the worst one as we obtain 69.80%. Regarding the second HR test dataset, as shown in Table III, the accuracies are
    less compared with the ﬁrst HR dataset due to the data-shift problem as the images are acquired at different locations.
    Here again, the text classiﬁer performs slightly better compared with the image classiﬁer and their fusion yields an (AA
    and OA) of (80.54% and 81.23%). Comparing these results to the method proposed in [20], we observe that our method performs
    better on both datasets.
  citations:
  - marker: '[20]'
    intent_label: Result Comparison
    topic_label: Image-Level Understanding Tasks
- block_id: 14
  content: As mentioned previously, around 100 questions were generated automatically for each image for both LR and HR datasets.
    In this experiment, we analyze the sensitivity of the model with respect to low regime training size scenarios. We repeat
    the above experiments using 20% and 10% of the training sets by sampling over questions. Table IV shows the classification
    results expressed in terms of OA and standard deviation for the LR dataset for three trials with different training subsets.
    Our model yields an (AA and OA) of (85.72% and 84.30%) and (83.83% and 82.85%) for the scenarios 20% and 10%, respectively,
    resulting a decrease of (1.06% and 1.26%) and (2.95% and 2.71%) compared with the original training set. Yet, it still
    provides better scores compared with [20] by using only 10% of the training set. Similar observations can be made for
    the HR datasets (see Tables V and VI), where the model reaches almost the same accuracies for the ﬁrst HR dataset with
    only 10% of the training set. Furthermore, it performs better for the second HR dataset with 10% of training set. This
    conﬁrms clearly the ability of the network in learning suitable visual and textual representations for the VQA task.
  citations:
  - marker: '[20]'
    intent_label: Result Comparison
    topic_label: Generalization and Data-Shift Assessment
- block_id: 15
  content: As mentioned previously, we append two transformer decoders to model the contextual relationship between the visual
    and textual image representations. To this end, we carry out experiments by removing this part and assess the performance
    of the model. The results are reported in Tables VII and VIII for LR and HR datasets, respectively. Evidently, discarding
    the decoder part incurs a significant drop in the performance on both datasets. For instance, considering the question
    of type “comparison” in the HR dataset, roughly 25% drop is observed when the decoder is discarded, and an OA accuracy
    drop of about 18% is noticed for the LR dataset, which underlines the importance of taking intermodality feature relationship
    into account.
  citations: []
- block_id: 16
  content: 'In the following, we show the attention maps provided by the proposed bi-modal transformer model over questions
    and images to understand relevance of text and image regions for generating the answer. To capture the attention maps
    from the output token to the input space of the network, we adopt the attention rollout strategy proposed in [36]. Basically,
    at every transformer block, an attention matrix that quantiﬁes attention ﬂow from a certain token in the previous layer
    to another token in the next layer is envisioned. Therefore, to track down attention ﬂow between two matrices, their respective
    weight matrices are recursively multiplied. As proposed in the original paper [36], we consider the average of the attention
    heads.


    Fig. 6 shows the attention for text and question pairs generated from HR dataset. It can be observed that when the question
    queries about more than one object, such as the top left example which compares the amount of “roads” with respect to
    “buildings,” the network seems to emphasize on both of the objects as shown in the attention map. It can also be noted
    that the object “roads” displays higher peaks in the attention map, owing to the fact that it is the object that is stressed
    in the query question (“more” roads than buildings).


    Interestingly, querying the amount of roads (top right example) incites the network to focus on the object “roads” in
    the image. The same holds for the two examples at the bottom where the network looks at the whereabouts of the objects
    that are present in the query questions (i.e., buildings and parkings, respectively). This suggests an evident consistency
    between the input question and the attention of the network.


    In Fig. 7, we depict the attention maps produced from the LR dataset. For instance, in the top example, when the question
    queries about the object “building,” the attention maps reflect relevant areas in the image, paying less attention to
    the bottom of the image where no buildings are observed. When the query regards the object “gardens,” however, more attention
    is given to the bottom of the image (rightmost attention map of the top example). As per the second example, querying
    the presence of the “square building” object stresses diverse areas across the image although the image consists mostly
    of agricultural fields, which may be due to the constraint “square” that characterizes both the buildings and the fields.
    When querying about the number of buildings (second map), the attention drops signiﬁcantly with respect to the previous
    case, while querying about the presence of water (last map), sparse attention can be observed. On this point, it is to
    note that the attention maps in the LR case suggest less relevance with the image compared with the HR case that offers
    major spatial details.


    Although the illustrated examples show a promising coherence between the questions and the network’s focus areas in particular
    for the HR dataset, we believe that even ﬁner results can be obtained. In particular, it is to point out that the network
    does not receive the bounding box annotations of the objects included in the questions, which is an important information
    that can support the network in the training. In contrast, we deal with images that were acquired from a top-view, which
    is known to disregard the facade of the objects of interest. Another contributing factor is the construction of the dataset
    itself, where most of the questions were produced automatically, yet many of them can be rather noisy and could be misleading
    to the network.'
  citations:
  - marker: '[36]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Explanations and Consistency Checks
- block_id: 17
  content: In this article, we have introduced a VQA approach based on vision and language transformers. We have proposed
    to capture the dependencies between visual and textual representations using co-attention mechanisms. In the experimental
    results, we have shown that our approach can yield promising results in terms of classification accuracy compared with
    the existing solutions with reduced training set size. For future developments, we propose to enhance the method by increasing
    the diversity among the two classiﬁers. In terms of dataset, we suggest adding object-based annotations to better capture
    the correlation between the textual and visual cues. Another possible interesting development is to extend the dataset
    to open-ended VQA scenarios where the answer can be given as a natural language.
  citations: []
