title: 'RSVQA: Visual Question Answering for Remote Sensing Data'
abstract: 'This paper introduces the task of visual question answering for remote
  sensing data (RSVQA). Remote sensing images contain a wealth of information which
  can be useful for a wide range of tasks including land cover classification, object
  counting or detection. However, most of the available methodologies are task-specific,
  thus inhibiting generic and easy access to the information contained in remote sensing
  data. As a consequence, accurate remote sensing product generation still requires
  expert knowledge. With RSVQA, we propose a system to extract information from remote
  sensing data that is accessible to every user: we use questions formulated in natural
  language and use them to interact with the images. With the system, images can be
  queried to obtain high level information specific to the image content or relational
  dependencies between objects visible in the images. Using an automatic method introduced
  in this article, we built two datasets (using low and high resolution data) of image/question/answer
  triplets. The information required to build the questions and answers is queried
  from OpenStreetMap (OSM). The datasets can be used to train (when using supervised
  methods) and evaluate models to solve the RSVQA task. We report the results obtained
  by applying a model based on Convolutional Neural Networks (CNNs) for the visual
  part and on a Recurrent Neural Network (RNN) for the natural language part to this
  task. The model is trained on the two datasets, yielding promising results in both
  cases.'
abstract_is_verbatim: true
segmented_markdown: "# RSVQA: Visual Question Answering for Remote Sensing Data\n\n\
  ## Abstract\n<block id=\"0\">\nThis paper introduces the task of visual question\
  \ answering for remote sensing data (RSVQA). Remote sensing images contain a wealth\
  \ of information which can be useful for a wide range of tasks including land cover\
  \ classification, object counting or detection. However, most of the available methodologies\
  \ are task-specific, thus inhibiting generic and easy access to the information\
  \ contained in remote sensing data. As a consequence, accurate remote sensing product\
  \ generation still requires expert knowledge. With RSVQA, we propose a system to\
  \ extract information from remote sensing data that is accessible to every user:\
  \ we use questions formulated in natural language and use them to interact with\
  \ the images. With the system, images can be queried to obtain high level information\
  \ specific to the image content or relational dependencies between objects visible\
  \ in the images. Using an automatic method introduced in this article, we built\
  \ two datasets (using low and high resolution data) of image/question/answer triplets.\
  \ The information required to build the questions and answers is queried from OpenStreetMap\
  \ (OSM). The datasets can be used to train (when using supervised methods) and evaluate\
  \ models to solve the RSVQA task. We report the results obtained by applying a model\
  \ based on Convolutional Neural Networks (CNNs) for the visual part and on a Recurrent\
  \ Neural Network (RNN) for the natural language part to this task. The model is\
  \ trained on the two datasets, yielding promising results in both cases.\n\n</block>\n\
  ## Introduction\n<block id=\"1\">\nREMOTE sensing data is widely used as an indirect\
  \ source of information. From land cover/land use to crowd estimation, environmental\
  \ or urban area monitoring, remote sensing images are used in a wide range of tasks\
  \ of high societal relevance. For instance, remote sensing data can be used as a\
  \ source of information for 6 of the 17 sustainable development goals as defined\
  \ by the United Nations [1]. Due to the critical nature of the problems that can\
  \ be addressed using remote sensing data, significant effort has been made to increase\
  \ its availability in the last decade. For instance, Sentinel-2 satellites provide\
  \ multispectral data with a relatively short revisiting time, in open-access. However,\
  \ while substantial effort has been dedicated to improving the means of direct information\
  \ extraction from Sentinel-2 data in the framework of a given task (e.g. classification\
  \ [2], [3]), the ability to use remote sensing data as a direct source of information\
  \ is currently limited to experts within the remote sensing and computer vision\
  \ communities. This constraint, imposed by the technical nature of the task, reduces\
  \ both the scale and variety of the problems that could be addressed with such information\
  \ as well as the number of potential end-users. This is particularly true when targeting\
  \ specific applications (detecting particular objects, e.g. thatched roofs or buildings\
  \ in a developing country [4]) which would today call for important research efforts.\
  \ The targeted tasks are often multiple and changing in the scope of a project calls\
  \ for strong expert knowledge, limiting the information which can be extracted from\
  \ remote sensing data. To address these constraints, we introduce the problem of\
  \ visual question answering (VQA) for remote sensing data.\n\nVQA is a new task\
  \ in computer vision, introduced in its current form by [5]. The objective of VQA\
  \ is to answer a free-form and open-ended question about a given image. As the questions\
  \ can be unconstrained, a VQA model applied to remote sensing data could serve as\
  \ a generic solution to classical problems involving remote sensing data (e.g. \"\
  Is there a thatched roof in this image?\" for thatched roof detection), but also\
  \ very specific tasks involving relations between objects of different nature (e.g.\
  \ \"Is there a thatched roof on the right of the river?\"). Examples of potential\
  \ questions are shown in Figure 1.\n\nTo the best of our knowledge, this is the\
  \ first time (after the first exploration in [6]) that VQA has been applied to extract\
  \ information from remote sensing data. It builds on the task of generating descriptions\
  \ of images through combining image and natural language processing to provide the\
  \ user with easily accessible, high-level semantic information. These descriptions\
  \ are then used for image retrieval and intelligence generation [7]. As seen in\
  \ this introduction, VQA systems rely on the recent advances in deep learning. Deep\
  \ learning based methods, thanks to their ability to extract high-level features,\
  \ have been successfully developed for remote sensing data as reviewed in [8]. Nowadays,\
  \ this family of methods is used to tackle a variety of tasks; for scene classification,\
  \ an early work by [9] evaluated the possibility to adapt networks pre-trained on\
  \ large natural image databases (such as ImageNet [10]) to classify hyperspectral\
  \ remote sensing images. More recently, [11] used an intermediate high level representation\
  \ using recurrent attention maps to classify images. Object detection is also often\
  \ approached using deep learning methods. To this effect, [12] introduced an object\
  \ detection dataset and evaluated classical deep learning approaches. Methods taking\
  \ into account the specificity of remote sensing data have been developed, such\
  \ as [13] which proposed to modify the classical approach by generating rotatable\
  \ region proposal which are particularly relevant for top-view imagery. Deep learning\
  \ methods have also been developed for semantic segmentation. In [14], the authors\
  \ evaluated different strategies for segmenting remote sensing data. More recently,\
  \ a contest organized on the dataset of building segmentation created by [15] has\
  \ motivated the development of a number of new methods to improve results on this\
  \ task [16]. Similarly, [17] introduced a contest including three tasks: road extraction,\
  \ building detection and land cover classification. Best results for each challenge\
  \ were obtained using deep neural networks: [18], [19], [20].\n\nNatural language\
  \ processing has also been used in remote sensing. For instance, [21] used a convolutional\
  \ neural network (CNN) to generate classification probabilities for a given image,\
  \ and used a recurrent neural network (RNN) to generate its description. In a similar\
  \ fashion, [7] used a CNN to obtain a multi semantic level representation of an\
  \ image (object, land class, landscape) and generate a description using a simple\
  \ static model. More recently, [22] uses an encoder/decoder type of architecture\
  \ where a CNN encodes the image and a RNN decodes it to a textual representation,\
  \ while [23] projects the textual representation and the image to a common space.\
  \ While these works are use cases of natural language processing, they do not enable\
  \ interactions with the user as we propose with VQA.\n\nA VQA model is generally\
  \ made of 4 distinct components: 1) a visual feature extractor, 2) a language feature\
  \ extractor, 3) a fusion step between the two modalities and 4) a prediction component.\
  \ Since VQA is a relatively new task, an important number of methodological developments\
  \ have been published in both the computer vision and natural language processing\
  \ communities during the past 5 years, reviewed in [24]. VQA models are able to\
  \ benefit from advances in the computer vision and automatic language processing\
  \ communities for the features extraction components. However, the multi-modal fusion\
  \ has been less explored and therefore, an important amount of work has been dedicated\
  \ to this step. First VQA models relied on a non-spatial fusion method, i.e. a point-wise\
  \ multiplication between the visual and language feature vectors [5]. Being straightforward,\
  \ this method does not allow every component from both feature vectors to interact\
  \ with each other. This interaction would ideally be achieved by multiplying the\
  \ first feature vector by the transpose of the other, but this operation would be\
  \ computationally intractable in practice. Instead, [25] proposed a fusion method\
  \ which first selects relevant visual features based on the textual feature (attention\
  \ step) and then, combines them with the textual feature. In [26], the authors used\
  \ Tucker decomposition to achieve a similar purpose in one step. While these attention\
  \ mechanisms are interesting for finding visual elements aligned with the words\
  \ within the question, they require the image to be divided in a regular grid for\
  \ the computation of the attention, and this is not suitable to objects of varying\
  \ size. A solution is presented in [27], which learns an object detector to select\
  \ relevant parts of the image. In this research, we use a non-spatial fusion step\
  \ to keep the model part relatively simple. Most traditional VQA works are designed\
  \ for a specific dataset, either composed of natural images (with questions covering\
  \ an unconstrained range of topics) or synthetic images. While interesting for the\
  \ methodological developments that they have facilitated, these datasets limit the\
  \ potential applications of such systems to other problems. Indeed, it has been\
  \ shown in [28] that VQA models trained on a specific dataset do not generalize\
  \ well to other datasets. This generalization gap raises questions concerning the\
  \ applicability of such models to specific tasks.\n\nA notable use-case of VQA is\
  \ helping visually impaired people through natural language interactions [29]. Images\
  \ acquired by visually impaired people represent an important domain shift, and\
  \ as such a challenge for the applicability of VQA models. In [30], the authors\
  \ confirm that networks trained on generic datasets do not generalize to their specific\
  \ one. However, they manage to obtain much better results by fine-tuning or training\
  \ models from scratch on their task-specific dataset.\n\nIn this study, we propose\
  \ a new application for VQA, specifically for the interaction with remote sensing\
  \ images. To this effect, we propose the first remote sensing-oriented VQA datasets,\
  \ and evaluate the applicability of this task on remote sensing images. We propose\
  \ a method to automatically generate remote sensing-oriented VQA datasets from already\
  \ available human annotations in section II and generate two datasets. We then use\
  \ this newly-generated data to train our proposed RSVQA model with a non-spatial\
  \ fusion step described in section III. Finally, the results are evaluated and discussed\
  \ in section IV.\n\nOur contribution are the following:\n- a method to generate\
  \ remote sensing-oriented VQA datasets;\n- 2 datasets;\n- the proposed RSVQA model.\n\
  \nThis work extends the preliminary study of [6] by considering and disclosing a\
  \ second larger dataset consisting of very high resolution images. This second dataset\
  \ helps testing the spatial generalization capability of VQA and provides an extensive\
  \ discussion highlighting remaining challenges. The method to generate the dataset,\
  \ the RSVQA model and the two datasets are available on https://rsvqa.sylvainlobry.com/.\n\
  \n</block>\n## Datasets\n<block id=\"2\">\n\n</block>\n### Method\n<block id=\"\
  3\">\nAs seen in the introduction, a main limiting factor for VQA is the availability\
  \ of task-specific datasets. As such, we aim at providing a collection of remote\
  \ sensing images with questions and answers associated to them. To do so, we took\
  \ inspiration from [31], in which the authors build a dataset of question/answer\
  \ pairs about synthetic images following an automated procedure. However, in this\
  \ study we are interested in real data (discussed in subsection II-B). Therefore,\
  \ we use the openly accessible OpenStreetMap data containing geo-localized information\
  \ provided by volunteers. By leveraging this data, we can automatically extract\
  \ the information required to obtain question/answer pairs relevant to real remotely\
  \ sensed data and create a dataset made of (image, question, answer) triplets.\n\
  \nThe first step of the database construction is to create the questions. The second\
  \ step is to compute the answers to the questions, using the OSM features belonging\
  \ to the image footprint. Note that multiple question/answer pairs are extracted\
  \ for each image.\n\n1) Question contruction: Our method to construct the questions\
  \ is illustrated in Figure 2. It consists of four main components:\n1) choice of\
  \ an element category (highlighted in red in Figure 2(a));\n2) application of attributes\
  \ to the element (highlighted in green in Figure 2(a));\n3) selection based on the\
  \ relative location to another element (highlighted in green in Figure 2(a));\n\
  4) construction of the question (highlighted in blue in Figure 2(a)).\n\nThese four\
  \ components are detailed in the following.\n\nElement category selection: First,\
  \ an element category is randomly selected from the element catalog. This catalog\
  \ is built by extracting the elements from one of the following OSM layers: road,\
  \ water area, building and land use. While roads and water areas are directly treated\
  \ as elements, buildings and land use related objects are defined based on their\
  \ \"type\" field, as defined in the OSM data specification. Examples of land use\
  \ objects include residential area, construction area, religious places, . . . Buildings\
  \ are divided in two categories: commercial (e.g. retail, supermarket, ...) and\
  \ residential (e.g. house, apartments, . . .).\n\nAttributes application: The second\
  \ (optional) step is to refine the previously selected element category. To do so,\
  \ we randomly select from one of the two possible attribute categories:\n- Shape:\
  \ each element can be either square, rectangular or circular. Whether an element\
  \ belongs to one of these shape types is decided based on basic geometrical properties\
  \ (i.e. hard thresholds on area-to-perimeter ratio and area-to-circumscribed circle\
  \ area ratio).\n- Size: using hard thresholds on the surface area, elements can\
  \ be considered \"small\", \"medium\" or \"large\". As we are interested in information\
  \ at different scales in the two datasets, we use different threshold values, which\
  \ are described in Table I.\n\nRelative position: Another possibility to refine\
  \ the element is to look at its relative position compared to another element. We\
  \ define 5 relations: \"left of\", \"top of\", \"right of\", \"bottom of\", \"next\
  \ to\". Note that these relative positions are understood in the image space (i.e.\
  \ geographically). The special case of \"next to\" is handled as a hard threshold\
  \ on the relative distance between the two objects (less than 1000m). When looking\
  \ at relative positions, we select the second element following the procedure previously\
  \ defined.\n\nQuestion construction: At this point of the procedure, we have an\
  \ element (e.g. road), with an optional attribute (e.g. small road) and an optional\
  \ relative position (e.g. small road on the left of a water area). The final step\
  \ is to generate a \"base question\" about this element. We define 5 types of questions\
  \ of interest (\"Question catalog\" in Figure 2(a)), from which a specific type\
  \ is randomly selected to obtain a base question. For instance, in the case of comparison\
  \ questions, we randomly choose among \"less than\", \"equals to\" and \"more than\"\
  \ and construct a second element.\n\nThis base question is then turned into a natural\
  \ language question using pre-defined templates for each question type and object.\
  \ For some question types (e.g. count), more than one template is defined (e.g.\
  \ 'How many are there?', 'What is the number of ?' or 'What is the amount of ?').\
  \ In this case, the template to be used is randomly selected. The stochastic process\
  \ ensures the diversity, both in the question types and the question templates used.\n\
  \n2) Answer construction: To obtain the answer to the constructed question, we extract\
  \ the objects from the OSM database corresponding to the image footprint. The objects\
  \ corresponding to the element category and its attributes are then selected and\
  \ used depending on the question type:\n- Count: In the case of counting, the answer\
  \ is simply the number of objects.\n- Presence: A presence question is answered\
  \ by comparing the number of objects to 0.\n- Area: The answer to a question about\
  \ the area is the sum of the areas of the objects.\n- Comparison: Comparison is\
  \ a specific case for which a second element and the relative position statement\
  \ is needed. This question is then answered by comparing the number of objects to\
  \ the ones of the second element.\n- Rural/Urban: The case of rural/urban questions\
  \ is handled in a specific way. In this case, we do not create a specific element,\
  \ but rather count the number of buildings (both commercial or residential). This\
  \ number of buildings is then thresholded to a predeﬁned number depending on the\
  \ resolution of the input data (to obtain a density) to answer the question. Note\
  \ that we are using a generic definition of rural and urban areas but this can be\
  \ easily adapted using the precise definition of each country.\n\n</block>\n###\
  \ Data\n<block id=\"4\">\nFollowing the method presented in subsection II-A, we\
  \ construct two datasets with different characteristics.\n\nLow resolution (LR):\
  \ this dataset is based on Sentinel-2 images acquired over the Netherlands. Sentinel-2\
  \ satellites provide 10m resolution (for the visible bands used in this dataset)\
  \ images with frequent updates (around 5 days) at a global scale. These images are\
  \ openly available through ESA’s Copernicus Open Access Hub.\n\nTo generate the\
  \ dataset, we selected 9 Sentinel-2 tiles covering the Netherlands with a low cloud\
  \ cover. These tiles were divided in 772 images of size 256 × 256 (covering 6.55km2)\
  \ retaining the RGB bands. From these, we constructed 77′232 questions and answers\
  \ following the methodology presented in subsection II-A. We split the data in a\
  \ training set (77.8% of the original tiles), a validation set (11.1%) and a test\
  \ set (11.1%) at the tile level. This allows to limit spatial correlation between\
  \ the different splits.\n\nHigh resolution (HR): this dataset uses 15cm resolution\
  \ aerial RGB images extracted from the High Resolution Orthoimagery (HRO) data collection\
  \ of the USGS. This collection covers most urban areas of the USA, along with a\
  \ few areas of interest (e.g. national parks). For most areas covered by the dataset,\
  \ only one tile is available with acquisition dates ranging from year 2000 to 2016,\
  \ with various sensors. The tiles are openly accessible through USGS’ EarthExplorer\
  \ tool.\n\nFrom this collection, we extracted 161 tiles belonging to the North-East\
  \ coast of the USA that were split into 10′659 images of size 512×512 (each covering\
  \ 5898m2). We constructed 1′066′316 questions and answers following the methodology\
  \ presented in subsection II-A. We split the data in a training set (61.5% of the\
  \ tiles), a validation set (11.2%), and test sets (20.5% for test set 1, 6.8% for\
  \ test set 2). Test set 1 covers similar regions as the training and validation\
  \ sets, while test set 2 covers the city of Philadelphia, which is not seen during\
  \ the training. Note that this second test set also uses another sensor (marked\
  \ as unknown on the USGS data catalog), not seen during training.\n\nDifferences\
  \ between the two datasets:\nDue to their characteristics, the two datasets represent\
  \ two different possible use cases of VQA:\n- The LR dataset allows for large spatial\
  \ and temporal coverage thanks to the frequent acquisitions made by Sentinel-2.\
  \ This characteristic could be of interest for future applications of VQA such as\
  \ large scale queries (e.g. rural/urban questions) or temporal (which is out of\
  \ the scope of this study). However, due to the relatively low resolution (10m),\
  \ some objects can not be seen on such images (such as small houses, roads, trees,\
  \ . . . ). This fact severely limits the questions to which the model could give\
  \ an accurate answer.\n- Thanks to the much finer resolution of the HR dataset,\
  \ a quantity of information of interest to answer typical questions is present.\
  \ Therefore, in contrast to the LR dataset, questions concerning objects’ coverage\
  \ or counting relatively small objects can possibly be answered from such data.\
  \ However, data of such resolution is generally less frequently updated and more\
  \ expensive to acquire.\n\nBased on these differences, we constructed different\
  \ types of questions for the two datasets. Questions concerning the area of objects\
  \ are only asked in the HR dataset. On the other hand, questions about urban/rural\
  \ area classification are only asked in the LR dataset, as the level of zoom of\
  \ images from the HR dataset would prevent a meaningful answer from being provided.\n\
  \nTo account for the data distributions and error margins we also quantize different\
  \ answers in both datasets:\n- Counting in LR: as the coverage is relatively large\
  \ (6.55km2), the number of small objects contained in one tile can be high, giving\
  \ a heavy tailed distribution for the numerical answers. More precisely, while 26.7%\
  \ of the numerical answers are '0' and 50% of the answers are less than '7', the\
  \ highest numerical answer goes up to '17139'. In addition to making the problem\
  \ complex, we can argue that allowing such a range of numerical answer does not\
  \ make sense on data of this resolution. Indeed, it would be in most cases impossible\
  \ to distinguish 17139 objects on an image of 65536 pixels. Therefore, numerical\
  \ answers are quantized into the following categories:\n  - '0';\n  - 'between 1\
  \ and 10';\n  - 'between 11 and 100';\n  - 'between 101 and 1000';\n  - 'more than\
  \ 1000'.\n- In a similar manner, we quantize questions regarding the area in the\
  \ HR dataset. A great majority (60.9%) of the answer of this type are '0m2', while\
  \ the distribution also presents a heavy tail. Therefore, we use the same quantization\
  \ as the one proposed for counts for the LR dataset. Note that we do not quantize\
  \ purely numerical answers (i.e. answers to questions of type 'count') as the maximum\
  \ number of objects is 89 in our dataset. Counting answers therefore correspond\
  \ to 89 classes in the model in this case.\n\n</block>\n### Discussion\n<block id=\"\
  5\">\nQuestions/Answers distributions:\nWe show the final distribution of answers\
  \ per question type for both datasets. We can see that most question types (with\
  \ the exception of 'rural/urban' questions in the LR dataset, asked only once per\
  \ image) are close to evenly distributed by construction. The answer 'no' is dominating\
  \ the answers’ distribution for the HR dataset with a frequency of 37.7%. In the\
  \ LR dataset, the answer 'yes' occurs 34.9% of the time while the 'no' frequency\
  \ is 34.3%. The strongest imbalance occurs for the answer '0' in the HR dataset\
  \ (with a frequency of 60.9% for the numerical answer). This imbalance is greatly\
  \ reduced by the quantization process described in the previous paragraph.\n\nLimitations\
  \ of the proposed method:\nWhile the proposed method for image/question/answer triplets\
  \ generation has the advantage of being automatic and easily scalable while using\
  \ data annotated by humans, a few limitations have been observed. First, it can\
  \ happen that some annotations are missing or badly registered [4]. Furthermore,\
  \ it was not possible to match the acquisition date of the imagery to the one of\
  \ OSM. The main reason being that it is impossible to know if a newly added element\
  \ appeared at the same time in reality or if it was just entered for the first time\
  \ in OSM. As OSM is the main source of data for our process, errors in OSM will\
  \ negatively impact the accuracy of our databases. Furthermore, due to the templates\
  \ used to automatically construct questions and provide answers, the set of questions\
  \ and answers is more limited than what it is in traditional VQA datasets (9 possible\
  \ answers for the LR dataset, 98 for the HR dataset).\n\n</block>\n## VQA Model\n\
  <block id=\"6\">\nWe investigate the difficulty of the VQA task for remote sensing\
  \ using a basic VQA model based on deep learning. In their simple form, VQA models\
  \ are composed of three parts [24]:\n\n</block>\n### Feature extraction\n<block\
  \ id=\"7\">\nThe first component of our VQA model is the feature extraction. Its\
  \ purpose is to obtain a low-dimensional representation of the information contained\
  \ in the image and the question.\n\n1) Visual part: To extract information from\
  \ a 2D image, a common choice is to use a Convolutional Neural Network (CNN). Specifically,\
  \ we use a Resnet-152 model [32] pre-trained on ImageNet [10]. The principal motivation\
  \ for this choice is that this architecture manages to avoid the undesirable degradation\
  \ problem (decreasing performance with deeper networks) by using residual mappings\
  \ of the layers’ inputs which are easier to learn than the common choice of direct\
  \ mappings. This architecture has been successfully used in a wide range of work\
  \ in the remote sensing community (e.g. [8], [17], [33]). The last average pooling\
  \ layer and fully connected layer are replaced by a 1 × 1 2D convolution which outputs\
  \ a total of 2048 features which are vectorized. A final fully connected layer is\
  \ learned to obtain a 1200 dimension vector.\n\n2) Language part: The feature vector\
  \ is obtained using the skip-thoughts model [34] trained on the BookCorpus dataset\
  \ [35]. This model is a recurrent neural network, which aims at producing a vector\
  \ representing a sequence of words (in our case, a question). To make this vector\
  \ informative, the model is trained in the following way: it encodes a sentence\
  \ from a book in a latent space, and tries to decode it to obtain the two adjacent\
  \ sentences in the book. By doing so, it ensures that the latent space embeds semantic\
  \ information. Note that this semantic information is not remote sensing specific\
  \ due to the BookCorpus dataset it has been trained on. However, several works,\
  \ including [36], have successfully applied non-domain specific NLP models to remote\
  \ sensing. In our model, we use the encoder which is then followed by a fully-connected\
  \ layer (from size 2400 elements to 1200).\n\n</block>\n### Fusion\n<block id=\"\
  8\">\nAt this step, we have two feature vectors (one representing the image, one\
  \ representing the question) of the same size. To merge them into a single vector,\
  \ we use a simple strategy: a point-wise multiplication after applying the hyperbolic\
  \ tangent function to the vectors’ elements. While being a fixed (i.e. not learnt)\
  \ operation, the end-to-end training of our model encourages both feature vectors\
  \ to be comparable with respect to this operation.\n\n</block>\n### Prediction\n\
  <block id=\"9\">\nFinally, we project this 1200 dimensional vector to the answer\
  \ space by using a MLP with one hidden layer of 256 elements. We formulate the problem\
  \ as a classification task, in which each possible answer is a class. Therefore,\
  \ the size of the output vector depends on the number of possible answers.\n\n</block>\n\
  ### Training procedure\n<block id=\"10\">\nWe train the model using the Adam optimizer\
  \ [37] with a learning rate of 10−5 until convergence (150 epochs in the case of\
  \ the LR dataset, and 35 epochs in the case of the HR dataset). We use a dropout\
  \ of 0.5 for every fully connected layer. Due to the difference of input size between\
  \ the two datasets (HR images are 4 times larger), we use batches of 70 instances\
  \ for the HR dataset and 280 for the LR dataset. Furthermore, when the questions\
  \ do not contain a positional component relative to the image space (i.e. \"left\
  \ of\", \"top of\", \"right of\" or \"bottom of\"), we augment the image space by\
  \ randomly applying vertical and/or horizontal flipping.\n\n</block>\n## Results\
  \ and Discussion\n<block id=\"11\">\nWe report the results obtained by our model\
  \ on the test sets of the LR and HR datasets. In both cases, 3 model runs have been\
  \ trained and we report both the average and the standard deviation of our results\
  \ to limit the variability coming from the stochastic nature of the optimization.\n\
  \nThe numerical evaluation is achieved using the accuracy, defined in our case as\
  \ the ratio of correct answers. We report the accuracy per question type (see subsection\
  \ II-A), the average of these accuracies (AA) and the overall accuracy (OA).\n\n\
  We show some predictions of the model on the different test sets to qualitatively\
  \ assess the results. Numerical performance of the proposed model on the LR dataset\
  \ is reported in Table II and the confusion matrix is shown in Figure 10. The performance\
  \ on both tests sets of the HR dataset are reported in Table III and the confusion\
  \ matrices are shown in Figure 11.\n\nGeneral accuracy assessment:\nThe proposed\
  \ model achieves an overall accuracy of 79% on the low resolution dataset and of\
  \ 83% on the first test set of the high resolution dataset, indicating that the\
  \ task of automatically answering question based on remote sensing images is possible.\
  \ When looking at the accuracies per question type, it can be noted that the model\
  \ performs inconsistently with respect to the task the question is tackling: while\
  \ a question about the presence of an object is generally well answered (87.46%\
  \ in the LR dataset, 90.43% in the first test set of the HR dataset), counting questions\
  \ gives poorer performances (67.01% and 68.63% respectively). This can be explained\
  \ by the fact that presence questions can be seen as simplified counting questions\
  \ to which the answers are restricted to two options: \"0\" or \"1 or more\". Classical\
  \ VQA models are known to struggle with the counting task [38]. An issue which partly\
  \ explains these performances in the counting task is the separation of connected\
  \ instances. This problem has been raised for the case of buildings in [33] and\
  \ is illustrated where the ground truth is indicating three buildings, which could\
  \ also be only one. We found another illustration of this phenomenon in the second\
  \ test set. This issue mostly arises when counting roads or buildings.\n\nThanks\
  \ to the answers’ quantization, questions regarding the areas of objects are generally\
  \ well answered with an accuracy of 85.24% in the first test set of the HR dataset.\
  \ Presence of buildings (by the mean of the covered area) is well detected in examples.\n\
  \nHowever, we found that our model performs poorly with questions regarding the\
  \ relative positions of objects. While some examples are correct, others show small\
  \ mistakes or are completely incorrect. These problems can be explained by the fact\
  \ that the questions are on high semantic level and therefore difficult for a model\
  \ considering a simple fusion scheme, as the one presented in section III.\n\nRegarding\
  \ the low resolution dataset, rural/urban questions are generally well answered\
  \ (90% of accuracy). Note that the ground truth for this type of questions is defined\
  \ as a hard threshold on the number of buildings, which causes some areas to be\
  \ labeled as urban.\n\nHowever, the low resolution of Sentinel-2 images can be problematic\
  \ when answering questions about relatively small objects. For instance, in some\
  \ examples, we cannot see any water area nor determine the type of buildings, which\
  \ causes the model’s answer to be unreliable.\n\nGeneralization to unseen areas:\n\
  The performances on the second test set of the HR dataset show that the generalization\
  \ to new geographic areas is problematic for the model, with an accuracy drop of\
  \ approximately 5%. This new domain has a stronger impact on the most difficult\
  \ tasks (counting and area computation). This can be explained when looking at some\
  \ examples. We can see that the domain shift is important on the image space, as\
  \ a different sensor was used for the acquisition. Furthermore, the urban organization\
  \ of Philadelphia is different from that of the city of New York. This causes the\
  \ buildings to go undetected by the model in some examples, while the parkings can\
  \ still be detected possibly thanks to the cars. This decrease in performance could\
  \ be reduced by using domain adaptation techniques. Such a method could be developed\
  \ for the image space only (a review of domain adaptation for remote sensing is\
  \ done in [39]) or at the question/image level (see [40], which presents a method\
  \ for domain adaptation in the context of VQA).\n\nAnswer’s categories:\nThe confusion\
  \ matrices indicate that the models generally provide logical answers, even when\
  \ making mistakes (e.g. it might answer \"yes\" instead of \"no\" to a question\
  \ about the presence of an object, but not a number). Rare exceptions to this are\
  \ observed on the first test set of the HR dataset, on which the model gives a small\
  \ number of illogical answers.\n\nLanguage biases:\nA common issue in VQA models,\
  \ raised in [41], is the fact that strong language biases are captured by the model.\
  \ When this is the case, the answer provided by the model mostly depends on the\
  \ question, rather than on the image. To assess this, we evaluated the proposed\
  \ models by randomly selecting an image from the test set for each question. We\
  \ obtained an overall accuracy of 73.78% on the LR test set, 73.78% on the first\
  \ test set of the HR dataset and 72.51% on the second test set. This small drop\
  \ of accuracy indicates that indeed, the models rely more on the questions than\
  \ on the image to provide an answer. Furthermore, the strongest drop of accuracy\
  \ is seen on the HR dataset, indicating that the proposed model extracts more information\
  \ from the high resolution data.\n\nImportance of the number of training samples:\n\
  We show the evolution of the accuracies when the model is trained with a fraction\
  \ of the HR training samples. When using only 1% of the available training samples,\
  \ the model already gets 65% in average accuracy (vs 83% for the model trained on\
  \ the whole training set). However, it can be seen that, for numerical tasks (counts\
  \ and area estimation), larger amounts of samples are needed to achieve the performances\
  \ reported. This experiment also shows that the performances start to plateau after\
  \ 10% of the training data is used: this indicates that the proposed model would\
  \ not profit substantially from a larger dataset.\n\nRestricted set of questions:\n\
  While not appearing in the numerical evaluation, an important issue with our results\
  \ is the relative lack of diversity in the dataset. Indeed, due to the source of\
  \ our data (OSM), the questions are only on a specific set of static objects (e.g.\
  \ buildings, roads, . . . ). Other objects of interest for applications of a VQA\
  \ system to remote sensing would also include different static objects (e.g. thatched\
  \ roofs mentioned in section I), moving objects (e.g. cars), or seasonal aspects\
  \ (e.g. for crop monitoring). Including these objects would require another source\
  \ of data, or manual construction of question/answer pairs.\n\nAnother limitation\
  \ comes from the dataset construction method described in subsection II-A. We defined\
  \ five types of questions (count, comparison, presence, area, rural/urban classification).\
  \ However, they only start to cover the range of questions which would be of interest.\
  \ For instance, questions about the distance between two points (defined by textual\
  \ descriptions), segmentation questions (e.g. \"where are the buildings in this\
  \ image?\") or higher semantic level question (e.g. \"does this area feel safe?\"\
  ) could be added.\n\nWhile the first limitation (due to the data source) could be\
  \ tackled using other databases (e.g. from national institutes) and the second limitation\
  \ (due to the proposed method) could be solved by adding other question construction\
  \ functions to the model, it would be beneficial to use human annotators using a\
  \ procedure similar to [5] to diversify the samples.\n\n</block>\n## Conclusion\n\
  <block id=\"12\">\nWe introduce the task of Visual Question Answering from remote\
  \ sensing images as a generic and accessible way of extracting information from\
  \ remotely sensed data. We present a method for building datasets for VQA, which\
  \ can be extended and adapted to different data sources, and we proposed two datasets\
  \ targeting different applications. The first dataset uses Sentinel-2 images, while\
  \ the second dataset uses very high resolution (30cm) aerial orthophotos from USGS.\n\
  \nWe analyze these datasets using a model based on deep learning, using both convolutional\
  \ and recurrent neural networks to analyze the images and associated questions.\
  \ The most probable answer from a predefined set is then selected. This first analysis\
  \ shows promising results, suggesting the potential for future applications of such\
  \ systems. These results outline future research directions which are needed to\
  \ overcome language biases and difficult tasks such as counting. The former can\
  \ be tackled using an attention mechanism [24], while the latter could be tackled\
  \ by using dedicated components for counting questions [33] in a modular approach.\n\
  \nIssues regarding the current database raised in section IV also need to be addressed\
  \ to obtain a system capable of answering a more realistic range of questions. This\
  \ can be done by making the proposed dataset construction method more complex or\
  \ by using human annotators.\n</block>"
