title: 'GeoGround: A Unified Large Vision-Language Model for Remote Sensing Visual
  Grounding'
abstract: 'Remote sensing (RS) visual grounding aims to use natural language expression
  to locate specific objects (in the form of the bounding box or segmentation mask)
  in RS images, enhancing human interaction with intelligent RS interpretation systems.
  Early research in this area was primarily based on horizontal bounding boxes (HBBs),
  but as more diverse RS datasets have become available, tasks involving oriented
  bounding boxes (OBBs) and segmentation masks have emerged. In practical applications,
  different targets require different grounding types: HBB can localize an object’s
  position, OBB provides its orientation, and mask depicts its shape. However, existing
  specialized methods are typically tailored to a single type of RS visual grounding
  task and are hard to generalize across tasks. In contrast, large vision-language
  models (VLMs) exhibit powerful multi-task learning capabilities but struggle to
  handle dense prediction tasks like segmentation. This paper proposes GeoGround,
  a novel framework that unifies support for HBB, OBB, and mask RS visual grounding
  tasks, allowing flexible output selection. Rather than customizing the architecture
  of VLM, our work aims to elegantly support pixel-level visual grounding output through
  the Text-Mask technique. We define prompt-assisted and geometry-guided learning
  to enhance consistency across different signals. Experimental results show that
  GeoGround demonstrates strong performance across four RS visual grounding tasks,
  matching the performance of specialized methods on multiple benchmarks. Code available
  at https://github.com/zytx121/GeoGround.'
abstract_is_verbatim: true
segmented_markdown: '# GeoGround: A Unified Large Vision-Language Model for Remote
  Sensing Visual Grounding


  Abstract

  Remote sensing (RS) visual grounding aims to use natural language expression to
  locate specific objects (in the form of the bounding box or segmentation mask) in
  RS images, enhancing human interaction with intelligent RS interpretation systems.
  Early research in this area was primarily based on horizontal bounding boxes (HBBs),
  but as more diverse RS datasets have become available, tasks involving oriented
  bounding boxes (OBBs) and segmentation masks have emerged. In practical applications,
  different targets require different grounding types: HBB can localize an object’s
  position, OBB provides its orientation, and mask depicts its shape. However, existing
  specialized methods are typically tailored to a single type of RS visual grounding
  task and are hard to generalize across tasks. In contrast, large vision-language
  models (VLMs) exhibit powerful multi-task learning capabilities but struggle to
  handle dense prediction tasks like segmentation. This paper proposes GeoGround,
  a novel framework that unifies support for HBB, OBB, and mask RS visual grounding
  tasks, allowing flexible output selection. Rather than customizing the architecture
  of VLM, our work aims to elegantly support pixel-level visual grounding output through
  the Text-Mask technique. We define prompt-assisted and geometry-guided learning
  to enhance consistency across different signals. Experimental results show that
  GeoGround demonstrates strong performance across four RS visual grounding tasks,
  matching the performance of specialized methods on multiple benchmarks. Code available
  at https://github.com/zytx121/GeoGround.


  ## 1. Introduction

  <block id="0">

  In the remote sensing (RS) community, the early visual grounding task [27, 38] specifically
  refers to the location of specific objects in terms of horizontal bounding boxes
  (HBBs), given a satellite image and related text query. With increasing availability
  of the RS dataset [12, 26, 35], researchers have started to use oriented bounding
  boxes (OBBs) [7] or segmentation masks [37] to more accurately depict the referred
  objects. RS visual grounding enables humans to interact with computers in a more
  intuitive manner, which has enormous promise for improving the efficiency of intelligent
  RS interpretation systems [30].


  Most existing RS visual grounding [27, 38] and referring segmentation [17, 37] methods
  are designed with task-specific modules and loss functions. Models based on HBB
  typically adopt loss functions in object detection tasks, such as Smooth L1, while
  models based on mask often use loss functions from semantic segmentation tasks,
  such as pixel-level cross-entropy. Enabling multi-task learning [28] in such models
  not only requires modifications to the network but also necessitates careful tuning
  of the weightings between various loss functions, making the process quite challenging.
  Although large vision-language models (VLMs) [1, 2, 5, 16] can support multiple
  multimodal RS tasks simultaneously by using a unified text regression loss function,
  they struggle with pixel-level tasks such as segmentation. This is because, as the
  output module of a VLM, the large language model (LLM) can only generate textual
  data and cannot produce output in the image modality [4].


  To address these challenges, we propose GeoGround, an elegant VLM that seamlessly
  unifies visual grounding tasks at the HBB, OBB, and pixel-level RS. Our key innovation
  lies in converting box-level and pixel-level signals into textual sequences, enabling
  the model to train diverse visual grounding tasks within a unified training pipeline.
  Specifically, we propose the Text-Mask paradigm, which distills and compresses the
  information embedded in the mask into a compact text sequence that can be efficiently
  learned by VLMs. Additionally, we introduce hybrid supervision, which incorporates
  prompt-assisted learning (PAL) and geometry-guided learning (GGL) to fine-tune the
  model using three types of signals, ensuring output consistency and enhancing the
  model’s understanding of the relationships between different grounding types.


  To support GeoGround training and promote the development of visual RS grounding,
  we introduce refGeo, a large-scale RS visual grounding instruction-following dataset.
  refGeo consolidates four existing visual grounding datasets from RS [7, 14, 27,
  38] and introduces a new aerial vehicle visual grounding dataset.


  In summary, our key contributions are as follows:

  - We propose GeoGround, a novel VLM framework that unifies box-level and pixel-level
  RS visual grounding tasks while maintaining its inherent dialogue and image understanding
  capabilities.

  - We introduce refGeo, the largest RS visual grounding instruction-following dataset,
  consisting of 161k image-text pairs and 80k RS images.

  - We conduct extensive experiments on various RS visual grounding tasks, providing
  valuable insights for future RS VLM research and opening new avenues for research
  in RS visual grounding.


  </block>

  ## 2. Related Work

  <block id="1">

  Remote Sensing Referring Detection and Segmentation. Compared to multimodal tasks
  like image captioning [15, 41], text-image retrieval [20], and visual question answering
  (VQA) [18] in RS, research on referring detection is a novel task with limited research.
  It was first introduced by GeoVG [37], which proposed the first RS visual grounding
  dataset. MGVLF [38] leverages multiscale visual features and multi-granularity textual
  embeddings to address the scale variation in RS images. LQVG [10] propose a language
  query-based Transformer framework for RSVG. LPVA [13] achieves precise attention
  on referred objects by adjusting visual features with progressive attention. RS
  referring segmentation is also in its early stages due to the challenges mentioned
  earlier. It was first introduced by RefSegRS [37], which proposed a new dataset
  and baseline model. Recently, the transformer-based method RMSIN [17] proposed an
  adaptive rotated convolution to tackle the issues arising from the scale variation
  and orientations prevalent in aerial imagery. FIANet [11] proposed a fine-grained
  image-text alignment module to better discriminative multimodal representation.
  However, these two types of models have always been studied separately, hindering
  progress in the field. In this paper, we unify these two tasks within a single framework,
  allowing them to share data and architecture.


  Generalist VLMs. Several efforts have been made to equip VLMs with visual grounding
  capabilities in the domain of natural images [3, 4, 8, 23, 31, 34, 39]. For instance,
  Shikra [3] directly textualizes HBB to support visual grounding tasks, but its discrete
  coordinate output is inadequate for pixel-level tasks. LISA [8] addresses this by
  incorporating a mask decoder to handle the RES task, while NExT-Chat [39] expands
  this paradigm by adding two decoders to support both box and mask outputs. In contrast,
  our approach elegantly unifies box-level and pixel-level visual grounding tasks
  based on general-purpose VLM, eliminating the need for additional encoders or decoders.


  Remote Sensing VLMs have shown promising results in image-level tasks such as scene
  classification, image captioning, and VQA [14, 32, 42]. However, works [7, 21, 22,
  40] on object-level tasks such as RS visual grounding remains comparatively unexplored.
  GeoChat [7] constructs a new dataset using OBB annotation and proposes the first
  RS visual grounding model based on OBB. However, the unsuitable choice of OBB angle
  representation limits its performance. Moreover, the small scale of the RS visual
  grounding dataset selected by LHRS-Bot [21] and H2RSVLM [22] hampers their generalization
  ability on this task. To address these issues, we introduce refGeo, a large-scale
  RS visual grounding dataset with multi-type annotations. For each type of annotation,
  we perform a systematic exploration to identify the most suitable format.


  </block>

  ## 3. GeoGround

  <block id="2">

  The architecture of GeoGround is highly streamlined, consisting of only a visual
  encoder (CLIP-ViT [24]), a connector (two-layer MLPs), and an LLM (Vicuna 1.5 [43]),
  without introducing additional encoders or decoders. The framework of our proposed
  model can flexibly output HBBs, OBBs, or segmentation masks based on user instructions.
  In addition to single-object outputs, the model is also capable of handling multi-object
  outputs.


  </block>

  ### 3.1. Signal Textualization

  <block id="3">

  To train three types of visual grounding tasks with a unified data pipeline, we
  textualize the three grounding supervision signals into three corresponding text
  strings. We refer to this process as signal textualization, which serves as the
  cornerstone of our method.


  Text-HBB and Text-OBB are generated by directly converting numerical coordinates
  into text sequences [3]. Specifically, the coordinates are normalized, multiplied
  by the resolution, and then rounded. The resulting numbers are separated by commas
  and enclosed in parentheses. In GeoGround, we set the resolution of the Text-HBB
  to 1000, allowing for more precise localization of small objects in RS images. Compared
  to Text-HBB, Text-OBB includes an additional angle parameter. Since there are various
  angle representations of OBB, the meaning of the first four numbers differs. Based
  on experiments, we adopt the long side 90-degree representation [44] in GeoGround,
  where the angle ranges from 0 to 90 degrees. To ensure that these values align with
  the angle in terms of range, we set the resolution of Text-OBB to 100 by default.


  Text-Mask should be generated by converting the mask into text sequences. However,
  this conversion is challenging due to the inherent differences between the image
  and text modalities. Inspired by Text4Seg [9], we propose a novel Text-Mask paradigm
  that treats the segmentation mask as text. Specifically, we downsample the mask
  into an N × N grid, where the object region is labeled 1 and the background region
  0. This results in a binary matrix that approximately represents the object’s location
  and shape. Higher resolution improves shape precision but results in longer text
  sequences, increasing training difficulty and slowing inference. To further reduce
  the token length required to represent a mask, we employ R-RLE [9] to compress redundant
  text sequences. It significantly reduces the length of Text-Mask and accelerates
  inference speed, without compromising performance. For RS visual grounding datasets,
  a resolution of 32 enables Text-Mask to effectively represent most objects.


  </block>

  ### 3.2. Hybrid Supervision

  <block id="4">

  We propose a hybrid supervision that simultaneously utilizes Text-HBB, Text-OBB,
  and Text-Mask to comprehensively enhance the visual grounding capabilities of VLMs.
  First, we adopt a basic supervised learning paradigm to train three types of visual
  grounding tasks, as follows:


  t = F (I, q) (1)


  where F denotes the LLM of our model, I represents the image embedding, q represents
  the query text embedding. t can represent the Text-HBB, Text-OBB, and Text-Mask,
  respectively. Next, we define the following two auxiliary tasks to establish connections
  between the different signals.


  Prompt-Assisted Learning refers to completing visual grounding with the help of
  additional prompts, such as predicting the OBB of an object based on its known HBB.
  This process can be understood as an increase in information entropy and aims to
  help the model acquire the ability to generate dense signals from sparse ones. Since
  dense signals contain more information than sparse signals, this process still requires
  the model to extract additional information from the image to bridge the gap between
  the signals. PAL can be expressed by the following equation:


  tdense = F (I, {q, tsparse}) (2)


  where tsparse represents the sparse textualized signal, which can be either Text-HBB
  or Text-OBB here. tdense represents the textualized signal that is denser than tsparse.


  Geometry-Guided Learning converts dense signals into sparse ones guided by geometric
  knowledge, reducing information entropy. This means that GGL does not require the
  image as input; the transformation process can be achieved solely based on geometric
  knowledge. For example, the HBB that encloses an OBB can be obtained by calculating
  its four corner points’ maximum and minimum values. GGL can be expressed as:


  tsparse = F ({q, tdense}) (3)


  where tdense denotes the dense textualized signal, which can be either Text-OBB
  or Text-Mask. Similarly to existing VLMs, GeoGround is supervised solely by the
  text regression loss.


  BBox Consistency Score. Ideally, the model outputs for the same object should have
  a similar enclosing bounding box. However, the positions of the HBB, OBB, and mask
  outputs may differ. To assess prediction consistency, we propose the BBox Consistency
  Score (BCS):


  BCS = 1/3 [ IoU(shbb, fobb2hbb(sobb)) + IoU(shbb, fmask2hbb(smask)) + IoU(fobb2hbb(sobb),
  fmask2hbb(smask)) ] (4)


  where shbb, sobb, and smask represent HBB, OBB, and mask signals, respectively.
  IoU denotes the Intersection over Union. fobb2hbb and fmask2hbb represent the functions
  that compute the enclosing HBB from the OBB and mask, respectively. The BCS ranges
  from 0 to 1. When the model predictions are completely consistent, the BCS equals
  1.


  </block>

  ### 3.3. Dataset

  <block id="5">

  To address the issue of limited generalization capability in VLMs caused by the
  relatively small size of existing RS visual grounding datasets, we present a large-scale
  RS referring expression comprehension dataset, refGeo. It integrates most existing
  RS visual grounding datasets. Since both GeoChat [7] and VRSBench [14] use DIOR
  [12] image data, which overlap with DIOR-RSVG [38], we remove the samples corresponding
  to images that appear in the DIOR-RSVG test and validation sets from the GeoChat
  and VRSBench training set to prevent data leakage. Moreover, we propose a new aerial
  vehicle visual grounding dataset using unmanned aerial vehicles.


  </block>

  ## 4. Experiments

  <block id="6">

  Our method is based on LLaVA-1.5-7B [16] with the input image resolution fixed at
  336×336. We utilize the AdamW optimizer [19], starting with an initial learning
  rate of 2e-4, followed by a linear decay schedule after a warm-up phase with a 0.03
  ratio. To reduce GPU memory consumption, all models are fine-tuned using LoRA with
  a rank of 64, in conjunction with ZeRO-2 stage memory optimization. All models are
  trained on 8 NVIDIA V100 GPUs (32GB) with a global batch size of 128 for 5 epochs.
  The inference batch size is set to 1 for all experiments. RS object detection datasets
  [12, 26, 35] are used during the fine-tuning of GeoGround to enhance its basic visual
  perception capabilities.


  </block>

  ### 4.1. Referring Expression Comprehension (REC)

  <block id="7">

  Settings. We follow standard evaluation protocols [14, 22] and assess the REC task
  using Acc@0.5 metric. Except for H2RSVLM [22] and EarthGPT [40], whose metrics are
  cited in the original articles due to the lack of open source code, the results
  for the other VLMs are obtained by inference with the official model weights provided.
  For the GeoChat [7], we convert its output OBBs to HBBs.


  Results. GeoGround achieves the best performance across multiple REC benchmarks,
  surpassing specialized models on some test sets. VLMs fine-tuned on our refGeo dataset,
  such as Qwen-VL [1] and GeoChat [7], showed significant improvements on the REC
  task, validating the effectiveness of the scaling law in the field of RS visual
  grounding. Benefiting from the wide range of image resolutions and GSD in refGeo,
  the fine-tuned model showed significant performance improvements on datasets with
  a high proportion of small objects, such as RSVG and A VVG. GeoGround achieves the
  best performance when the resolution of Text-Mask is set to 16. This could be due
  to the increased difficulty of training at higher resolutions. Although low resolution
  leads to coarse masks, they can be seen as attention mechanisms that aid in localizing
  the approximate object area.


  </block>

  ### 4.2. REC with OBB

  <block id="8">

  Settings. Following GeoChat [7], we also use Acc@0.5 as metric, with the difference
  being that rotated IoU [36] is used instead of normal IoU during the calculation.


  Results. The results demonstrate GeoGround’s dominance in RS visual grounding tasks
  based on OBB, further validating the effectiveness of our hybrid supervision approach.
  Due to the increased number of parameters to learn, this task is more challenging
  than standard REC, resulting in lower scores on the OBB task compared to the HBB
  task, even on the same test set.


  </block>

  ### 4.3. Referring Expression Segmentation (RES)

  <block id="9">

  Settings. We utilize Acc@0.5 and Mean Intersection-over-Union (mIoU) as evaluation
  metrics, similar to prior studies [33, 37]. As GeoGround is currently the only RS
  VLM that supports the RES task, we compare it with three generalist VLMs that possess
  native segmentation capabilities on the RES task in the RRSIS-D dataset [17].


  Results. GeoGround exhibits superior performance in the pixel-level RS vision grounding
  task. GeoGround (N=32) achieve better results than directly using HBB to prompt
  SAM even without relying on SAM. Moreover, we attempt to use HBB and coarse mask
  to prompt SAM [6], which allowed GeoGround to achieve results that match the performance
  of the best RS referring segmentation model [17]. See appendix for more details.


  </block>

  ### 4.4. Ablation Study

  <block id="10">

  Effect of Hybrid Supervision. The ablation study of the components in our proposed
  hybrid supervision method confirms their effectiveness and further highlights the
  importance of output consistency in improving performance. To compute BCS, we first
  convert both OBB and mask into HBB before calculating Acc@0.5. The results show
  that PAL improves performance when predicting dense signals from sparse ones, while
  GGL, which requires no visual input, yields better results when predicting sparse
  signals from dense ones. Experiments further explore the effect of multiple signals
  on model performance. The results show that direct training with three signals can
  enhance the visual grounding capability of the VLM on HBB and OBB tasks.


  Design Options of Text-Mask. To our best knowledge, Text4Seg [9] is the only work
  to attempt to treat masks as text. However, with longer referring expressions, its
  semantic descriptors become overly redundant. Comparisons between the performance
  of our proposed Text-Mask with Text4Seg show that mapping the mask to a binary matrix
  not only reduces the text length of Text4Seg by 40% but also improves its performance
  by 18%. Since the objects in RS are relatively small, using the nearest downsampling
  method leads to the loss of mask information for small objects, resulting in significant
  performance degradation. While increasing the mask quantization resolution can further
  improve the segmentation accuracy, longer output text sequences increase the inference
  time and training difficulty.


  </block>

  ### 4.5. Visualization Examples

  <block id="11">

  Qualitative comparisons between GeoGround and GeoChat include HBBs and OBBs. It
  can be observed that GeoGround consistently demonstrates superior localization accuracy,
  whether handling simple or relatively complex referential expressions. Additionally,
  it exhibits 3D spatial understanding, enabling it to infer 3D distances from 2D
  images. Comparisons of GeoGround in the RES task under different resolutions of
  Text-Mask show that when the resolution is 32, although the coarse mask edges still
  exhibit small jaggedness, the result is already very close to the ground truth.
  These results fully validate the effectiveness of GeoGround in addressing the pixel-level
  visual grounding task in RS.


  </block>

  ## 5. Conclusion

  <block id="12">

  Although remote sensing (RS) visual grounding tasks using horizontal bounding boxes,
  oriented bounding boxes, and segmentation masks have progressed, no model has unified
  these tasks due to framework limitations. To address this, we propose GeoGround,
  a novel framework that unifies box-level and pixel-level visual grounding tasks
  in a single model. Instead of adding extra encoders or decoders, GeoGround empowers
  large vision-language models (VLMs) to perform pixel-level visual grounding by treating
  the segmentation mask as text using our Text-Mask method. It does not compromise
  the model’s conversational abilities or its image-level understanding capabilities.
  We also introduce a large-scale RS visual grounding instruction-following dataset,
  refGeo, that offers a comprehensive benchmark for various visual grounding tasks
  of RS and serves as a valuable corpus for RS VLMs. Our comprehensive benchmarks
  and ablation studies provide important insights for the development of VLMs in the
  RS domain.

  </block>'
