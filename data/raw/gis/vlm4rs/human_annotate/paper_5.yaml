title: 'GeoChat : Grounded Large Vision-Language Model for Remote Sensing'
blocks:
- block_id: 0
  content: Recent advancements in Large Vision-Language Models (VLMs) have shown great promise in natural image domains, allowing
    users to hold a dialogue about given visual content. However, such general-domain VLMs perform poorly for Remote Sensing
    (RS) scenarios, leading to inaccurate or fabricated information when presented with RS domain-specific queries. Such a
    behavior emerges due to the unique challenges introduced by RS imagery. For example, to handle high-resolution RS imagery
    with diverse scale changes across categories and many small objects, region-level reasoning is necessary alongside holistic
    scene interpretation. Furthermore, the lack of domain-specific multimodal instruction following data as well as strong
    backbone models for RS make it hard for the models to align their behavior with user queries. To address these limitations,
    we propose GeoChat - the first versatile remote sensing VLM that offers multitask conversational capabilities with high-resolution
    RS images. Specifically, GeoChat can not only answer image-level queries but also accepts region inputs to hold region-specific
    dialogue. Furthermore, it can visually ground objects in its responses by referring to their spatial coordinates. To address
    the lack of domain-specific datasets, we generate a novel RS multimodal instruction-following dataset by extending image-text
    pairs from existing diverse RS datasets. We establish a comprehensive benchmark for RS multitask conversations and compare
    with a number of baseline methods. GeoChat demonstrates robust zero-shot performance on various RS tasks, e.g., image
    and region captioning, visual question answering, scene classification, visually grounded conversations and referring
    detection. Our code is available here.
  citations: []
- block_id: 1
  content: 'In the natural image domain, the abundance of aligned image-text data sourced from web imagery or manual annotations
    facilitate effective self-supervised vision-language modeling, as demonstrated by multimodal GPT-4 [23] and open-source
    initiatives like LLaVA [19]. These vision-language models (VLMs), developed through generative pretraining and instruction-tuning,
    exhibit robust zero-shot task completion across various user-oriented multimodal tasks. The resulting capabilities open
    the door to the development of versatile multimodal conversational assistants with broad applications in real-world scenarios
    [12].


    However, general-domain VLMs designed for natural images, exhibit poor performance when presented with remotely sensed
    visual imagery. The performance disparity arises primarily from the distinct nature of content found in remote sensing
    image-text pairings compared to the publicly available web data. As a result, general-domain VLMs can provide inaccurate
    information or hallucinate when presented with spatial images from RS sensors. Although there has been significant progress
    in the field of remote sensing visual question answering (VQA) [38, 40], earlier methods have framed the task as a classification
    problem. Here, the model chooses answers from predetermined responses found in the training data. It limits their applicability
    to open-ended answer generation and instruction-following.


    In this paper, we introduce GeoChat, an attempt to extend multimodal instruction-tuning to the remote sensing domain for
    training a multitask conversational assistant. However, remote-sensing domain lacks a multimodal instruction-tuning conversational
    dataset. Inspired by recent work in instruction-tuning [14, 19, 41], GeoChat uses Vicuna-v1.5 [7] and an automated pipeline
    to generate diverse remote sensing multimodal instruction-following data comprising of nearly 318k instructions. We create
    the image-text pairs from various existing remote sensing datasets developed for diverse tasks. These includes LR-BEN
    for VQA [20], NWPU-RESISC-45 for scene classification [5] and SAMRS for object detection [30].


    A crucial capability of GeoChat is the unification of multiple image and region-level reasoning tasks for RS imagery within
    a single pipeline. We achieve this via distinct task tokens that help suitably direct the model’s responses according
    to user requirements. In addition, the model uses spatial location representations in its inputs to seamlessly reason
    about local regions and can also generate object locations in its responses to visually ground objects. This enables a
    diverse set of tasks possible with GeoChat including referring expression detection, image/region captioning, scene classification,
    natural language conversations and VQA, besides visually grounded conversations.


    In summary, this work has the following contributions:

    - RS multimodal instruction following dataset. We present a novel data generation pipeline, to leverage existing object
    detection dataset [30] to create short descriptions of the images, followed by using Vicuna-v1.5 [7] to create conversations
    using the generated text alone. Further, we add visual question-answering and scene classification abilities using their
    corresponding datasets [5, 20]. This results in a total of 318k instruction pairs for RS domain.

    - GeoChat. Leveraging our dataset, we finetune LLaVA-1.5 [14] to create the remote sensing-domain vision-language model
    - GeoChat. Our LoRA [11] fine-tuning is efficient and avoids forgetting the necessary context embedded in fully-tuned
    LLaVA model, whose MLP projection is trained to align images into the word embedding space of the LLM (Vicuna-v1.5 [7]).
    This allows GeoChat to retain the conversation and instruction following abilities of LLaVA and extend its domain-knowledge
    to remote sensing tasks.

    - We also address the lack of evaluation benchmarks to assess the capability of existing VLMs on remote-sensing conversations.
    To this end, we setup evaluation protocols for conversation grounding in RS, as well as a setup a suite of tasks to allow
    comparisons with future efforts in this direction. We show various supervised as well as zero-shot evaluations for different
    remote sensing tasks, including image captioning, visual question answering and scene classification to demonstrate the
    generalisability of GeoChat conversational VLM.'
  citations:
  - marker: '[23]'
    intent_label: Domain Overview
    topic_label: Joint Vision–Language Backbones
  - marker: '[19]'
    intent_label: Domain Overview
    topic_label: Joint Vision–Language Backbones
  - marker: '[12]'
    intent_label: Prospective Application
    topic_label: Other Topics
  - marker: '[38, 40]'
    intent_label: Research Gap
    topic_label: Image-Level Understanding
  - marker: '[14, 19, 41]'
    intent_label: Prior Methods
    topic_label: Instruction Tuning and Supervised Fine-Tuning (SFT)
  - marker: '[7]'
    intent_label: Model/Architecture Adoption
    topic_label: Language Models and Tokenization
  - marker: '[20]'
    intent_label: Resource Utilization
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[5]'
    intent_label: Resource Utilization
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[30]'
    intent_label: Resource Utilization
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[14]'
    intent_label: Model/Architecture Adoption
    topic_label: Instruction Tuning and Supervised Fine-Tuning (SFT)
  - marker: '[11]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Parameter-Efficient Adaptation
- block_id: 2
  content: 'Large Vision-Language Models. The typical architecture of instruction-following Vision Language Models (VLMs)
    consists of utilising a pre-trained visual backbone[9] to encode visual data, a large language model [7] for interpreting
    user instructions and generating responses, and a vision-language cross-modal connector, e.g., a linear projection layer
    [18, 41] or an MLP [17], for fusing visual information with language models. The results achieved with VLMs show great
    promise; for example, LLaVA [18], InstructBLIP [8], Otter [13] and MiniGPT-4 [41] show remarkable gains in language instruction
    following and visual reasoning ability for natural scenes. More recent studies have shown that these models can be adapted
    to other domains such as videos [22], biomedical [14, 29] and remote sensing [12].


    Remote Sensing VLMs. The application of generalized VLMs in remote sensing is comparatively sparse. The majority of research
    so far has neglected the semantic understanding of the items and their relationships towards a deep visual comprehension.
    Beyond merely identifying the objects in an image, vision-language models are also capable of generating natural language
    descriptions of the image and inferring the connections between the objects. This makes them more appropriate for tasks
    like text-based image retrieval, captioning images, and answering visual questions that call for both visual and linguistic
    knowledge. Although there has been progress in vision language models for remote sensing tasks, such as image captioning
    [42], zero-shot classification [16] and visual question answering [3, 38], these models can only perform a specific task
    they are trained for, lack conversational capability and do not possess generic semantic knowledge about the remote sensing
    images. A major gap exists in the remote sensing domain towards developing general-purpose models to solve all tasks together,
    while also maintaining conversation abilities. While RSGPT [12] is an initial effort that has shown good conversation
    ability along with solving multiple tasks, it requires finetuning the model for each task separately, which makes it cumbersome
    and not generalizable. Further, RSGPT cannot work for region-level reasoning or visual grounding, which our work aims
    to address.'
  citations:
  - marker: '[12]'
    intent_label: Prospective Application
    topic_label: Multi-Task and Joint Training
  - marker: '[7]'
    intent_label: Prior Methods
    topic_label: Language Models and Tokenization
  - marker: '[9]'
    intent_label: Prior Methods
    topic_label: Vision Encoder Choices and Scales
  - marker: '[18, 41]'
    intent_label: Prior Methods
    topic_label: Feature-Level and Representation Fusion
  - marker: '[17]'
    intent_label: Prior Methods
    topic_label: Feature-Level and Representation Fusion
  - marker: '[18]'
    intent_label: Prior Methods
    topic_label: Language Models and Tokenization
  - marker: '[8]'
    intent_label: Prior Methods
    topic_label: Language Models and Tokenization
  - marker: '[13]'
    intent_label: Prior Methods
    topic_label: Language Models and Tokenization
  - marker: '[41]'
    intent_label: Prior Methods
    topic_label: Language Models and Tokenization
  - marker: '[22]'
    intent_label: Prospective Application
    topic_label: Temporal and Multimodal Variants
  - marker: '[14, 29]'
    intent_label: Prospective Application
    topic_label: Other Topics
  - marker: '[42]'
    intent_label: Research Gap
    topic_label: Image-Level Understanding
  - marker: '[16]'
    intent_label: Research Gap
    topic_label: Image-Level Understanding
  - marker: '[3, 38]'
    intent_label: Research Gap
    topic_label: Image-Level Understanding
- block_id: 3
  content: 'Visually grounded conversations for remote sensing aim to generate textual responses interleaved with corresponding
    object locations. Further, a user can also provide visual prompts (e.g., a bounding box) besides natural language questions,
    and the model should be able to answer questions about the specified Region of Interest (RoI). Such seamless interplay
    between visual and language modalities necessitate a deep comprehension of linguistic constructions that denote particular
    objects or elements in a visual scene.


    As mentioned above, GeoChat is the first model capable of holding visually grounded conversations about remotely sensed
    images. By construction, GeoChat can address not only the challenging task of visually grounded conversations, but can
    also perform a spectrum of other spatial reasoning tasks that span varying levels of granularity in visual imagery understanding
    e.g., image/region captioning, referring object detection and image/region-level conversations about remotely sensed images.
    We formally outline the tasks possible with GeoChat below.'
  citations: []
- block_id: 4
  content: In this task, GeoChat processes an image x and a user text query q without any specific spatial coordinates in
    its inputs or outputs. The goal is to perform conversation-based tasks at a holistic level with image-wide context, such
    as visual question answering (VQA), scene classification and image captioning.
  citations: []
- block_id: 5
  content: This task involves providing spatial box locations b in the input to GeoChat besides x and q. Region locations
    b guide the model’s attention to specific regions within the image, so that the model can perform tasks such as region-level
    captioning, region-specific VQA or multi-turn conversation.
  citations: []
- block_id: 6
  content: With the use of special tokens, termed as task-specification tokens t, GeoChat can be guided to provide object
    locations at different granularities, while maintaining conversation abilities. It helps in tasks including grounded image
    captioning/conversation, object grounding and referring expression detection.
  citations: []
- block_id: 7
  content: 'GeoChat follows the architecture as of LLaVA-v1.5 [17], which consists of three core components, i) Global Image
    encoder, ii) an MLP adaptor (two linear layers) and iii) LLM. Different to LLaVA, we add specific task prompt that indicates
    the type of task desired from the model i.e., grounding, image-level or region-level conversations. Additionally, we allow
    spatial positions within both inputs and outputs, enabling visual prompts as inputs and grounded objects in GeoChat outputs.
    Notably, the original LLaVA model cannot perform object grounding or accept region inputs. Further, the original LLaVA
    can not reason about remote sensing images which is enabled via our domain-specific dataset. We describe each component
    in the architecture as follows:


    Task Token: The unique quality of GeoChat is its ability to easily switch between different types of remote sensing visual
    interpretation tasks. To eliminate uncertainty among tasks, our approach assigns a unique task identification to each
    one. We suggest three distinct task identities, t ∈ {grounding, identify, refer}, each for grounded conversations, region
    captioning and referring expression comprehension. As for the case of visual question answering and scene classification,
    we directly ask the model to output the answer in a single word or phrase. Our approach does not employ any task identification
    tokens for vision-irrelevant commands. This unified approach is supported by a modular design that efficiently integrates
    spatial data, giving the model flexibility in its reasoning about visual content.


    Spatial Location Representation. Our model must precisely identify the spatial position of the referenced items for tasks
    such as grounded conversations, referring expression generation, and comprehension. To this end, we represent the box
    locations in a textual format to express the geographical position: b = {bx_left, by_top, bx_right, by_bottom | θ}. Here,
    bx_left, by_top denote the top left corner point of box while the bx_right, by_bottom represent the bottom right corner
    coordinates. The angle θ represents the angle of rotation for the bounding box, from the lower edge. Numerical values
    normalised within the interval [0, 100] are used to represent the x and y coordinates. Region locations in this format
    are used to interact with the model via its inputs and outputs.


    Visual Backbone. GeoChat adapts the pretrained vision backbone of CLIP-ViT(L-14) [28], which has an input resolution of
    336×336. This results in effectively 576 patches per image. Since this resolution is not sufficient to understand details
    presented in remote sensing imagery (e.g., small objects and object details), we interpolate the positional encoding in
    the transformer-based CLIP [28] model to scale with input image sizes of 504×504. Although this leads to an increase in
    the number of patches to almost double (i.e., 1296 per image), this enhanced resolution allows us to handle larger image
    sizes and also supports better visual grounding in high-resolution RS images.


    MLP Cross-modal Adaptor. From the frozen CLIP-ViT[28], we project the output tokens (∈ R1296×1024) with dimensions 1024
    onto the language model space, using an MLP adaptor with one hidden layer. The adaptor has an input dimensionality of
    1024 and outputs a vector of size 4096, corresponding to the input size of the LLM [7]. A GeLU [10] is used as the activation
    function.


    Large Language Model. The open source Vicuna-v1.5(7B) [7] large language model is utilised as the foundation for GeoChat.
    The language model functions as a single interface for diverse vision-language inputs in our framework. To accomplish
    different vision-language tasks, we directly depend on the Vicuna-v1.5(7B) [7] language tokens. We explicitly interact
    with the language model to construct textual representations of bounding boxes to express their spatial coordinates for
    the visual grounding tasks that require the production of spatial locations. Similarly, the safe, aligned and effective
    behavior of LLM is ensured via system prompts appended together with given inputs.


    Low-Rank Adaptation (LoRA) [11] based strategy is used for fine-tuning the LLM. While training, instead of finetuning
    all of the weights that comprise the weight matrix of the pre-trained Vicuna-v1.5[7], we finetune two smaller matrices
    in LoRA [11] that approximate the original larger matrix. After that, the fine-tuned adaptor is fed into the pre-trained
    model and utilised for inference. The LoRA adaptation ensures faster training and avoids forgetting original knowledge
    embedded in the LLM trained and fine-tuned on generic natural language instructions. This is an important feature since
    it allows the model to bring in external context about generic object types, landmarks and affordances in the remote-sensing
    reasoning framework of GeoChat.'
  citations:
  - marker: '[7]'
    intent_label: Model/Architecture Adoption
    topic_label: Language Models and Tokenization
  - marker: '[11]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Parameter-Efficient Adaptation
  - marker: '[17]'
    intent_label: Model/Architecture Adoption
    topic_label: Joint Vision–Language Backbones
  - marker: '[28]'
    intent_label: Model/Architecture Adoption
    topic_label: Vision Encoder Choices and Scales
  - marker: '[10]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Training and Adaptation Regime
- block_id: 8
  content: To enhance the effectiveness of our model on general visual tasks and optimize training efficiency, we employ a
    strategy that involves initializing the network with pre-trained weights and fine-tuning specific segments for remote
    sensing related tasks. We use a pre-trained CLIP-ViT(L-14) encoder [28], a pretrained MLP adaptor [17], pretrained on
    a 558K subset of the LAION-CC-SBU [26] dataset with BLIP [15] captions, and Vicuna-v1.5 [7] to initialize our model. To
    adapt our model to remote sensing images, we subsequently LoRA [11] fine-tune the LLM, while keeping the MLP adaptor and
    the CLIP encoder [28] frozen during training.
  citations:
  - marker: '[7]'
    intent_label: Model/Architecture Adoption
    topic_label: Language Models and Tokenization
  - marker: '[11]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Parameter-Efficient Adaptation
  - marker: '[17]'
    intent_label: Model/Architecture Adoption
    topic_label: Feature-Level and Representation Fusion
  - marker: '[28]'
    intent_label: Model/Architecture Adoption
    topic_label: Joint Vision–Language Backbones
  - marker: '[26]'
    intent_label: Resource Utilization
    topic_label: RS Dataset Usage and Construction
  - marker: '[15]'
    intent_label: Resource Utilization
    topic_label: Language- or Instruction-Derived RS Supervision
- block_id: 9
  content: 'By using LLM Vicuna [7], we align the model to follow a range of instructions by presenting and curating varied
    instruction-following data with multi-round conversations regarding remote sensing imagery. We specifically provide system
    instructions as prompts that ask Vicuna [7] to generate multi-round question and answer pairs in a manner as if it could
    visualize the image (although it only has access to the text). This is achieved by providing few-shot in-context examples
    manually composed within the prompt to show Vicuna [7] how to build high-quality instruction-response pairs based on the
    caption and information supplied. Specifically, from our short descriptions created using the below pipeline, we randomly
    sample 65k images to create multi-round conversations, 10k images to generate complex question answers and 30k images
    to generate detailed descriptions for the given short descriptions. In combination, after conversion to instruction format,
    we obtain a total of nearly 306k image-instruction pairs for training and 12k for testing. Next, we outline the instruction-set
    creation process.


    Constituent Datasets: In the compilation of our instruction set, we incorporate three distinct types of datasets, encompassing
    the ones designed for object detection, scene classification, and visual question answering (VQA). Specifically, we integrate
    three object detection (DOTA [34], DIOR [6], and FAIR1M [27] which together form the SAMRS [30] dataset), one scene classification
    (NWPU-RESISC-45 [5]), one VQA (LRBEN [20]), and one flood detection [25] VQA dataset. The object detection datasets allow
    region-level reasoning capability as they offer segmentation masks along with bounding boxes.


    Addition of Missing Classes: Although a wide variety of object classes are included in the object detection databases,
    several essential categories like buildings, roads, and trees are missing. To address this, we propose to utilize ViTAE-RVSA
    [31] model, pre-trained on the LoveDA dataset [32], which encompasses the required important classes. The model [31] is
    used to infer these classes on the SAMRS [30] dataset, yielding pseudo labels. To mitigate potential noise in these predictions,
    we remove the predictions of ViTAE-RVSA [31] for which we already have ground truth from the SAMRS [30] dataset to refine
    the results.


    Attribute extraction: For referring expression annotations, it is important to derive a variety of attributes in RS images.
    To this end, we have selected five distinct types of attributes: category (e.g. “plane, ship”), color (e.g. “gray, white”),
    relative size (e.g. “small, large”), relative location (e.g. “top right, bottom”) and relation (e.g. “parked at, driving
    through”). Object category information can be directly obtained from the SAMRS dataset. For color extraction, we use the
    K-Means clustering algorithm. Specifically, we extract the object’s pixels from the image using ground-truth box and cluster
    them into K groups. The center of the largest cluster is then selected as the object’s color. To specify the relative
    size of the object, we categorize objects into three sizes: small, normal, and large. This categorization is determined
    by measuring the area of all instances of a class in the entire dataset and assigning the 80th percentile as the large
    label. Similarly, the 20th percentile is designated as small size, with the remaining falling into the normal category.
    To determine the object’s relative position within the images, we partition the entire image into a 3×3 grid, defining
    regions such as Top Right, Top, Top Left, Left, Center, Right, Bottom Right, Bottom Left, and Bottom. Based on the object’s
    center pixel coordinates, we assign its relative position accordingly.


    To define the relation between objects in a given image, we group different objects based on their distance between the
    bounding boxes, and for each sub-graph, we assign different relationships between objects based on their class labels.
    Various examples of object relationships are used. To establish relationships like “surrounded by,” we cross-reference
    pixel-level coordinates to verify if one object is entirely contained within another object.


    Expression Generation: To emulate natural language expressions, we employ predefined textual templates based on [39].
    The phrase template encompasses the attributes {a1, . . . , a5}. The expression for a group of objects of the same class
    is formulated as:

    "The/A ⟨a3⟩ ⟨a2⟩ a1 ⟨ in/on the a4⟩."

    Attributes that may be absent are enclosed in 〈〉, and attributes {a2, a3} can be arranged in any sequence.


    Similarly, the sentence template incorporates the relational attributes a5 to establish connections between two objects
    through this structure:

    "The/A ⟨ai3⟩ ⟨ai2⟩ ai1 ai5 aj1 ⟨ in/on the aj4⟩."

    Here, the indices i and j represent the ith and jth object.


    Visual Grounding: Although referring expression datasets are available in the natural image domain [36, 37], they lack
    for the remote sensing domain. To this end, we use our short descriptions as referring expressions to create three different
    kinds of question answering pairs, i.e. grounding image description, referring expression, and region level captioning.'
  citations:
  - marker: '[7]'
    intent_label: Resource Utilization
    topic_label: Language Models and Tokenization
  - marker: '[20]'
    intent_label: Resource Utilization
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[5]'
    intent_label: Resource Utilization
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[30]'
    intent_label: Resource Utilization
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[34]'
    intent_label: Resource Utilization
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[6]'
    intent_label: Resource Utilization
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[27]'
    intent_label: Resource Utilization
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[25]'
    intent_label: Resource Utilization
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[31]'
    intent_label: Resource Utilization
    topic_label: Vision Encoder Choices and Scales
  - marker: '[32]'
    intent_label: Resource Utilization
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[39]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Language- or Instruction-Derived RS Supervision
  - marker: '[36]'
    intent_label: Research Gap
    topic_label: Object-Level Understanding
  - marker: '[37]'
    intent_label: Research Gap
    topic_label: Object-Level Understanding
- block_id: 10
  content: ''
  citations: []
- block_id: 11
  content: We initialize the weights of our model with the pretrained CLIP-ViT [24], and LLM (Vicuna-v1.5 [7]) and apply LoRA
    [11] finetuning. Utilizing LoRA, we refine the parameters Wq and Wv through low-rank adaptation, with a designated rank
    r set to 64 in our implementation. The model undergoes training consistently at an image resolution of 504×504 throughout
    the whole process. Each training step incorporates specifically crafted multi-modal instructional templates designed for
    a variety of vision-language tasks during the training process. We use AdamW [21] optimizer with a cosine learning rate
    scheduler to train our model. We keep the global batch size as 144. We train our model in two stages, first, we train
    using all of our datasets for 1 epoch, correspondingly 2400 steps, followed by stage 2, where we only train on the grounding
    dataset for 1600 more steps.
  citations:
  - marker: '[7]'
    intent_label: Model/Architecture Adoption
    topic_label: Language Models and Tokenization
  - marker: '[11]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Parameter-Efficient Adaptation
  - marker: '[24]'
    intent_label: Model/Architecture Adoption
    topic_label: Joint Vision–Language Backbones
  - marker: '[21]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Training and Adaptation Regimes
- block_id: 12
  content: 'Datasets for evaluation. For scene classification, we evaluate our model using AID [33] and UCMerced [35]. AID
    [33] is a large-scale aerial image collection compiled from Google Earth imagery, with 30 classes, such as a river, dense
    residential area, etc. The images are labeled by specialists in the field of remote sensing image interpretation. In total,
    the AID [33] dataset has 10,000 images within 30 classes. The images have been taken from different countries as well
    as different weather conditions. For evaluation, we use a 20% split of the AID [33] dataset. UCMerced [35] is a Land Use
    scene classification dataset, with 2,100 images and 21 classes. Each image is of size 256×256. We use the whole UCMerced
    [35] dataset as a zero-shot test set.


    Results. We prompt the models with all of the classes and prompt to classify the image using just one word/phrase. For
    example, we input a prompt like "Classify the image within one of the given classes: dense residential area, ..., school.
    Answer with one word or short phrase." We calculate zero-shot accuracy on both AID and UCMerced. GeoChat significantly
    outperforms other VLM’s with an accuracy of 84.43% on UCMerced [35] and 72.03% on AID [33]. Notably, the recent MiniGPT-4-v2
    [4] fails to follow the instructions provided for this specific task and returns unrelated classes that are not a part
    of the dataset. Its accuracy is close to 5% if we pass the answers from Vicuna-v1.5 [7] and ask it to check if the output
    sentence refers to the ground truth class or not. In comparison, Qwen-VL and LLaVA-1.5 perform well in instruction following,
    but fall short to GeoChat, due to lack of domain knowledge.'
  citations:
  - marker: '[7]'
    intent_label: Resource Utilization
    topic_label: Evaluation Baselines and Tooling
  - marker: '[33]'
    intent_label: Benchmark Utilization
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[35]'
    intent_label: Benchmark Utilization
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[4]'
    intent_label: Result Comparison
    topic_label: Evaluation Protocols and Diagnostics
- block_id: 13
  content: 'Datasets for evaluation. RSVQA-HRBEN [20] comprises 10,569 high-resolution photos and 1,066,316 question-answer
    pairs, with 61.5%, 11.2%, 20.5%, and 6.8% divided into training, validation, test 1, and test 2 sets, respectively. This
    dataset has three question types: presence, comparison, and count. For evaluation, we use the test set-2 for RSVQA-HRBEN
    [20] with 47k question answer pairs.


    RSVQA-LR [20] is made up of 772 low-resolution images and 77,232 question-answer pairs, with 77.8%, 11.1%, and 11.1% used
    for training, validation, and testing, respectively. There are four different categories of questions: presence, comparison,
    rural/urban, and count. We omitted area and count questions during evaluation because the responses are numerical and
    quantifiable into numerous categories. In the RSVQA-LRBEN [20] dataset, for example, counting questions are quantified
    into five categories: 0, between 1 and 10, between 11 and 100, between 101 and 1000, and greater than 1000. For evaluation,
    we use the test set of RSVQA-LRBEN [20] with 7k question-answer pairs.


    Results. To constrain the answers to a simple yes/no and for rural/urban question types, we add a suitable prompt at the
    end of each question. GeoChat performs close to the SOTA specialist models on RSVQA-LRBEN test set, which is RSGPT [12],
    finetuned on the target dataset for 5 iterations in comparison. We also match the SOTA on urban-rural classification subset.
    For RSVQA-HRBEN, GeoChat outperforms other VLM’s in zero-shot setting on average accuracy by 3.9%, while beating the Comparison
    subset by 15.9% on LLaVA-v1.5 [17].'
  citations:
  - marker: '[12]'
    intent_label: Result Comparison
    topic_label: Evaluation Baselines and Tooling
  - marker: '[20]'
    intent_label: Benchmark Utilization
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[17]'
    intent_label: Result Comparison
    topic_label: Evaluation Baselines and Tooling
- block_id: 14
  content: 'Datasets for evaluation. For the evaluation of grounding tasks, we propose a new benchmark that contains different
    referring and grounding tasks. We use the validation set from [30] and used the same dataset creation pipeline as in Sec.
    4 to construct the test benchmark. There are a total of 7653 [refer], 758 [grounding], and 555 grounding description questions.
    We use accuracy@0.5 as the evaluation metric. Accuracy is calculated if the predicted box has an overlap of more than
    0.5 IoU with the ground-truth box.


    Results. The overall model performance is low on small objects or when it has to predict multiple boxes. Compared to MiniGPT-4-v2
    [4], our model works better on medium size images. On the grounding description task, we calculate both, the IoU for the
    multiple bounding boxes generated as well as the text answer generated. Our model provides a better description with slightly
    better box accuracy than MiniGPT-4-v2 [4]. As for region-level captioning, we evaluate both models based on the text accuracy
    with ground truth region-level captions. Our model significantly outperforms MiniGPT-4-v2 in terms of ROUGE and METEOR
    score.'
  citations:
  - marker: '[30]'
    intent_label: Benchmark Utilization
    topic_label: RS Dataset Usage and Construction
  - marker: '[4]'
    intent_label: Result Comparison
    topic_label: Evaluation Baselines and Tooling
- block_id: 15
  content: Although recent advancements in large Vision-Language Models (VLMs) have shown promise in natural image domains,
    their performance in Remote Sensing (RS) scenarios is still limited due to the unique domain-specific challenges. Addressing
    this gap, we present GeoChat, the first unified remote sensing VLM that excels in multitask conversational capabilities
    with high-resolution RS images. GeoChat not only answers image-level queries but also engages in region-specific dialogue,
    grounding responses with precise spatial coordinates. We create a novel RS multimodal instruction-following dataset comprising
    of 318k image-instruction pairs with a diverse multitask format. GeoChat achieves robust zero-shot performance across
    various RS tasks including scene classification, VQA, multi-turn dialogue, visual grounding and referring object detection,
    thus establishing a comprehensive benchmark.
  citations: []
