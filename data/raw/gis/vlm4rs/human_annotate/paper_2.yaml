title: Boosting Few-shot Remote Sensing Image Scene Classification with Language-guided Multimodal Prompt Tuning
blocks:
- block_id: 0
  content: 'Remote sensing image Scene classification is an important research topic in remote sensing community and has evoked
    a growing concern with the recent development of deep learning techniques. However, the requirement of a large amount
    of annotations brings great challenges to deep learning-based scene classification approaches. Visual-linguistic pretraining
    models, which improve the transferability of visual models using the supervision information of text, create a new way
    for the task under label scarcity scenario. In this paper, we explore the novel approach of prompt engineering, aiming
    to achieve satisfactory performance of multi-modal pretraining models on downstream remote sensing image scene classification
    task with minimal amounts of training data. Experiments were conducted on multiple publicly available datasets. The results
    indicate that training the learnable prompts with a small number of samples can yield impressive results, surpassing the
    few-shot transfer learning results of the best-performing pre-trained models.


    Index Terms—Remote sensing image scene classification, Prompt tuning, Few-shot learning, Multi-modal pretraining.'
  citations: []
- block_id: 1
  content: 'Remote sensing image scene classification, which aims to classify scene images into different semantic categories,
    is a crucial component in remote sensing image processing realm. It has been widely applied in various fields, such as
    land resource management and environmental monitoring [1]. The early remote sensing image scene classification methods
    utilize manually crafted features, such as spectral features and texture features, and then performs classification with
    classifiers [2]. These handcrafted features-based methods require the involvement of numerous efforts of human experts,
    and their generalization ability is weak [3], [4].


    Deep learning methods have become one of the research focuses in the field of remote sensing image scene classification
    due to their powerful hierarchical feature extraction capabilities [5]. Literature [6] utilizes multi-layer convolutional
    layers and dense residual blocks to obtain more detailed features for remote sensing images. [7] combines graph neural
    network (CNN) and graph neural network (GNN) to make full use of the adjacency and disjointness relationship among geographical
    objects. However, the training of these models requires a large amount of annotations, while annotating is a cumbersome
    and costly task [8], [9], [10]. Therefore, improving the scene classification performance with a meager amount of labels
    is still a challenging task in remote sensing field.


    Most recently, pretrained models based on multimodal data have shown tremendous potential in representation learning.
    The deep features learned by such models exhibit strong generalization abilities and demonstrate great versatility across
    various downstream tasks. The most outstanding multimodal pretrained models include CLIP [11], ALBEF [12] and VLMo [13].
    It should be noted that these models successfully promoted the performances of vision tasks by exploring the heterogeneity
    and correlation between lanaguage and vision. For instance, the CLIP pretrained model [11] achieves comparable accuracy
    to ResNet-50 [14] on the ImageNet dataset, even without using its 1.28 million training samples. Some advanced CLIP models
    even achieved zero-shot inference accuracy of over 80.0% on the ImageNet-1K dataset.


    Motivated by the strong generalization performance of the vision-language multimodal pretrained models, we propose the
    language-guided remote sensing image scene classification approach in this paper. Specifically, to overcome the poor performance
    of handcrafted hard prompts when encountering domain-specific tasks or data, we explore the learnable prompt tuning-based
    transfer learning paradigm for remote sensing image scene classification task. Experimental results on two benchmark datasets
    show that learnable prompts can significantly enhance model performance, even outperforming specialized remote sensing
    pretrained models in few-shot learning scenario.


    The rest of this paper is organized as follows. Section II introduces the the proposed method. Section III presents the
    experimental results. Section IV concludes the paper.'
  citations:
  - marker: '[1]'
    intent_label: Prospective Application
    topic_label: Other Topics
  - marker: '[2]'
    intent_label: Prior Methods
    topic_label: Image-Level Understanding
  - marker: '[3]'
    intent_label: Research Gap
    topic_label: Robustness, Reliability, and Practical Constraints
  - marker: '[4]'
    intent_label: Research Gap
    topic_label: Robustness, Reliability, and Practical Constraints
  - marker: '[5]'
    intent_label: Prior Methods
    topic_label: Image-Level Understanding
  - marker: '[6]'
    intent_label: Prior Methods
    topic_label: Vision Encoder Choices and Scales
  - marker: '[7]'
    intent_label: Prior Methods
    topic_label: Feature-Level and Representation Fusion
  - marker: '[8]'
    intent_label: Research Gap
    topic_label: Manual Expert or Crowdsourced Annotation
  - marker: '[9]'
    intent_label: Research Gap
    topic_label: Manual Expert or Crowdsourced Annotation
  - marker: '[10]'
    intent_label: Research Gap
    topic_label: Manual Expert or Crowdsourced Annotation
  - marker: '[11]'
    intent_label: Domain Overview
    topic_label: Joint Vision–Language Backbones
  - marker: '[12]'
    intent_label: Domain Overview
    topic_label: Joint Vision–Language Backbones
  - marker: '[13]'
    intent_label: Domain Overview
    topic_label: Joint Vision–Language Backbones
  - marker: '[14]'
    intent_label: Result Comparison
    topic_label: Vision Encoder Choices and Scales
- block_id: 2
  content: In this section, we will first introduce the utilized vision-language multimodal model CLIP, and then explain the
    prompt tuning-based transfer learning method.
  citations: []
- block_id: 3
  content: 'CLIP is a groundbreaking multimodal pretrained model that provides a new approach by aligning text and images
    in a shared feature space, with separate encoders for images and text. The text encoder is based on BERT [15], while the
    visual encoder can utilize ResNet [14] or ViT [16]. Through large-scale pretraining, the model can learn diverse visual
    concepts and easily transfer to any downstream task using prompts. CLIP represents a successful practice of leveraging
    large-scale multimodal pretraining for zero-shot learning.


    CLIP achieves remarkable zero-shot task transfer capability by only employing contrastive pretraining on a large-scale
    dataset of image-text pairs. As CLIP utilizes multimodal pretraining to transform classification into a retrieval task,
    specifically predicting whether an image matches a given text description, it naturally lends itself to zero-shot recognition.
    This is accomplished by comparing the synthesized classification weights of image features with the text encoder, which
    takes class-specific text descriptions as input.


    For a given image I, it undergoes segment embedding to obtain E0 ∈ R(M × dv). The image encoder V consists of K Transformer
    layers Vi_K (i=1). We introduce a special learnable token as the classification token ci. For the i-th layer, we have:

    [ci, Ei] = V ([ci−1, Ei−1]), i = 1, ..., K (1)


    To obtain the final image representation, the ImageProj(·) function projects the class token cK into the latent embedding
    space of visual-textual domain:

    x = ImageProj(cK), x ∈ Rd, (2)

    where d denotes the dimension of the embedding space.


    Similarly, for the text encoder L, with an input of a word embedding sequence W0 = [w1_0, w2_0, ..., wN_0] ∈ RN × dl,
    for the i-th layer:

    Wi = Li(Wi−1), i = 1, 2, . . . , K (3)


    The final text representation is obtained by projecting the embedding of the last token from the last layer using TextProj(·)
    into the latent embedding space of visual-textual representations:

    z = TextProj(wN_K), z ∈ Rd (4)


    So the image I extracts image features as x, and the text encoder generates a set of feature vectors {zi}C_i=1, where
    C represents the number of classes, and each zi is derived from prompts of the form “a photo of [class]”, with class labels
    replaced by specific class names like “cat”, “dog”, or “car”. Then, the predicted probabilities are computed as:

    p(y = i|I) = exp(cos(zi, x) / τ) / Σ_{j=1}^C exp(cos(zj, x) / τ), (5)

    where τ is a parameter learned during the training process of CLIP, and cos(·) represents cosine similarity.


    Compared to traditional classifier learning methods that learn closed-set visual concepts from random vectors, visual-language
    pretraining allows exploration of open-set visual concepts through high-capacity text encoders, enabling a broader semantic
    space and facilitating transferability of learned representations to downstream tasks. Moreover, CLIP possesses powerful
    zero-shot transfer capabilities, endowing it with open-ended detection abilities.'
  citations:
  - marker: '[14]'
    intent_label: Model/Architecture Adoption
    topic_label: Vision Encoder Choices and Scales
  - marker: '[15]'
    intent_label: Model/Architecture Adoption
    topic_label: Language Models and Tokenization
  - marker: '[16]'
    intent_label: Model/Architecture Adoption
    topic_label: Vision Encoder Choices and Scales
- block_id: 4
  content: 'In this section, we will introduce the employed prompt tuning method called MaPle. Compared to other prompt tuning
    approaches, MaPle incorporates learnable prompts in both modal branches, allowing both modalities to adapt to downstream
    tasks. The learnable prompts are based on the pretrained visual-textual model CLIP, with the visual Transformer serving
    as the visual encoder.


    1) Deep Language Prompts:: To learn language prompts, we introduce b learnable tokens {P^i ∈ R^{dl}}_{i=1}^b to the CLIP
    text branch. The input embeddings are then [P^1, P^2, ..., P^b, W0], where W0 = [w1, w2, ..., wN] represents fixed token
    embeddings of the input text. We incorporate such learnable tokens at deep levels as well. Specifically, for the i-th
    layer:

    [, Wi] = Li([Pi−1, Wi−1]), i = 1, 2, . . . , J, (6)

    where [·, ·] denotes the concatenation operation, and J represents the layer at which we need to insert the prompts. After
    the J-th layer, we handle the prompts in the following manner:

    [Pi, Wi] = Li([Pi−1, Wi−1]), i = J + 1, . . . , K, (7)

    where K represents the total number of layers in that branch. The final text representation is obtained by projecting
    the embedding of the last token from the final layer using TextProj(·) into the visual-textual latent embedding space:

    z = TextProj(wN_K), z ∈ Rd, (8)

    where d represents the dimension of the embedding space.


    2) Deep Visual Prompts:: Similar to the language branch, we introduce b learnable prompt tokens {˜P^i ∈ R^{dv}}_{i=1}^b
    to the visual branch. For the i-th layer of the visual encoder:

    [ci, Ei, ] = V ([ci−1, Ei−1, ˜Pi−1]), i = 1, 2, . . . , J (9)

    [ci, Ei, ˜Pi) = V ([ci−1, Ei−1, ˜Pi−1]), i = J + 1, . . . , K (10)

    To obtain the final image representation, we utilize the function Imageproj(·) as well to project the category token cK
    into the latent embedding space:

    x = ImageProj(cK), x ∈ Rd (11)

    Compared to independent prompts, cross-stage shared prompts are more effective. Therefore, unlike the early stages, late
    stages do not provide free prompts for independent learning.


    3) Coupling Visual and Language Prompts:: If there is no interaction during the learning and adjustment process of these
    prompts, the two modal branches naturally lack synergy. In order to couple the prompts from both modalities, we introduce
    the language prompt token from the J-th layer into the corresponding visual branch using a linear layer as the coupling
    function:

    [ci, Ei, ] = V ([ci−1, Ei−1, Fi−1(˜Pi−1)]), i = 1, 2, . . . , J, (12)

    [ci, Ei, ˜Pi] = V ([ci−1, Ei−1, ˜Pi−1]), i = J + 1, . . . , K, (13)

    where F(·) represents the coupling function.'
  citations: []
- block_id: 5
  content: 'We conducted a series of experiments using two publicly available remote sensing scene classification datasets:
    the AID dataset [17] and the UCM dataset [18], to evaluate the performance of prompt tuning in few-shot transfer learning
    scenario. We first introduce the experimental datasets and deployment details used in this study, then validate the effectiveness
    of our proposed method on these two datasets. Additionally, we compare our approach with the state-of-the-art algorithm,
    namely the ViT model [16] pre-trained on large-scale remote sensing datasets, on both the AID and UCM datasets to highlight
    the strengths and weaknesses of both methods.'
  citations:
  - marker: '[16]'
    intent_label: Result Comparison
    topic_label: Vision Encoder Choices and Scales
  - marker: '[17]'
    intent_label: Benchmark Utilization
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[18]'
    intent_label: Benchmark Utilization
    topic_label: Public Remote Sensing Benchmarks
- block_id: 6
  content: 'AID Dataset: It is a large-scale dataset used for aerial image classification in remote sensing scenarios. It
    consists of 10,000 sample images of 30 different types of aerial scenes. Each category contains 200 to 400 samples. The
    images have a resolution of 600×600 pixels.


    UCM dataset: It consists of 2,100 images in total. It includes 21 different land-use categories, with 100 samples per
    category. The images have a resolution of 256×256 pixels. These images were manually extracted from large-scale images
    of urban areas in various cities across the United States. The pixel resolution of these publicly available images is
    1 foot.


    Evaluation Metrics: We use accuracy as the metric to measure the performance of our algorithm, which is defined as:

    Acc = (Σ_{i=1}^k x_{ii}) / (Σ_{i=1}^k Σ_{j=1}^k x_{ij}), (14)

    where k represents the number of categories. In this case, the dataset used has 30 classes and 21 classes respectively.
    x_{ij} denotes the number of samples from class i that are classified as class j. x_{ii} represents the number of samples
    correctly classified in class i.


    1) Results for Few-shot Transfer Learning: This section discusses the performance of multimodal pretraining models like
    CLIP on remote sensing scene classification datasets using fine-tuning. In the subsequent use of the CLIP model, unless
    otherwise specified, we will be using the OpenAI pretrained CLIP model. We utilize the data Transformer base model (ViT-base)
    as our visual encoder, where H=8, dm=768, K=12 layers, and the image patch resolution is 16×16. The input image size is
    set to 224×224. Therefore, the visual encoder branch used in this paper has approximately 86M parameters. The CLIP model
    is pretrained on a dataset of 400 million images.


    To begin with, we employ manually crafted prompt templates to fine-tune the CLIP model on the AID dataset and UCM dataset.
    The prompt template used is “an aerial remote sensing photo of [class]”, where “[class]” represents the specific remote
    sensing scene category. To evaluate the performance of the algorithm, we select 50% of the data as the test set, while
    the remaining 50% as training set.


    We conducted 8-shot and 16-shot transfer experiments using the prompt tuning approach described in Section 2. Specifically,
    we created new training sets by randomly selecting 8 (or 16) samples per class from the original training set. We then
    trained the model with prompts following the parameter settings of MaPLe. For the 16-shot transfer training, we performed
    15 epochs, while for the 8-shot transfer, we conducted 10 epochs. All experiments were deployed on a compute platform
    consisting of 4×NVIDIA RTX A5000 24G GPUs. The experimental results on two datasets are presented in Table I and Table
    II respectively.


    From the results, it is evident that the CLIP model achieves impressive performance using manually crafted templates.
    However, there are two prominent issues with using manually designed templates for zero-shot transfer learning. 1) during
    the process of manually designing templates, the performance of the CLIP model on these datasets is sensitive to the selection
    of manual templates. For instance, if we choose “a photo of [class]” as the prompt template, where “[class]” represents
    the remote sensing scene category, the test accuracy on the AID dataset is only 55.3%. 2) looking at the specific test
    accuracies for different categories, it is observed that manually designed templates perform poorly on certain classes.
    For example, on the AID dataset, categories such as “Bare land”, “Meadow”, “Storage tanks”, “playground” and “Sparse residential”
    exhibit low accuracies. Similarly, on the UCM dataset, categories like “Dense residential”, “Medium residential”, “parse
    residential” and “Storage tanks” also have almost 0 accuracy.


    The main reasons for these issues can be attributed to two factors. Firstly, during the CLIP pretraining phase, there
    is limited exposure to remote sensing-specific image-text pairs, resulting in a lack of specific visual information related
    to remote sensing domains. Secondly, some categories exhibit minimal differences in text or images, making it challenging
    to distinguish between them. For example, on the AID dataset, categories like “Playground” and “Square” or “Commercial”
    and “Railway station,” as well as on the UCM dataset, categories like “Mobilehome park” and “Parking lot” are easily confused
    due to their subtle distinctions.


    2) Comparative Results: To further investigate effective transfer methods with different types of parameters, we designed
    an experimental comparison with existing transfer learning approaches based on the CLIP model. These include Clip-Adapter,
    CoOp and the MaPle method used in this paper.


    Clip-Adapter is a transfer learning approach that adds an additional adapter module after the backbone network of the
    CLIP model, rather than inserting adapters into the backbone network itself, thus preserving the integrity of the structure.
    In addition, this method utilizes residual connections to mix the original zero-shot visual-linguistic embeddings with
    the corresponding fine-tuned features, enabling the model to leverage both the knowledge stored in the original CLIP and
    the newly learned knowledge from few-shot training.


    From the results, it can be observed that using Clip-Adapter has limited performance on both datasets. This may be due
    to the significant disparity between remote sensing images and natural images. Inserting the adapter module after the
    model makes it challenging to enhance the model’s ability to extract remote sensing-specific features at the frontend
    of the model. In addition, compared with CoOp which inserts tokens only in the first layer of the text modality, the coupling
    MaPle prompt tuning achieves the best performance, which validates the effectiveness of coupling the language and visual
    prompts.'
  citations: []
- block_id: 7
  content: This paper explores the novel transfer learning approach of prompt tuning, which enables the effective utilization
    of limited downstream data and achieves promising results for downstream tasks using multimodal pretrained models. Based
    on this efficient prompt tuning approach, the multimodal pretrained model is applied to remote sensing scene classification.
    The method is experimented on multiple publicly available remote sensing scene classification datasets, and the results
    demonstrate that training the learnable prompts with a small amount of samples can yield impressive performance, surpassing
    the few-shot transfer results of the best-performing pretrained models on these datasets.
  citations: []
