title: 'RS-RAG: Bridging Remote Sensing Imagery and Comprehensive Knowledge with a Multi-Modal Dataset and Retrieval-Augmented
  Generation Model'
blocks:
- block_id: 0
  content: Recent progress in Vision-Language Models (VLMs) has demonstrated impressive capabilities across a variety of tasks
    in the natural image domain. Motivated by these advancements, the remote sensing community has begun to adopt VLMs for
    remote sensing vision-language tasks, including scene understanding, image captioning, and visual question answering.
    However, existing remote sensing VLMs typically rely on closed-set scene understanding and focus on generic scene descriptions,
    yet lack the ability to incorporate external knowledge. This limitation hinders their capacity for semantic reasoning
    over complex or context-dependent queries that involve domain-specific or world knowledge. To address these challenges,
    we first introduced a multimodal Remote Sensing World Knowledge (RSWK) dataset, which comprises high-resolution satellite
    imagery and detailed textual descriptions for 14,141 well-known landmarks from 175 countries, integrating both remote
    sensing domain knowledge and broader world knowledge. Building upon this dataset, we proposed a novel Remote Sensing Retrieval-Augmented
    Generation (RS-RAG) framework, which consists of two key components. The Multi-Modal Knowledge Vector Database Construction
    module encodes remote sensing imagery and associated textual knowledge into a unified vector space. The Knowledge Retrieval
    and Response Generation module retrieves and re-ranks relevant knowledge based on image and/or text queries, and incorporates
    the retrieved content into a knowledge-augmented prompt to guide the VLM in producing contextually grounded responses.
    We validated the effectiveness of our approach on three representative vision-language tasks, including image captioning,
    image classification, and visual question answering, where RS-RAG significantly outperformed state-of-the-art baselines.
    By bridging remote sensing imagery and comprehensive knowledge, RS-RAG empowers remote sensing VLMs with enhanced contextual
    reasoning, enabling them to generate more accurate, informative, and semantically grounded outputs across a wide range
    of tasks.
  citations: []
- block_id: 1
  content: 'Remote sensing imagery, as a critical source of information for Earth observation and monitoring, plays an essential
    role in urban planning [1], agricultural assessment [2], and environmental protection [3]. However, as remote sensing
    technology advances, the scale and complexity of imagery data have rapidly increased, making it increasingly difficult
    for traditional manual analysis or image processing methods to meet practical demands. Deep learning methods [4], [5]
    have significantly improved the accuracy and efficiency of tasks like classification, segmentation and object detection
    by automatically extracting features from vast amounts of remote sensing data. While these methods have made notable progress,
    most deep learning models rely predominantly on single-modal visual information, lacking deep semantic understanding of
    image content. This limitation results in reduced generalization and adaptability, particularly for tasks that require
    in-depth semantic analysis and comprehensive scene understanding.


    The emergence of Vision-Language Models (VLMs) [6], [7], [8], [9], [10], [11] offers a novel solution for the semantic
    analysis of remote sensing data. By leveraging multimodal fusion techniques, VLMs combine visual features with language
    information to automatically generate descriptive insights for remote sensing imagery. This semantic enhancement improves
    image classification and object detection performance while enabling the transformation of recognition results into natural
    language descriptions, making them more interpretable and accessible for various applications. Additionally, VLMs perform
    well even in weakly supervised or zero-shot learning scenarios, providing reliable analysis with limited labeled data
    and thus reducing dependency on extensive data annotations. This cross-modal integration not only enhances the model’s
    cognitive ability to interpret remote sensing imagery but also facilitates detailed semantic descriptions of complex scenes,
    paving the way for broader intelligent applications in remote sensing. However, existing remote sensing VLMs primarily
    focus on identifying image features and providing basic scene descriptions, lacking deeper background understanding of
    the objects within the images. Particularly when it comes to rich semantic information requiring remote sensing domain
    expertise or other general world knowledge, such as historical, cultural, and social contexts, these VLM models often
    struggle to provide comprehensive contextual support.


    To address this issue, we first introduce the Remote Sensing World Knowledge (RSWK) dataset, a multimodal remote sensing
    dataset that contains high-resolution imagery and natural language descriptions for approximately 14,141 well-known locations
    worldwide from 175 conuties. Unlike most existing remote sensing vision-language datasets that only provide basic descriptions
    of current scenes, our RSWK dataset will include richer remote sensing domain expertise and world knowledge about the
    objects within these scenes. For instance, from a remote sensing perspective, it will provide information on surface reflectance,
    spectral indices, and atmospheric. From a world knowledge perspective, it will include historical background, cultural
    significance, construction period, and major events. This combination of remote sensing expertise and world knowledge
    will not only enhance the RSWK dataset’s utility for visual analysis of remote sensing images, but also provide the model
    with deeper semantic context, overcoming the limitations of traditional datasets and enabling remote sensing VLMs to perform
    more complex cognitive tasks. Furthermore, our dataset incorporates historical, cultural, and social backgrounds from
    various countries and regions, allowing VLM models to be trained across diverse geographical and cultural contexts, thereby
    improving their generalization ability and understanding of different cultural settings.


    Furthermore, to effectively leverage the comprehensive information provided by the RSWK dataset, we propose the Remote
    Sensing Retrieval-Augmented Generation (RS-RAG) model. RS-RAG is designed to enhance the capacity of vision-language models
    to generate contextually enriched and knowledge-grounded responses for remote sensing imagery. It operates by integrating
    external knowledge, both domain-specific and general world knowledge, retrieved from a multimodal knowledge base constructed
    using the RSWK dataset. The model consists of two main components: (1) the Multi-Modal Knowledge Vector Database Construction
    module, which encodes satellite imagery and textual descriptions into a shared embedding space using unified image and
    text encoders to enable efficient cross-modal retrieval; and (2) the Knowledge Retrieval and Response Generation module,
    which retrieves top-ranked knowledge entries based on image and text queries, re-ranks them via a fused similarity score,
    and incorporates the selected content into a knowledge-augmented prompt. This prompt is then passed to the vision-language
    model, enabling it to generate responses that go beyond semantic understanding and reflect deeper background knowledge.
    By coupling visual input with relevant contextual information, RS-RAG significantly improves the interpretability and
    accuracy of vision-language outputs, particularly for complex queries involving geospatial, historical, or environmental
    reasoning. To assess the effectiveness of our proposed model, we construct a lightweight benchmark and conduct comprehensive
    experiments on three representative tasks: image captioning, image classification, and visual question answering. Results
    on these tasks demonstrate the model’s ability to produce accurate, context-rich descriptions, deliver semantically informed
    scene classifications, and provide precise answers to knowledge-intensive queries by leveraging both visual and textual
    modalities. Through these findings, RS-RAG demonstrates its potential to substantially advance the capabilities of vision-language
    models in remote sensing, effectively bridging the gap between imagery and comprehensive contextual knowledge. Our main
    contributions are summarized as follows:


    - We construct the Remote Sensing World Knowledge (RSWK) dataset, a large-scale multimodal benchmark containing 14,141
    high-resolution remote sensing images and rich textual descriptions of globally recognized landmarks from 175 countries.
    The descriptions incorporate both domain-specific knowledge (e.g., land use, temperature, wind direction) and general
    world knowledge (e.g., historical, cultural, and societal context).

    - We propose RS-RAG, a novel Retrieval-Augmented Generation framework tailored for remote sensing vision-language tasks.
    RS-RAG retrieves semantically relevant knowledge from a multimodal vector database and integrates it with the input via
    knowledge-conditioned prompt construction, significantly enhancing contextual reasoning capabilities.

    - We design a lightweight benchmark for evaluating remote sensing vision-language models across three core tasks: image
    captioning, image classification, and visual question answering. This benchmark enables systematic assessment of both
    semantic understanding and deeper knowledge-grounded reasoning.

    - Extensive experiments demonstrate that RS-RAG consistently outperforms state-of-the-art vision-language models across
    all tasks, particularly on queries requiring external world knowledge. These results highlight the effectiveness of RS-RAG
    in bridging remote sensing imagery with structured knowledge, and point to promising future directions for research on
    remote sensing vision-language models.'
  citations:
  - marker: '[1]'
    intent_label: Prospective Application
    topic_label: Other Topics
  - marker: '[2]'
    intent_label: Prospective Application
    topic_label: Other Topics
  - marker: '[3]'
    intent_label: Prospective Application
    topic_label: Other Topics
  - marker: '[4]'
    intent_label: Prior Methods
    topic_label: Vision Encoder Choices and Scales
  - marker: '[5]'
    intent_label: Prior Methods
    topic_label: Vision Encoder Choices and Scales
  - marker: '[6]'
    intent_label: Prior Methods
    topic_label: Joint Vision–Language Backbones
  - marker: '[7]'
    intent_label: Prior Methods
    topic_label: Joint Vision–Language Backbones
  - marker: '[8]'
    intent_label: Prior Methods
    topic_label: Joint Vision–Language Backbones
  - marker: '[9]'
    intent_label: Prior Methods
    topic_label: Joint Vision–Language Backbones
  - marker: '[10]'
    intent_label: Prior Methods
    topic_label: Joint Vision–Language Backbones
  - marker: '[11]'
    intent_label: Prior Methods
    topic_label: Joint Vision–Language Backbones
- block_id: 2
  content: ''
  citations: []
- block_id: 3
  content: Several multimodal datasets [12], [13], [14], [15], [16] have been developed to bridge the gap between vision and
    language in the remote sensing domain. UCM Captions [17], Sydney Captions [17], and RSICD [18] are among the earliest
    datasets that provide textual descriptions for remote sensing images. Each image in these datasets is paired with five
    relatively simple human-written sentences, offering only a basic level of semantic information. To enhance the quality
    and richness of textual annotations, RSGPT [6] recently introduced RSICap, a high-quality image captioning dataset with
    detailed human-annotated descriptions of aerial scenes. RSICap serves as a valuable resource for fine-tuning and developing
    domain-specific vision-language models in remote sensing. Beyond manual annotation, researchers have explored the construction
    of large-scale datasets using automatic or hybrid approaches applied to existing data sources. For instance, RS5M [19]
    was created by aggregating 11 publicly available image–text paired datasets along with three large-scale class-level labeled
    datasets. Captions were generated using BLIP-2, resulting in a diverse and large-scale dataset suitable for training foundational
    multimodal models. Similarly, RemoteCLIP [20] compiled a large-scale dataset by integrating 10 object detection datasets,
    4 semantic segmentation datasets, and 3 remote sensing image–text datasets, enabling contrastive pretraining for cross-modal
    alignment. In addition, GeoChat introduced the RS Multimodal Instruction Dataset, which incorporates heterogeneous data
    sources, including three object detection datasets, one scene classification dataset, and a visual question answering
    dataset focused on flood detection. More recently, FedRSCLIP [21] proposed a new multimodal remote sensing dataset specifically
    designed for federated learning scenarios, further expanding the applicability of vision-language research in distributed
    settings. These efforts collectively advance the development of remote sensing VLMs by providing diverse, rich, and large-scale
    multimodal data resources tailored to various downstream tasks.
  citations:
  - marker: '[6]'
    intent_label: Prior Methods
    topic_label: Manual Expert or Crowdsourced Annotation
  - marker: '[12]'
    intent_label: Prior Methods
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[13]'
    intent_label: Prior Methods
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[14]'
    intent_label: Prior Methods
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[15]'
    intent_label: Prior Methods
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[16]'
    intent_label: Prior Methods
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[17]'
    intent_label: Prior Methods
    topic_label: Manual Expert or Crowdsourced Annotation
  - marker: '[18]'
    intent_label: Prior Methods
    topic_label: Manual Expert or Crowdsourced Annotation
  - marker: '[19]'
    intent_label: Prior Methods
    topic_label: Language- or Instruction-Derived RS Supervision
  - marker: '[20]'
    intent_label: Prior Methods
    topic_label: Public Remote Sensing Benchmarks
  - marker: '[21]'
    intent_label: Prior Methods
    topic_label: Public Remote Sensing Benchmarks
- block_id: 4
  content: With the growing availability of remote sensing multimodal datasets, a number of vision-language models [6], [7],
    [8], [9], [10], [11] have been proposed to enhance the understanding and interpretation of aerial imagery through natural
    language. [22] provides the first comprehensive review of vision-language models in remote sensing, systematically summarizing
    tasks, datasets, and methods, and identifying key challenges and future directions. RSCLIP [23] is a pioneering vision-language
    model designed for remote sensing scene classification, which leverages contrastive vision-language supervision and incorporates
    a pseudo-labeling technique along with a curriculum learning strategy to improve zero-shot classification performance
    through multi-stage fine-tuning. RSGPT [6] represents one of the earliest attempts to build a generative pretrained model
    for remote sensing. It fine-tunes only the Q-Former and a linear projection layer on the proposed RSI-Cap dataset, achieving
    notable improvements in both image captioning and visual question answering tasks. Similarly, GeoChat [24] adapts the
    LLaVA-1.5 architecture and fine-tunes it on its proposed remote sensing multimodal instruction-following dataset, offering
    multi-task conversational capabilities grounded in high-resolution satellite imagery. In addition, to address the common
    issue of hallucination in remote sensing vision-language models, a Helpful and Honest Remote Sensing Vision-Language Model
    [8], named H2RSVLM, is proposed and fine-tuned on the RSSA dataset, the first dataset specifically designed to enhance
    self-awareness in remote sensing VLMs. More recently, RSMoE [25] was proposed as the first Mixture-of-Experts-based VLM
    tailored for remote sensing. It features a novel instruction router that dynamically dispatches tasks to multiple lightweight
    expert LLMs, allowing each expert to specialize in a specific subset of tasks.
  citations:
  - marker: '[6]'
    intent_label: Prior Methods
    topic_label: Parameter-Efficient Adaptation
  - marker: '[7]'
    intent_label: Prior Methods
    topic_label: Joint Vision–Language Backbones
  - marker: '[8]'
    intent_label: Prior Methods
    topic_label: Output Consistency and Error Characteristics
  - marker: '[9]'
    intent_label: Prior Methods
    topic_label: Joint Vision–Language Backbones
  - marker: '[10]'
    intent_label: Prior Methods
    topic_label: Joint Vision–Language Backbones
  - marker: '[11]'
    intent_label: Prior Methods
    topic_label: Joint Vision–Language Backbones
  - marker: '[22]'
    intent_label: Domain Overview
    topic_label: Other Topics
  - marker: '[23]'
    intent_label: Prior Methods
    topic_label: Image-Level Understanding
  - marker: '[24]'
    intent_label: Prior Methods
    topic_label: Instruction Tuning and Supervised Fine-Tuning (SFT)
  - marker: '[25]'
    intent_label: Prior Methods
    topic_label: Multi-Task and Joint Training
- block_id: 5
  content: 'Most existing remote sensing VLM datasets focus primarily on basic descriptions of objects in a scene, typically
    providing only information about the objects present. While these datasets serve a purpose in supporting fundamental tasks
    such as scene classification and image captioning, they fall short in applications requiring complex semantic understanding
    or contextual awareness. To address this limitation, in our full paper, we would like to propose the Remote Sensing World
    Knowledge (RSWK) dataset, which encompasses high-resolution remote sensing imagery of well-known locations worldwide,
    along with domain knowledge and world knowledge described in natural language. This dataset not only fills the gap in
    knowledge depth and breadth found in current remote sensing datasets but also provides a new foundation for advancing
    remote sensing technology toward intelligent applications.


    The core value of remote sensing data lies in its ability to deliver rich geographic and spatial distribution information.
    By integrating high-resolution imagery from across the globe with detailed domain knowledge and world knowledge, the RSWK
    dataset extends this value, expanding the application of remote sensing data from traditional foundational tasks to complex
    scenarios that require deeper semantic understanding.'
  citations: []
- block_id: 6
  content: 'The end-to-end pipeline for constructing the Remote Sensing World Knowledge (RSWK) dataset aims to bridge the
    gap between remote sensing domain knowledge and encyclopedic world knowledge of globally distributed landmarks. This pipeline
    is designed to automatically curate and align multimodal information from global sources, integrating high-resolution
    remote sensing imagery, remote sensing expert knowledge, and contextual world knowledge.


    The process begins with GPT-4o [26], a state-of-the-art large language model, which is employed to generate a comprehensive
    list of globally recognized landmarks across diverse countries and regions, ensuring broad cultural and geographic coverage
    that includes both natural and man-made sites of historical and societal relevance. To acquire world knowledge, we utilize
    the Wikipedia API to extract descriptive information for each landmark, such as historical background, cultural relevance,
    architectural details, and notable events. Post-processing is performed using DeepSeek [27] to remove low-quality or irrelevant
    entries. In parallel, the Google Geocoding API is used to obtain precise geographic coordinates for each location; to
    resolve potential ambiguities in place names, we implement a validation mechanism that cross-references multiple sources
    and applies regional constraints. High-resolution satellite imagery is retrieved from the ArcGIS Tile Map Service, with
    spatial resolutions ranging from 0.6m to 0.15m, followed by normalization, cropping, and resizing to ensure consistency
    in image quality and dimensions. Concurrently, remote sensing expert knowledge is derived using Google Earth Engine (GEE)
    [28]. This includes a wide range of satellite-derived geophysical and meteorological variables such as land surface temperature,
    surface albedo, land cover classification, vegetation indices, and precipitation, which are sourced from authoritative
    datasets like MODIS, Landsat, and ERA5. By integrating these three complementary modalities, remote sensing imagery, remote
    sensing expert knowledge, and contextual world knowledge, the RSWK dataset provides a rich foundation for multimodal learning
    in remote sensing, enabling downstream tasks such as image captioning, image classification, and visual question answering.'
  citations:
  - marker: '[26]'
    intent_label: Resource Utilization
    topic_label: Language Models and Tokenization
  - marker: '[27]'
    intent_label: Resource Utilization
    topic_label: Language Models and Tokenization
  - marker: '[28]'
    intent_label: Resource Utilization
    topic_label: Rule-Based or GIS/Map-Derived
- block_id: 7
  content: 'Through the above data construction pipeline, the RSWK dataset successfully collected a total of 14,141 landmark
    instances from 175 countries, each accompanied by high-resolution satellite imagery, domain-specific remote sensing attributes,
    and structured world knowledge descriptions. The global spatial distribution of the landmarks demonstrates wide coverage
    across all major continents. Notably, the dataset includes landmarks from a diverse range of regions, effectively covering
    most major countries worldwide. Countries such as the United States, the United Kingdom, China, and Japan contribute the
    largest number of landmarks. In addition, the dataset shows the frequency distribution of the top landmark categories,
    including parks, museums, natural scenery, universities, and historic sites. The category distribution is relatively balanced,
    highlighting the diverse types of landmarks captured in the dataset and ensuring a rich set of semantic concepts for downstream
    tasks. Each RSWK entry contains three core components: the satellite imagery, the domain knowledge (e.g., albedo, emissivity,
    land cover type, and meteorological variables), and the structured world knowledge (e.g., historical background, architectural
    characteristics, and cultural significance). This tri-modal representation demonstrates the depth and richness of the
    dataset, supporting a wide range of geospatial understanding and vision-language reasoning tasks.'
  citations: []
- block_id: 8
  content: 'To bridge the semantic gap between remote sensing imagery and comprehensive external knowledge, we propose RS-RAG,
    a Retrieval-Augmented Generation framework designed to integrate both domain-specific and world knowledge into vision-language
    reasoning. RS-RAG consists of two main components: the Multi-Modal Knowledge Vector Database Construction module, which
    encodes remote sensing imagery and textual knowledge into a unified embedding space; and the Knowledge Retrieval and Response
    Generation module, which retrieves and fuses the most relevant knowledge to support downstream tasks. By conditioning
    the vision-language model on retrieved context, RS-RAG enables knowledge-grounded understanding for diverse applications
    such as image captioning, scene classification, and visual question answering.'
  citations: []
- block_id: 9
  content: 'VLMs are designed to generate natural language outputs conditioned on multimodal inputs, typically a visual observation
    and a textual prompt. Let qI denote the input image (e.g., a remote sensing image), and qT the associated textual prompt
    (e.g., a question or instruction). A conventional VLM seeks to generate a natural language response ˆy by modeling the
    conditional probability distribution over output space and selecting the most probable response. Formally, this can be
    expressed as:

    ˆy = arg max_y P (y | qT , qI ; θVLM), (1)

    where θVLM represents the parameters of the VLM, ˆy can represent an image caption, a classification label, or an answer
    to a visual question. This closed-form generation framework assumes that all necessary information for reasoning is either
    visually grounded in the input image qI or implicitly encoded within the model parameters θVLM. While this assumption
    often holds for natural image datasets, it becomes problematic in the domain of remote sensing. Remote sensing images
    typically capture large-scale scenes, such as entire cities or extensive geographic regions, where accurate interpretation
    often relies on understanding cultural, historical, or geographical significance. Such knowledge is rarely discernible
    from visual features and is not explicitly modeled in conventional VLMs.


    To address this limitation, we extend the standard VLM formulation by incorporating external, query-relevant knowledge.
    Specifically, we adopt a Retrieval-Augmented Generation (RAG) framework, wherein a set of top-k relevant knowledge snippets
    R is retrieved from an external corpus based on the multimodal similarity between the input image–text pair (qI , qT )
    and the knowledge index. The retrieved context R is then integrated with the original inputs to guide the response generation
    process. Formally, the objective becomes:

    ˆy = arg max_y P (y | qT , qI , R; θVLM), (2)

    where the additional context R allows the model to produce more informative and contextually grounded outputs. This open-book
    generation paradigm is particularly well-suited for remote sensing applications, where high-level semantic understanding
    often depends on both domain-specific knowledge (e.g., land use categories) and broader world knowledge (e.g., cultural
    or geopolitical significance)—information that is typically absent from raw pixel data alone.'
  citations: []
- block_id: 10
  content: 'To enable retrieval-augmented generation, we construct a Multi-Modal Knowledge Vector Database (MKVD) by encoding
    the RSWK dataset, which contains high-resolution remote sensing images Ii and their paired textual descriptions Ti. These
    data are transformed into dense embeddings using CLIP and stored in a shared semantic space to support efficient and flexible
    cross-modal retrieval. We adopt CLIP as a unified encoder consisting of an image encoder fI(·) and a text encoder fT(·),
    each mapping input into a shared embedding space Rd. Each image Ii is encoded into a visual embedding:

    vi = fI(Ii) ∈ Rd. (3)


    In parallel, the corresponding textual document Ti is segmented into mi semantically coherent chunks {Ti,1, Ti,2, . .
    . , Ti,mi}, and each chunk is encoded using the text encoder:

    ti,j = fT(Ti,j) ∈ Rd. (4)


    The resulting image embeddings {vi} and text embeddings {ti,j} are indexed in Qdrant, a high-performance vector database
    optimized for approximate nearest neighbor (ANN) search. To organize the data, embeddings are stored in two separate collections:
    Dimage for image embeddings and Dtext for text embeddings. Each image-text pair is linked via a unique identifier IDi.
    Specifically, each vi ∈ Dimage is associated with a set {ti,j}mi_{j=1} ⊂ Dtext, which encodes domain-specific and general
    world knowledge about the same location or object. Metadata such as raw textual descriptions, image paths, and geospatial
    attributes are stored alongside each entry as payloads. This database structure supports modality-specific and cross-modal
    retrieval, serving as the foundation for external knowledge integration in the RS-RAG framework.'
  citations: []
- block_id: 11
  content: 'After constructing the Multi-Modal Knowledge Vector Database, we implement a retrieval-augmented generation pipeline
    that enhances vision-language understanding by incorporating external knowledge retrieved via cross-modal similarity.
    Given a user query q, composed of an image component qI and a textual component qT, we first encode each input into dense
    embeddings:

    vT = fT(qT), vI = fI(qI), (5)

    where fT(·) and fI(·) denote the CLIP-based text and image encoders, respectively. To retrieve semantically relevant knowledge,
    we perform similarity search in both the text and image embedding spaces, retrieving the top-τ candidates from each modality:

    Rτ_T = Topτ (vT, Dtext), Rτ_I = Topτ (vI, Dimage), (6)

    where Dtext and Dimage represent the text and image embedding collections in the vector database. While the initial retrieval
    from each modality yields candidates based on unimodal similarity, these results may contain semantically redundant, irrelevant,
    or inconsistent entries due to the disjoint nature of visual and textual embedding spaces. To address this issue, we introduce
    a retrieval re-ranking step that jointly considers both modalities. Specifically, we first merge the retrieved sets from
    each modality, Rfused = Rτ_T ∪ Rτ_I. Each candidate is then assigned a fused similarity score via weighted combination:

    score(ri) = (1 − α) · sT(ri) + α · sI(ri), (7)

    where sT(ri) and sI(ri) are the cosine similarities between the query and the candidate in the respective embedding spaces.
    The weighting parameter α ∈ [0, 1] controls the relative influence of each modality. Based on these fused scores, we select
    the top-K most relevant candidates:

    {k1, . . . , kK} = TopK ({score(ri) | ri ∈ Rfused}) . (8)


    To enhance semantic coherence and eliminate redundancy among the retrieved segments, we apply a Knowledge-Conditioned
    Context Fusion module that consolidates them into a single, contextually grounded representation. Specifically, a frozen
    large language model Lfuse is employed to synthesize the knowledge-conditioned context R from the top-ranked knowledge
    snippets:

    R = Lfuse({k1, . . . , kK}). (9)


    This fusion step consolidates salient content from the retrieved segments into a compact, context-aware representation,
    thereby facilitating structured prompt construction. Given the original user query qT and the fused knowledge context
    R, we construct a retrieval-augmented prompt Pq via Knowledge-Augmented Prompt Construction as follows:

    Pq = Concat [ϕ, qT , ψ, R] (10)

    where ϕ is a task-specific instruction token (e.g., “Answer the following question based on the retrieved knowledge:”),
    ψ is a knowledge header (e.g., “Retrieved context:” ), and R is the fused knowledge used to support reasoning. Finally,
    the composed prompt Pq, along with the visual input qI, is provided to the VLM, which performs Knowledge-Grounded Response
    Generation via joint multimodal reasoning and generates the final output:

    ˆy = GenerateθVLM (y | Image = qI, Prompt = Pq). (11)


    By leveraging retrieval from a multi-modal knowledge base, this framework empowers the RSVLMs to go beyond purely visual
    grounding by integrating both domain-specific knowledge and broader world knowledge, including cultural, historical, and
    geopolitical context. This enriched understanding enables the model to generate more accurate, context-aware, and semantically
    comprehensive outputs. In doing so, our RS-RAG framework effectively bridges the gap between remote sensing imagery and
    external knowledge sources, establishing a retrieval-augmented generation paradigm tailored to the unique demands of the
    remote sensing domain.'
  citations: []
- block_id: 12
  content: ''
  citations: []
- block_id: 13
  content: 'All input images are resized to 512 by 512 pixels before being fed into the models. We evaluate several state-of-the-art
    vision-language models as baselines, including InternVL2.5-Instruct-8B [29], Janus-Pro-7B [30], Qwen-2.5-VL-7B-Instruct
    [31], and LLaMA-3.2-Vision-11B-Instruct [32]. Building upon Qwen-2.5-VL and LLaMA-3.2-Vision, we further develop our retrieval-augmented
    variants, named RS-RAG-7B and RS-RAG-11B, respectively. To better adapt these models to the remote sensing domain, we
    fine-tune them via Low-Rank Adaptation (LoRA) [33] using instruction-following data consisting of 1380 curated image-text
    pairs. Fine-tuning is conducted for 3 epochs with a batch size of 1, using the Adamw optimizer and an initial learning
    rate of 1 × 10−4. To systematically evaluate instruction-tuned vision-language models in the remote sensing domain, we
    curate a new benchmark consisting of task-specific subsets for image captioning (348 samples), image classification (910
    samples), and visual question answering (300 samples). All experiments are performed using 3 NVIDIA RTX A6000 GPUs, each
    with 48 GB of memory.


    To comprehensively evaluate model performance, we adopt a set of standard metrics tailored to each task. For image captioning
    and visual question answering, we report BLEU-1, BLEU-2, BLEU-3, BLEU-4, METEOR, ROUGE L, and CIDEr to measure the fluency,
    relevance, and informativeness of generated text. For the classification task, we evaluate both overall accuracy and per
    class accuracy to reflect model performance across diverse scene categories.'
  citations:
  - marker: '[29]'
    intent_label: Result Comparison
    topic_label: Evaluation Baselines and Tooling
  - marker: '[30]'
    intent_label: Result Comparison
    topic_label: Evaluation Baselines and Tooling
  - marker: '[31]'
    intent_label: Result Comparison
    topic_label: Evaluation Baselines and Tooling
  - marker: '[32]'
    intent_label: Result Comparison
    topic_label: Evaluation Baselines and Tooling
  - marker: '[33]'
    intent_label: Algorithm/Principle Adoption
    topic_label: Parameter-Efficient Adaptation
- block_id: 14
  content: 'The performance of baseline and proposed models on the image captioning task over remote sensing imagery is evaluated
    using a comprehensive set of metrics including BLEU-1 to BLEU-4, METEOR, ROUGE-L, and CIDEr. Among the baseline models,
    LLaMA3.2-Vision-11B achieves the strongest performance, attaining a BLEU-4 score of 0.173 and METEOR of 0.225, highlighting
    its relative strength in generating syntactically and semantically coherent captions. However, it still exhibits limitations
    in domain-specific understanding and factual richness due to its lack of grounding in external knowledge. In contrast,
    both of our proposed retrieval-augmented models, RS-RAG-7B and RS-RAG-11B, demonstrate consistent improvements across
    all evaluation metrics. Notably, RS-RAG-11B outperforms all baselines with a BLEU-4 of 0.266 and METEOR of 0.276, representing
    absolute gains over the strongest baseline. It also achieves the highest ROUGE-L score of 0.322, indicating improved phrase-level
    overlap with human-written descriptions. While RS-RAG-7B uses a smaller backbone, it still yields a significant performance
    boost, achieving a strong CIDEr score, which suggests enhanced alignment with human judgment of caption quality.


    To better understand the effectiveness of our RS-RAG framework, we conduct a qualitative comparison of image captioning
    outputs generated by baseline models and our RS-RAG model. The selected case depicts the Great Seto Bridge, a large-scale
    landmark in Japan. Baseline models, such as InternVL2.5-8B and Qwen2.5-VL-7B, either misidentify the landmark or generate
    overly generic descriptions without domain-grounded facts. Although LLaMA3.2-Vision-11B produces a correct name, it lacks
    detailed evidence from the image or environmental context. Notably, our proposed RS-RAG-7B and RS-RAG-11B models generate
    accurate, entity-aware, and knowledge-rich descriptions that correctly identify the Great Seto Bridge and provide factual
    details about its coordinates, construction history, and structural type. Furthermore, the descriptions include domain-specific
    knowledge (e.g., surface albedo, emissivity, and land cover classification), demonstrating the ability of our models to
    integrate retrieved remote sensing metadata. The use of world knowledge, such as historical background and engineering
    facts, further enhances the semantic richness and correctness of the generated captions. These results highlight the benefits
    of retrieval-augmented generation in grounding VLM outputs with both domain and general-purpose knowledge.'
  citations: []
- block_id: 15
  content: 'The performance of all models on the image classification task using a subset of the RSWK dataset is measured
    by overall accuracy and per-class accuracy across 15 scene categories. Among the baselines, LLaMA-3.2-Vision-11B achieves
    the highest overall accuracy at 34.0 percent, followed closely by InternVL2.5-8B and Qwen2.5-VL-7B. Despite these results,
    baseline models perform poorly on several knowledge-dependent categories such as Church, Mansion, Historic Site, and Museum,
    indicating their limited capacity to reason about semantically nuanced or infrequent classes.


    Our RS-RAG-11B model achieves a substantial improvement, attaining 84.2 percent overall accuracy. It outperforms all baselines
    by a large margin across nearly all categories, including challenging ones like Church with 82.0 percent accuracy and
    Historic Site with 65.5 percent accuracy. RS-RAG-7B also surpasses all baselines, reaching 65.9 percent overall accuracy,
    despite using a smaller backbone. These results demonstrate the effectiveness of integrating retrieved world and domain
    knowledge, which helps the model resolve semantic ambiguities and enhances recognition of visually similar or context-dependent
    categories in remote sensing imagery.


    Qualitative comparisons of image classification outputs generated by baseline models and our RS-RAG variants on representative
    examples show that existing vision-language baselines often misclassify scenes, assigning incorrect labels such as "University,"
    "Government Building," or "Historic Site." In contrast, RS-RAG-7B and RS-RAG-11B correctly identify the ground-truth categories,
    demonstrating a better understanding of scene semantics. These results qualitatively highlight the strength of retrieval-augmented
    generation in capturing contextual cues and leveraging external knowledge to improve fine-grained classification in complex
    remote sensing scenarios.'
  citations: []
- block_id: 16
  content: 'The quantitative evaluation of various models on the VQA task using a subset of the RSWK dataset shows that across
    all metrics—including BLEU, METEOR, ROUGE-L, and CIDEr—the proposed RS-RAG variants outperform all baselines by substantial
    margins. RS-RAG-11B achieves the highest scores on BLEU-1, BLEU-4, and METEOR, indicating more fluent and semantically
    relevant answers. RS-RAG-7B also delivers competitive results, achieving a strong CIDEr score, suggesting strong alignment
    with human-annotated answers in terms of informativeness and precision. In contrast, baseline models such as Qwen2.5-VL-7B
    and Janus-Pro-7B struggle to generate accurate or coherent responses, especially in knowledge-intensive queries.


    Representative qualitative examples illustrate that in some cases baseline responses are either factually incorrect or
    overly generic, whereas both RS-RAG models retrieve and accurately generate correct, knowledge-grounded answers. In historically
    grounded questions, RS-RAG models correctly identify specific events, while other models either hallucinate unrelated
    content or fail to recognize the event entirely. These examples underscore the strength of RS-RAG in incorporating relevant
    domain and world knowledge to produce accurate, informative, and context-aware answers in remote sensing VQA tasks.'
  citations: []
- block_id: 17
  content: 'Effect of the number of retrieved candidates. To investigate the impact of the number of retrieved candidates
    in the retrieval-augmented generation process, we conduct an ablation study on the top-k parameter in RS-RAG. With the
    fusion weight α in Eq.7 fixed to 0.9, we evaluate the model’s performance on the image captioning task under different
    values of k. The results show that retrieving a single, highly relevant knowledge snippet (k = 1) yields the best performance
    across all evaluation metrics. As k increases to 3 and 5, the performance consistently degrades, indicating that incorporating
    more candidates introduces semantic redundancy or irrelevant noise. Such noise can interfere with the model’s ability
    to generate concise and accurate captions. These findings underscore the importance of retrieval precision in the remote
    sensing domain and confirm that fewer, but higher-quality, knowledge segments are more effective for guiding the generation
    process.


    Effect of fusion weight α. We further explore the effect of the fusion weight α that balances visual and textual similarity
    in the re-ranking step (see Eq.7). The results show that α = 0.5 achieves the best overall performance, indicating that
    equal weighting of visual and text similarity provides the most informative guidance for downstream caption generation.
    When α is too low (e.g., 0.3), text similarity dominates, leading to less visually grounded results. Conversely, when
    α increases beyond 0.5, performance slightly degrades, suggesting that over-reliance on visual similarity may overlook
    relevant textual semantics. These findings confirm that effective cross-modal fusion is critical for high-quality retrieval
    and generation.'
  citations: []
- block_id: 18
  content: In this work, we presented RS-RAG, a retrieval-augmented vision-language framework designed to bridge remote sensing
    imagery with structured domain and world knowledge. To support this framework, we constructed the RSWK dataset, a large-scale
    multimodal benchmark that integrates high-resolution satellite imagery with rich textual descriptions covering over 14,000
    globally recognized locations from 175 countries. By leveraging this curated knowledge base, RS-RAG significantly improves
    contextual reasoning and semantic understanding across key vision-language tasks, including image captioning, image classification,
    and visual question answering. Extensive experiments validate the effectiveness of our approach, particularly in handling
    complex, knowledge-intensive queries. We believe that both the RSWK dataset and the RS-RAG framework provide a strong
    foundation for advancing research in remote sensing vision-language understanding and knowledge-grounded geospatial AI.
  citations: []
