# SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model

## Abstract
Large language models (LLMs) have recently been extended to the vision-language realm, obtaining impressive general multi-modal capabilities. However, the exploration of multi-modal large language models (MLLMs) for remote sensing (RS) data is still in its infancy, and the performance is not satisfactory. In this work, we introduce SkyEyeGPT, a unified multi-modal large language model specifically designed for RS vision-language understanding. To this end, we meticulously curate an RS multi-modal instruction tuning dataset, including single-task and multi-task conversation instructions. After manual verification, we obtain a high-quality RS instruction-following dataset with 968k samples. Our research demonstrates that with a simple yet effective design, SkyEyeGPT works surprisingly well on considerably different tasks without the need for extra encoding modules. Specifically, after projecting RS visual features to the language domain via an alignment layer, they are fed jointly with task-specific instructions into an LLM-based RS decoder to predict answers for RS open-ended tasks. In addition, we design a two-stage tuning method to enhance instruction-following and multi-turn dialogue ability at different granularities. Experiments on 8 datasets for RS vision-language tasks demonstrate SkyEyeGPT’s superiority in image-level and region-level tasks, such as captioning and visual grounding. In particular, SkyEyeGPT exhibits encouraging results compared to GPT-4V in some qualitative tests. The online demo, code, and dataset will be released.

## 1 Introduction
With the rapid advancement of Large Language Models (LLMs), Vision-Language Models (VLMs) like Shikra[Chen et al., 2023b] and MiniGPT-v2[Chen et al., 2023a] have profoundly changed the landscape of Multi-modal Large Language Models (MLLMs). These models exhibit a remarkable ability to engage in fluent vision-language conversations with humans and have generated new state-of-the-art (SoTA) on multi-granularity vision-language tasks [Zhan et al., 2023b]. LLaVA[Liu et al., 2023] has achieved great success in constructing instruction-following data to fine-tune the model, bringing new possibilities to the field of MLLMs. Despite these strides, it is crucial to note that the triumph of generalized MLLMs has not seamlessly extended to remote sensing (RS) vision-language tasks due to inherent differences between the natural and remote sensing domains.

Recently, there has been significant attention on remote sensing vision-language tasks [Yuan et al., 2023a] of diverse granularity levels, including RS image captioning [Hoxha et al., 2023], RS visual question answering (VQA) [Yuan et al., 2023b], RS visual grounding [Zhan et al., 2023a], and UAV video captioning [Bashmal et al., 2023]. Although efforts have been made to explore large vision-language models for remote sensing, it remains an emerging field with many challenges. The pioneering work, RSGPT [Bashmal et al., 2023] is designed to address RS image captioning and VQA tasks using LLMs. However, it lacks the capability for multi-task conversation. RSGPT needs to train task-specific models on different datasets to solve tasks independently, which greatly limits its open-ended task capability.

Toward an open generalist framework that seamlessly combines the advantages of LLMs with remote sensing tasks, we introduce SkyEyeGPT, a unified model capable of handling open-ended RS vision-language tasks. The input and output of each task are represented in natural language, including bounding box coordinates. The SkyEyeGPT’s architecture consists of a visual encoder, an alignment layer, and an LLM-based decoder for RS open-ended tasks. We do not design any extra encoder or external plugin modules, making SkyEyeGPT a unified and efficient model, and also simple to train and deploy. Recent studies [Liu et al., 2023] have demonstrated the impressive results achieved by training MLLMs via instruction tuning, to connect LLMs and vision. Instruction tuning in the multi-modal domain of remote sensing is still underexplored. The challenge is the lack of large-scale RS multimodal instruction-following data.

To foster the research of RS VLMs, we meticulously curate an RS vision-language instruction-following dataset with 968k training samples, namely SkyEye-968k. Our instructions consist of the reorganization of public data and a few generated data. To guarantee correctness, data is manually verified and selected by our team members. We involve humans in the loop to ensure the high quality of the conversation instruction. The SkyEye-968k is divided into single-task image-text instruction and multi-task conversation instruction. We set task-specific identifiers for different tasks to improve the ability of SkyEyeGPT on various specific tasks. To further explore multi-turn multi-task dialogue capabilities, we design a two-stage tuning that utilizes single-task and multi-task conversation instructions in two stages, respectively.

Experiments on 8 remote sensing vision-language datasets demonstrate SkyEyeGPT’s superiority. To further investigate whether SkyEyeGPT possesses good instruction-following ability, we compare it with MiniGPT-4, Shikra, MiniGPT-v2, and GPT-4V. We provide several real conversations with users and comparisons. Surprisingly, SkyEyeGPT, trained on our SkyEye-968k, shows results comparable to or even better than GPT-4V and can provide a more comprehensive and detailed understanding of remote sensing images. To demonstrate the effectiveness of the simple SkyEyeGPT structure, we conduct extensive and adequate ablation studies.

Our contributions can be summarized as follows:
- Unified RS vision-language instruction dataset, SkyEye-968k. One challenge is the lack of instruction data for RS multi-modal large language model. We create high-quality instruction-following data, including single-task and multi-task conversation instruction.
- RS multi-modal large language model. We develop SkyEyeGPT, which unifies RS vision-language tasks and breaks new ground in enabling the unified modeling of RS vision and LLM.
- Superior performance. SkyEyeGPT achieves competitive performance on the image-level and region-level RS vision-language tasks. Specially, it has shown encouraging results in some tests, compared with GPT-4V.
- Open source SkyEyeGPT for real-world applications. We release the following assets to the public community for applications in real-world scenarios: an online RS multi-modal chatbot, the model checkpoint, the instruction-following dataset, and the codebase.

## 2 Related Work

### 2.1 Remote Sensing Vision-Language Tasks
Recently, there has been significant attention on multi-modal tasks in remote sensing vision-language understanding [Yuan et al., 2023a]. Traditional image-level tasks, such as RS image captioning and RS VQA, have made significant progress [Yuan et al., 2023b]. Emerging region-level and spatio-temporal tasks, such as RSVG [Zhan et al., 2023a] and UAV video captioning [Bashmal et al., 2023], have raised novel challenges and garnered increasing interest. Despite the availability of numerous state-of-the-art methods capable of performing these tasks [Xiong et al., 2022], they are typically trained on a specific dataset to perform a specific task. This work primarily focuses on unifying the diverse RS vision-language tasks.

### 2.2 LLMs for Vision-Language
With the rise of advanced LLMs, ChatGPT [OpenAI, 2022], LLaMA [Touvron et al., 2023a], GPT-4 [OpenAI, 2023], and Vicuna [Chiang et al., 2023] have shown remarkable abilities in various language tasks. BLIP-2 [Li et al., 2023] extends LLMs into the realm of multimodal by connecting the frozen LLM with a visual encoder via Q-Former. Some approaches employ the simplest linear layer as a mediator to link LLMs and visual encoders, achieving notable success, such as LLaVA [Liu et al., 2023], MiniGPT-4 [Zhu et al., 2023]. Recent contributions from VisionLLM [Wang et al., 2023], Shikra [Chen et al., 2023b], and MiniGPT-v2 [Chen et al., 2023a] further substantiate that spatial coordinates in visual grounding tasks can be effectively handled in language form by LLM. These approaches showcase the potential and versatility of LLM for seamless integration of vision and language modalities. The application and research on generalized MLLMs in RS have been comparatively limited. RS-GPT [Hu et al., 2023] was the first attempt, but it could only handle coarse-grained tasks of image-text and doesn’t support open-ended multi-tasks and multi-task conversations.

### 2.3 Vision-Language Instruction Tuning
The purpose of instruction tuning is to enhance the instruction following ability of the model. Drawing inspiration from LLMs in instruction tuning, LLaVA [Liu et al., 2023] fine-tunes the model based on synthetic multi-modal instruction-following data. Instruct-BLIP [Dai et al., 2023] collects a larger set of instruction data, resulting in improved performance for BLIP. These methods primarily focus on image-level coarse-grained tasks, and cannot effectively address fine-grained perception challenges. Recent VisionLLM [Wang et al., 2023], Shikra [Chen et al., 2023b], and MiniGPT-v2 [Chen et al., 2023a] further utilize instruction-following data to tackle fine-grained visual perception tasks such as visual grounding, region caption, and object detection. These methods demonstrate the potential of instruction tuning strategies to mine the LLM’s ability to understand and respond to multi-grained multi-modal instructions. Our method aims to provide a unified framework for handling open-ended RS vision-language tasks and develop multi-task conversational capability via instruction tuning.

## 3 Method of SkyEyeGPT

### 3.1 Overall Architecture
SkyEyeGPT consists of a visual encoder, an alignment layer, and an LLM-based decoder for RS open-ended tasks. More detailed comparisons of existing MLLMs are presented in the supplementary materials.

Visual Encoder. The pre-trained vision transformer, EVA-CLIP [Fang et al., 2023], is employed as the visual encoder. The parameters are frozen during our training. Given an input RS image I ∈ R^{H×W×3}, H and W represent the height and width, respectively. Initially, the resolution of remote sensing images is standardized to 448×448. Subsequently, we apply the EVA model to segment the image into patches and extract image embeddings Zv ∈ R^{N×D} from these patches, where N is the number of patches and D is the hidden dimension. The UAV video features are formed by the concatenation of features from multiple frame images.

Alignment Layer. We consider a linear layer to bridge the modality gap, aligning RS visual features from the visual encoder with the language features from advanced LLM. The input resolution is crucial for accurately understanding detailed RS image-text representations. However, the high resolution of 448 × 448 will generate an excessive number of patches N, which reduces the efficiency of processing contextual input in the LLM and is highly resource-demanding. Therefore, we opt not to directly project the RS image embeddings into the linear layer. A simple yet effective method [Chen et al., 2023a] is adopted to directly concatenate four adjacent visual tokens to reduce the number of patches by four times. The linear layer converts the visual tokens Z'_v ∈ R^{N/4 × (4×D)} into embeddings Fv ∈ R^{N/4 × d} in the language space, where d is the hidden dimension size of LLM.

LLM-based Decoder for RS Open-Ended Tasks. We choose open-sourced LLaMA2-chat [Touvron et al., 2023b] as our language model, which is a decoder-only LLM. Our decoder takes a sequence of visual tokens Fv and language instructions as input, generating task-specific answers. We acknowledge the existence of more sophisticated (but expensive) methods for connecting remote sensing images and language, such as Q-former in BLIP-2 [Li et al., 2023], or other encoders like RemoteCLIP pre-trained on remote sensing data. We explore potentially more efficient or sophisticated architectures for SkyEyeGPT in ablation experiments.

### 3.2 Unified RS Vision-Language Instruction
While acquiring instruction fine-tuning datasets in the general domain is straightforward, there are no equivalent datasets in the remote sensing domain. To address this gap, a unified RS vision-language instruction data, SkyEye-968k, is carefully planned and specifically tailored for the RS vision-language large model. Our instruction data with 968k training samples consists of the reorganization of public data and a few generated data verified manually.

We ensure that no images from the validation or test sets appear in the instructions, thus eliminating the risk of data leakage. The SkyEye-968k dataset is divided into two parts:

Single-task Image-text Instruction.
- Captioning task. Specifically, we integrate five RS image captioning datasets and one UAV video captioning dataset.
- VQA task. We integrate three public RS VQA datasets. The ERA-VQA dataset is generated based on the event recognition in aerial videos (ERA) dataset [Mou et al., 2020]. Take frame images and questions about the event theme as input.
- Grounding task. We integrate two public RS visual grounding datasets. Following the method of generating object parsing and grounding instructions [Chen et al., 2023a], we created an RS phrase grounding dataset (RSPG). With RS images and phrases as input, the output target bounding box can be either single or multiple.

Multi-task Conversation Instruction. Single-task instruction focuses only on high-quality aligned image-text data to improve SkyEyeGPT’s performance on each specific task. After the first stage of tuning, when engaging in multiple rounds of conversations with the user, the model may struggle to handle subsequent tasks effectively as the context becomes more complex. To transform SkyEyeGPT into a proficient chatbot, we must focus on how to enhance its multi-task conversation capabilities, ensuring a good and seamless user experience. To tackle this challenge, we create the RS multi-task conversation instruction by mixing or reorganizing datasets from different tasks.

Specifically, we mix the corresponding captioning and VQA datasets to get UCM-Conversa and Sydney-Conversa instruction. Using the DIOR-RSVG dataset and DIOR dataset [Li et al., 2020], we construct DIOR-Conversa instruction which contains visual grounding, phrase grounding, and referring expression generation tasks. Similarly, we leverage RSIVQA and the DOTA object detection dataset [Xia et al., 2018] to build a conversation instruction, DOTA-Conversa, that includes VQA and phrase grounding tasks. To guarantee correctness, data is manually verified and selected by our team members. We involve humans in the loop to ensure the high quality of the instruction.

Conversation input template of SkyEyeGPT includes task-specific identifiers, such as “[caption], [vqa], [refer]”. This design achieves the unification of RS vision-language tasks while allowing the model to flexibly produce task-specific outputs. The answer or response, i.e., the model output, follows after the [/INST]. The input or output of region-level tasks requires bounding boxes of objects. We represent the coordinates in the natural language form {<x1><y1><x2><y2>}. Specifically, (x1, y1) and (x2, y2) denote the coordinates of the top-left and bottom-right corners of the box, respectively. The coordinate values are normalized, multiplied by 100, and rounded to integers.

### 3.3 Instruction Tuning
The model is trained to follow a series of task-specific instructions on the RS multi-modal instruction-following data. To achieve an effective SkyEyeGPT, we design a two-stage instruction tuning approach.

Input and Output Template. We build a variety of task inputs, following the conversation input template. We introduce task-specific identifiers, such as “[caption], [vqa], [refer]”. This design achieves the unification of RS vision-language tasks while allowing the model to flexibly produce task-specific outputs. The answer or response follows after the [/INST]. The input or output of region-level tasks requires bounding boxes of objects. We represent the coordinates in the natural language form {<x1><y1><x2><y2>}. The coordinate values are normalized, multiplied by 100, and rounded to integers.

Stage 1: Remote Sensing Image-Text Alignment. This stage trains the model using the single-task image-text instruction. This helps SkyEyeGPT build remote sensing fine-grained knowledge of multi-tasking. Treat each sample as a single-round conversation Xc = (Xinstruct, Xa). For the given RS image features Fv, connect it with the instruction tokens Xinstruct from the text modality. This concatenated input is then fed into the LLM. SkyEyeGPT generates the answer Xa with a length of L. Maximizing the likelihood function that is defined as follows:

L = log P(Xa | Fv, Xinstruct; θ) = ∑_{i=1}^{L} log P(xi | Fv, Xinstruct, Xa,<i; θ),

where P and θ are the conditional probability and the trainable parameters, and Xa,<i is the answer tokens preceding the current prediction tokens xi.

Stage 2: Multi-task Conversation Fine-tuning. This stage uses the multi-task conversation instruction to better answer questions for multiple rounds and multiple tasks, enabling SkyEyeGPT to generate more natural and convincing outputs in multi-task conversations. The multi-task conversation is represented as a list Xc = (X^1_instruct, X^1_a, ..., X^n_instruct, X^n_a), where X^n_instruct is the instruction for n-th turn. Similarly, the objective function is as follows:

L = log P(Xa | Fv, Xinstruct; θ) = ∑_{i=1}^{L} log P(xi | Fv, Xinstruct,<i, Xa,<i; θ),

where Xinstruct,<i is the instruction tokens in all turns before the current prediction tokens xi. Therefore, the instructions and answers from previous rounds serve as references for the current task’s response.

In the above two stages, we employ the Low-Rank Adaptation (LoRA) method to fine-tune the alignment layer and LLM. This approach can fine-tune the model with limited resources and promote alignment between the two modalities of remote sensing vision and language.

## 4 Experiments

### 4.1 Experimental Details
The parameters of the linear layer and LLM are initialized from MiniGPT-v2’s checkpoint [Chen et al., 2023a]. In the first stage, we finetune our SkyEyeGPT end-to-end for 35 epochs on the single-task image-text instruction. In the second stage, we add the multi-task conversation instruction and reduced the sampling ratio of single-task instruction to train 5 epochs of SkyEyeGPT. Our setting is the same for the two training stages. The AdamW is used as the optimizer. We set the batch size to 1 with 10^{-5} learning rate and a cosine learning rate scheduler. To control overfitting, we apply a weight decay of 0.05. The rank in LoRA is 64. All training is conducted on four NVIDIA 3090 GPUs.

### 4.2 Remote Sensing Multi-modal Chatbot
We have developed a demonstration of a remote sensing multi-modal chatbot, showcasing the vision-language understanding and conversational capabilities of SkyEyeGPT. We also provide several real conversations with users or comparisons. Surprisingly, SkyEyeGPT, trained on our RS instruction-following dataset, demonstrates results comparable to or even better than GPT-4V.

### 4.3 Main Results
We conduct experiments on four representative tasks: RS image captioning, UAV video captioning, RS visual question answering, and RS visual grounding. Specialist models are designed for specific tasks, and we only report a few latest SoTA methods. Generalist models can perform various vision-language tasks.

RS Captioning: Image caption is crucial for assessing the quality of the alignment between RS vision and language. For this task, the model generates a description based on the user-input RS image and instruction. We achieved the best performance in most of the metrics except for CIDEr on the UCM-caption and achieved SoTA results on the CapERA dataset for aerial video captioning. MiniGPT-4 generates longer captions with rich details, so it is difficult for existing captioning evaluation metrics to provide accurate evaluation, especially CIDEr. RSGPT achieves high CIDEr scores due to its fine-tuning on each dataset to produce results with similar lengths. We employ a novel ChatGPT-based evaluation method in the supplementary material.

RS Visual Grounding: The model receives an RS image and a referring expression, then outputs the bounding box referring to the target object. The RSVG dataset requires the model to have a stronger numerical geospatial relations understanding. Our testing has shown that Shikra has poor robustness, performing poorly outside of the training set domain, and cannot be used on the more challenging RSVG dataset. SkyEyeGPT outperforms the SoTA specialist models by about 10%. SkyEyeGPT has good robustness and precise localization ability for small objects.

RS VQA: Our average accuracy is lower than RSGPT, which separately fine-tuned on the RSVQA dataset. Moreover, the images in RSVQA belong to satellite imagery in our SkyEye-968k, while other RS images belong to aerial images. The image modality difference of RSVQA leads to performance loss. Addressing the modality difference problem of RS images from different sources is a key focus of our future work.

### 4.4 Ablation Studies
In this section, we analyze the impact of key components and hyperparameters of SkyEyeGPT in detail. To explore the impact of SkyEyeGPT’s multi-task learning and two-stage instruction tuning approach, we develop two variants (SkyEyeGPT_single and SkyEyeGPT_one-stage) to compare single-task tuning and one-stage instruction tuning. single indicates that the model is trained separately on each task and lacks open-ended task ability. Except for the VQA task where the model without open-ended task ability is significantly better than SkyEyeGPT, their performance is comparable in other tasks, indicating a balance between SkyEyeGPT’s accuracy and generalization. The model trained with the multi-task conversation instruction in the second stage can significantly improve performance on various tasks.

To demonstrate that our alignment layer is sufficient to align RS visual and textual features, we design three variants: (a) w/o Linear Layer, (b) + Multiple Linear Layers, and (c) + Q-Former. We also compare the impact with and without task identifiers. We set different ranks in LoRA to explore the impact on the results. Detailed ablation experiments show the design choices and hyperparameters influence, with LoRA rank = 64 giving strong performance.

## 5 Conclusion
In this work, we introduce SkyEyeGPT, a unified open MLLM tailored specifically for remote sensing. We construct an RS multi-modal instruction-following dataset, including single-task and multi-task conversation instruction. We design a two-stage tuning method to develop the model’s multi-task and multi-round conversational ability. Task-specific identifiers are set to facilitate a unified treatment of open-ended tasks. The effectiveness and superiority of SkyEyeGPT are validated on a range of different granularity tasks. SkyEyeGPT achieves the new SoTA accuracy on many tasks and provides an exceptional remote sensing multi-modal chatting experience. This work represents a significant advancement in the remote sensing multi-modal domain, offering a versatile and high-performing solution for open-ended tasks in a unified framework via LLM.