GeoGround: A Unified Large Vision-Language Model
for Remote Sensing Visual Grounding
Yue Zhou1 Mengcheng Lan1 Xiang Li2 Litong Feng5 Yiping Ke1
Xue Jiang3 Qingyun Li4 Xue Yang3 Wayne Zhang5
1Nanyang Technological University,2University of Reading
3Shanghai Jiao Tong University,4Harbin Institute of Technology
5SenseTime Research
Abstract
Remote sensing (RS) visual grounding aims to use natu-
ral language expression to locate specific objects (in the
form of the bounding box or segmentation mask) in RS im-
ages, enhancing human interaction with intelligent RS in-
terpretation systems. Early research in this area was pri-
marily based on horizontal bounding boxes (HBBs), but
as more diverse RS datasets have become available, tasks
involving oriented bounding boxes (OBBs) and segmenta-
tion masks have emerged. In practical applications, dif-
ferent targets require different grounding types: HBB can
localize an object‚Äôs position, OBB provides its orienta-
tion, and mask depicts its shape. However, existing spe-
cialized methods are typically tailored to a single type
of RS visual grounding task and are hard to generalize
across tasks. In contrast, large vision-language models
(VLMs) exhibit powerful multi-task learning capabilities
but struggle to handle dense prediction tasks like segmen-
tation. This paper proposes GeoGround, a novel frame-
work that unifies support for HBB, OBB, and mask RS
visual grounding tasks, allowing flexible output selection.
Rather than customizing the architecture of VLM, our work
aims to elegantly support pixel-level visual grounding out-
put through the Text-Mask technique. We define prompt-
assisted and geometry-guided learning to enhance consis-
tency across different signals. Experimental results show
that GeoGround demonstrates strong performance across
four RS visual grounding tasks, matching the performance
of specialized methods on multiple benchmarks. Code
available at https://github.com/zytx121/GeoGround.
1. Introduction
In the remote sensing (RS) community, the early visual
grounding task [27, 38] specifically refers to the location
of specific objects in terms of horizontal bounding boxes
HBB Signal
OBB Signal
Mask Signal
Text-HBB
Text-OBB
Text-Mask
‚Äú[ùë•ùëê, ùë¶ùëê, ùë§, ‚Ñé, ùúÉ]‚Äù
‚Äú[‚Ä¶ ; ‚Ä¶ 0,1,1,0, ‚Ä¶ ; ‚Ä¶ ]‚Äù
‚Äú[ùë•1, ùë¶1, ùë•2, ùë¶2]‚Äù
Multiple Signals Prompt-Assisted Geometry-Guided
(5 parameters)
(N √ó N parameters)
(4 parameters)
Figure 1. An overview of the hybrid supervision of GeoGround.
(HBBs), given a satellite image and related text query.
With increasing availability of the RS dataset [12, 26, 35],
researchers have started to use oriented bounding boxes
(OBBs) [7] or segmentation masks [37] to more accurately
depict the referred objects. RS visual grounding enables hu-
mans to interact with computers in a more intuitive manner,
which has enormous promise for improving the efficiency
of intelligent RS interpretation systems [30].
Most existing RS visual grounding [27, 38] and refer-
ring segmentation [17, 37] methods are designed with task-
specific modules and loss functions. Models based on HBB
typically adopt loss functions in object detection tasks, such
as Smooth L1, while models based on mask often use loss
functions from semantic segmentation tasks, such as pixel-
level cross-entropy. Enabling multi-task learning [28] in
such models not only requires modifications to the net-
work but also necessitates careful tuning of the weight-
ings between various loss functions, making the process
quite challenging. Although large vision-language models
(VLMs) [1, 2, 5, 16] can support multiple multimodal RS
tasks simultaneously by using a unified text regression loss
1
arXiv:2411.11904v3  [cs.CV]  10 May 2025
function, they struggle with pixel-level tasks such as seg-
mentation. This is because, as the output module of a VLM,
the large language model (LLM) can only generate textual
data and cannot produce output in the image modality [4].
To address these challenges, we propose GeoGround, an
elegant VLM that seamlessly unifies visual grounding tasks
at the HBB, OBB, and pixel-level RS. Our key innovation
lies in converting box-level and pixel-level signals into tex-
tual sequences, enabling the model to train diverse visual
grounding tasks within a unified training pipeline. Specif-
ically, we propose the Text-Mask paradigm, which distills
and compresses the information embedded in the mask into
a compact text sequence that can be efficiently learned by
VLMs. Additionally, we introduce hybrid supervision, as
shown in Fig. 1, which incorporates prompt-assisted learn-
ing (PAL) and geometry-guided learning (GGL) to fine-tune
the model using three types of signals, ensuring output con-
sistency and enhancing the model‚Äôs understanding of the re-
lationships between different grounding types.
To support GeoGround training and promote the de-
velopment of visual RS grounding, we introduce refGeo,
a large-scale RS visual grounding instruction-following
dataset. refGeo consolidates four existing visual ground-
ing datasets from RS [7, 14, 27, 38] and introduces a new
aerial vehicle visual grounding dataset.
In summary, our key contributions are as follows:
‚Ä¢ We propose GeoGround, a novel VLM framework that
unifies box-level and pixel-level RS visual grounding
tasks while maintaining its inherent dialogue and image
understanding capabilities.
‚Ä¢ We introduce refGeo, the largest RS visual grounding
instruction-following dataset, consisting of 161k image-
text pairs and 80k RS images.
‚Ä¢ We conduct extensive experiments on various RS visual
grounding tasks, providing valuable insights for future RS
VLM research and opening new avenues for research in
RS visual grounding.
2. Related Work
Remote Sensing Referring Detection and Segmenta-
tion. Compared to multimodal tasks like image caption-
ing [15, 41], text-image retrieval [20], and visual question
answering (VQA) [18] in RS, research on referring detec-
tion is a novel task with limited research. It was first intro-
duced by GeoVG [37], which proposed the first RS visual
grounding dataset. MGVLF [38] leverages multiscale vi-
sual features and multi-granularity textual embeddings to
address the scale variation in RS images. LQVG [10] pro-
pose a language query-based Transformer framework for
RSVG. LPV A [13] achieves precise attention on referred
objects by adjusting visual features with progressive atten-
tion. RS referring segmentation is also in its early stages
due to the challenges mentioned earlier. It was first intro-
duced by RefSegRS [37], which proposed a new dataset and
baseline model. Recently, the transformer-based method
RMSIN [17] proposed an adaptive rotated convolution to
tackle the issues arising from the scale variation and orien-
tations prevalent in aerial imagery. FIANet [11] proposed
a fine-grained image-text alignment module to better dis-
criminative multimodal representation. However, these two
types of models have always been studied separately, hin-
dering progress in the field. In this paper, we unify these
two tasks within a single framework, allowing them to share
data and architecture.
Generalist VLMs. Several efforts have been made to
equip VLMs with visual grounding capabilities in the do-
main of natural images [3, 4, 8, 23, 31, 34, 39]. For in-
stance, Shikra [3] directly textualizes HBB to support vi-
sual grounding tasks, but its discrete coordinate output is
inadequate for pixel-level tasks. LISA [8] addresses this by
incorporating a mask decoder to handle the RES task, while
NExT-Chat [39] expands this paradigm by adding two de-
coders to support both box and mask outputs. In contrast,
our approach elegantly unifies box-level and pixel-level vi-
sual grounding tasks based on general-purpose VLM, elim-
inating the need for additional encoders or decoders.
Remote Sensing VLMs have shown promising results in
image-level tasks such as scene classification, image cap-
tioning, and VQA [14, 32, 42]. However, works [7, 21,
22, 40] on object-level tasks such as RS visual ground-
ing remains comparatively unexplored. GeoChat [7] con-
structs a new dataset using OBB annotation and proposes
the first RS visual grounding model based on OBB. How-
ever, the unsuitable choice of OBB angle representation
limits its performance. Moreover, the small scale of the RS
visual grounding dataset selected by LHRS-Bot [21] and
H2RSVLM [22] hampers their generalization ability on this
task. To address these issues, we introduce refGeo, a large-
scale RS visual grounding dataset with multi-type annota-
tions. For each type of annotation, we perform a systematic
exploration to identify the most suitable format.
3. GeoGround
The architecture of GeoGround is highly streamlined, con-
sisting of only a visual encoder (CLIP-ViT [24]), a con-
nector (two-layer MLPs), and an LLM (Vicuna 1.5 [43]),
without introducing additional encoders or decoders. Fig. 2
illustrates the framework of our proposed model, which can
flexibly output HBBs, OBBs, or segmentation masks based
on user instructions. In addition to single-object outputs,
the model is also capable of handling multi-object outputs.
2
LLM
Connector
Vision
Encoder
Output the bounding box of <ref> 
the vehicle on the right</ref>
Give me the oriented bounding 
box of <ref>the vehicle on the 
left</ref>
Can you segment the <ref> 
airplane</ref> in the given image?
Query
Response
Output the bounding box of all 
<ref>vehicles</ref>
<box>[[906,47,1000,141]]</box>
<obb>[[20,19,2,10,52]]</obb>
<seg> 0, 0, 0, ‚Ä¶, 0, 0, 0;
‚Ä¶0, 0, 1,1, 0, 0, ‚Ä¶</seg>
<box>[[155,143,250,238], [906,47,1000,141]]</box>
Detection:
REC with HBB :
REC with OBB:
RES:
Figure 2. An overview of GeoGround ‚Äì the first model to unify box-level and pixel-level visual grounding tasks in remote sensing.
3.1. Signal Textualization
To train three types of visual grounding tasks with a unified
data pipeline, we textualize the three grounding supervision
signals into three corresponding text strings. We refer to this
process as signal textualization, which serves as the corner-
stone of our method.
Text-HBB and Text-OBB are generated by directly con-
verting numerical coordinates into text sequences [3].
Specifically, the coordinates are normalized, multiplied by
the resolution, and then rounded. The resulting numbers are
separated by commas and enclosed in parentheses, as illus-
trated in Fig. 2. In GeoGround, we set the resolution of the
Text-HBB to 1000, allowing for more precise localization
of small objects in RS images. Compared to Text-HBB,
Text-OBB includes an additional angle parameter. Since
there are various angle representations of OBB, the mean-
ing of the first four numbers differs. Based on experiments,
we adopt the long side 90-degree representation [44] in Ge-
oGround, where the angle ranges from 0 to 90 degrees. To
ensure that these values align with the angle in terms of
range, we set the resolution of Text-OBB to 100 by default.
Text-Mask should be generated by converting the mask
into text sequences. However, this conversion is challenging
due to the inherent differences between the image and text
modalities. Inspired by Text4Seg [9], we propose a novel
Text-Mask paradigm that treats the segmentation mask as
text. Specifically, we downsample the mask into an N √óN
grid, where the object region is labeled 1 and the back-
ground region 0, as shown in Fig. 2. This results in a bi-
nary matrix that approximately represents the object‚Äôs loca-
tion and shape. Higher resolution improves shape precision
but results in longer text sequences, increasing training dif-
ficulty and slowing inference. To further reduce the token
length required to represent a mask, we employ R-RLE [9]
to compress redundant text sequences. It significantly re-
duces the length of Text-Mask and accelerates inference
speed, without compromising performance. For RS visual
grounding datasets, a resolution of 32 enables Text-Mask to
effectively represent most objects.
3.2. Hybrid Supervision
We propose a hybrid supervision that simultaneously uti-
lizes Text-HBB, Text-OBB, and Text-Mask to comprehen-
sively enhance the visual grounding capabilities of VLMs.
First, we adopt a basic supervised learning paradigm to train
three types of visual grounding tasks, as follows:
t = F (I, q) (1)
where F denotes the LLM of our model, I represents the
image embedding, q represents the query text embedding.
t can represent the Text-HBB, Text-OBB, and Text-Mask,
respectively. Next, we define the following two auxiliary
tasks to establish connections between the different signals.
Prompt-Assisted Learning refers to completing visual
grounding with the help of additional prompts, such as pre-
dicting the OBB of an object based on its known HBB. This
process can be understood as an increase in information
3
Prompt-Assisted Learning:   
Geometry-Guided Learning:
+ Query +        ->
Query +        ->
Sparse Signal:               Dense Signal:
Figure 3. Illustrations of PAL and GGL. The concepts of sparse
and dense signals are relative. For example, the sparse signal cor-
responds to HBB, while the dense signal corresponds to OBB.
entropy and aims to help the model acquire the ability to
generate dense signals from sparse ones. Since dense sig-
nals contain more information than sparse signals, this pro-
cess still requires the model to extract additional informa-
tion from the image to bridge the gap between the signals.
PAL can be expressed by the following equation:
tdense = F (I, {q, tsparse}) (2)
where tsparse represents the sparse textualized signal,
which can be either Text-HBB or Text-OBB here. tdense
represents the textualized signal that is denser than tsparse.
Geometry-Guided Learning converts dense signals into
sparse ones guided by geometric knowledge, reducing in-
formation entropy. This means that GGL does not re-
quire the image as input; the transformation process can
be achieved solely based on geometric knowledge. For ex-
ample, the HBB that encloses an OBB can be obtained by
calculating its four corner points‚Äô maximum and minimum
values. GGL can be expressed as:
tsparse = F ({q, tdense}) (3)
where tdense denotes the dense textualized signal, which
can be either Text-OBB or Text-Mask. Fig. 3 presents a
demo of PAL and GGL. Similarly to existing VLMs, Ge-
oGround is supervised solely by the text regression loss.
BBox Consistency Score. Ideally, the model outputs for
the same object should have a similar enclosing bounding
box. However, the positions of the HBB, OBB, and mask
outputs may differ. To assess prediction consistency, we
propose the BBox Consistency Score (BCS):
BCS = 1
3

IoU(shbb, fobb2hbb(sobb))
+ IoU(shbb, fmask2hbb(smask))
+ IoU(fobb2hbb(sobb), fmask2hbb(smask))

(4)
where shbb, sobb, and smask represent HBB, OBB, and
mask signals, respectively. IoU denotes the Intersection
over Union. fobb2hbb and fmask2hbb represent the functions
Dataset Format # Refers GSD (m) Image Width
RSVG [27] HBB 5,505 0.24 ‚àº4.88 1,024
DIOR-RSVG [38] HBB 27,133 0.5 ‚àº30 800
GeoChat [7] OBB 63,883 0.3 ‚àº0.8 600 ‚àº1,024
VRSBench [14] OBB 38,689 0.1 ‚àº30 512
Aerial vehicle dataset (Ours) OBB 26,465 0.007‚àº0.04 4,000
refGeo Mixed161,6750.007‚àº30 512‚àº4,000
Table 1. List of datasets used to create our referring expression
instruction set for GeoGround VLM training. To ensure diver-
sity, we include RS visual grounding datasets with varying ground
sampling distance (GSD) and sizes.
that compute the enclosing HBB from the OBB and mask,
respectively. The BCS ranges from 0 to 1. When the model
predictions are completely consistent, the BCS equals 1.
3.3. Dataset
To address the issue of limited generalization capability in
VLMs caused by the relatively small size of existing RS
visual grounding datasets, we present a large-scale RS re-
ferring expression comprehension dataset, refGeo. It in-
tegrates most existing RS visual grounding datasets. The
details of each dataset are provided in Tab. 1. Since both
GeoChat [7] and VRSBench [14] use DIOR [12] image
data, which overlap with DIOR-RSVG [38], we remove the
samples corresponding to images that appear in the DIOR-
RSVG test and validation sets from the GeoChat and VRS-
Bench training set to prevent data leakage. Moreover, we
propose a new aerial vehicle visual grounding dataset using
unmanned aerial vehicles.
4. Experiments
Our method is based on LLaV A-1.5-7B [16] with the input
image resolution fixed at 336√ó336. We utilize the AdamW
optimizer [19], starting with an initial learning rate of 2e-4,
followed by a linear decay schedule after a warm-up phase
with a 0.03 ratio. To reduce GPU memory consumption,
all models are fine-tuned using LoRA with a rank of 64, in
conjunction with ZeRO-2 stage memory optimization. All
models are trained on 8 NVIDIA V100 GPUs (32GB) with
a global batch size of 128 for 5 epochs. The inference batch
size is set to 1 for all experiments. 3 RS object detection
datasets [12, 26, 35] are used during the fine-tuning of Ge-
oGround to enhance its basic visual perception capabilities.
4.1. Referring Expression Comprehension (REC)
Settings. We follow standard evaluation protocols [14,
22] and assess the REC task using Acc@0.5 metric. Ex-
cept for H2RSVLM [22] and EarthGPT [40], whose met-
rics are cited in the original articles due to the lack of open
source code, the results for the other VLMs are obtained by
inference with the official model weights provided. For the
GeoChat [7], we convert its output OBBs to HBBs.
4
Model DIOR-RSVG-Test DIOR-RSVG-Val RSVG-Test RSVG-Val GeoChat* VRSBench* A VVG A VG
Specialized Models
MGVLF [38] 76.78 - - - - - -
GeoVG [27] - - 59.40 58.20 - - - -
Generalist VLMs
InternVL2-8B [5] 14.42 12.99 0.16 0.67 9.91 5.47 0.34 6.28
InternVL2-40B [5] 15.06 14.87 0.41 0.67 21.13 13.64 8.23 10.57
Qwen-VL [1] 32.22 32.01 2.04 4.66 35.36 31.07 0.31 19.66
Qwen2-VL [29] 44.25 43.32 20.13 19.15 30.92 32.88 17.73 29.77
Remote Sensing VLMs
GeoChat [7] 24.05 23.35 2.04 3.08 22.74 11.52 0.28 12.44
LHRS-Bot [21] 17.59 17.04 1.56 0.95 3.25 1.19 0.00 5.94
H2RSVLM [22] 48.04 - - - - - - -
EarthGPT [40] 76.65 - - - - - - -
Supervised Fine-Tuning on refGeo
Qwen-VL (sft) 58.76 58.65 10.59 12.99 41.75 47.38 9.53 34.24
GeoChat (sft) 61.96 60.27 14.67 16.32 56.99 51.36 11.52 39.01
LLaV A-1.5-7B (sft) 65.98 64.46 20.95 19.98 63.76 57.17 15.05 43.91
GeoGround (N=32) 76.42 76.18 26.57 26.73 68.65 65.35 21.34 51.61
GeoGround (N=16) 77.73 77.18 26.65 27.64 70.24 66.04 21.58 52.44
Improvement over Runner-up‚Üë 1.4% 19.7% 27.2% 38.3% 10.2% 15.5% 21.7% 19.4%
Table 2. Performance (Acc@0.5%) comparison on 7 benchmarks. ‚àó indicates that the test set has been modified. The last row indicates
the performance improvement of our method compared to the runner-up VLM, further demonstrating the superiority of the GeoGround.
Results. Tab. 2 compares the performance of GeoGround
with 2 specialized models and 8 mainstream VLMs on 7
REC benchmarks. GeoGround achieves the best perfor-
mance across all benchmarks, surpassing the specialized
model on the DIOR-RSVG test set. VLMs fine-tuned on
our refGeo dataset, such as Qwen-VL [1] and GeoChat [7],
showed significant improvements on the REC task, validat-
ing the effectiveness of the scaling law in the field of RS
visual grounding. Benefiting from the wide range of im-
age resolutions and GSD in refGeo, the fine-tuned model
showed significant performance improvements on datasets
with a high proportion of small objects, such as RSVG and
A VVG. GeoGround achieves the best performance when
the resolution of Text-Mask is set to 16. This could be due
to the increased difficulty of training at higher resolutions.
Although low resolution leads to coarse masks, they can be
seen as attention mechanisms that aid in localizing the ap-
proximate object area.
4.2. REC with OBB
Settings. Following GeoChat [7], we also use Acc@0.5
as metric, with the difference being that rotated IoU [36] is
used instead of normal IoU during the calculation.
Results. Tab. 3 compares the performance of GeoGround
with GeoChat and LLaV A-1.5 on three REC benchmarks
that provide OBB annotations. The results demonstrate Ge-
oGround‚Äôs dominance in RS visual grounding tasks based
Method GeoChat ‚àó VRSBench‚àó A VVG A VG
GeoChat 31.88 11.54 0.00 14.47
LLaV A-1.5-7B (sft) 51.47 43.71 12.56 35.91
GeoGround (N=16) 59.72 53.22 13.93 42.29
Table 3. Performance (Acc@0.5) comparison on 3 REC bench-
marks that provide OBB annotations.
on OBB, further validating the effectiveness of our hybrid
supervision approach. Due to the increased number of pa-
rameters to learn, this task is more challenging than stan-
dard REC, resulting in lower scores on the OBB task com-
pared to the HBB task, even on the same test set.
4.3. Referring Expression Segmentation (RES)
Settings. We utilize Acc@0.5 and Mean Intersection-
over-Union (mIoU) as evaluation metrics, similar to prior
studies [33, 37]. As GeoGround is currently the only RS
VLM that supports the RES task, we compare it with three
generalist VLMs that possess native segmentation capabili-
ties on the RES task in the RRSIS-D dataset [17].
Results. Tab. 4 demonstrates that GeoGround exhibits su-
perior performance in the pixel-level RS vision grounding
task. GeoGround (N=32) achieve better results than di-
rectly using HBB to prompt SAM even without relying on
SAM. Moreover, we attempt to use HBB and coarse mask
to prompt SAM [6], which allowed GeoGround to achieve
results that match the performance of the best RS referring
5
Method Acc@0.5 mIoU
Val Test Val Test
RSMIN [17] 74.66 74.26 65.10 64.20
LISA [8] 27.07 24.51 27.84 26.78
PixelLM [25] 33.46 28.81 33.89 31.65
NExT-Chat [39] 28.97 26.37 26.98 24.98
Qwen2-VL (HBB) + SAM 44.97 44.38 42.40 41.30
LLaV A (sft) + SAM 60.41 59.38 55.93 54.61
GeoGround (N=16) 42.93 40.57 42.24 41.05
GeoGround (N=32) 63.25 60.97 56.36 54.92
GeoGround (N=16) + SAM68.69 67.50 61.10 60.50
GeoGround (N=32) + SAM68.88 66.06 60.80 58.93
Table 4. Performance comparison of RES task on the RRSIS-D.
Options Acc@0.5 BCSHBB OBB ‚ÜíHBB Mask‚ÜíHBB
LLaV A-1.5-7B 79.84 - - -
+ Multiple Signals 87.56 80.76 61.40 0.62
+ PAL 86.76 80.82 63.24 0.63
+ GGL 87.87 81.83 64.95 0.64
Table 5. Ablation Study of Hybrid Supervision on RRSIS-D.
HBB‚ÜíOBB HBB‚ÜêOBB Acc@0.5 (HBB) Acc@0.5 (OBB) BCS
/times-circle /times-circle60.87 58.68 0.67
PAL /times-circle61.37 59.12 0.68
/times-circlePAL 60.83 58.27 0.68
/times-circleGGL 60.60 58.81 0.69
PAL PAL 61.18 59.53 0.69
PAL GGL 61.39 59.71 0.70
Table 6. Influence of signal consistency on DIOR-RSVG.
HBB OBB Mask Acc@0.5 (HBB) Acc@0.5 (OBB)
DIOR-RSVG RSVG VRSBench GeoChat
/check-circle/times-circle /times-circle65.98 20.95 41.45 42.95
/times-circle/check-circle/times-circle60.75 14.51 43.71 51.47
/check-circle /check-circle/times-circle68.49 22.66 48.01 55.64
/check-circle/check-circle/check-circle68.61 23.47 49.66 56.43
Table 7. Influence of multiple supervised signals on GeoGround.
segmentation model [17]. See appendix for more details.
4.4. Ablation Study
Effect of Hybrid Supervision. Tab. 5 presents the abla-
tion study of the components in our proposed hybrid su-
pervision method. To compute BCS, we first convert both
OBB and mask into HBB before calculating Acc@0.5.
The results confirm their effectiveness and further high-
light the importance of output consistency in improving per-
formance. Tab. 6 illustrates the impact of different learn-
ing strategies on the performance of VLM. All models are
trained on the training set and evaluated on the DIOR-
RSVG test set. The results show that PAL improves per-
formance when predicting dense signals from sparse ones,
while GM, which requires no visual input, yields better re-
sults when predicting sparse signals from dense ones. Tab. 7
Method DownSample Resolution Length Acc@0.5 (HBB)
Text4Seg [9] NEAREST 16 330 17.68
Text4Seg [9] MaxPooling 16 397 43.09
Text-Mask MaxPooling 16 157 50.73
Text-Mask MaxPooling 24 237 56.10
Text-Mask MaxPooling 32 316 57.50
Table 8. Discrepancies between Text-Mask and Text4Seg. Length
represents the average length of the model outputs.
further explores the effect of multiple signals on model per-
formance. The results show that direct training with three
signals can enhance the visual grounding capability of the
VLM on HBB and OBB tasks.
Design Options of Text-Mask To our best knowledge,
Text4Seg [9] is the only work to attempt to treat masks as
text. However, with longer referring expressions, its seman-
tic descriptors become overly redundant. Tab. 8 compares
the performance of our proposed Text-Mask with Text4Seg
on the DIOR-RSVG test set. The HBB prediction is ob-
tained by calculating the bounding box from the boundary
of the segmentation mask. Experiments show that mapping
the mask to a binary matrix not only reduces the text length
of Text4Seg by 40% but also improves its performance by
18%. Since the objects in RS are relatively small, using
the nearest downsampling method leads to the loss of mask
information for small objects, resulting in significant per-
formance degradation. While increasing the mask quanti-
zation resolution can further improve the segmentation ac-
curacy, longer output text sequences increase the inference
time and training difficulty.
4.5. Visualization Examples
Qualitative comparisons between GeoGround and GeoChat
are presented in Fig. 4, which includes HBBs and OBBs. It
can be observed that GeoGround consistently demonstrates
superior localization accuracy, whether handling simple or
relatively complex referential expressions. Additionally, it
exhibits 3D spatial understanding, enabling it to infer 3D
distances from 2D images. Fig. 5 compares the perfor-
mance of GeoGround in the RES task under different reso-
lutions of Text-Mask. When the resolution is 32, although
the coarse mask edges still exhibit small jaggedness, the re-
sult is already very close to the ground truth. These results
fully validate the effectiveness of GeoGround in addressing
the pixel-level visual grounding task in RS.
5. Conclusion
Although remote sensing (RS) visual grounding tasks us-
ing horizontal bounding boxes, oriented bounding boxes,
and segmentation masks have progressed, no model has
6
GT GeoChatGeoGround
The airport is on the lower 
right of the bridge on the left.
A gray rectangular 
basketball court on the right.
 Silver a321 airplane
 Gray passenger-ship
 The car farthest from the camera.
Figure 4. Visualizations of GeoGround and GeoChat on the REC task with HBB and OBB.
GT 32 √ó 32
The ground track 
field in the middle.
The expressway service 
area in the middle.
The large gray 
chimney.
The gray dam in 
the middle.
A slender train 
station.
The small harbor.
16 √ó 16
The bridge.
Figure 5. Visualizations of GeoGround with different resolutions of Text-Mask on the RRSIS-D test set.
unified these tasks due to framework limitations. To ad-
dress this, we propose GeoGround, a novel framework that
unifies box-level and pixel-level visual grounding tasks in
a single model. Instead of adding extra encoders or de-
coders, GeoGround empowers large vision-language mod-
els (VLMs) to perform pixel-level visual grounding by
treating the segmentation mask as text using our Text-Mask
method. It does not compromise the model‚Äôs conversa-
tional abilities or its image-level understanding capabili-
ties. We also introduce a large-scale RS visual grounding
instruction-following dataset, refGeo, that offers a compre-
hensive benchmark for various visual grounding tasks of RS
and serves as a valuable corpus for RS VLMs. Our compre-
hensive benchmarks and ablation studies provide important
insights for the development of VLMs in the RS domain.
References
[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
7
Zhou. Qwen-vl: A frontier large vision-language model with
versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1,
5
[2] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun
Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi,
Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.
Minigpt-v2: large language model as a unified interface
for vision-language multi-task learning. arXiv preprint
arXiv:2310.09478, 2023. 1
[3] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,
Feng Zhu, and Rui Zhao. Shikra: Unleashing multi-
modal llm‚Äôs referential dialogue magic. arXiv preprint
arXiv:2306.15195, 2023. 2, 3
[4] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Ge-
offrey Hinton. Pix2seq: A language modeling framework for
object detection. arXiv preprint arXiv:2109.10852, 2021. 2
[5] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,
Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,
Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng
Dai. Internvl: Scaling up vision foundation models and
aligning for generic visual-linguistic tasks. arXiv preprint
arXiv:2312.14238, 2023. 1, 5
[6] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision, pages 4015‚Äì4026, 2023. 5
[7] Kartik Kuckreja, Muhammad Sohail Danish, Muzammal
Naseer, Abhijit Das, Salman Khan, and Fahad Shahbaz
Khan. Geochat: Grounded large vision-language model for
remote sensing. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 27831‚Äì
27840, 2024. 1, 2, 4, 5
[8] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui
Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation
via large language model. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 9579‚Äì9589, 2024. 2, 6
[9] Mengcheng Lan, Chaofeng Chen, Yue Zhou, Jiaxing Xu,
Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang.
Text4seg: Reimagining image segmentation as text genera-
tion. arXiv preprint arXiv:2410.09855, 2024. 3, 6
[10] Meng Lan, Fu Rong, Hongzan Jiao, Zhi Gao, and Lefei
Zhang. Language query-based transformer with multiscale
cross-modal alignment for visual grounding on remote sens-
ing images. IEEE Transactions on Geoscience and Remote
Sensing, 62:1‚Äì13, 2024. 2
[11] Sen Lei, Xinyu Xiao, Tianlin Zhang, Heng-Chao Li, Zhen-
wei Shi, and Qing Zhu. Exploring fine-grained image-text
alignment for referring remote sensing image segmentation.
IEEE Transactions on Geoscience and Remote Sensing , 63:
1‚Äì11, 2025. 2
[12] Ke Li, Gang Wan, Gong Cheng, Liqiu Meng, and Junwei
Han. Object detection in optical remote sensing images: A
survey and a new benchmark. ISPRS journal of photogram-
metry and remote sensing, 159:296‚Äì307, 2020. 1, 4
[13] Ke Li, Di Wang, Haojie Xu, Haodi Zhong, and Cong Wang.
Language-guided progressive attention for visual grounding
in remote sensing images. IEEE Transactions on Geoscience
and Remote Sensing, 62:1‚Äì13, 2024. 2
[14] Xiang Li, Jian Ding, and Mohamed Elhoseiny. Vrsbench: A
versatile vision-language benchmark dataset for remote sens-
ing image understanding. arXiv preprint arXiv:2406.12384,
2024. 2, 4
[15] Yunpeng Li, Xiangrong Zhang, Jing Gu, Chen Li, Xin Wang,
Xu Tang, and Licheng Jiao. Recurrent attention and seman-
tic gate for remote sensing image captioning. IEEE Trans-
actions on Geoscience and Remote Sensing , 60:1‚Äì16, 2021.
2
[16] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. arXiv preprint arXiv:2304.08485,
2023. 1, 4
[17] Sihan Liu, Yiwei Ma, Xiaoqing Zhang, Haowei Wang, Ji-
ayi Ji, Xiaoshuai Sun, and Rongrong Ji. Rotated multi-scale
interaction network for referring remote sensing image seg-
mentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 26658‚Äì
26668, 2024. 1, 2, 5, 6
[18] Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia.
Rsvqa: Visual question answering for remote sensing data.
IEEE Transactions on Geoscience and Remote Sensing , 58
(12):8555‚Äì8566, 2020. 2
[19] I Loshchilov. Decoupled weight decay regularization. arXiv
preprint arXiv:1711.05101, 2017. 4
[20] Georgii Mikriukov, Mahdyar Ravanbakhsh, and Beg ¬®um
Demir. Deep unsupervised contrastive hashing for large-
scale cross-modal text-image retrieval in remote sensing.
arXiv preprint arXiv:2201.08125, 2022. 2
[21] Dilxat Muhtar, Zhenshi Li, Feng Gu, Xueliang Zhang, and
Pengfeng Xiao. Lhrs-bot: Empowering remote sensing
with vgi-enhanced large multimodal language model. arXiv
preprint arXiv:2402.02544, 2024. 2, 5
[22] Chao Pang, Jiang Wu, Jiayu Li, Yi Liu, Jiaxing Sun, Wei-
jia Li, Xingxing Weng, Shuai Wang, Litong Feng, Gui-
Song Xia, et al. H2rsvlm: Towards helpful and honest re-
mote sensing large vision language model. arXiv preprint
arXiv:2403.20213, 2024. 2, 4, 5
[23] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan
Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-
ing multimodal large language models to the world. arXiv
preprint arXiv:2306.14824, 2023. 2
[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning, pages
8748‚Äì8763. PMLR, 2021. 2
[25] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao,
Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel
reasoning with large multimodal model. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 26374‚Äì26383, 2024. 6
[26] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping
Wang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng,
Tao Xu, et al. Fair1m: A benchmark dataset for fine-
8
grained object recognition in high-resolution remote sens-
ing imagery. ISPRS Journal of Photogrammetry and Remote
Sensing, 184:116‚Äì130, 2022. 1, 4
[27] Yuxi Sun, Shanshan Feng, Xutao Li, Yunming Ye, Jian
Kang, and Xu Huang. Visual grounding in remote sensing
images. In Proceedings of the 30th ACM International Con-
ference on Multimedia, pages 404‚Äì412, 2022. 1, 2, 4, 5
[28] Michele V olpi and Devis Tuia. Deep multi-task learning for
a geographically-regularized semantic segmentation of aerial
images. ISPRS journal of photogrammetry and remote sens-
ing, 144:48‚Äì60, 2018. 1
[29] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,
Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin
Ge, et al. Qwen2-vl: Enhancing vision-language model‚Äôs
perception of the world at any resolution. arXiv preprint
arXiv:2409.12191, 2024. 5
[30] Sheng Wang, Wei Han, Xiaohui Huang, Xiaohan Zhang,
Lizhe Wang, and Jun Li. Trustworthy remote sensing inter-
pretation: Concepts, technologies, and applications. ISPRS
Journal of Photogrammetry and Remote Sensing , 209:150‚Äì
172, 2024. 1
[31] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,
Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu
Qiao, et al. Visionllm: Large language model is also an open-
ended decoder for vision-centric tasks. Advances in Neural
Information Processing Systems, 36, 2024. 2
[32] Zhecheng Wang, Rajanie Prabha, Tianyuan Huang, Jiajun
Wu, and Ram Rajagopal. Skyscript: A large and seman-
tically diverse vision-language dataset for remote sensing.
In Proceedings of the AAAI Conference on Artificial Intel-
ligence, pages 5805‚Äì5813, 2024. 2
[33] Chenyun Wu, Zhe Lin, Scott Cohen, Trung Bui, and
Subhransu Maji. Phrasecut: Language-based image segmen-
tation in the wild. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
10216‚Äì10225, 2020. 5
[34] Jiannan Wu, Muyan Zhong, Sen Xing, Zeqiang Lai,
Zhaoyang Liu, Wenhai Wang, Zhe Chen, Xizhou Zhu, Lewei
Lu, Tong Lu, et al. Visionllm v2: An end-to-end general-
ist multimodal large language model for hundreds of vision-
language tasks. arXiv preprint arXiv:2406.08394, 2024. 2
[35] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Be-
longie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liang-
pei Zhang. Dota: A large-scale dataset for object detection
in aerial images. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 3974‚Äì3983,
2018. 1, 4
[36] Xue Yang, Gefan Zhang, Xiaojiang Yang, Yue Zhou, Wentao
Wang, Jin Tang, Tao He, and Junchi Yan. Detecting rotated
objects as gaussian distributions and its 3-d generalization.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 45(4):4335‚Äì4354, 2022. 5
[37] Zhenghang Yuan, Lichao Mou, Yuansheng Hua, and
Xiao Xiang Zhu. Rrsis: Referring remote sensing image seg-
mentation. IEEE Transactions on Geoscience and Remote
Sensing, 2024. 1, 2, 5
[38] Yang Zhan, Zhitong Xiong, and Yuan Yuan. Rsvg: Exploring
data and models for visual grounding on remote sensing data.
IEEE Transactions on Geoscience and Remote Sensing , 61:
1‚Äì13, 2023. 1, 2, 4, 5
[39] Ao Zhang, Liming Zhao, Chen-Wei Xie, Yun Zheng, Wei Ji,
and Tat-Seng Chua. Next-chat: An lmm for chat, detection
and segmentation. arXiv preprint arXiv:2311.04498, 2023.
2, 6
[40] Wei Zhang, Miaoxin Cai, Tong Zhang, Yin Zhuang, and
Xuerui Mao. Earthgpt: A universal multi-modal large lan-
guage model for multi-sensor image comprehension in re-
mote sensing domain. IEEE Transactions on Geoscience and
Remote Sensing, 2024. 2, 4, 5
[41] Zhengyuan Zhang, Wenkai Zhang, Menglong Yan, Xin Gao,
Kun Fu, and Xian Sun. Global visual feature and linguistic
state guided attention for remote sensing image captioning.
IEEE Transactions on Geoscience and Remote Sensing , 60:
1‚Äì16, 2021. 2
[42] Zilun Zhang, Tiancheng Zhao, Yulong Guo, and Jianwei Yin.
Rs5m and georsclip: A large scale vision-language dataset
and a large vision-language model for remote sensing. IEEE
Transactions on Geoscience and Remote Sensing, 2024. 2
[43] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with
mt-bench and chatbot arena.Advances in Neural Information
Processing Systems, 36:46595‚Äì46623, 2023. 2
[44] Yue Zhou, Xue Yang, Gefan Zhang, Jiabao Wang, Yanyi Liu,
Liping Hou, Xue Jiang, Xingzhao Liu, Junchi Yan, Chengqi
Lyu, et al. Mmrotate: A rotated object detection benchmark
using pytorch. In Proceedings of the 30th ACM International
Conference on Multimedia, pages 7331‚Äì7334, 2022. 3
9