# Vision Foundation Models in Remote Sensing: A Survey

## Abstract
Artificial Intelligence (AI) technologies have profoundly transformed the field of remote sensing, revolutionizing data collection, processing, and analysis. Traditionally reliant on manual interpretation and task-specific models, remote sensing research has been significantly enhanced by the advent of foundation models—large-scale, pre-trained AI models capable of performing a wide array of tasks with unprecedented accuracy and efficiency. This paper provides a comprehensive survey of foundation models in the remote sensing domain. We categorize these models based on their architectures, pre-training datasets, and methodologies. Through detailed performance comparisons, we highlight emerging trends and the significant advancements achieved by those foundation models. Additionally, we discuss technical challenges, practical implications, and future research directions, addressing the need for high-quality data, computational resources, and improved model generalization. Our research also finds that pre-training methods, particularly self-supervised learning techniques like contrastive learning and masked autoencoders, remarkably enhance the performance and robustness of foundation models. This survey aims to serve as a resource for researchers and practitioners by providing a panorama of advances and promising pathways for continued development and application of foundation models in remote sensing.

## I. INTRODUCTION
ARTIFICIAL Intelligence (AI) technologies have profoundly transformed the field of remote sensing, revolutionizing how data is collected, processed, and analyzed. Traditionally, remote sensing projects relied heavily on manual interpretation and task-specific models that required extensive labeled datasets and significant computational resources. However, the advent of AI and deep learning (DL) has ushered in a new era in which large-scale, pre-trained models, known as foundation models, are capable of performing a wide array of tasks with unprecedented accuracy and efficiency. These advancements have not only enhanced the potential applications of remote sensing but have also opened new avenues for its usage across various domains.

In recent years, numerous vision foundation models have emerged, demonstrating remarkable performance in handling diverse remote sensing tasks. These models have shown the potential to significantly improve performance on multiple downstream tasks such as scene classification, semantic segmentation, object detection, and more. By leveraging vast amounts of pre-training data and sophisticated architectures, these foundation models have set new benchmarks in the field, making them indispensable tools for researchers and engineers alike.

This paper aims to provide a comprehensive survey of vision foundation models in the remote sensing domain, and is limited to foundation models released between June 2021 and June 2024. This timeframe marks a surge in the development of modern foundation models, including vision transformers and advanced self-supervised learning techniques. Although early models like Tile2Vec [47] and others laid the groundwork for representation learning in remote sensing, they were typically limited in scale and generalization capabilities. Furthermore, numerous review papers have already provided comprehensive overviews of these pre-2021 models. Our review, therefore, focuses on recent developments to highlight the unique contributions and innovations that have emerged in the past few years.

To facilitate navigation and enhance utility for researchers, we categorized existing models based on their perception levels (e.g., image-level, region-level, pixel-level). This organization helps clarify which models have been tested for general image-based challenges or specialized applications such as environmental monitoring, land cover mapping, archaeological exploration, disaster management, and more. It is essential to distinguish between applications that models have been explicitly tested on and those for which they could potentially be effective. In this review, the fact that a model has not been tested on a particular application does not mean it won’t perform well. Foundation models, especially convolutional neural network (CNN) backbones like residual networks (ResNet) [36] and vision transformers (ViT) [25], may still be suitable for various downstream tasks, even if prior work has not yet demonstrated this.

Our contributions include:
1) An exhaustive review of current state of vision foundation models proposed in the field of remote sensing, starting from the background and methodologies of these models to specific applications across different domains and tasks in a hierarchical and structured manner.
2) Categorization and analysis of the models based on their application in both image analysis (table I) and practical applications (table VI-B). We discuss the architecture, pre-training datasets, pre-training methods, and performance of each model.
3) Discussion of challenges and unresolved aspects related to foundation models in remote sensing. We pinpoint new trends, raise important questions, and proposed future directions for further exploration.

## II. BACKGROUND

### A. Remote Sensing
Remote sensing (RS) refers to the process of acquiring information about objects or areas from a distance, typically using satellite or airborne sensors. These technologies and techniques serve vital roles in diverse fields, enabling the collection of data over geographic areas without physical contact. Applications of remote sensing include earth observation, digital archaeology, urban planning and development, and disaster management. The field of remote sensing has developed rapidly since the mid 20th century. Initially, remote sensing predominately consisted of analog photographic techniques via aerial and satellite platforms, which provided limited spectral and spatial resolution. The launch of early Earth observation satellites, such as Landsat program commenced in 1967 [112], marked a significant advancement, enabling consistent and wide-ranging data collection for environmental monitoring.

Modern remote sensing employs a variety of sensors suited for specific types of data collection, including optical, thermal, and radar. Optical sensors capture a wide variety of spectral bands, including visible and near-infrared light, allowing for detailed imaging of land cover and vegetation health. Thermal sensors detect heat emitted or reflected from the Earth’s surface, useful for monitoring volcanic activity, forest fires, and climate change monitoring. Radar sensors can penetrate clouds and vegetation, providing critical information in all-weather conditions and for applications such as soil moisture estimation and urban infrastructure mapping [17], [71].

In recent years, remote sensing has found applications in many fields. With regard to environmental monitoring, it is used to track deforestation, to monitor air and water quality, and to assess the impacts of climate change [30], [39]. In agriculture, remote sensing helps in crop health monitoring, yield estimation, and efficient resource management [71]. Urban planning and development benefit from remote sensing through the monitoring of urban sprawl, infrastructure development, and land-use planning [17], [48]. Furthermore, in disaster management, remote sensing is crucial for assessing the damage caused by natural disasters, aiding in the planning and execution of relief operations [1], [30].

The integration of remote sensing data with Geographic Information Systems (GIS) has further enhanced its utility. GIS provides a framework for capturing, storing, analyzing, and visualizing spatial and geographic data. When combined with remote sensing data, GIS can be used to create detailed and dynamic maps and models for various applications. This synergy is particularly valuable in resource management, urban planning, and disaster response, where accurate and timely information is critical [17], [30], [71].

### B. Foundation Models for Remote Sensing
Foundation models (FMs) refer to large-scale, pre-trained models that provide a robust starting point for various downstream tasks across different domains [50]. These models leverage extensive datasets and advanced architectures, enabling them to capture complex patterns and features that can be fine-tuned for specific applications with minimal additional training. In remote sensing, FMs are particularly valuable due to the diverse and complex nature of the data, including multi-spectral and multi-temporal imagery. Techniques such as self-supervised learning (SSL) [51] and transformers [93] have significantly enhanced the performance and efficiency of tasks such as image classification, object detection, and change detection, addressing the unique challenges posed by remote sensing data [19].

A major strength of these models lies in their ability to utilize SSL to learn effective representations from largely unlabeled data, which is often abundant in remote sensing scenarios [126]. By integrating advanced architectures like transformers [93], FMs in remote sensing can handle the unique characteristics of geospatial data, such as varying spatial resolutions and temporal dynamics, without requiring separate task-specific models.

The evolution of FMs has been driven by advancements in deep learning and the availability of large datasets. Initially, convolutional neural networks (CNNs) like ResNet [37] paved the way for improved image recognition and classification tasks [65]. The introduction of transformers, which use self-attention mechanisms to model long-range dependencies, has further advanced the capabilities of FMs in handling large-scale image data [16]. Vision transformers (ViTs) [25] extend the transformer architecture to process image data by treating image patches as sequences of tokens, enabling models to learn both local and global relationships. This capability makes transformers particularly effective for semantic segmentation and change detection tasks, where capturing long-range dependencies is crucial, especially in high-resolution satellite imagery.

Notable foundation models in remote sensing include SatMAE [16], which pre-trains transformers for temporal and multi-spectral satellite imagery; Scale-MAE [78], a scale-aware masked autoencoder for multiscale geospatial representation learning; and DINO-MC [111], which extends global-local view alignment for SSL with remote sensing imagery. These models have shown remarkable performance in various remote sensing tasks such as scene classification, object detection, and change detection.

Despite their success, FMs face several challenges, including the need for high-quality and diverse training data, significant computational resources, and effective domain adaptation to specific remote sensing tasks [73]. Addressing these challenges will be crucial for the continued advancement of FMs in remote sensing.

## III. RELATED REVIEW PAPERS
Artificial intelligence in remote sensing has been a growing area of research, with numerous review papers providing insights into AI advancements and their applications. In this section, we summarize the most influential reviews on foundation models in remote sensing.

Zhang et al. (2016), in their foundational review "Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art " [121], introduced deep learning techniques to RS, focusing on convolutional neural networks (CNNs) for tasks such as image classification and object detection. This work highlighted both the promise and challenges of early AI integration in RS, setting the stage for subsequent advancements.

In 2017, Zhu et al.’s "Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources " [129] delved into diverse AI applications, including hyperspectral analysis and synthetic aperture radar (SAR) interpretation. It also provided an extensive resource list, capturing the rapid adoption of deep learning in addressing complex RS challenges, paving the way for more advanced AI models in the following years.

More recent reviews have focused on advanced AI models and methods. Wang et al.’s 2022 review, " Self-Supervised Learning in Remote Sensing " [103], highlighted the ability of self-supervised learning (SSL) methods to utilize large volumes of unlabeled data, significantly reducing dependence on labeled datasets while maintaining high performance in RS tasks. The review also identified key challenges and future directions, emphasizing SSL’s potential to handle large-scale RS data complexities.

Zhang et al. (2022), in " Artificial Intelligence for Remote Sensing Data Analysis: A Review of Challenges and Opportunities" [120], offered a comprehensive overview of AI algorithms, synthesizing findings from over 270 studies. It emphasized ongoing challenges such as explainability, security, and integrating AI with other computational techniques, serving as a roadmap for future innovation in AI-driven RS.

Aleissaee et al.’s 2023 survey, " Transformers in Remote Sensing " [3], explored the impact of transformer-based models across various RS tasks, comparing them with CNNs. It identified both strengths and limitations, along with unresolved challenges, providing a detailed roadmap for future research on transformers’ role in RS.

Li et al.’s 2024 review, " Vision-Language Models in Remote Sensing " [60], examined the increasing significance of vision-language models (VLMs), which combine visual and textual data. It highlighted VLMs’ potential in applications like image captioning and visual question answering, emphasizing a shift toward richer semantic understanding in RS tasks.

Additionally, the recent work, " On the Foundations of Earth and Climate Foundation Models " [130], provided a comprehensive review of existing foundation models, proposing features like geolocation embedding and multisensory capability. It outlined key traits for future Earth and climate models, contributing to a broader discussion on foundational advancements in geospatial AI.

Building on these reviews, our study provides a comprehensive analysis of foundation models developed from June 2021 to June 2024, focusing on advances in self-supervised learning and transformer-based architectures. Unlike previous reviews, which focused mainly on individual techniques, we explore their combined potential in remote sensing tasks like semantic segmentation, multi-spectral analysis, and change detection. For instance, SatMAE [16] demonstrates effective use of SSL for pre-training transformers, enabling improved segmentation in complex multi-spectral imagery, while Scale-MAE employs scale-aware masked autoencoders for better handling of varied spatial resolutions in remote sensing data.

Our study also highlights new models like DINO-MC [111], which integrates global-local view alignment for SSL, making it particularly effective for identifying changes in high-resolution satellite imagery. By systematically examining these innovations, we illustrate how recent models address persistent challenges like domain adaptation and computational efficiency. For example, efficient self-attention mechanisms in Scale-MAE [78] help reduce computation costs, while enhanced geolocation embeddings in models like SatMAE improve performance in geospatial feature extraction.

In contrast to earlier reviews, which often remained theoretical, we emphasize both the theoretical advancements and practical applications of recent models. For example, DINO-MC [111] and ORBIT’s [101] real-world applications in environmental monitoring and disaster response highlight its practical impact, demonstrating how new FMs can be effectively leveraged to address pressing challenges in geospatial analysis.

## IV. PRETRAINING METHODS
Pretraining serves as a critical step in developing foundation models (FM), enabling them to learn transferable and generalized representations from large-scale datasets. This process leverages self-supervised or supervised learning methods to extract domain-agnostic features that can be adapted to various downstream tasks. In this section, we explore the key pretraining methods utilized commonly in foundation models for remote sensing, explaining the mechanism of these methods and their roles in enhancing model performance and addressing challenges in this field.

### A. Self-Supervised Learning
Self-supervised learning has emerged as a cornerstone of pre-training foundation models, offering a paradigm where models learn representations by predicting parts of the input data from other parts. This approach reduces reliance on expensive and time-consuming labeled datasets, making it particularly advantageous in fields like remote sensing, where labeled data is often scarce or challenging to obtain.

SSL allows models to exploit vast amounts of unlabeled data, learning rich, generalizable representations that transfer well to downstream tasks such as scene classification, semantic segmentation, object detection, and change detection. By uncovering underlying data structures and patterns, SSL not only enhances model robustness but also improves adaptability across diverse domains and resolutions of remote sensing imagery [104].

Two SSL methods commonly used in vision foundation models for remote sensing are contrastive learning and predictive coding, each offering unique mechanisms to harness information from unlabeled data.

1) Predictive Coding: Predictive coding leverages a generative approach, where the model learns to predict missing or occluded parts of an image based on visible portions. This strategy helps capture spatial and contextual relationships in remote sensing imagery, which often contains diverse textures, complex scenes, and varying resolutions.

In remote sensing, predictive coding can be applied to tasks such as gap filling in satellite imagery, where the model learns to infer missing data caused by sensor limitations or occlusions like cloud cover. Popular implementations of predictive coding frameworks include autoencoder-based architectures, masked image modeling techniques like those used in MAE (Masked Autoencoders) [34], and autoregressive models. These methods are particularly effective in learning fine-grained details critical for high-resolution imagery and specialized tasks.

2) Contrastive Learning: Contrastive learning is another powerful SSL technique that focuses on distinguishing between similar and dissimilar samples in the data. The key idea is to bring representations of similar (positive) samples closer together while pushing apart those of dissimilar (negative) samples. This encourages the model to learn discriminative and invariant features that are crucial for remote sensing tasks.

Contrastive learning frameworks such as SimCLR [13], MoCo [35], DINO [9], and BYOL [29] have shown promise in remote sensing applications. They use augmentations like random cropping, rotations, or spectral band dropping to generate positive pairs, enabling the model to learn robust representations invariant to these transformations. For instance, in multispectral or hyperspectral imagery, contrastive learning can help models capture spectral signatures across varying conditions, improving performance in tasks like crop classification or land cover mapping [104].

Contrastive learning is especially relevant in remote sensing when labeled datasets are highly imbalanced, as it enables models to learn from underrepresented classes or regions without explicit labels.

By combining approaches like predictive coding and contrastive learning, self-supervised learning has significantly advanced the development of vision foundation models in remote sensing. These methods allow models to leverage vast, unlabeled datasets while maintaining adaptability across diverse spatial resolutions, spectral bands, and application scenarios. On the other hand, it is important to note that there are many other SSL methods that can be employed to such tasks. Other innovative methods, such as teacher-student self-distillation frameworks, have also demonstrated potential in remote sensing applications. For example, CMID [70] achieves promising performance by combining contrastive learning and masked image modeling in a teacher-student self-distillation framework. This structure enables it to capture both global and local features, making it effective for diverse remote sensing tasks. The diversity of SSL techniques highlights the versatility and evolving nature of self-supervised learning, underscoring its critical role in unlocking the full potential of remote sensing imagery.

### B. Supervised Pretraining
Supervised pretraining is a fundamental approach in deep learning, where models are trained using labeled datasets to minimize prediction errors for specific tasks, such as image classification. This method allows models to learn direct mappings between input features and target labels, fostering the development of detailed, task-specific representations. For instance, models like ResNet [36] and VGGNet (Visual Geometry Group Network) [81] trained on large-scale datasets such as ImageNet [18] have demonstrated how supervised pretraining can capture robust feature hierarchies that are highly transferable to related tasks, including semantic segmentation or object detection.

In remote sensing, supervised pretraining has shown promise for tasks such as land cover classification and object detection using high-resolution satellite imagery [97]. However, the dependency on large-scale labeled datasets presents a major limitation. Creating labeled datasets for remote sensing tasks, particularly when involving multispectral or hyperspectral data, is resource-intensive and often requires domain expertise for annotation. For example, labeling pixel-level data for land cover classification or delineating objects in complex urban environments can be prohibitively time-consuming. Furthermore, labeled data in remote sensing is often domain-specific, limiting the generalizability of models trained on one dataset to other applications or regions [129].

These challenges highlight the need for innovative strategies to address the reliance on labeled data. Such limitations have motivated the development of alternative approaches, including self-supervised pretraining methods, which leverage the abundance of unlabeled data to learn general-purpose representations without manual annotation.

## V. IMAGE ANALYSIS METHODS

### A. Image Perception at Different Levels
Foundation models in remote sensing enable image analysis at three primary levels: image-level, region-level, and pixel-level. These levels address varying spatial, contextual, and application-specific needs, providing the foundation for a wide range of tasks such as environmental monitoring, urban planning, disaster response, and more. The following subsections outline the distinct objectives and applications at each level.

1) Image-Level: Image-level analysis focuses on classification tasks, categorizing entire images or large image segments into predefined classes, such as urban, forest, water bodies, or agricultural areas. This approach provides broad, high-level insights into geographic regions and is instrumental in large-scale applications like land use mapping, land cover classification, and resource management. By classifying entire scenes, this level of analysis enables efficient monitoring of extensive areas, supporting decision-making in environmental management and policy planning.

2) Region-Level: Region-level analysis identifies and localizes specific objects within an image, such as buildings, vehicles, ships, or other structures. Unlike image-level analysis, which provides holistic classifications, region-level tasks focus on object detection which is to detect individual entities and their spatial locations. This analysis is critical for targeted applications like urban planning, where the detection of infrastructure is essential, as well as disaster response and security, where identifying damaged buildings or vulnerable areas can significantly aid in timely interventions.

3) Pixel-Level: Pixel-level analysis offers the most granular form of image perception, assigning a label to every pixel within an image. This includes tasks such as semantic segmentation, where each pixel is classified into categories like vegetation, water, or buildings; it also include change detection, which identifies temporal differences between images captured at different times. Pixel-level analysis is indispensable for creating highly detailed maps used in applications like precision agriculture, deforestation tracking, and disaster management. The ability to analyze fine-grained details enables more accurate assessments and actionable insights for these critical areas.

### B. Backbone
1) Convolutional Neural Networks (CNNs): Convolutional Neural Networks [74] are a fundamental architecture in deep learning, designed to extract hierarchical spatial features from images through the use of convolutional layers. Each convolutional layer applies filters to the input data, detecting patterns like edges, textures, and shapes at different levels of abstraction. This makes CNNs well-suited for handling complex visual tasks in remote sensing, such as image classification, segmentation, and object detection.

Residual Neural Networks (ResNet) [36], a type of Convolutional Neural Network (CNN), address the degradation problem in deep neural networks by introducing residual connections, which allow gradients to bypass certain layers, facilitating the training of very deep networks. This capability is particularly beneficial in remote sensing, where deep models are often required to capture the intricate details and variations in satellite images. ResNet, as an example, is characterized by their residual blocks, which include shortcut connections that bypass one or more layers. The residual block can be described by the following equation:
y = F(x, {Wi})+ x
where y is the output, F represents the residual mapping to be learned, x is the input, and {Wi} are the layer weights [37].

ResNet has various architectures like ResNet-50, ResNet-101, and ResNet-152, with the number indicating the total layers. These networks have shown remarkable performance in various vision tasks due to their ability to train deeper networks without degradation. In remote sensing, ResNets are widely used for image classification, object detection and change detection tasks [31]. For example, ResNet-based models can classify different land cover types [125], [128], detect objects like buildings and vehicles [31], and monitor changes [75], [128] in the landscape over time by comparing temporal sequences of satellite images.

2) Transformers and Vision Transformers (ViTs): Transformers, adapted for computer vision (CV) as Vision Transformers (ViT), model long-range dependencies through self-attention, making them effective for complex geospatial data. ViTs treat images as sequences of patches, capturing global and local patterns, which is useful for segmentation and change detection. The self-attention mechanism computes:
Attention(Q, K, V )= softmax (QK T / √dk )V
where Q (query), K (key), and V (value) are the input matrices, and dk is the dimension of the key vectors [93].

By incorporating these methodologies, foundation models for remote sensing can leverage vast amounts of data, handle complex structures, and achieve state-of-the-art performance across various applications. These methodologies enable models to effectively address the unique challenges of remote sensing, such as large image sizes, diverse data sources, and the need for high accuracy in environmental monitoring and analysis.

In the following sections, we will explore specific applications of these methodologies in different remote sensing tasks, analyze their performance, and discuss the datasets used to train and evaluate these models.

## VI. DATA AND TASKS

### A. Data
Datasets play a crucial role in remote sensing, providing the foundation for training and evaluating models. High-quality datasets enable models to learn accurate representations of the Earth’s surface, improving their performance on various remote sensing tasks. In this section, we provide an overview of commonly used datasets in remote sensing, discussing their characteristics, applications, and relevance to foundation models. These datasets, with their varying resolutions, categories, and geographic coverage, provide a rich resource for advancing remote sensing research and applications. They facilitate the development of robust models capable of addressing diverse challenges in understanding and interpreting Earth’s surface through remote sensing technologies.

Datasets used in remote sensing vary significantly in size, from hundreds of thousands of samples, as in RSD46-WHU [62], [116], to over a million, as seen in MillionAID [63], [64]. Generally, larger datasets contribute to model generalization by encompassing diverse geographic areas, seasonal variations, and environmental conditions. Dataset resolutions also range from high (sub-meter), suitable for tasks requiring detailed spatial analysis, to moderate (10-60 meters), as with SEN12MS [80] and SSL4EO-S12 [107], which support broader pattern recognition applications.

These datasets leverage various sensor types, including RGB, multispectral, hyperspectral, and synthetic aperture radar (SAR). For instance, SEN12MS [80] integrates both SAR and multispectral imagery, enabling models to learn from distinct data modalities. This diversity in sensor types is critical for robust model development, as each sensor type captures unique surface characteristics, supporting tasks that benefit from cross-modal information.

Foundation models, in particular, benefit from such large-scale, multimodal datasets, which support self-supervised and supervised training approaches across tasks such as scene classification, segmentation, and object detection.

### B. Tasks
Different applications in remote sensing address particular real-world challenges by leveraging the capabilities of foundation models. These tasks include environmental monitoring, archaeology, agriculture, urban planning and development, and disaster management. To highlight the versatility of foundation models in remote sensing, we categorize models based on their applicability to various applications, as well as the different image analysis methods used.

1) Environmental Monitoring: Environmental monitoring utilizes remote sensing models to observe and track environmental changes, including deforestation, desertification, and pollution. These models play a crucial role in analyzing the effects of human activities and natural phenomena on the environment [39].

2) Agriculture: In agriculture, remote sensing models are used to monitor crop health, estimate yields, and manage agricultural practices. These models help optimize resource use and improve agricultural productivity [52].

3) Archaeology: In archaeology, remote sensing models have been used to identify and analyze archaeological features and sites. These models help detect features such as ruins, artifacts, and ancient structures from satellite imagery, leveraging technologies like Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to process high-resolution images and capture fine details [5]. Mantovan et al. (2020) also highlight the effectiveness of AI models, particularly CNNs, in locating challenging terrestrial archaeological sites and processing multispectral data [68].

4) Urban Planning and Development: In urban planning and development, remote sensing models are used to monitor and analyze urban expansion, infrastructure development, and land use changes. These models play a critical role in managing urban growth, planning new developments, and assessing the impact of urbanization by providing essential data for smart city planning and sustainable development [48].

5) Disaster Management: Remote sensing models play a crucial role in disaster management by providing timely information on affected areas. These models are used to detect and assess damage from natural disasters like earthquakes, hurricanes, and floods, enabling rapid response and recovery efforts [1].

## VII. DISCUSSION
The rapid advancement in foundation models for remote sensing underscore their transformative potential across various applications. As the field continues to evolve, it is crucial to synthesize the findings, address technical challenges, understand practical implications, and identify future research directions. In this section, we make a comprehensive analysis of these aspects, aiming to offer insights and guidance for future development and application of remote sensing foundation models.

### A. Synthesis of Findings
In our survey of foundation models for remote sensing, we identified significant advancements and trends that highlight the evolving capabilities and applications of these models. The performance metrics of various models across different downstream tasks such as scene classification, semantic segmentation, object detection, and change detection reveal the following key findings:

1) Model Performance: We present the performance metrics of recent foundation models in remote sensing based on results reported in the original papers. All performance numbers mentioned here are sourced directly from the original studies to ensure accuracy and consistency in evaluating these models. These metrics provide insights into the models’ effectiveness across tasks like semantic segmentation, object detection, and change detection, highlighting their strengths and limitations under different experimental setups.

- Image-Level: The performance of foundation models on the BigEarthNet dataset [85] for classification tasks shows variations in accuracy. Overall, msGFM [33] has the top performance of 92.90% (mAP), followed closely by SkySense [32] with a performance of 92.09%. Other notable performers include DeCUR [102], which achieved an mAP of 89.70%, and DINO-MC [111], with an mAP of 88.75%. SeCo [66] also demonstrated strong performance with an mAP of 87.81%, while DINO-MM [105] reached an mAP of 87.10%. On the other hand, models like CACo [67] and FoMo-Bench [8] have mAP of 74.98% and F1-Score of 68.33% respectively, showing competitiveness but room for improvement.

The high mAP scores of msGFM [33] and SkySense [32] highlight their efficiency in classification tasks, making them suitable for applications requiring high accuracy. The classification advancements observed in remote sensing models stem from sophisticated pre-training techniques that capture both spatial and spectral complexity across vast datasets. SkySense, for example, shows an average improvement of 2.76% over recent models by implementing multi-granularity contrastive learning on a diverse dataset of 21.5 million optical and SAR sequences [32]. HyperSIGMA [95], pre-trained on the expansive HyperGlobal-450K hyperspectral dataset [95], leverages its sparse sampling attention mechanism to optimize spectral-spatial feature extraction in high-dimensional hyperspectral data.

- Pixel-Level: For segmentation tasks, comparisons across models tested on datasets such as ISPRS Potsdam reveal varying performance. SkySense [32] has strong performance with a high mF1 score. CMID [70] stands out with a high mIoU, demonstrating its superior capability in accurately segmenting different regions within datasets. Cross-Scale MAE [88], UPetu [24] and RSP [96] have competitive segmentation capabilities. These insights can guide the selection and optimization of models for specific remote sensing applications.

For change detection tasks, models were evaluated on datasets like OSCD and LEVIR-CD. SkySense [32] achieves the highest F1 Score on the OSCD dataset, demonstrating strong ability to accurately detect changes. On LEVIR-CD, models like MTP [99] and SkySense [32] achieve high F1 scores, demonstrating robust performance.

- Region-Level: For object detection on datasets such as DOTA, DIOR, and DIOR-R, models like RVSA [100], SMLFR [22], RSP [96], MTP [99], and SkySense [32] achieve strong performance in mAP and AP50 metrics, showcasing their object detection capabilities across challenging aerial datasets.

Influence of Pre-training Methods: Various pre-training methods have a substantial impact on the performance of foundation models in remote sensing. Models pre-trained using SSL techniques, such as contrastive learning (CL) and masked autoencoders (MAE), consistently exhibit superior performance compared to those pre-trained with traditional supervised learning. For instance, SkySense uses a multi-granularity contrastive learning approach and outperforms other models in scene classification and object detection tasks [32]. SeCo, based on seasonal contrast learning, yields superior performance for land-cover classification, improving metrics over ImageNet-pre-trained models [66]. MAE-based models like SatMAE [16] and Scale-MAE [78] achieve improvements in change detection and segmentation across varied resolutions.

Foundation models like SatMAE, RingMo, A2-MAE, and ORBIT each demonstrate strong performance, but practical trade-offs are essential to consider, especially for application-specific constraints [16], [87], [101], [122]. SatMAE effectively leverages temporal and multi-spectral embeddings to capture complex spatiotemporal patterns, but this strength comes with significant computational requirements. RingMo provides a more lightweight vision transformer architecture, offering efficient model inference and a balance between performance and computational demands [87]. A2-MAE introduces an anchor-aware masking strategy, optimizing spatial-temporal-spectral representations and allowing effective integration of multi-source data [122]. ORBIT, designed with a very large number of parameters, is exceptionally scalable for Earth system predictability tasks but requires substantial computational resources [101].

Recent studies comparing SSL approaches highlight the distinct advantages of generative methods like Masked Autoencoders (MAE) over contrastive methods for time-series data, especially when labeled data is limited [61]. Generative methods reconstruct data from masked segments, allowing them to capture complex underlying structures and relationships within the data. This reconstruction-based learning proves particularly advantageous for time-series and multi-spectral applications in remote sensing.

2) Practical Implications: Foundation models offer transformative capabilities in remote sensing by building upon established applications like multi-spectral and time-series data analysis. While these applications have traditionally relied on machine learning and deep learning, foundation models reduce the need for labeled data and enable rapid adaptation to new tasks, providing robust solutions in areas previously limited by data constraints and task-specific architectures. Consequently, the advancements in foundation models have significant practical implications across various areas:

- Environmental Monitoring: Models like GASSL [6] and SatMAE [16] offer detailed assessments of environmental changes, aiding in conservation efforts and policy-making. These models excel in monitoring deforestation, desertification, and pollution levels, providing actionable insights for environmental management.

- Agriculture and Forestry: Foundation models such as EarthPT [82] and GeCo [57] deliver valuable insights into crop health, yield predictions, and land use management, optimizing agricultural practices and resource allocation. RSP [96], leveraging multi-spectral data, enhances precision agriculture by accurately monitoring crop conditions and predicting yields.

- Archaeology: The use of foundation models in archaeology revolutionizes the discovery, mapping, and analysis of archaeological features and sites. Models like GeoKR [56] and RingMo [87] can process high-resolution satellite imagery and multi-spectral data to enhance detection and mapping of archaeological features. MATTER [2] can accomplish texture and material analysis to help identify various surfaces.

- Urban Planning and Development: Remote sensing models like CMID [70] and SkySense [32] are pivotal for monitoring urban expansion, infrastructure development, and land use changes. These models facilitate sustainable urban growth and development planning by providing high-resolution data analysis and trend forecasting.

- Disaster Management: Models such as OFA-Net [118], DOFA [117] and Prithvi [46] are instrumental in flood mapping and fire detection. These models provide critical real-time data that helps in identifying affected areas quickly, enabling timely and effective response measures.

The improvements in accuracy across the models discussed have profound implications for real-world remote sensing applications. For example, models like GFM achieve high pixel-level accuracy in semantic segmentation, enhancing the precision of mapping forest cover changes [101]. HyperSIGMA achieves notable accuracy boosts in hyperspectral vegetation monitoring [95]. In urban planning, UPetu excels in infrastructure mapping by integrating multi-modal data, achieving higher accuracy compared to single-modality models [24]. RingMo enhances object detection accuracy over traditional supervised models [87]. ORBIT demonstrates exceptional scalability for processing large climate datasets, supporting long-term environmental monitoring and predictive modeling [101].

While remote sensing has long benefited from multi-spectral and temporal data, the adaptability, scalability, and efficiency of foundation models unlock a new level of precision and accessibility in these applications. This advancement opens up opportunities to tackle complex and evolving challenges across domains—from environmental conservation to urban planning—that traditional models have struggled to address at scale.

### B. Future Direction
Future research should prioritize several key areas:

- Efficient Model Development: Exploring techniques such as model distillation, pruning, and quantization to reduce computational requirements without compromising performance is crucial. Additionally, developing scalable architectures that efficiently handle ultra-high-resolution images is essential. Model adaptation techniques such as LoRA (Low-Rank Adaptation) [41] have emerged as effective methods for fine-tuning large-scale models with minimal computational overhead. By decomposing weight updates into low-rank matrices, LoRA enables efficient adaptation without the need to modify the entire set of model parameters.

- Multi-Modal Data Integration: Enhancing methods for integrating and processing multi-modal data (e.g., combining optical and radar imagery) will provide more comprehensive insights. Research on advanced SSL techniques capable of leveraging multi-modal data is necessary. The OFA-Net [118] framework, which integrates multi-modal data, serves as a promising direction for future models to emulate and improve upon.

- Interdisciplinary Collaboration: Promoting collaboration between remote sensing experts, AI researchers, and domain specialists can address complex challenges and drive innovation. For example, partnerships between AI researchers and environmental scientists can refine models like GASSL [6] for better environmental monitoring and conservation efforts.

Looking ahead, the consistent success of self-supervised learning methods in foundation models marks an exciting frontier for future research. These models’ ability to learn from unlabeled data and adapt to diverse remote sensing tasks with minimal fine-tuning suggests that advancements in unsupervised learning techniques could greatly reduce reliance on large labeled datasets. However, as these models grow in size and complexity, balancing computational demands with the need for efficiency will become increasingly crucial. Future work may focus on developing more resource-efficient versions of foundation models that maintain high performance, particularly for deployment in real-time monitoring systems or environments with limited computational resources.

### C. Limitation
This survey has several limitations:

- Scope and Coverage. The review focuses on foundation models released between June 2021 and June 2024. While the scope of this review is extensive and covers many significant developments, it is not exhaustive. Some recent advancements and innovations in the field may not be included due to their release timing or the lack of sufficient evaluation metrics at the time of writing. Consequently, certain cutting-edge models that have emerged in the latter part of this period or that have not yet been thoroughly evaluated might be omitted. This limitation underscores the need for readers to seek out the most current research and updates beyond the scope of this survey. Additionally, while foundation models have been empirically tested on a specific set of downstream applications, their robust architectures and general-purpose training paradigms, such as convolutional networks (e.g., ResNet) and vision transformers (e.g., ViT), indicate their potential to perform well across a much broader range of tasks.

- Evolving Field. The field of AI and remote sensing is rapidly evolving, with continuous advancements and breakthroughs occurring at a fast pace. This dynamic nature necessitates ongoing reviews and updates to ensure the relevance and comprehensiveness of the survey. New techniques, methodologies, and models are constantly being developed, which can significantly impact the state of the art. Therefore, it is essential to recognize that this survey represents a snapshot in time and that continuous monitoring of the literature is required to capture the latest advancements and emerging trends.

## VIII. CONCLUSION
In this comprehensive survey, we have reviewed the recent advancements in foundation models for remote sensing. We categorized these models based on their pretraining methods, image analysis techniques, and applications across different areas, highlighting their unique methodologies and capabilities.

Our analysis covered various advanced techniques, including self-supervised learning, vision transformers, and residual neural networks. These models have significantly improved performance on different image perception levels like region-level, pixel level, and image-level, as well as in applications like environmental monitoring, digital archaeology, agriculture, urban planning, and disaster management.

While significant progress has been made, several challenges persist, such as the need for more diverse and high-quality datasets, high computational requirements, and difficulties for different applications. Addressing these challenges will require further research and collaboration across disciplines.

In summary, this survey provides a detailed overview of the current state of foundation models in remote sensing, offering valuable insights and identifying future research directions. We recommend continued efforts in developing efficient model architectures, enhancing multi-modal data integration, and expanding dataset diversity to fully realize the potential of these models in remote sensing.