# Vision-Language Modeling Meets Remote Sensing

## Abstract
Vision-language modeling (VLM) aims to bridge the information gap between images and natural language. Under the new paradigm of first pretraining on massive image-text pairs and then fine-tuning on task-specific data, VLM in the remote sensing (RS) domain has made significant progress. The resulting models benefit from the absorption of extensive general knowledge and demonstrate strong performance across a variety of remote sensing data analysis tasks. Moreover, they are capable of interacting with users in a conversational manner. In this article, we aim to provide the remote sensing community with a timely and comprehensive review of the developments in VLM using the two-stage paradigm. Specifically, we first cover a taxonomy of VLM in remote sensing: contrastive learning, visual instruction tuning, and text-conditioned image generation. For each category, we detail the commonly used network architecture and pretraining objectives. Second, we conduct a thorough review of existing works, examining foundation models and task-specific adaptation methods in contrastive-based VLM, architectural upgrades, training strategies, and model capabilities in instruction-based VLM as well as generative foundation models with their representative downstream applications. Third, we summarize datasets used for VLM pretraining, fine-tuning, and evaluation, with an analysis of their construction methodologies (including image sources and caption generation) and key properties, such as scale and task adaptability. Finally, we conclude this survey with insights and discussions on future research directions: cross-modal representation alignment, vague requirement comprehension, explanation-driven model reliability, continually scalable model capabilities, and large-scale datasets featuring richer modalities and greater challenges.

## Introduction
VLM in remote sensing, aiming to bridge the information gap between remote sensing images and natural language, facilitates a deeper understanding of remote sensing scene semantics, like the attributes of ground objects and their relationship, and enables more natural human interaction with intelligent remote sensing data analysis models or methods [1], [2]. Since the introduction of remote sensing tasks, such as image captioning [3], visual question answering (VQA) [4], text-image (or image-text) retrieval [5], and text-based image generation [6], VLM in remote sensing has achieved significant success, driven by advancements in deep learning.

Early works on VLM primarily emphasize the careful design of model architectures, followed by supervised training from scratch on small-scale datasets. For example, in image captioning research, many efforts [7], [8], [9], [10] have been made to effectively combine convolutional neural networks (e.g., VGG [11] and ResNet [12]) with sequential models (e.g., LSTM [13] and Transformer [14]) before training on UCM-captions [3] and Sydney-captions [3] datasets. Under this classical construction paradigm, deep models often excel on test datasets but struggle to perform satisfactorily in large-scale deployments. Moreover, although these models are capable of describing image content, they fall short when tasked with answering questions about the images. In other words, they struggle to accomplish related tasks, such as VQA. The task-specific nature of these models seriously limits their applicability across diverse scenarios.

Recently, a new paradigm of pretraining followed by fine-tuning provides a promising solution to address the challenges mentioned previously. The core idea is to first pretrain a model on massive image-text data, enabling it to capture general knowledge that covers a wide range of visual and textual concepts, along with their underlying correspondence. The pretrained model is then fine-tuned on task-specific training data. The integration of general knowledge has been shown to enhance the model’s generalization ability in a single task [15], [16] while also making it more versatile and capable of handling a variety of downstream tasks [17], [18]. Consequently, VLM with this new paradigm has emerged as a prominent research focus in the field of remote sensing.

To date, significant progress has been achieved. This includes works based on the following:
1) contrastive learning [19], such as GeoRSCLIP [15], SkyCLIP [16], and RemoteCLIP [20], which have driven substantial advancements in various cross-modal tasks and zero-shot image understanding tasks;
2) learning an implicit joint distribution between text and images, like RS-SD [15], DiffusionSat [21], and CRS-Diff [22], which allow for image generation from text prompts; and
3) visual instruction tuning [23], such as GeoChat [18], LHRS-Bot [24], and SkySenseGPT [25], which have demonstrated improved performance, diverse capabilities, and conversational interactions in remote sensing data analysis.

Despite these remarkable achievements, it is widely acknowledged that VLM remains an open challenge. Indeed, existing works have not yet achieved the level of remote sensing experts in processing remote sensing data. To provide clarity and motivation for further advances in the research community, several surveys have reviewed VLM in remote sensing. For instance, Li et al. [2] summarize vision-language models from an application perspective and suggest potential research opportunities. However, due to time constraints, they primarily concentrate on vision-only foundation models and early works. Zhou et al. [26] review recent developments but lack an in-depth analysis of key designs, which is significant for inspiring future research. Moreover, datasets, as a prerequisite of visual-language modeling research, have not been given adequate attention in existing surveys.

In this work, we aim to provide a timely and comprehensive review of the literature, with a focus on VLM based on the pretraining and fine-tuning paradigm in the field of remote sensing. Specifically, we cover the following:
1) a taxonomy of VLM in remote sensing, detailing commonly used network architectures and pretraining objectives for each category;
2) the latest advancements in contrastive-based, instruction-based, and generation-based VLM in remote sensing, highlighting key designs and downstream applications;
3) progress in datasets for VLM pretraining, fine-tuning, and evaluation; and
4) several challenges and potential research directions.

## The Taxonomy of Visual-Language Modeling in Remote Sensing
Under the pretraining and fine-tuning paradigm, VLM in remote sensing can be divided into three distinct groups based on their strategies for bridging the two modalities in the pretraining phase: contrastive learning, visual instruction tuning, and text-conditioned image generation. In this section, we present commonly used network architectures and pretraining objectives within each group.

## Contrastive Learning
The motivation behind applying contrastive learning to VLM is training a model to map vision and language into a shared representation space, where an image and its corresponding text share similar representations while differing from other texts, which was first implemented by the pioneering work CLIP [27] in the field of computer vision. CLIP utilizes two independent encoders responsible for encoding visual and textual information. Given a batch of N image-text pairs, the embeddings extracted by the two encoders followed by normalization are used to achieve similarity between visual and textual representations. CLIP is trained to maximize the cosine similarity of the embeddings of the ith image and the corresponding ith text while minimizing the cosine similarity of the embeddings of the ith image and jth text. The loss is computed using InfoNCE [29].

The original CLIP model was trained on 400 million image-text pairs collected from the Internet and has demonstrated impressive results across various computer vision tasks [30], [31], [32]. These advancements spark interest in extending its capability to advance VLM in remote sensing. Two primary lines of research have been actively explored. The first, following the CLIP learning way, focuses on pretraining foundation models that are task agnostic but specifically adapted for the remote sensing domain. This includes efforts such as constructing large-scale image-text datasets [15], [16] and developing novel pretraining objectives [33], [34]. The second line explores the effective adaptation of pretrained CLIP models toward diverse downstream tasks, including image captioning [35], [36], zero-shot scene classification [37], [38], image-text retrieval [39], [40], etc.

## Visual Instruction Tuning
Optimizing a model from scratch for image-text alignment is extremely resource intensive due to the need for vast amounts of data and computational power. Fortunately, many pretrained vision encoders and language models have been released. Pretrained vision encoders can provide high-quality visual representations, while pretrained language models, particularly large language models (LLMs), demonstrate advanced language understanding capabilities. As a result, recent works increasingly leverage these models to achieve image-text alignment through visual instruction tuning, as introduced by LLaVA [23] and MiniGPT-4 [41].

A network architecture for this type of work typically consists of three key components: a pretrained vision encoder, a connector, and an LLM. Specifically, the vision encoder compresses remote sensing images into compact visual representations, while the connector maps image embeddings into the word-embedding space of the LLM. The LLM then receives both visual information and language instructions to perform reasoning tasks. Different from CLIP, which directly takes images and corresponding texts as input, this type of work preprocesses image-text pairs to instruction-following data. In this setup, each image is accompanied by a question or a language instruction, requiring the LLM to describe the image, while the corresponding text serves as the ground truth for the LLM’s predictions. Denote a batch of instruction-following data as image-question-answer triplets where qi is the question associated with the ith image. The pretraining objective is the conditional probability of generating the caption word given the image, question and the previous words in the caption. During pretraining, the vision encoder and LLM are typically kept frozen, with only the parameters of the connector being trainable.

In [23] and [41], the authors demonstrated that pretraining with visual instruction tuning can align vision and language representations while preserving extensive knowledge. Since then, advances have been made by modifying network architectures and creating high-quality pretraining datasets [24], [42]. In addition to improving alignment, another line of research focuses on supervised fine-tuning (SFT), aiming to enable the model to perform a variety of remote sensing image analysis tasks and interact with users in a conversational manner. This includes efforts to generate task-specific instruction-following data [18], [25], [43], design novel training strategies [42], [44], and incorporate cutting-edge vision encoders [44].

## Text-Conditioned Image Generation
Taking advantage of the advances in conditional image generation, a group of works [15], [21], [22] uses off-the-shelf generative models, primarily Stable Diffusion [28], to generate remote sensing images given text prompts, which essentially learn an implicit joint distribution between images and texts. Their network architecture comprises three main components: a text encoder, a variational autoencoder (VAE) [45], and a denoising U-Net. During training, the VAE encoder first transforms an image into a latent representation. Gaussian noise is then added to this latent representation at different time steps, resulting in noisy latents where a time-dependent scaling factor applies. Next, the conditioning representation extracted from the text encoder is provided as input alongside the noisy latents to the denoising U-Net, which predicts the noise added at each time step. Finally, the VAE decoder upsamples the denoised latent representation to reconstruct the input image. Based on image-text pairs, the training objective minimizes the error between the model’s predicted noise and the actual noise.

The application of diffusion models in remote sensing has shown rapid development, encompassing areas such as remote sensing image generation, enhancement, and interpretation [46]. Two major research groups have emerged. The first group aims to develop generative foundation models for various remote sensing images, including satellite [21], aerial [47], hyperspectral [48], and multiresolution images [49]. The second group extends text-conditioned diffusion models to specific remote sensing tasks, such as image or change captioning [50], [51], pansharpening [52], and zero-shot target recognition [53].

## Contrastive-Based Vision-Language Modeling
Most existing works on VLM fall into the group employing contrastive learning. There are two main research directions being actively investigated, namely foundation model construction and effective adaptation. Specifically, foundation model construction concerns the large domain gap between natural and remote sensing images, aiming to learn visual representations with rich remote sensing scene semantics and well aligned with textual representations. On the other hand, effective adaptation answers the question of how to leverage pretrained CLIP models for specific remote sensing tasks. In the following sections, we analyze the existing works from these two directions.

### Foundation Model Construction
To build foundation models, three key components need to be carefully designed: training datasets, pretraining objectives, and encoder architectures.

#### Training Datasets
Large-scale image-text datasets form the basis for constructing foundation models. Ready-made image-text datasets in remote sensing, e.g., UCM-captions and Sydney-captions, suffer from limited data volume and insufficient image diversity, rendering them inadequate for pretraining models to capture general knowledge of the domain. Recognizing the availability of numerous remote sensing image datasets, some works use open source datasets as the image source and develop image captioning methods to generate corresponding textual descriptions. Notably, one work filters 11 commonly used image-text datasets using RS-related keywords and captions three large-scale RS image datasets (Million-AID, fMoW, and BigEarthNet) with the aid of the tuned BLIP2 model, resulting in the RS5M dataset, which contains more than 5 million image-text pairs. Off-the-shelf vision-language models are indeed powerful tools for building large-scale image-text datasets due to their availability and ease of use, but ensuring captioning accuracy remains a significant challenge. To address this, another work proposes a rule-based method called mask-to-box (M2B) and box-to-caption (B2C), which converts pixel-wise or bounding box annotations into natural language captions.

Another concern is that the semantic diversity of the generated captions is constrained by the limited number of predefined classes in open source remote sensing image datasets. Given this, Wang et al. [16] attempt to leverage rich semantic information contained in OpenStreetMap (OSM), allowing the textual descriptions to encompass not only a wide variety of object categories but also fine-grained subcategories and object attributes. Similar to the rule-based approach, captions are assembled from object tags following predefined rules.

#### Pretraining Objectives
Instead of creating large-scale datasets, several works explore new training objectives to facilitate model pretraining with a few remote sensing image-text pairs. For instance, S-CLIP [55] introduces caption-level and keyword-level pseudolabels to fine-tune the original CLIP model on massive unpaired remote sensing images alongside a few image-text pairs. The caption-level pseudolabel is based on the assumption that the semantics of an unpaired image can be represented as a combination of those of paired images. The keyword-level pseudolabel relies on the assumption that an unpaired image shares keywords with visually similar images, representing the similarity between the embeddings of the unpaired image and the keywords (drawn from the nearest paired image). The overall training objective also includes the InfoNCE loss.

With a similar training data setup consisting of massive unpaired images and texts and limited image-text pairs, Set-CLIP [34] transforms the representation alignment between images and texts into a manifold matching problem, developing a multikernel maximum mean discrepancy loss and a semantic density distribution loss. The loss constrains the consistency of whole representation distributions of images and texts, thereby achieving macrolevel alignment. The semantic density distribution loss refines the alignment between the two modalities by ensuring that their probability density distributions remain similar in the representation space. Additionally, a self-supervised contrastive loss is introduced to obtain robust feature representations for each modality independently. Ultimately, the overall training objective is a weighted combination of these losses.

To avoid textual annotations entirely, GRAFT [33] proposes utilizing colocated ground images as the bridge between satellite images and language. In doing so, a dataset of 2 million ground-satellite image pairs is collected to support model training. Building on this, a feature extractor is designed to map satellite images to the representation space of the CLIP model, which was trained on Internet image-text pairs. Since a satellite image can cover a large ground area and thus be associated with multiple ground images, the extractor is optimized using an image-level contrastive loss focusing solely on aligning image-level representations between the two types of images while also training with an additional pixel-level contrastive loss to map ground image features to specific pixel locations in the corresponding satellite image.

#### Encoder Architectures
Unlike natural images, where corresponding textual information typically describes the image content, the textual information for remote sensing images can be represented by their geographic coordinates (longitude and latitude), which are beneficial for tasks such as scene classification and object recognition [60]. Therefore, some works propose aligning representations from remote sensing images and their geographic coordinates, with particular attention given to the choice of location encoders. A location encoder generally consists of a nonparametric functional positional encoding combined with a small neural network. In CSP [58], an existing 2D location encoder, namely Space2Vec’s grid, is utilized. This encoder maps image coordinates into high-dimensional representations using sinusoid transforms for position encoding, followed by fully connected rectified linear unit layers. GeoCLIP [61], on the other hand, first applies equal Earth projection to the image coordinates to reduce distortions inherent in standard geographic coordinate systems. It then adopts random Fourier features to capture high-frequency details, varying the frequency to construct hierarchical representations. These hierarchical representations are processed through separate multilayer perceptrons (MLPs), followed by element-wise addition, resulting in a joint representation. This design enables the location encoder to effectively capture the features of a specific location across multiple scales. In SatCLIP [64], the authors use a location encoder that combines spherical harmonics basis functions with sinusoidal representation networks to map geographic coordinates into latent representations [65].

In addition to adapting encoders for different types of textual information, another purpose of adjusting encoders is to improve the representations of visual concepts. Remote sensing images typically cover a large field of view and include a variety of objects. However, the corresponding textual descriptions often center on specific objects of interest and their relationships. Semantic noise, such as irrelevant objects and background, can interfere with the representation of key content in images, thereby obstructing the alignment between image and text representations. In [66], this problem is addressed by using prior knowledge of remote sensing scenes to instruct the pretrained model in filtering out semantic noise before calculating the similarity between image and text embeddings. This is practically achieved by adding an instruction encoder and a transformer encoder layer on top of the vision encoder. The instruction encoder, pretrained on the scene classification dataset AID [70], generates instruction embeddings, which are used to filter image embeddings via a soft belief strategy. The filtered embeddings are then activated using instruction information through the transformer encoder layer, producing relevant image embeddings for subsequent alignment.

### Performance Evaluation
Zero-shot scene classification and image-text retrieval are commonly used tasks to assess foundation models’ capabilities to capture a wide range of visual and textual concepts, along with their correspondence. For zero-shot scene classification, several observations emerge from evaluations: AID [70] is the most widely used dataset for evaluation, with RemoteCLIP achieving strong accuracy; models pretrained on images collected from Google Earth may outperform others on datasets sourced from Google Earth; and models pretrained on limited image-text pairs can perform comparably to those trained on larger datasets, highlighting the importance of data-efficient pretraining. For cross-modal retrieval, PIR-CLIP achieves state-of-the-art performance on several retrieval datasets, while models trained on limited image-text pairs (e.g., S-CLIP and SetCLIP) may perform significantly worse, indicating challenges in learning relationships between modalities with limited paired data.

### Effective Adaptation Methods
Inspired by the outstanding performance of foundation models, numerous methods have been developed to effectively adapt pretrained models for specific remote sensing tasks, including image captioning, cross-modal retrieval, zero-shot classification, dense prediction, etc. This section presents the development of adaptation methods within the context of different downstream tasks.

#### Image Captioning
Image captioning aims to describe the content of a given image using natural language. Vision-language foundation models are well suited for this task as they align image and text representations. One can use the vision encoder of these models to extract representations of image content, which prepares the input for language models to generate captions [36], [83], [85]. Commonly used vision encoders for remote sensing image captioning are from CLIP, while language models for this task include GPT-2 [105], BERT [106], and OPT [107]. To improve caption accuracy, a few works are devoted to enhancing models’ visual representation capabilities or further aligning images and texts.

For representation enhancement, VLCA [83] introduces external attention [84] to capture potential correlations between different images, improving visual representations. MVP [36] fuses visual representations from pretrained vision-language models and pretrained vision models through stacked transformer encoders. It also leverages CLIP’s vision encoder, followed by adaptive average pooling layers, to generate visual prefixes, which are concatenated with token embeddings. A BERT-based caption generator is subsequently developed to combine the fused visual representations and concatenated embeddings, enabling the generation of accurate captions. BITA [85] focuses on improving the alignment of images and texts in the remote sensing domain by introducing an interactive Fourier transformer. In this design, learnable visual prompts are fed to the Fourier layer and interact with image embeddings from a pretrained vision encoder, capturing the most relevant visual representations. Through contrastive learning, these visual representations are aligned with textual representations, also extracted by the Fourier-based transformer. The interactive Fourier transformer then connects the frozen vision encoder with the frozen language model, leveraging the language model’s generation and reasoning capabilities.

Beyond improving caption accuracy, generating detailed captions has also been explored. One work proposes a two-stage instruction fine-tuning for the vision-to-language mapping layer to generate geographically detailed captions. The first stage aligns geographic object regions with their attribute descriptions. The second stage focuses on understanding the spatial distribution of geographic objects within images. In another approach, image captioning is defined as the aggregation of information from multiturn dialogues, where each turn serves to query the image content. In each turn, CLIP’s vision encoder extracts image features, which are input into an auto-regressive language model along with previous questions, answers, and the current question to generate the response. After several dialogue turns, GPT-3 [108] summarizes the dialogue information to produce an enriched textual image description.

Prompt-CC [88] utilizes pretrained models to describe differences between bitemporal images, a task known as change captioning. Based on bitemporal visual representations from CLIP, Prompt-CC introduces an image-level classifier to detect the presence of changes and a feature-level encoder to extract discriminative features that identify the specific changes.

#### Cross-Modal Retrieval
Cross-modal retrieval is the task of retrieving data from one modality by using queries from another modality. Based on vision-language foundation models, research explores retrieval between images and text [40], [89], [91] as well as between images captured with different imaging parameters [39]. Image-text retrieval involves the challenges of finding textual descriptions that correspond to given image queries and vice versa. Existing works use pretrained encoders to encode images and text separately, mapping them into a shared representation space for similarity measurement. Typically, CISEN [40] adopts encoders from CLIP and GeoRSCLIP as the backbone for its retrieval network. Given the abundance of objects in remote sensing images, CISEN is trained in two stages to enhance visual representation. In the first stage, an image adapter is trained to map global visual features into textual-like features. In the second stage, a feature pyramid network is used to integrate the textual-like features into local visual features, which are then utilized to enrich global visual features.

Rather than focusing on representation, KTIR [89] aims to improve alignment by explicitly incorporating knowledge into text features. The knowledge is derived from textual information using off-the-shelf knowledge sources like RSKG [110] and ConceptNet [111]. Once converted to knowledge sentences, the knowledge is processed by the text encoder to extract knowledge features, which are then fused with text features via a single cross-attention layer.

Image-image retrieval involves searching for relevant images across different imaging parameters, such as matching RGB images with multispectral images. The authors in [39] use a pretrained CLIP text encoder as the classification head and fine-tune the CLIP vision encoder alongside a newly added multispectral-specific encoder. The training is conducted in stages; first, the CLIP vision encoder is fine-tuned on RGB images composited from multispectral images, followed by fine-tuning the new encoder on multispectral images. This encoder is guided to produce discriminative representations that can achieve accurate image classification while ensuring that its representations are similar to those of RGB images extracted by the CLIP vision encoder.

Considering the complexity of Earth’s surface, single-modality queries, such as text queries, require users to fully articulate their needs to pinpoint relevant images. To address this issue, composed-to-image retrieval searches for remote sensing images based on a composed query of image and text. The retrieved images share the same scene or object category as the image query and reflect the attribute defined by the text query. To achieve this, the similarity scores for the image and text query are calculated separately and then normalized and combined using a convex combination controlled by a weighting parameter that adjusts the contribution of each modality.

#### Zero-Shot Scene Classification
Zero-shot scene classification challenges models to identify remote sensing images from scene categories that were not seen during training. The idea of applying vision-language foundation models to this task is straightforward; since these models align images and texts, zero-shot scene classification can be achieved by comparing image embeddings with text embeddings extracted by the text encoder, which takes as input textual descriptions specifying the unseen classes. By default, these descriptions follow the format “a photo of a [CLASS]” where the class token is replaced by the specific class name, such as “farmland,” “forest,” or “playground.” The input text, also known as the prompt, plays a crucial role in the performance of foundation models on downstream tasks. As a result, several works pay attention to prompt tuning and suggest adding task-relevant context words to improve performance.

To avoid manual prompt tuning, Lan et al. [92] model context words in a prompt as learnable vectors, which are combined with the class token embeddings before being input to the text encoder. Moreover, the class token embeddings for each category are organized across multiple levels of granularity for few-shot fine-grained ship classification. Compared to hand-crafted prompts, these hierarchical learnable prompts incorporate richer task-specific knowledge.

Most works employ and freeze pretrained CLIP models. However, due to the significant domain gap between web images and remote sensing images, their performance tends to be limited. To mitigate this, one can inject remote sensing domain priors into the vision encoder [92] or fine-tune the entire model using pseudolabeling techniques [35]. Typically, Lan et al. [92] introduce a lightweight network that is trained on data from seen classes to capture the domain prior. This prior is then combined with image embeddings output by the vision encoder, allowing the pretrained model to adapt better. On the other hand, RS-CLIP [35] leverages the strong transferability of the pretrained model to automatically generate pseudolabels from unlabeled images, which are used to fine-tune the model. Additionally, a curriculum learning strategy is developed to gradually select more pseudolabeled images for model training in multiple rounds, further boosting the model’s performance in zero-shot scene classification.

In addition to comparing the similarity between embeddings of images and unseen-class texts, DSVA [38] introduces a solution that uses pretrained models to annotate attributes for each scene class and predict scene categories by evaluating the similarity between attribute values derived from image embeddings and those associated with scene classes. For automatic attribute annotation, textual descriptions have the form of “This photo contains [ATTRIBUTE],” where the attribute token is replaced by specific attribute names, such as “red,” “cement,” or “rectangle.” Meanwhile, the attribute value for each class is calculated by measuring the similarity between embeddings of attribute text and images belonging to that class.

#### Dense Prediction Tasks
Dense prediction tasks, such as semantic segmentation and change detection, which produce pixel-level predictions for input images, have recently benefited from the application of vision-language foundation models. Text2Seg [94] uses a CLIP model to classify category-agnostic segmentation masks generated by the Segment Anything Model (SAM) [112], enabling zero-shot semantic segmentation of remote sensing images. Lin et al. [95] modify the CLIPSeg decoder [96] to receive joint image-text embeddings from the CLIP model as input, producing a binary segmentation mask.

Without the need for additional decoders or segmentation models, one can perform an upsampling operation on image embeddings from CLIP and compare the similarity between image patch embeddings and text embeddings to produce segmentation results. However, empirical findings suggest that for CLIP with a vision encoder based on ViT-B/16, the image embeddings are downsampled to 1/16 of the input image size. This downsampling leads to distorted object shapes and poorly fitting boundaries in segmentation masks. Furthermore, CLIP’s self-attention causes global information from the class token embedding to be attached to the patch embeddings, which significantly degrades performance in semantic segmentation. To handle these issues, SegEarth-OV incorporates SimFeatUp on top of the CLIP vision encoder to restore lost spatial information in image embeddings. Subsequently, subtraction operations are executed between patch embeddings and the class token embeddings before similarity measurement, alleviating global bias in patch embeddings. In particular, SimFeatUp is a learnable upsampler consisting of a single parameterized Joint Bilateral Upsampling (JBU) operator. It is trained with a frozen CLIP model, a learnable downsampler, and a lightweight content retention network. The training objective is to ensure that the image embeddings, after the up-down-sampling process, remain similar to those from CLIP. Meanwhile, the image generated by the content retention network, which takes the upsampled image embeddings as input, should closely resemble the input image to CLIP.

When it comes to change detection, similar adaptation strategies used in segmentation tasks, such as combining SAM with CLIP and designing decoders, are applied. For instance, SCM [98] applies CLIP after SAM to identify objects of interest in bitemporal images, helping to filter out pseudo-changes. ChangeCLIP [99] uses CLIP to construct and encode multimodal input data for change detection tasks, while a transformer-based decoder combines vision-language features with image features to predict change maps. Bitemporal texts are formatted to highlight the changing object, providing additional information for change detection.

Alternatively, only the vision encoders of foundation models are adopted as most existing datasets provide only bitemporal images for identifying changes. In BAN [100], the authors introduce bridging modules to inject general knowledge extracted from the vision encoders of foundation models into existing change detection models like BiT [113] and ChangeFormer [114]. Since the input image sizes for foundation models and change detection models may differ and not all general knowledge contributes to predicting changes, the bridging modules are responsible for selecting, aligning, and injecting this knowledge. These modules consist of layer normalization and a linear layer to mitigate the distribution inconsistency between the image features from the two models, cross-attention to obtain valuable information from general knowledge, and bilinear interpolation to solve the misalignment problem. The bridging modules are placed between the two encoders, executing multi-level knowledge injection.

### Broader Scope of Application
In addition to the remote sensing tasks previously discussed, there are several emerging applications of contrastive-based vision-language foundation models, including:
1) VQA: VQA attempts to provide answers to questions related to the content of images. Works employ CLIP to extract both visual and textual representations from images and questions, designing decoders and classifiers to capture intra- and inter-dependencies within and between these representations.
2) Cloud presence detection: This involves identifying satellite images that are affected by clouds. Strategies include prompt tuning, learnable context combined with the class token embeddings, or employing the vision encoder with a linear classifier on top.
3) Text-based image generation: Text-based image generation refers to creating images from textual descriptions, which can help mitigate class imbalance in remote sensing data. Typically, CLIP is used to classify generated images, ensuring semantic class consistency.
4) Image address localization: Image address localization aims to predict the readable textual address where an image was taken. A solution is predicting GPS coordinates with a foundation model and then converting them into readable addresses. AddressCLIP introduces contrastive learning to align images with scene captions and address texts and develops image-geography matching to bring features of geographically proximate images closer together while distancing features of images that are far apart geographically.

## Instruction-Based Vision-Language Modeling
Instruction-based VLM is advancing rapidly. Since 2023, many impressive vision-language models have emerged. They are versatile, capable of performing a range of remote sensing image analysis tasks, and able to interact with users in a conversational manner. This broadens the accessibility of intelligent models beyond experts in remote sensing, facilitating their widespread deployment and application. This section presents critical developments in terms of model architecture, training strategy, and model capability.

### Model Architecture
As CLIP has been trained to align image and text representations, most works directly employ its vision encoder. The commonly used LLMs include the LLaMA family and its derivative, the Vicuna family. Regarding model architecture, existing works are devoted to improving visual encoders to enhance visual perception and designing connectors to promote the alignment between the two modalities.

#### Vision Encoder
Through a mask image modeling pretext task, EVA incorporates geometry and structure information into CLIP’s visual representations, leading to improved performances across a wide range of visual perception tasks. Consequently, RSGPT and SkyEyeGPT adopt EVA as their vision encoder. An alternative to complementing CLIP is utilizing the strengths of diverse vision encoders. DINOv2 learns visual representation from images alone via self-supervised learning, enabling it to capture both image-level and pixel-level information. Given the varied object sizes in remote sensing images, some works refine visual representations by incorporating multiscale information. In certain works, the input image is downsampled to different resolutions and then respectively fed into two vision encoders. The encoded visual features are transformed to the same dimension and concatenated channel-wise.

#### Vision-Language Connector
The linear layer and MLP are widely used as vision-language connectors, serving as key components in most models. In contrast, RSGPT and LHRS-Bot explore alternative connector architectures. Following InstructBLIP, RSGPT includes an instruction-aware query transformer (Q-Former) as an intermediate module between the vision encoder and LLM. The Q-Former is designed to extract task-relevant visual representations by interacting additional query embeddings with instruction and image embeddings via attention mechanisms. The resulting output from the Q-Former, after passing through a linear layer, is then input into the LLM along with the instruction embeddings to generate responses. LHRS-Bot proposes incorporating multilevel image embeddings to sufficiently capture the semantic content of images. It introduces a set of learnable queries for each level of visual representation to summarize the semantic information of each level through stacked cross-attention and MLP layers, resulting in a dedicated visual perceiver.

### Training Strategy
Training instruction-based vision-language models typically involves two stages: pretraining for modality alignment and SFT for following task-specific instructions.

#### Only Supervised Fine-Tuning
Due to the lack of large-scale image-text datasets specifically designed for the remote sensing domain, most works target the SFT stage using carefully crafted instruction-following datasets. To preserve the general knowledge embedded in pretrained vision encoders, the vision encoder is typically kept frozen during training, with the connector or the LLM undergoing fine-tuning. For instance, RSGPT fine-tunes the connector, while GeoChat and TEOChat fine-tune the LLM. SkyEyeGPT, SkySenseGPT, and IFShip fine-tune both the connector and LLM. To avoid the expense of full-parameter tuning, Low-Rank Adaptation (LoRA) is often adopted, which introduces low-rank learnable matrices into the layers of the connector or LLM.

#### Pretraining Followed by Supervised Fine-Tuning
A couple of recent works have investigated how to implement the pretraining stage to boost model performance. Based on the choice of training data, they can be categorized into two groups: those that combine available image-text pairs from multiple domains for pretraining and those that direct attention toward creating large-scale RS image-text datasets. For combining available data, some models utilize natural image-text datasets, while others integrate data from both computer vision and remote sensing domains. COCO Caption is a commonly used pretraining dataset. Some models perform pretraining for the connector and the LLM.

To address the domain gap between remote sensing images and natural images, researchers have developed large-scale RS image-text datasets, such as LHRS-Align and VersaD, both of which contain more than 1 million training samples. Leveraging LHRS-Align, a three-stage curriculum learning strategy is designed for LHRS-Bot: pretrain the vision perceiver on LHRS-Align; fine-tune the vision perceiver and LLM on the LHRS-Instruct subset and multitask dataset; and further fine-tune the LLM on all instruction data to fully unlock LHRS-Bot’s capabilities. Note that these models still keep the vision encoder frozen. In contrast, Pang et al. unfreeze the vision encoder, MLP, and LLM for pretraining on VersaD. They subsequently fine-tune the MLP and LLM on customized datasets, showing that models pretrained with RS data significantly outperform those only fine-tuned with RS data across multiple tasks, confirming the importance of incorporating extensive RS visual knowledge by pretraining.

### Model Capability
Most instruction-based models are developed for general-purpose remote sensing data analysis, with only a few specifically tailored for remote sensing images of ships. These models primarily process optical images but have expanded to include synthetic aperture radar (SAR) and infrared (IR) images. Most models support conversational interaction with users and can perform a variety of tasks, ranging from single-image analysis to temporal series analysis. The granularity of analyzed information has progressed from the image level to the region level and even to the point level.

#### Common Capabilities
Most models are capable of performing remote sensing tasks such as image captioning, scene classification, VQA, and visual grounding (VG). For image captioning, models generate descriptions based on the input image and language instruction. For scene classification, absorption of extensive remote sensing visual knowledge improves model accuracy and generalization. For VQA, datasets such as RSVQA-HR and RSVQA-LR assess model performance in question answering tasks. For VG, models locate specific objects within an image based on a natural language expression, typically outputting coordinates of the target object.

#### Unique Capabilities
Current research seeks to develop versatile vision-language models capable of handling various remote sensing image analysis tasks in a conversational manner. Examples include:
1) Fine-grained image understanding: Region-level image understanding can be guided by coordinates or visual prompts. Visual prompting marks guide the model to interpret specific regions or points, enabling multigranularity interpretation at the image, region, and point levels.
2) Time-series image analysis: Change detection and other temporal tasks are handled by models that process sequences of remote sensing images and produce results such as bounding boxes for detected changes. Instruction-following datasets tailored for time-series analysis support such capabilities.
3) From qualitative recognition to quantitative analysis: Models like LHRS-Bot and VHM broaden capabilities to include quantitative image analysis, such as object counting and measuring object size, using open-ended question formats for practical application.
4) Endowing models with honesty: Datasets containing factual and deceptive categories help models learn to refuse to answer questions about non-existent objects, improving honesty and reliability.
5) Object relationship understanding: Recent models extend capabilities to understand spatial, functional, and semantic relationships between objects in images, benefiting from specialized instruction-following datasets.

## Generation-Based Vision-Language Modeling
Similar to contrastive-based VLM, generation-based VLM follows two major research directions: the construction of foundation models concerning the characteristics of remote sensing images and their application to promote various remote sensing data analysis tasks. This section presents the development of generative foundation models and representative downstream applications.

### Generative Foundation Models
Building an effective generative foundation model is a formidable task because one needs to consider how to ensure the reliability and diversity of the generated images. Enhancing reliability and improving diversity are two primary concerns.

#### Enhancing Reliability
Text descriptions alone struggle to fully encapsulate the variety of objects and intricate relationships present within a satellite image. The lack of sufficient constraint information poses a challenge to generating reliable images. To address this challenge, additional conditions, such as metadata or images, are increasingly utilized to constrain the generation process.

1) Metadata: Metadata, such as latitude, longitude, ground sampling distance, cloud cover, and imaging time, are adopted in several works. Compared to text conditions, metadata are more easily available as they are inherently embedded within remote sensing images. They allow generative foundation models to be trained on large-scale image datasets, benefiting from the diverse geographic distribution of these datasets. Injecting metadata conditions into diffusion models is typically done by processing metadata values through sinusoidal encoding followed by MLPs, and adding the resulting metadata embeddings with time step embeddings before feeding into the diffusion model. For latitude and longitude, an alternative approach is to utilize a pretrained location encoder.

2) Image: Image-form conditions enable more precise control over the image generation process and are split into low-level visual conditions and high-level semantic conditions. Low-level conditions pertain to geometric information such as edges, line segments, and sketches. High-level semantic conditions refer to semantic information of images, providing constraints on object categories and their relationships. These conditions can be associated remote sensing images or abstract representations like road maps and segmentation masks. Image-form conditions are typically injected into diffusion models using ControlNet [154], which replicates encoder blocks and incorporates zero convolutions to process conditioning representations and noisy latent representations. For multicondition injection, works extend ControlNet into 3D versions or perform multiscale condition injection with attentional feature fusion. Other approaches directly concatenate latent and image condition representations.

#### Improving Diversity
Diversity refers to both varied semantic categories and a broad range of variations in imaging conditions and sensors. By conditioning on metadata, the diversity of generated data is improved. Some works explore frameworks for generating multiresolution images, using cascaded diffusion models or a resolution-guided self-cascading framework to generate images at multiple resolutions in stages, with each stage conditioned on the low-resolution image output from the previous stage and its corresponding spatial resolution. Techniques like sliding windows with overlaps and noise sampling strategies enable generating continuous unbounded scenes.

### Performance Evaluation
Generative foundation models have demonstrated the capability to generate optical or hyperspectral remote sensing images. Their performance is typically evaluated using metrics such as Fréchet Inception Distance (FID) and Inception Score (IS). For text-conditioned models, CLIP is employed to measure the similarity between generated images and their corresponding textual descriptions. In addition to direct evaluation metrics, some works assess model performance by applying generated images to downstream tasks and measuring their impact, such as superresolution, temporal generation, in-painting, and augmenting training datasets for downstream detection tasks.

### Downstream Applications
Generative foundation models serve as powerful tools for advancing various remote sensing tasks. Their image generation capabilities have been utilized to tackle limited image availability and the high costs associated with annotation. Their conditionally controllable nature makes them suitable for image enhancement tasks like superresolution and cloud removal. Moreover, their advantages in learning diverse remote sensing image distributions improve the accuracy of interpretation tasks such as change detection and land cover classification.

Representative applications include:
- Captioning: Generative models have been applied to generate textual descriptions conditioned on visual features, extracting global and local features to enhance visual conditions and improving interactions between noisy representations and visual conditions in the decoder.
- Pansharpening: Formulated as an image generation problem conditioned on panchromatic and multispectral images, with text descriptions used as identifiers to specify satellites, enhancing generalizability across satellites with different spectral configurations.
- Zero-shot SAR target recognition: Generative models can create optical images conditioned on target semantic information, which are then transformed into 3D models for SAR target simulation and recognition.
- Cloud removal: Conditioning on cloud-contaminated images and SAR images, generative models have been employed for cloud removal, though geographic alignment of multimodal pairs is challenging in practice.
- Urban prediction: Generative models forecast future urban layouts based on current layouts and planned change maps, injecting layout embeddings and text conditions into diffusion decoders.

## Datasets
Large-scale datasets are an essential prerequisite for vision-language research under the two-stage paradigm. Considerable research focuses on dataset construction. Existing datasets can be broadly categorized into three groups: pretraining datasets, instruction-following datasets, and benchmark datasets.

### Pretraining Datasets
Pretraining datasets, which consist of remote sensing images and corresponding texts, play a crucial role in infusing the model with a broad range of visual and language concepts. Two alternative sources for collecting images are combining various open source remote sensing image datasets and utilizing public geographic databases. Once images are collected, corresponding textual descriptions can be generated through manual annotation, rule-based methods, or off-the-shelf models.

#### Image Collection
Object detection datasets in remote sensing typically feature diverse ground objects and have been instrumental in constructing pretraining datasets (e.g., RSICap and DIOR-Captions). Combining multiple datasets further enriches image and object diversity while significantly increasing dataset size. Some pretraining datasets prioritize large-scale image datasets, integrating sources like BigEarthNet, fMoW, and Million-AID. Other datasets derive images from Google Maps, Google Earth, or Google Earth Engine, ensuring diverse topographies and geographic coverage. Manual screening and image enhancement models are sometimes applied to improve dataset quality.

#### Caption Generation
Texts associated with remote sensing images in pretraining datasets are typically human-understandable sentences that describe various aspects of the images. Caption generation methods include:
1) Rule-based captioning: Converting heterogeneous annotations like bounding boxes and segmentation masks into natural language captions using methods like B2C and M2B. Leveraging semantic information from OSM, filtered for visibility, to craft descriptions.
2) Model-based captioning: Using LLMs (e.g., ChatGPT, Gemini, Vicuna, Mixtral) to generate captions. Prompt design is critical, and prompts are optimized by adding constraints on the model’s response or incorporating semantic and meta information about the image. For text-only LLMs, providing processed semantic information in the prompt allows the model to simulate “seeing” the image and generate captions. Generated captions may be further enriched by combining metadata or producing multiple revisions in different tones.

#### Dataset Property
Key observations on pretraining datasets include:
1) Geographic coverage: Datasets like RS5M, VersaD, and ChatEarthNet benefit from large-scale sources, while others like RSTeller may have limited geographic coverage.
2) Scene diversity: Large-scale datasets ensure diversity in scenes, with careful global sampling and manual selection enhancing coverage.
3) Caption quality: Manually annotated or rule-generated captions typically achieve high accuracy. Model-generated captions inevitably contain errors, but large-scale noisy datasets can still yield effective pretrained models.
4) Distinctive characteristics: Most pretraining datasets predominantly contain English text; ChatEarthNet provides multispectral images paired with textual descriptions to support deeper multispectral understanding.

### Instruction-Following Datasets
Instruction-following datasets are specifically designed for the SFT of instruction-based vision-language models, allowing them to perform specific remote sensing tasks. These datasets consist of images paired with conversations, structured as instructions or questions and answers. Their construction involves image collection and conversation generation using template-based transformation, large model assistance, and manual annotation.

#### Conversation Generation
1) Template-based transformation: Converting task-specific annotations into dialogues using templates for instructions and answers derived from dataset annotations. Separate instruction pools can be constructed per task to generate diverse instruction phrasing.
2) Large model assistance: LLMs are prompted with a few in-context examples to generate instruction-answer pairs based on image captions and related information. GPT-4 and similar models can generate detailed descriptions, multiturn conversations, and complex reasoning examples.
3) Manual annotation: Manual efforts provide accurate information to support other generation methods and to build high-quality labels for specialized tasks.

The three methods can be combined to construct rich and diverse instruction-following datasets. To help models distinguish between tasks, task-specific identifiers are commonly incorporated into instructions.

#### Impressive Datasets
Several instruction-following datasets have been developed for remote sensing tasks. Examples include:
- RSVP-3M: A visual prompting instruction dataset where samples consist of images, visual prompts (masks, bounding boxes, or points), and conversations. GPT-4V is employed to automatically generate instruction data, resulting in more than 3 million samples.
- TEOChatlas: An instruction-following dataset for time-series image analysis, supporting temporal scene classification, change detection, temporal referring expressions, and more. Conversation generation is assisted by GPT-4o, resulting in hundreds of thousands of samples.
- LHRS-Instruct and VariousRS-Instruct: Datasets exploring quantitative image analysis, including object counting and geometric measurement tasks, derived from datasets such as DOTA and FAIR1M.
- HnstD: A dataset designed to enhance model honesty, incorporating factual and deceptive instructions across tasks like presence, color, and positional reasoning.
- FIT-RS: A dataset focused on fine-grained semantic relationships and scene graph generation, challenging models to interpret relationships between objects.

### Benchmark Datasets
Benchmark datasets are essential for evaluating and fairly comparing the performance of different models. Recent benchmark datasets fall into two types: instruction-specific datasets and general-purpose datasets.

#### Instruction-Specific Datasets
These datasets are designed for instruction-based VLMs and typically provide image-instruction-answer pairs. They often include a variety of tasks, from image captioning and VQA to complex reasoning and temporal analysis. Notable benchmarks include RSIEval, VRSBench, FIT-RSRC, LHRS-Bench, COREval, VLEO-Bench, GEOBench-VLM, UrBench, and others. Key observations include the expanding variety of tasks, increasing dataset scales, and diverse question formats (single choice, multiple choice, open-ended).

#### General-Purpose Datasets
General-purpose datasets evaluate various types of VLMs and target specific remote sensing multimodal tasks, such as geo-localization, visual grounding, object counting, and classification. Examples include GeoText-1652, DIOR-RSVG, RemoteCount, and SATIN. These datasets pose significant challenges and help assess models’ real-world applicability.

## Conclusion and Future Directions
From the perspectives of models and datasets, we have covered the advancements in VLM for remote sensing, knowing how remote sensing images and natural language can be effectively bridged, which remote sensing tasks existing vision-language models can address, and which datasets are suitable for developing and testing vision-language models. Naturally, this raises two important questions.
1) Are existing vision-language models adequate for practical applications?
2) If not, which directions are worth pursuing to advance this field further?

The answer to the first question is, unsurprisingly, no. VLM remains a highly challenging task and is far from meeting practical needs. In this section, we share insights on future research directions from two perspectives: models and datasets.

### Effective Representation and Alignment for Cross-Modal Data
A growing trend seeks to advance vision-language models to accommodate a wide range of remote sensing images, including optical, SAR, and IR, thereby enabling the acquisition of more comprehensive information about Earth’s surface. However, in applications such as disaster risk assessment, these models may need to integrate additional information sources beyond remote sensing images, such as geospatial vector data and social media, to perform complex reasoning [294], [295]. Geospatial vector data present complex data structures in the form of points, polylines, polygons, and networks. Meanwhile, social media encompasses texts in various languages, diverse types of images, videos, and more. This complexity and diversity pose challenges for models in comprehending the embedded information and associating it with remote sensing data. Consequently, there is a pressing need for effective representation and alignment across a broader scope of cross-modal data.

### Requirements Arbitrarily Described in Natural Language
Existing instructions for prompting vision-language models are usually definite, such as using task identifiers to specify particular remote sensing tasks or providing candidate answers. However, in more realistic and practical scenarios, requirements described in natural language tend to be vague and complex, involving the sequential execution of multiple tasks. For example, given an instruction like “Please assess the water quality in the indicated area of the image,” the model must be capable of decomposing water quality assessment into two subtasks—water detection and quantitative retrieval—and then accomplish each task in sequence. This creates the need to advance models’ language understanding capability to adapt to users’ arbitrary or flexible demands.

### Enhancing the Reliability of Model Answers via Expert Explanations
Existing vision-language models take language instructions and visual representations as input, producing image analysis results in the form of natural language. However, these models typically do not provide expert explanations for their answers, leading users to doubt the reliability of the outputs, especially in tasks requiring complex reasoning rather than simple recognition. For instance, in precision agriculture, where the model may be used to guide pesticide spraying schedules, the lack of explanations regarding crop diseases and pest conditions makes it hard to convince farmers to follow the recommendations. Consequently, expert explanations accompanying image analysis results are necessary as they not only enhance the reliability and interpretability of the model’s answers but also offer insight into the decision-making process, thereby fostering user trust. Recent initiatives have begun exploring the feasibility of developing vision-language models capable of performing fine-grained classification while providing reasoning behind their classifications by integrating domain knowledge into the construction of the instruction-following dataset.

### Continually Adapting Vision-Language Models
New remote sensing images are collected daily from around the world, and human demands may change accordingly. This necessitates continually adapting vision-language models to these dynamic changes rather than relying solely on one-time pretraining and fine-tuning. Combining newly collected data with previously gathered data for continual learning is a straightforward and effective approach. However, the growing volume of data poses significant challenges in terms of computational and storage costs. Training the model exclusively on new data risks catastrophic forgetting [296], [297]. Therefore, it is crucial for the research community to explore effective learning strategies that enable vision-language models to learn new knowledge from new data while maintaining old knowledge.

### More Diverse and Rich Remote Sensing Multimodal Dataset
The original CLIP model was trained on 400 million image-text pairs, whereas the largest remote sensing pretraining dataset contains only 10 million pairs, predominantly limited to optical images. This data scarcity restricts models’ capacity to capture a broad range of visual concepts, a challenge that is particularly pressing in remote sensing. Variations in imaging conditions, sensor parameters, and geographic locations result in highly varied visual characteristics of objects in remote sensing images. Therefore, substantial efforts are required to create image-text datasets that encompass diverse ground objects and a rich variety of remote sensing image modalities.

### Challenging and Application-Specific Benchmarks
Most benchmark datasets for model evaluation are limited to a narrow range of remote sensing tasks (e.g., VQA and image captioning), whereas contemporary vision-language models in remote sensing demonstrate versatile capabilities across diverse image analysis tasks. Furthermore, while some benchmarks include multiple tasks, each task typically has a limited number of samples. Such datasets are insufficient for thoroughly testing and comparing the performance of different vision-language models. Beyond general-purpose benchmarks, exploring application-specific benchmarks is also a promising research direction as some efforts have focused on developing versatile vision-language models tailored to specific real-world applications.

