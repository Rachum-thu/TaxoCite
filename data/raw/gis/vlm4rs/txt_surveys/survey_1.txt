276 
 IEE E GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 20252473-2397 © 2025 IEEE. All rights reserved, including rights for text and 
data mining, and training of artificial intelligence and similar technologies.
V
ision-language modeling (VLM) aims to bridge 
the information gap between images and 
natural language. Under the new paradigm 
of first pretraining on massive image-
text pairs and then fine-tuning on 
task-specific data, VLM in the re -
mote sensing (RS) domain has 
made significant progress. 
The resulting models benefit 
from  the absorption of ex -
tensive  general knowledge 
and demonstrate strong 
performance across a vari -
ety of remote sensing data 
analysis tasks. Moreover, 
they are capable of inter -
acting with users in a con -
versational manner. In this 
article, we aim to provide 
the remote sensing com -
munity with a timely and 
comprehensive review of the 
developments in VLM using the 
two-stage paradigm. Specifically, 
we first cover a taxonomy of VLM 
in remote sensing: contrastive learn -
ing, visual instruction tuning, and text-
conditioned image generation. For each 
category, we detail the commonly used net -
work architecture and pretraining objectives. Sec-
ond, we conduct a thorough review of existing works, 
examining foundation models and task-specific adaptation 
methods in contrastive-based VLM, architectural upgrades, 
training strategies, and model capabilities in instruction-
based VLM as well as generative foundation models with 
their representative downstream applications. Third, we 
summarize datasets used for VLM pretraining, fine-tun -
ing, and evaluation, with an analysis of their construction 
methodologies (including image sources and caption gen-
eration) and key properties, such as scale and task adapt -
ability. Finally, we conclude this survey with insights and 
discussions on future research directions: cross-modal rep-
resentation alignment, vague requirement comprehension, 
explanation-driven model reliability, continually scalable 
model capabilities, and large-scale datasets featuring richer 
modalities and greater challenges. 
INTRODUCTION
VLM in remote sensing, aiming to bridge the information 
gap between remote sensing images and natural language, 
facilitates a deeper understanding of remote sensing scene 
Digital Object Identifier 10.1 109/MGRS.2025.3572702 
Date of publication 9 June 2025; date of current version 19 September 2025.
Vision-Language Modeling 
Meets Remote Sensing
Models, datasets, and perspectives XINGXING WENG , CHAO PANG , 
AND GUI-SONG XIA
©SHUTTERSTOCK.COM/RANJITH RAVINDRAN
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
277 
semantics, like the attributes of ground objects and their 
relationship, and enables more natural human interaction 
with intelligent remote sensing data analysis models or 
methods [1], [2]. Since the introduction of remote sensing 
tasks, such as image captioning [3], visual question answer-
ing (VQA) [4], text-image (or image-text) retrieval [5], and 
text-based image generation [6] , VLM in remote sensing 
has achieved significant success, driven by advancements 
in deep learning.
Early works on VLM primarily emphasize the careful 
design of model architectures, followed by supervised 
training from scratch on small-scale datasets. For exam -
ple, in image captioning research, many efforts [7], [8], [9], 
[10] have been made to effectively combine convolutional 
neural networks (e.g., VGG [11] and ResNet [12]) with se -
quential models (e.g., LSTM [13]  and Transformer [14] ) 
before training on UCM-captions [3] and Sydney-captions 
[3] datasets. Under this classical construction paradigm, 
deep models often excel on test datasets but struggle to 
perform satisfactorily in large-scale deployments. More -
over, although these models are capable of describing im -
age content, they fall short when tasked with answering 
questions about the images. In other words, they struggle 
to accomplish related tasks, such as VQA. The task-specific 
nature of these models seriously limits their applicability 
across diverse scenarios.
Recently, a new paradigm of pretraining followed by 
fine-tuning provides a promising solution to address the 
challenges mentioned previously. The core idea is to first 
pretrain a model on massive image-text data, enabling it 
to capture general knowledge that covers a wide range of 
visual and textual concepts, along with their underlying 
correspondence. The pretrained model is then fine-tuned 
on task-specific training data. The integration of general 
knowledge has been shown to enhance the model’s gener-
alization ability in a single task [15], [16] while also making 
it more versatile and capable of handling a variety of down-
stream tasks [17], [18]. Consequently, VLM with this new 
paradigm has emerged as a prominent research focus in the 
field of remote sensing.
To date, significant progress has been achieved, as il -
lustrated in Figure 1 . This includes works based on the 
following:
1)
 con
trastive learning [19], such as GeoRSCLIP [15], Sky-
CLIP [16], and RemoteCLIP [20], which have driven sub-
stantial advancements in various cross-modal tasks and 
zero-shot image understanding tasks
2)
 lea
rning an implicit joint distribution between text and 
images, like RS-SD [15] , DiffusionSat [21] , and CRS-
Diff [22], which allow for image generation from text 
prompts
3) vis
ual instruction tuning [23] , such as GeoChat [18] , 
LHRS-Bot [24], and SkySenseGPT [25] , which have 
demonstrated improved performance, diverse capabili -
ties, and conversational interactions in remote sensing 
data analysis.
Despite these remarkable achievements, it is widely 
acknowledged that VLM remains an open challenge. In -
deed, existing works have not yet achieved the level of re -
mote sensing experts in processing remote sensing data. To 
provide clarity and motivation for further advances in the 
research community, several surveys have reviewed VLM 
in remote sensing. For instance, Li et al. [2] summarize vi -
sion-language models from an application perspective and 
suggest potential research opportunities. However, due to 
time constraints, they primarily concentrate on vision-only 
foundation models and early works. Zhou et al. [26] review 
recent developments but lack an in-depth analysis of key 
designs, which is significant for inspiring future research. 
Moreover, datasets, as a prerequisite of visual-language 
modeling research, have not been given adequate attention 
in existing surveys.
In this work, we aim to provide a timely and compre -
hensive review of the literature, with a focus on VLM based 
on the pretraining and fine-tuning paradigm in the field of 
remote sensing. Specifically, we cover the following: 
1)
 a ta
xonomy of VLM in remote sensing, detailing com -
monly used network architectures and pretraining ob -
jectives for each category
2) the l
atest advancements in contrastive-based, instruc -
tion-based, and generation-based VLM in remote 
sensing, highlighting key designs and downstream  
applications
3)
 pro
gress in datasets for VLM pretraining, fine-tuning, 
and evaluation
4) sev
eral challenges and potential research directions. 
Figure 2 presents an overview of this article.
THE TAXONOMY OF VISUAL-LANGUAGE  
MODELING IN REMOTE SENSING
Under the pretraining and fine-tuning paradigm, VLM in 
remote sensing can be divided into three distinct groups 
based on their strategies for bridging the two modalities in 
the pretraining phase: contrastive learning, visual instruc -
tion tuning, and text-conditioned image generation. In this 
Research Hotspot
Number of Papers
Contrastive-Based VLM
Instruction-Based VLM
Generation-Based VLM
Datasets For VLM
GeoCLIP , CSP ,
GeoChat, SATIN,
DIOR-RSVG, …
GeoRSCLIP,
SkyCLIP ,
EarthGPT,
LHRS-Bot,
DiffusionSat,
CRS-Diff,
VRSBench,
UrBench, …
2022
0
5
10
15
20
25
2023 2024
FIGURE 1. The number of publications on visual-language 
modeling in remote sensing using the pretraining and fine-tuning 
paradigm.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
278 
section, we present commonly used network architectures 
and pretraining objectives within each group.
CONTRASTIVE LEARNING
The motivation behind applying contrastive learning to 
VLM is training a model to map vision and language into 
a shared representation space, where an image and its cor -
responding text share similar representations while differ -
ing from other texts, which was first implemented by the 
pioneering work CLIP [27] in the field of computer vision. 
As illustrated in Figure 3(a), CLIP utilizes two independent 
encoders responsible for encoding visual and textual infor-
mation. Given a batch of N  image-text pairs 
{( ,) },xyii i
N
1=  
the embeddings extracted by the two encoders followed 
by normalization are 
{, }.e ei
I
i
T
i
N
1=  To achieve the similarity 
between visual and textual representations, CLIP is trained 
to maximize the cosine similarity of the embeddings of 
the ith image and the corresponding ith text 
([ ]iN!  with 
[] {, ,, })NN 12 f=  while minimizing the cosine similarity 
of the embeddings of the ith image and jth text (, [] ,ij N!  
).ij!  The loss is computed using InfoNCE [29] as follows:
() (, {} )( ,{ })logl ogN ss ss2
1L ii
I
ij
I
j
N
ii
T
ij
T
j
N
i
N
11
1
InfoNCE vv=- +xx ==
=
/  
 (1)
wh
ere (, )s eeSij
I
i
I
j
T=  and (, )s eeSij
T
i
T
j
I=  are the similar -
ity scores between the image and text embeddings, 
as computed by the function .(, )/ee ee eeS $ ;; ;;=ll l  
(, {} )( /) /( (/ ))exp expss ssij j
N
i j
N
j11vx xR=x ==  is the softmax 
function, which normalizes the similarity score sii
I  or sii
T  
over the corresponding sets {}sij
I
j
N
1=  or {} .sij
T
j
N
1=  x is the 
temperature.
The original CLIP model was trained on 400 million 
image-text pairs collected from the Internet and has dem -
onstrated impressive results across various computer vision 
tasks [30], [31], [32]. These advancements spark interest in 
extending its capability to advance VLM in remote sensing. 
Two primary lines of research have been actively explored. 
The first, following the CLIP learning way, focuses on pre -
training foundation models that are task agnostic but specif-
ically adapted for the remote sensing domain. This includes 
efforts such as constructing large-scale image-text datasets 
[15], [16] and developing novel pretraining objectives [33], 
[34]. The second line explores the effective adaptation of 
pretrained CLIP models toward diverse downstream tasks, 
including image captioning [35], [36], zero-shot scene clas-
sification [37], [38], image-text retrieval [39], [40], etc.
VISUAL INSTRUCTION TUNING
Optimizing a model from scratch for image-text alignment 
is extremely resource intensive due to the need for vast 
amounts of data and computational power. Fortunately, 
many pretrained vision encoders and language models have 
been released. Pretrained vision encoders can provide high-
quality visual representations, while pretrained language 
models, particularly large language models (LLMs), dem -
onstrate advanced language understanding capabilities. As 
a result, recent works increasingly leverage these models to 
achieve image-text alignment through visual instruction 
tuning, as introduced by LLaVA [23]  and MiniGPT-4 [41]. 
Figure 3(b) illustrates a network architecture for this type 
of work, consisting of three key components: a pretrained 
vision encoder, a connector, and an LLM. Specifically, the 
vision encoder compresses remote sensing images into 
compact visual representations, while the connector maps 
image embeddings into the word-embedding space of the 
LLM. The LLM then receives both visual information and 
language instructions to perform reasoning tasks. Different 
from CLIP, which directly takes images and corresponding 
texts as input, this type of work preprocesses image-text 
pairs to instruction-following data. In this setup, each im -
age is accompanied by a question or a language instruc -
tion, requiring the LLM to describe the image, while the 
corresponding text serves as the ground truth for the LLM’s 
Vision-Language Modeling Meets Remote Sensing
Contrastive-Based VLM Instruction-Based VLM Generation-Based VLM Datasets
Foundation Model Construction
Effective Adaptation Methods
Training Strategy Instruction-Following Datasets
Benchmark DatasetsModel Capability
Downstream Applications
Training Datasets Vision Encoder
Vision-Language Connector
Only SFT
Pretraining Followed by SFT
Common Capabilities
Captioning
Conversation Generation
Impressive Datasets
Instruction-Specific Datasets
General-Purpose Datasets
Pansharpening
Zero-Shot Target Recognition
Cloud Removal
Urban PredictionUnique Capabilities
Enhancing Reliability
Improving Diversity
Performance Evaluation
Image Collection
Caption Generation
Dataset Property
Pretraining Objectives
Encoder Architectures
Performance Evaluation
Image Captioning
Cross-Modal Retrieval
Zero-Shot Scene Classification
Dense Prediction Tasks
Broader Scope of Application
Model Architecture Pretraining DatasetsGenerative Foundation Models
FIGURE 2. An overview of this survey.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
279 
predictions. Denote a batch of instruction-following data as 
{( ,, )} ,xq yii i i
N
1=  where qi  is the question associated with the 
ith image. The pretraining objective is defined as follows:
(, ,)logN L Pw xq y11L ,ii
N
j
L
ji ii jVIT
11
i
;=- 1
==
//  (2)
whe
re Li  is the length of the cap -
tion {, ,, },yw wwiL 12 if=  y ,ij1  de -
notes the masked sequence that 
contains only the first j 1-  words, 
and (, ,)Pw xq y ,ji ii j; 1  is the condi -
tional probability of generating the 
caption word 
wj  given the image ,xi  
question ,qi  and the previous words 
y ,ij1  in the caption. During pretrain-
ing, the vision encoder and LLM are 
typically kept frozen, with only the 
parameters of the connector being 
trainable.
In [23] and [41], the authors dem-
onstrated that pretraining with visu -
al instruction tuning can align vision 
and language representations while 
preserving extensive knowledge. 
Since then, advances have been made 
by modifying network architectures 
and creating high-quality pretrain -
ing datasets [24], [42]. In addition to 
improving alignment, another line 
of research focuses on supervised 
fine-tuning (SFT), aiming to enable 
the model to perform a variety of 
remote sensing image analysis tasks 
and interact with users in a conversa-
tional manner. This includes efforts 
to generate task-specific instruction-
following data [18], [25], [43], design 
novel training strategies [42] , [44], 
and incorporate cutting-edge vision 
encoders [44].
TEXT-CONDITIONED IMAGE 
GENERATION
Taking advantage of the advances 
in conditional image generation, a 
group of works [15] , [21], [22] uses 
off-the-shelf generative models, pri -
marily Stable Diffusion [28], to gen-
erate remote sensing images given 
text prompts, which essentially learn 
an implicit joint distribution be -
tween images and texts. As illustrated 
in Figure 3(c), their network architec-
ture comprises three main compo -
nents: a text encoder, a variational 
autoencoder (VAE) [45] , and a denoising U-Net. During 
training, the VAE encoder first transforms an image into 
a latent representation 
.z0  Gaussian noise e is then added 
to this latent representation at different time steps t, result-
ing in ,zz 1tt t0aa e=+ -rr  where tar  is a time-dependent 
scaling factor. Next, the conditioning representation c, 
An Aerial Photo
of Cars on the
Road
Text
Encoder
Language Response: An Aerial Photo of Cars on the Road
Pretrained Large Language Model
Vision
Encoder
VAE
Encoder
z0
z0 zT–1
zT
zT
Text
Encoder
(c)
(b)
(a)
An Aerial
Photo of Cars
on the Road
VAE
Decoder
Diffusion Process
Denoising U-Net
Connector
Language
Instruction: Please
describe this image.
T1
I1T1 I1T2 I1TN
I2T1 I2T2 I2TN
I1
I2
T2 TN
IN
Vision
Encoder
···
···
···
INT1 INT2 INTN···
··· ···
···
···
···
···
······
~
×(T–1)
FIGURE 3. Based on the strategies employed to bridge remote sensing images and natural 
language during the pretraining phase, VLM in remote sensing can be categorized into three 
groups: (a) contrastive learning, (b) visual instruction tuning, and (c) text-conditioned image 
generation. This image is recreated by us based on [23] , [27], and [28]. (a). Contrastive-based 
VLM. (b) Instruction-based VLM. (c) Generation-based VLM. 
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
280 
extracted from the text encoder, is provided as input along-
side zt  to the denoising U-Net, which predicts the noise 
added at time step t . Finally, the VAE decoder upsamples 
the denoised latent representation to reconstruct the input 
image. Based on image-text pairs, the training objective is 
defined as follows:
 (, ,)zt cEL ,, ,zc ttLDM 2
2
0 ee=- ei 7 A (3)
whe
re (, ,)zt ctei  is the model’s predicted noise, parameter-
ized by .i  The application of diffusion models in remote 
sensing has shown rapid development, encompassing 
areas such as remote sensing image generation, enhance -
ment, and interpretation [46] . As this article focuses on 
VLM using diffusion models, we do not attempt to cover 
every instance of their application in remote sensing tasks. 
Instead, we specifically highlight works that integrate text 
conditions with diffusion models. For a more comprehen-
sive overview of the advancements in diffusion models for 
remote sensing, please refer to [46].
Based on the fundamental principles outlined previous-
ly, two major research groups have emerged. The first group 
aims to develop generative foundation models for various 
remote sensing images, including satellite [21], aerial [47], 
hyperspectral [48], and multiresolution images [49] . The 
second group, in contrast, extends text-conditioned diffu -
sion models to specific remote sensing tasks, such as image 
or change captioning [50] , [51], pansharpening [52] , and 
zero-shot target recognition [53].
CONTRASTIVE-BASED VISION-LANGUAGE MODELING
Most existing works on VLM fall into the group of employ-
ing contrastive learning. As mentioned previously, there are 
two main research directions being actively investigated, 
namely foundation model construction  and effective adapta -
tion. Specifically, foundation model construction concerns 
the large domain gap between natural and remote sensing 
images, aiming to learn visual representations with rich re-
mote sensing scene semantics and well aligned with textual 
representations. On the other hand, effective adaptation 
answers the question of how to leverage pretrained CLIP 
models for specific remote sensing tasks. In the following 
sections, we analyze the existing works from these two  
directions.
FOUNDATION MODEL CONSTRUCTION
Table 1 summarizes contrastive-based vision-language 
models in remote sensing. To build foundation models, 
three key components need to be carefully designed: 
training datasets, pretraining objectives, and encoder 
architectures.
TRAINING DATASETS
Large-scale image-text datasets form the basis for construct-
ing foundation models. Ready-made image-text datasets in 
remote sensing, e.g., UCM-captions and Sydney-captions, 
suffer from limited data volume and insufficient image di-
versity, rendering them inadequate for pretraining models 
to capture general knowledge of the domain. Recognizing 
the availability of numerous remote sensing image data -
sets, [15] and [20] use open source datasets as the image 
source and develop image captioning methods to gener -
ate corresponding textual descriptions. Notably, [15] filters 
11 commonly used image-text datasets using RS-related 
keywords and caption three large-scale RS image datas -
ets (Million-AID [67] , fMoW [60], and BigEarthNet [68] ) 
with the aid of the tuned BLIP2 model [69] , resulting in 
the RS5M dataset, which contains more than 5 million 
image-text pairs. Off-the-shelf vision-language models are 
indeed powerful tools for building large-scale image-text 
datasets due to their availability and ease of use, but ensur-
ing captioning accuracy remains a significant challenge. 
To address this, [20] proposes a rule-based method called 
mask-to-box (M2B) and box-to-caption (B2C), which converts 
pixel-wise or bounding box annotations into natural lan -
guage captions. 
Another concern is that the semantic diversity of the 
generated captions is constrained by the limited number 
of predefined classes in open source remote sensing image 
datasets. Given this, Wang et al. [16]  attempt to leverage 
rich semantic information contained in OpenStreetMap 
(OSM), allowing the textual descriptions to encompass 
not only a wide variety of object categories but also fine-
grained subcategories and object attributes. Similar to 
[20], captions are assembled from object tags following 
predefined rules.
PRETRAINING OBJECTIVES
Instead of creating large-scale datasets, several works 
[33], [34], [55] explore new training objectives to fa -
cilitate model pretraining with a few remote sensing 
image-text pairs. For instance, S-CLIP [55]  introduces 
caption-level and keyword-level pseudolabels to fine-
tune the original CLIP model on massive unpaired re -
mote sensing images alongside a few image-text pairs. 
Denote a large number of unpaired images as 
{}ui i
M
1=  
( ).MN&  The caption-level pseudolabel q Ri
c N!  is based 
on the assumption that the semantics of an unpaired im -
age ui  can be represented as a combination of those of 
paired images. Thus, qi
c  represents a probability distribu -
tion over the captions of N  paired images, derived from 
the relationships between unpaired and paired images, 
which are formulated as an optimal transport problem. 
The keyword-level pseudolabel 
q Ri
k K!  relies on the as -
sumption that ui  shares keywords with visually similar 
images, representing the similarity between the embed -
dings of ui  and the keywords (drawn from the nearest 
paired image) {} .kj j
K
1=  The loss functions for two pseudo -
labels are defined as follows:
 (, {} )logM qs s1L ij
c
j
N
i
M
ij
U
il
U
l
N
1 1
1caption
v=- x
= =
=/ /  (4)
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
281 
 (, {} ).logM qs s1L ij
k
j
K
i
M
ij
U
il
U
l
K
1 1
1keyword
v=- x
= =
=/ /  (5)
In (4), (, )s eeSil
U
i
U
l
T=  refers to the similarity score of the un-
paired image embedding ei
U  and the caption embedding 
el
T
 of the paired image. Meanwhile, in (5) , (, )s eeSil
U
i
U
l
k=  
denotes the similarity score between ei
U  and the keyword-
embedding .el
k  Since few image-text pairs are involved 
in the fine-tuning process, the overall training objective 
also includes the InfoNCE loss, as shown in the following 
equation:
 () .2
1LL LLSC LIPI nfoNCE captionk eyword- =+ +  (6)
With a si
milar training data setup consisting of mas -
sive unpaired images and texts {, }uvii i
M
1=  and limited 
image-text pairs {( ,) },xyii i
N
1=  Set-CLIP [34]  transforms 
the representation alignment between images and texts 
into a manifold matching problem, developing a mul -
tikernel maximum mean discrepancy loss 
LMK MMD - 
and a semantic density distribution loss .LSDD  The loss 
LMK MMD - constrains the consistency of whole representa -
tion distributions of images and texts, thereby achieving 
macrolevel alignment, as formulated in the following 
equation:
 () ()MN MN
11 eeL
i
MN
i
I
i
MN
i
T
11
2
MK MMD -
HRKHS
zz= + - +=
+
=
+
//  
 (7)
wher
e (·)z  is a linear combination of Gaussian and Poly-
nomial kernel functions, used to map image and text 
TABLE 1. A SUMMARY OF CONTRASTIVE-BASED VISION-LANGUAGE FOUNDATION MODELS IN REMOTE SENSING. 
MODEL 
VISION 
ENCODER TEXT ENCODER 
TRAINING 
DATASET 
PRETRAINING 
OBJECTIVE 
EVALUATION TASK PUBLIC
GeoRSCLIP [15] a ViT-B/32,  
ViT-H/14 [54] 
Transformer [14] RS5M [15]  InfoNCE loss [29]  Zero-shot scene classification 
Image-text retrieval 
Semantic localization
✓
RemoteCLIP 
[20]
b
ResNet-50 [12], 
ViT-B/32,  
ViT-L/14 [54] 
Transformer [14] RET-3+SEG-
4+DET-10 [20] 
InfoNCE loss [29]  Zero/few-shot scene classification 
Image-text retrieval 
Object counting 
Linear/k-NN classification
✓
SkyCLIP [16]
c ViT-B/32,  
ViT-L/14 [54] 
Transformer [14] SkyScript [16]  InfoNCE loss [29]  Zero-shot scene classification 
Image-text retrieval 
Zero-shot fine-grained classifica -
tion
✓
S-CLIP [55]
d ResNet-50 [12], 
ViT-B/32,  
ViT-B/16 [54] 
Transformer [14] RS-ALL [55]  
NWPU-RESISC45 
[56] 
InfoNCE loss [29]  
Pseudolabel losses 
[55] 
Zero-Shot scene classification 
Image-text retrieval
✓
Set-CLIP [34]  ResNet [12]  Transformer [14] RS-ALL [55]  InfoNCE loss [29]  
SSL loss [57]
MK-MMD, SDD 
losses [34]  
Zero-shot scene classification 
Image-text retrieval
✗
GRAFT [33]
e ViT-B/16 [54] — Internet-NAIP 
 Data
set [33] 
Internet-Senti -
nel-2 Dataset [33]  
Image/Pixel-level 
contrastive losses 
[33] 
Zero-shot scene classification 
Zero-shot text-to-image retrieval 
Zero-Shot semantic segmentation 
Zero-shot VQA
✓
CSP [58]
f ResNet-50 [12] Sinusoidal 
transform+FcNet [59] 
fMoW [60] InfoNCE loss [29]  Few-shot geo-aware scene clas -
sification
✓
GeoCLIP [61]
g ViT-L/14 [54] Random Fourier 
features+MLP [62]  
MP-16 [63] InfoNCE loss [29]  
SSL loss [57]  
Image geo-localization 
Text-query geo-localization 
Geo-aware image classification
✓
SatCLIP [64]
h ResNet-18, 
ResNet-50 [12], 
ViT-S/16 [54] 
Spherical 
harmonics+Siren [65] 
S2-100K [64] InfoNCE loss [29]  Air temperature prediction 
Elevation prediction 
Median income estimation 
California housing price estimation 
Population density estimation 
Biome classification 
Ecoregion classification 
Country code classification 
Species recognition
✓
PIR-CLIP [66]
i ViT-B/32 [54] + 
ResNet-50 [12] 
Transformer [14] RS5M [15]  InfoNCE loss [29]  
Affiliation loss [66]  
Image-text retrieval ✓
ahttps://huggingface.co/Zilun/GeoRSCLIP; bhttps://github.com/ChenDelong1999/RemoteCLIP; chttps://github.com/wangzhecheng/SkyScript; dhttps://github.com/alinlab/s-clip; ehttps://
graft.cs.cornell.edu/; fhttps://gengchenmai.github.io/csp-website/; ghttps://github.com/VicenteVivan/geo-clip; hhttps://github.com/microsoft/satclip; ihttps://github.com/jaychempan/
PIR- CLIP.
Image-text retrieval involves image-to-text and text-to-image retrieval tasks. In the “ Vision Encoder ” column, we separate different architectures of vision encoders with commas, 
 whereas
 in [66], the vision encoder is a combination of two architectures. “ Public” refers to the availability of both code and model weights. S-CLIP has only open-sourced its code.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
282 
embeddings ,ei
I  ei
T  into Reproducing Kernel Hilbert Space 
(RKHS) .HRKHS  Meanwhile, the loss LSDD  refines the align-
ment between the two modalities by ensuring that their 
probability density distributions remain similar in the rep-
resentation space. It can be formulated as follows:
 (( ,) (, ))IT TI2
1LSDD CC=+  (8)
whe
re {}I ei
I
i
MN
1= =
+  and {}T ei
T
i
MN
1= =
+  refer to embedding 
distributions of images and texts, respectively. (·,·)C  rep-
resents the Kullback-Leibler divergence, which measures 
the dissimilarity between two distributions. The format of 
(, )ITC  is given by the following:
 (, )
(, )
(, )
(, )/ (, )
(, )/ (, )
logIT
I
I
TT
II
e
e
ee
ee
j
I
MN
i
I
i
MN
i
I
MN
j
I
i
I
j
I
MN
jj
j
1
1
1
1
l
l
ll
ll
C =
=
+
=
+
=
+
=
+
/
/
/
/
 (9)
wit
h (, )TIC  defined similarly by swapping the roles of I 
and T. Here, (·,·)l  is an exponential probability density 
function that estimates the density value for each embed -
ding in the corresponding distribution. Additionally, the 
self-supervised contrastive loss LSSL  is introduced to obtain 
robust feature representations for each modality indepen -
dently, defined as follows:
 
(( ,) /)
(( ,) /)log exp
expMN
1
ee
eeL
S
S
i
MN
j
MN
ij
i i
1 1
SSL
x
x=- + =
+
=
+
+
/ /  (10)
wh
ere ei  is an embedding, and ei
+  is the representation of 
the positive sample generated by augmentation techniques. 
Ultimately, the overall training objective is formulated as 
follows:
 LL LLLSetC LIPI nfoNCE SS LM KM MD SDD-- an dh=+ ++  (11)
whe
re ,a  ,n  ,d  and h are tunable hyperparameters to bal -
ance the loss terms.
Even a small set of image-text pairs requires expert 
knowledge to craft textual descriptions. To avoid textual 
annotations entirely, GRAFT [33]  proposes utilizing colo -
cated ground images as the bridge between satellite images 
and language. In doing so, a dataset of 2 million ground-
satellite image pairs, denoted as 
{, {} },xgii j j
N
i
N
1 1
i
= =  is collect-
ed to support model training. Building on this, a feature 
extractor is designed to map satellite images to the repre -
sentation space of the CLIP model, which was trained on 
Internet image-text pairs. Since a satellite image 
xi  can cov-
er a large ground area and thus be associated with multiple 
ground images 
{} ,gij j
N
1
i
=  the extractor is optimized using the 
following loss:
 
(( ,) /)
(( ,) /)log
exp
exp
NN
11
ee
ee
S
L S
ii
N
j
N
b
N
a
N
i
I
ab
G
i
I
ij
G
11
1 1
GRAFT
i
a
x
x=-
==
= =
//
/ /
 (12)
whe
re eij
G  is the embedding of the j th ground image corre-
sponding to the ith satellite image. This loss focuses solely 
on aligning image-level representations between the two 
types of images while ignoring the fact that a ground im -
age 
gij  can be mapped to a specific pixel location pij  in the 
corresponding satellite image .xi  Thus, GRAFT trains the 
feature extractor with an additional pixel-level contrastive 
loss as follows:
 
(( ,) /)
(( ,) /)log
exp
exp
NN
11
ee
eeL
S
S
ii
N
j
N
b
N
a
N
ij
P
ab
G
ij
P
ij
G
11
1 1
GRAFT
i
a
x
x=-
==
= =
//
/ /
 (13)
whe
re eij
P  denotes the feature vector for pixel .pij
ENCODER ARCHITECTURES
Unlike natural images, where corresponding textual infor -
mation typically describes the image content, the textual 
information for remote sensing images can be represented 
by their geographic coordinates (longitude and latitude), 
which are beneficial for tasks such as scene classification 
and object recognition [60] . Therefore, some works pro -
pose aligning representations from remote sensing images 
and their geographic coordinates, with particular atten -
tion given to the choice of location encoders [58] , [61], 
[64]. A location encoder generally consists of a nonpara -
metric functional positional encoding combined with a 
small neural network. In CSP [58] , an existing 2D loca -
tion encoder, namely Space2Vec’s grid proposed in [59], is 
utilized. This encoder maps image coordinates into high-
dimensional representations using sinusoid transforms for 
position encoding, followed by fully connected rectified 
linear unit layers. GeoCLIP [61] , on the other hand, first 
applies equal Earth projection to the image coordinates to 
reduce distortions inherent in standard geographic coordi-
nate systems. It then adopts random Fourier features [62]  
to capture high-frequency details, varying the frequency to 
construct hierarchical representations. These hierarchical 
representations are processed through separate multilayer 
perceptrons (MLPs), followed by element-wise addition, 
resulting in a joint representation. This design enables 
the location encoder to effectively capture the features of 
a specific location across multiple scales. In SatCLIP [64] , 
the authors use a location encoder that combines spherical 
harmonics basis functions with sinusoidal representation 
networks to map geographic coordinates into latent repre-
sentations [65].
In addition to adapting encoders for different types of 
textual information, another purpose of adjusting encod -
ers is to improve the representations of visual concepts. 
Remote sensing images typically cover a large field of view 
and include a variety of objects. However, the correspond-
ing textual descriptions often center on specific objects of 
interest and their relationships. Semantic noise, such as ir-
relevant objects and background, can interfere with the rep-
resentation of key content in images, thereby obstructing 
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
283 
the alignment between image and text representations. In 
[66], this problem is addressed by using prior knowledge 
of remote sensing scenes to instruct the pretrained model 
in filtering out semantic noise before calculating the simi -
larity between image and text embeddings. This is practi -
cally achieved by adding an instruction encoder and a 
transformer encoder layer on top of the vision encoder. 
The instruction encoder, pretrained on the scene classifi -
cation dataset AID [70], generates instruction embeddings, 
which are used to filter image embeddings via a soft belief 
strategy. The filtered embeddings are then activated using 
instruction information through the transformer encoder 
layer, producing relevant image embeddings for subsequent 
alignment.
PERFORMANCE EVALUATION
Zero-shot scene classification and image-text retrieval are 
commonly used tasks to assess foundation models’ capa -
bilities to capture a wide range of visual and textual con -
cepts, along with their correspondence. Table 2 lists the 
performance of contrastive-based vision-language founda-
tion models on these two tasks. Note that it shows the best 
model performance in the original articles. For zero-shot 
scene classification, three conclusions can be drawn: 
1)
 AID [7
0] is the most widely used dataset for evaluation, 
with RemoteCLIP achieving the best accuracy of 87.9%.
2) Usi
ng the same vision encoder (ViT-L/14), SkyCLIP, 
which is pretrained on images collected from Google 
Earth, outperforms RemoteCLIP only on PatternNet 
[74], a dataset sourced from Google Earth.
3)
 Com
paring models pretrained on limited image-text 
pairs (i.e., S-CLIP, SetCLIP, and GRAFT) with Remote -
CLIP, their performance is somewhat comparable, 
showing the importance of exploring data-effective pre-
training. For cross-modal retrieval, PIR-CLIP achieves 
state-of-the-art performance on RSICD [78]  and RSIT -
MD [82]. In comparison, S-CLIP and SetCLIP perform 
significantly worse, with a performance gap exceeding 
20%. This suggests that existing foundation models 
pretrained on limited image-text pairs are capable of ac-
quiring general visual and textual concepts but may face 
challenges in learning the relationships between them. 
EFFECTIVE ADAPTATION METHODS
Inspired by the outstanding performance of foundation 
models, numerous methods have been developed to effec-
tively adapt pretrained models for specific remote sensing 
tasks, including image captioning, cross-modal retrieval, 
zero-shot classification, dense prediction, etc. This section 
presents the development of adaptation methods within 
the context of different downstream tasks as summarized 
in Table 3.
IMAGE CAPTIONING
Image captioning aims to describe the content of a given 
image using natural language. Vision-language foundation 
models are well suited for this task as they align image and 
text representations. One can use the vision encoder of 
these models to extract representations of image content, 
which prepares the input for language models to generate 
captions [36], [83], [85]. Commonly used vision encoders 
for remote sensing image captioning are from CLIP, while 
language models for this task include GPT-2 [105] , BERT 
[106], and OPT [107]. To improve caption accuracy, a few 
works are devoted to enhancing models’ visual representa-
tion capabilities or further aligning images and texts.
For example, regarding representation enhancement, 
VLCA [83] introduces external attention [84]  to capture 
potential correlations between different images, improv -
ing visual representations. MVP [36]  fuses visual repre -
sentations from pretrained vision-language models and 
pretrained vision models through stacked transformer 
encoders. It also leverages CLIP’s vision encoder, followed 
by adaptive average pooling layers, to generate visual pre -
fixes, which are concatenated with token embeddings. A 
BERT-based caption generator is subsequently developed 
to combine the fused visual representations and concat -
enated embeddings, enabling the generation of accurate 
captions. In contrast, BITA [85]  focuses on improving the 
alignment of images and texts in the remote sensing do -
main by introducing an interactive Fourier transformer. 
In this design, learnable visual prompts are fed to the 
Fourier layer and interact with image embeddings from a 
pretrained vision encoder, capturing the most relevant vi -
sual representations. Through contrastive learning, these 
visual representations are aligned with textual represen -
tations, also extracted by the Fourier-based transformer. 
The interactive Fourier transformer then connects the fro -
zen vision encoder with the frozen language model, le -
veraging the language model’s generation and reasoning 
capabilities.
Beyond improving caption accuracy, generating detailed 
captions has also been explored [86] , [87]. In [86], a two-
stage instruction fine-tuning is proposed for the vision-to-
language mapping layer to generate geographically detailed 
captions. The first stage, guided by the instruction “[region] 
Based on the provided region of the remote sensing image, 
describe the basic attributes of the main objects in that re -
gion,” aligns geographic object regions with their attribute 
descriptions. The second stage, guided by the instruction 
“[image] Please provide a detailed description of this im -
age,” focuses on understanding the spatial distribution of 
geographic objects within images. In [87] , image caption-
ing is defined as the aggregation of information from mul-
titurn dialogues, where each turn serves to query the image 
content. In each turn, CLIP’s vision encoder extracts im -
age features, which are then input into an auto-regressive 
language model [105] along with previous questions, an -
swers, and the current question to generate the response. 
After several dialogue turns, GPT-3 [108]  summarizes the 
dialogue information to produce an enriched textual image 
description.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
284 
TABLE 2. THE PERFORMANCE OF CONTRASTIVE-BASED VISION-LANGUAGE FOUNDATION MODELS.
ZERO-SHOT SCENE CLASSIFICATION: TOP-1 ACCURACY
MODEL 
VISION 
ENCODER 
AID 
[70] 
EuroSAT 
[71] 
fMoW 
[60] 
MILLION-
AID [67] 
MLRSNet 
[72] 
RESISC 
[56] 
OPTIMAL- 
31 [73] 
PatternNet 
[74] 
RSSCN7 
[75] 
RSC11 
[76] 
RSI-
CB128 
[77] 
RSI-
CB256 
[77] 
RSICD 
[78] 
SIRI-
WHU 
[79] 
UCM 
[80] 
WHU-
RS19 
[81] 
SkyCLIP [16]  ViT-L/14 [54] 71.7 51.33 27.12 67.45 — 70.94 — 80.88 — — — 50.09 — — — —
Set-CLIP [34]  ResNet-50 [12]  76.2 — — — — — — — 66.2 — — — 69.2 — 67.5 89
GRAFT [33]  ViT-B/16 [54] — 63.76 — — — — — — — — — — — — — —
GeoRSCLIP [15]  ViT-H/14 [54] 76.33 67.47 — — — 73.83 — — — — — — — — — —
S-CLIP [55]  ViT-B/16 [54] 85.2 — — — — — — — 76.3 — — — 79.5 — 82.3 93.9
RemoteCLIP [20]  ViT-L/14 [54] 87.9 59.94 — — 66.32 79.84 90.05 68.75 72.32 74.9 37.22 52.82 — 70.83 — 94.66 
IMAGE-TEXT RETRIEVAL: RECALL@1, RECALL@5, AND RECALL@10
MODEL 
VISION 
ENCODER
RSICD [78] RSITMD [82] UCM-captions  [3] Sydney-captions [3] 
R@1 R@5 R@10 Mean R@1 R@5 R@10 Mean R@1 R@5 R@10 Mean R@1 R@5 R@10 Mean
IMAGE-TO-TEXT RETRIEVAL
S-CLIP [55]  ResNet-50 [12]  4.2 18.4 — — — — — — 11.6 45.7 — — 14.9 50 — —
Set-CLIP [34]  ResNet-50 [12]  — 19.6 — — — — — — — 46.3 — — — 51.1 — —
SkyCLIP [16] † ViT-L/14 [54] — — — 23.7 — — — 30.75 — — — 72.22 — — — —
RemoteCLIP [20]  ViT-L/14 [54] 18.39 37.42 51.05 35.62 28.76 52.43 63.94 48.38 19.05 54.29 80.95 51.43 — — — —
GeoRSCLIP [15]  ViT-B/32 [54]  21.13 41.72 55.63 39.49 32.3 53.32 67.92 51.18 — — — — — — — —
PIR-CLIP [66]  ViT-B/32 [54]  
+ResNet-50 [12] 
27.63 45.38 55.26 42.76 45.58 65.49 75 62.02 — — — — — — — —
TEXT-TO-IMAGE RETRIEVAL
S-CLIP [55]  ResNet-50 [12]  4.2 16.8 — — — — — — 11.1 43.5 — — 17.8 55.1 — —
Set-CLIP [34]  ResNet-50 [12]  — 17.4 — — — — — — — 44.1 — — — 55.2 — —
SkyCLIP [16] † ViT-L/14 [54] — — — 19.97 — — — 30.58 — — — 59.33 — — — —
RemoteCLIP [20]  ViT-L/14 [54] 14.73 39.93 56.58 37.08 23.76 59.51 74.73 52.67 17.71 62.19 93.9 57.93 — — — —
GeoRSCLIP [15]  ViT-B/32 [54]  15.59 41.19 57.99 38.26 25.04 57.88 74.38 52.43 — — — — — — — —
PIR-CLIP [66]  ViT-B/32 [54]  
+ResNet-50 [12] 
21.1 44.87 56.12 40.7 30.13 55.44 68.54 51.37 — — — — — — — —
RESISC: NWPU-RESISC45 [56] . Mean refers to the mean recall average of recall@1 (R@1), recall@5 (R@5), and recall@10 (R@10). †indicates that the training data corresponding to the test dataset were not used to pretrain the model. Best results are 
highlighted in bold. 
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
285 
TABLE 3. SUMMARY OF CONTRASTIVE-BASED VISION-LANGUAGE FOUNDATION MODELS APPLIED TO REMOTE SENSING TASKS.
WORK TASK 
VISION-LANGUAGE FOUNDATION 
MODEL ADAPTATION 
VLCA [83]  Image captioning CLIP ResNet-50, ViT [27]  Utilize external attention [84]  to enhance visual representations extracted 
by CLIP’s vision encoder.
MVP [36] Image captioning CLIP ViT-B/16 [27]  Fuse visual representations from CLIP and pretrained vision models.
BITA [85] Image captioning CLIP ViT-L/14 [27]  Introduce the Fourier-based transformer to align images and texts in the 
remote sensing domain.
MGIMM [86]  Image captioning CLIP ViT-L/14 [27]  Use region-level and image-level instruction tuning for generating 
captions with detailed geographical information.
CFD-C [87]  Image captioning CLIP ViT-B/16 [27]  Define image captioning as the aggregation of information from multiturn 
dialogues.
Prompt-CC 
[88] 
Change captioning CLIP ViT-B/32 [27]  Develop an image-level classifier, a feature-level encoder on top of the 
pretrained vision encoder for detecting the presence of changes and 
extracting discriminative features.
CISEN [40] Image-text retrieval CLIP ResNet-50, ViT-
B/32+Transformer [27]  
GeoRSCLIP ViT-B/32+Transformer 
[15] 
Use two-stage training to progressively fuse fine-grained semantic 
features to enrich visual representation.
KTIR [89]  Image-text retrieval BLIP ViT-B/16+BERT [90]  Incorporate knowledge into text features for better image-text alignment.
Zavras et al. 
[39] 
Zero-shot RGB-
multispectral 
retrieval 
Zero-shot scene 
classification
CLIP ViT-B/32, ViT-B/16, ViT-L/14, 
ViT-L/14@336 [27]  
Use two-stage fine-tuning to achieve alignment between distinct image 
modalities in the CLIP representation space.
WEICOM [91] Composed-to-Image 
retrieval
CLIP ViT-L/14+Transformer [27] 
RemoteCLIP ViT-
L/14+Transformer [20]  
Use a weighting parameter for combining image-query and text-query 
retrieval results, achieving a composed query.
Rahhal et al. 
[37] 
Zero-shot scene 
classification
Thirteen CLIP/Open-CLIP Models 
[27] 
Use tuning prompts by adding task-relevant context words, including 
“remote sensing image” and “scene.”
Lan et al. [92] Few-shot fine-
grained ship 
classification
CLIP ResNet-50, ViT-
B/16+Transformer [27]  
Inject domain priors to adapt frozen vision encoder to remote sensing 
scenes.
Design hierarchical learnable prompts to capture rich task-specific 
knowledge.
RS-CLIP [35]  Zero-shot scene 
classification
CLIP ViT-L/14+Transformer [93] Use pseudolabeling and curriculum learning to fine-tune the pretrained 
model in multiple rounds.
DSVA [38] Zero-shot scene 
classification
CLIP ViT-B/32+Transformer [27]  Employ the pretrained model to annotate attributes for each class.
Text2Seg [94]  Zero-shot semantic 
segmentation
CLIP ViT-B/16+Transformer [27]  Employ CLIP to endow segmentation masks with semantic categories.
Lin et al. [95]  Semantic 
segmentation
CLIP ViT-B/16+Transformer [27]  Modify CLIPSeg decoder [96]  to receive CLIP joint image-text embeddings 
as input and generate segmentation masks.
SegEarth-OV 
[97] 
Open-vocabulary 
semantic 
segmentation
CLIP ViT-B/16+Transformer [27] Develop SimFeatUp to robustly upsample low-resolution CLIP 
embeddings. 
Execute subtraction operations between patch embeddings and the class 
token embedding to alleviate global bias.
SCM [98] Unsupervised 
change detection
CLIP [27] Employ CLIP to identify objects of interest in bitemporal images, helping 
filter out pseudo-changes.
ChangeCLIP 
[99] 
Change detection CLIP ResNet-50, ViT-
B/16+Transformer [27]  
Employ CLIP to construct and encode multimodal input data for change 
detection. 
Design a transformer-based decoder to combine vision-language features 
with image features, predicting change maps.
BAN [100] Change detection CLIP ViT-B/16, ViT-L/14 [27] 
RemoteCLIP ViT-B/32, ViT-L/14 [20] 
GeoRSCLIP ViT-B/32, ViT-L/14 [15] 
Design bridging modules to inject general knowledge from foundation 
models into existing change detection models.
Bazi et al. 
[101] 
VQA CLIP ViT+Transformer [27]  Employ CLIP to encode images and questions.
Czerkawski  
et al. [102] 
Cloud presence 
detection
CLIP [27] Apply CLIP to this task via tuning prompts or adding linear classifier.
TGN [103] Text-based image 
generation
CLIP [27] Utilize CLIP to classify generated images, ensuring semantic class 
consistency.
AddressCLIP 
[104] 
Image address 
localization
CLIP ViT-B/16+Transformer [27]  Align images with scene captions and address texts by contrastive learning. 
Introduce image-geography matching to constrain image features with the 
spatial distance.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
286 
Unlike previous works that caption a single image, 
Prompt-CC [88] utilizes pretrained models to describe 
differences between bitemporal images, a task known as 
change captioning. Based on bitemporal visual representa -
tions from CLIP, Prompt-CC introduces an image-level 
classifier to detect the presence of changes and a feature-
level encoder to extract discriminative features that identify 
the specific changes.
CROSS-MODAL RETRIEVAL
Cross-modal retrieval is the task of retrieving data from 
one modality by using queries from another modality. 
Based on vision-language foundation models, there is 
ongoing research that explores retrieval between images 
and text [40] , [89], [91] as well as between images cap -
tured with different imaging parameters [39] . Image-text 
retrieval involves the challenges of finding textual de -
scriptions that correspond to given image queries and vice 
versa. Taking advantage of advances in vision-language 
foundation models, existing works use pretrained encod -
ers to encode images and text separately, mapping them 
into a shared representation space for similarity measure -
ment. Typically, CISEN [40]  adopts encoders from CLIP 
and GeoRSCLIP as the backbone for its retrieval network. 
Given the abundance of objects in remote sensing im -
ages, CISEN is trained in two stages to enhance visual 
representation. In the first stage, an image adapter [109]  
is trained to map global visual features into textual-like 
features. In the second stage, a feature pyramid network is 
used to integrate the textual-like features into local visual 
features, which are then utilized to enrich global visual 
features. Rather than focusing on effective representation, 
KTIR [89] aims to improve alignment by explicitly incor -
porating knowledge into text features. The knowledge 
is derived from textual information using off-the-shelf 
knowledge sources like RSKG [110] and ConceptNet [111]. 
Once converted to knowledge sentences, the knowledge 
is processed by the text encoder to extract knowledge fea -
tures, which are then fused with text features via a single 
cross-attention layer.
Image-image retrieval involves searching for relevant 
images across different imaging parameters, such as match-
ing red, green, blue (RGB) images with multispectral im -
ages, as explored in [39]. The authors use a pretrained CLIP 
text encoder as the classification head and fine-tune the 
CLIP vision encoder alongside a newly added multispectral-
specific encoder. The training is conducted in stages; first, 
the CLIP vision encoder is fine-tuned on RGB images com-
posited from multispectral images, followed by fine-tuning 
the new encoder on multispectral images. This encoder is 
guided to produce discriminative representations that can 
achieve accurate image classification while ensuring that its 
representations are similar to those of RGB images extract-
ed by the CLIP vision encoder.
Considering the complexity of Earth’s surface, single-
modality queries, such as text queries, require users to fully 
articulate their needs to pinpoint relevant images. This 
raises the barrier to using retrieval models. To address this 
issue, Psomas et al. [91] introduce the composed-to-image 
retrieval task, which searches for remote sensing images 
based on a composed query of image and text. The retrieved 
images share the same scene or object category as the image 
query and reflect the attribute defined by the text query. To 
achieve this, the authors in [91]  propose calculating simi -
larity scores for the image and text query separately and 
then normalizing and combining these scores using a con-
vex combination controlled by a weighting parameter 
m,  
which adjusts the contribution of each modality. As shown 
in Figure 4, when 
, 0m =  composed-to-image retrieval sim-
plifies to image-to-image retrieval, while at , 1m =  it simpli-
fies to text-to-image retrieval.
ZERO-SHOT SCENE CLASSIFICATION
Zero-shot scene classification challenges models to iden -
tify remote sensing images from scene categories that were 
not seen during training. The idea of applying vision-lan -
guage foundation models to this task is straightforward; 
since these models align images and texts, zero-shot scene 
classification can be achieved by comparing image embed-
dings with text embeddings extracted by the text encoder, 
Query Image Image Only: /lambda.0 = 0T ext Only: /lambda.0 = 1 Query Text
Concrete
Dense
/lambda.0 = 0.5 /lambda.0 = 0.75 /lambda.0 = 0.95
FIGURE 4. An illustration of composed image retrieval for remote sensing [91] .
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
287 
which takes as input textual descriptions specifying the 
unseen classes. By default, these descriptions follow the 
format “a photo of a [CLASS],” where the class token is 
replaced by the specific class name, such as “farmland,” 
“forest,” or “playground.” The input text, also known as 
the prompt, plays a crucial role in the performance of foun-
dation models on downstream tasks. As a result, several 
works [35], [37] pay attention to prompt tuning and sug -
gest adding task-relevant context words to improve perfor-
mance. An example is provided by [37], where the authors 
replace “photo” with terms such as “remote sensing im -
age,” “top view image,” “satellite image,” and “scene.” This 
slight modification results in more than a 5% increase in 
the accuracy of the CLIP model with the ViT-B/32 vision 
encoder on the UCM dataset [80] . However, as noted in 
other experiments from [37] , prompts subjected to exten -
sive tuning are not guaranteed to be optimal for the task 
and may not be suitable for other test datasets. 
To avoid manual prompt tuning, Lan et al. [92]  model 
context words in a prompt as learnable vectors, which are 
combined with the class token embeddings before being 
input to the text encoder. Moreover, the class token em -
beddings for each category are organized across multiple 
levels of granularity, taking the form: “a photo of a ship, 
the primary type is [CLASS1], secondary type is [CLASS2], 
final type is [CLASS3],” aimed at the task of few-shot fine-
grained ship classification. Compared to hand-crafted 
prompts, these hierarchical learnable prompts incorporate 
richer task-specific knowledge.
Most works employ and freeze pretrained CLIP mod -
els. However, due to the significant domain gap between 
web images and remote sensing images, their performance 
tends to be limited. To mitigate this, one can inject remote 
sensing domain priors into the vision encoder [92] or fine-
tune the entire model using pseudolabeling techniques 
[35]. Typically, Lan et al. [92]  introduce a lightweight net -
work that is trained on data from seen classes to capture 
the domain prior. This prior is then combined with im -
age embeddings output by the vision encoder, allowing 
the pretrained model to adapt better. On the other hand, 
RS-CLIP [35] leverages the strong transferability of the pre-
trained model to automatically generate pseudolabels from 
unlabeled images, which are used to fine-tune the model. 
Additionally, a curriculum learning strategy is developed 
to gradually select more pseudolabeled images for model 
training in multiple rounds, further boosting the model’s 
performance in zero-shot scene classification.
In addition to comparing the similarity between embed-
dings of images and unseen-class texts, DSVA [38]  intro -
duces a solution that uses pretrained models to annotate 
attributes for each scene class and predict scene categories 
by evaluating the similarity between attribute values de -
rived from image embeddings and those associated with 
scene classes. For automatic attribute annotation, textual 
descriptions have the form of “This photo contains [ATTRI-
BUTE],” where the attribute token is replaced by specific 
attribute names, such as “red,” “cement,” or “rectangle.” 
Meanwhile, the attribute value for each class is calculated 
by measuring the similarity between embeddings of attri -
bute text and images belonging to that class.
DENSE PREDICTION TASKS
Dense prediction tasks, such as semantic segmentation 
and change detection, which produce pixel-level predic -
tions for input images, have recently benefited from the 
application of vision-language foundation models. Typi -
cally, Text2Seg [94] uses a CLIP model to classify category-
agnostic segmentation masks generated by the Segment 
Anything Model (SAM) [112] , enabling zero-shot seman -
tic segmentation of remote sensing images. Lin et al. [95]  
instead modify the CLIPSeg decoder [96]  to receive joint 
image-text embeddings from the CLIP model as input, 
producing a binary segmentation mask. The decoder is 
composed of transformer blocks, convolution layers, and 
linear interpolation layers.
Without the need for additional decoders or segmenta -
tion models, one can perform an upsampling operation on 
image embeddings from CLIP and compare the similarity 
between image patch embeddings and text embeddings to 
produce segmentation results. However, empirical findings 
in [97] suggest that for CLIP with a vision encoder based 
on ViT-B/16, the image embeddings are downsampled to 
1/16 of the input image size, This downsampling leads to 
distorted object shapes and poorly fitting boundaries in 
segmentation masks. Furthermore, CLIP’s self-attention 
causes global information from the class token embedding 
to be attached to the patch embeddings, which signifi -
cantly degrades performance in semantic segmentation. To 
handle these issues, SegEarth-OV [97] incorporates SimFea-
tUp on top of the CLIP vision encoder to restore lost spatial 
information in image embeddings. Subsequently, subtrac-
tion operations are executed between patch embeddings 
and the class token embeddings before similarity mea -
surement, alleviating global bias in patch embeddings. In 
particular, SimFeatUp is a learnable upsampler consisting 
of a single parameterized Joint Bilateral Upsampling (JBU) 
operator. It is trained with a frozen CLIP model, a learnable 
downsampler, and a lightweight content retention network. 
The training objective is to ensure that the image embed -
dings, after the up-down-sampling process, remain similar 
to those from CLIP. Meanwhile, the image generated by the 
content retention network, which takes the upsampled im-
age embeddings as input, should closely resemble the input 
image to CLIP.
When it comes to change detection, similar adaptation 
strategies used in segmentation tasks, such as combining 
SAM with CLIP and designing decoders, are applied. For 
instance, in SCM [98], CLIP is applied after SAM to identify 
objects of interest in bitemporal images, helping to filter 
out pseudo-changes. In ChangeCLIP [99] , CLIP is used to 
construct and encode multimodal input data for change de-
tection tasks, while a transformer-based decoder combines 
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
288 
bitemporal vision-language features with image features to 
produce change maps. To be specific, the authors predefine 
56 common land cover categories in remote sensing im -
ages and use the CLIP model to identify bitemporal images 
from these categories. Each image is paired with textual 
descriptions of the foreground and background, format -
ted as “remote sensing image foreground objects, [Predict 
Classes]” and “remote sensing image background objects,” 
respectively. This way allows bitemporal texts to highlight 
the changing object, providing additional information for 
change detection.
Alternatively, only the vision encoders of founda -
tion models are adopted as most existing datasets pro -
vide only bitemporal images for identifying changes. In 
BAN [100], the authors introduce bridging modules to 
inject general knowledge extracted from the vision en -
coders of foundation models into existing change de -
tection models like BiT [113]  and ChangeFormer [114] . 
Since the input image sizes for foundation models and 
change detection models may differ and not all general 
knowledge contributes to predicting changes, the bridg -
ing modules are responsible for selecting, aligning, and 
injecting this knowledge. As shown in Figure 5 , these 
modules consist of three main components: layer nor -
malization and a linear layer to mitigate the distribution 
inconsistency between the image features from the two 
models, cross-attention to obtain valuable information 
from general knowledge, and bilinear interpolation to 
solve the misalignment problem. The bridging modules 
are placed between the two encoders, executing multi -
level knowledge injection.
BROADER SCOPE OF APPLICATION
In addition to the remote sensing tasks previously dis -
cussed, there are several emerging applications of contras -
tive-based vision-language foundation models, including 
the following:
1)
 VQA: VQ
A attempts to provide answers to questions 
related to the content of images. In [101], CLIP is used 
to extract both visual and textual representations from 
images and questions. Two decoders, followed by two 
classifiers, are then developed to capture the intrade -
pendencies and interdependencies within and between 
these representations. The final answer is determined by 
averaging the predictions from both classifiers.
2)
 Clou
d presence detection:  Cloud presence detection in -
volves identifying satellite images that are affected by 
clouds. The authors in [102] explore several strategies to 
adapt CLIP for this task. One approach, similar to zero-
shot scene classification [37], involves selecting prompts 
such as “This is a satellite image with clouds” and “This 
is a satellite image with clear sky” and then classifying 
satellite images via similarity measurement. Another 
strategy follows CoOp [115], combining learnable con-
text with the class token embeddings. Alternatively, the 
vision encoder can be employed by itself, with a linear 
classifier added on top. Unlike the first strategy, the other 
approaches necessitate training for prompts or classifiers.
3)
 Tex
t-based image generation: Text-based image generation 
refers to the task of creating images from textual descrip-
tions, which can help mitigate the issue of class imbal -
ance commonly found in remote sensing data. Typically, 
TGN [103] utilizes CLIP to classify generated images, 
Bridging 
Module
Bridging 
Module
Bridging 
Module
Bridging 
Module
Bi-TAB
e.g. BiT,
ChangeFormer,
Stacked-Blocks, etc.
Pretrained
Foundation Model
e.g. ViTs of CLIP, 
RemoteCLIP, etc. LN + Linear
Hf × Wf × Cf
Hf × Wf × Cc
Hc × Wc × Cc
Hc × Wc × Cc Hc × Wc × Cc Hc × Wc × Cc
Hc × Wc × Cc
HcWc × Hf Wf
Hf Wf × Cc Hf Wf × Cc
Resize
BridgingM odule
(a)
(b)
The Basic Block of Bi-TAB
The Basic Block of Foundation Model
Stop Gradient
Dot Product
Element-Wise Addition
Bi-Linear InterpolationResize
x1
x2
×
+
++
××
FIGURE 5. Illustration of BAN architecture [100] . (a) The detailed structure of BAN. (b) The detailed structure of the bridging module.  
LN: layer normalization. 
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
289 
 ensuring semantic class consistency between input im -
ages and generated ones.
4) Ima
ge address localization: Image address localization 
aims to predict the readable textual address where 
an image was taken [104] . Different from image geo-
localization in [61] , which predicts GPS coordinates 
(i.e., latitude and longitude) from images, this task 
outputs semantic address text such as “Grant Street, 
Downtown.” Utilizing existing foundation models 
like CSP [58]  and GeoCLP [61] , a viable solution is 
predicting GPS coordinates with a foundation model 
and then converting them into readable addresses. 
However, this mapping from coordinates to addresses 
often presents ambiguities. A recent work, Address -
CLIP [104], introduces a novel idea for performing im -
age address localization in an end-to-end manner. In 
particular, AddressCLIP applies contrastive learning 
to align images with scene captions and address texts. 
Furthermore, image-geography matching is developed 
to bring features of geographically proximate images 
closer together while distancing features of images 
that are far apart geographically. Figure 6 shows the 
framework of AddressCLIP.
INSTRUCTION-BASED VISION-LANGUAGE MODELING
Today, instruction-based VLM is advancing rapidly. Since 
2023, more than 10 impressive vision-language models 
have emerged, as shown in Table 4. They are not only ver-
satile, capable of performing a range of remote sensing im-
age analysis tasks, but also able to interact with users in a 
conversational manner. This broadens the accessibility of 
intelligent models beyond experts in remote sensing, facili-
tating their widespread deployment and application. This 
section presents critical developments in terms of model ar-
chitecture, and training strategy, model capability.
MODEL ARCHITECTURE
As CLIP has been trained to align image and text represen-
tations, most works [18], [24], [25], [42], [132] directly em-
ploy its vision encoder. The commonly used LLMs include 
the LLaMA family [119], [130] and its derivative, the Vicuna 
family [118]. Developed by Meta, the first-generation LLa -
MA [130] adopts transformer architectures ranging from 7 
to 65 billion parameters, pretrained on approximately 1.4 
trillion tokens of publicly available text corpora. Despite 
its smaller scale, LLaMA outperforms proprietary models 
like GPT-3 [108]. Its successor, LLaMA 2 [119] , introduces 
significant improvements, including expanded pretrain -
ing data (2 trillion tokens), architectural upgrades such as 
grouped-query attention [136], and a scaled-up 70-billion-
parameter variant. Through SFT and reinforcement learning 
with human feedback, LLaMA 2 was further optimized for 
dialogue, yielding the LLaMA 2-Chat series [119] . Parallel 
to this development, the research community developed 
Vicuna [118] by fine-tuning LLaMA models on 70,000 con-
versations from ShareGPT. These models achieve more than 
90% of ChatGPT’s quality. Regarding model architecture, 
existing works are devoted to improving visual encoders 
to enhance visual perception and designing connectors to 
promote the alignment between the two modalities. The fol-
lowing encoders were developed.
VISION ENCODER
Through a mask image modeling pretext task, EVA [116] in-
corporates geometry and structure information into CLIP’s 
visual representations, leading to improved performances 
Image Address
located in the Downtown, specifically
on a section of Third Avenue that near
Grant Building
a street view of a tall brick building
with a street light in front of it and a
street sign on the corner located in
the Downtown, specifically on a
section of Third Avenue that near
Grant Building
located in the Downtown, specifically
on a section of Third Avenue that near
Grant Building
Map
Image
Image Encoder
Image Embedding
Address Embedding
(a) (b)
located in the South Side Flats,
specifically on a section of South 13th
Street that near Muriel Street
located in the South Side Flats,
specifically on a section of South 13th 
Street that near Muriel Street lying
Text Encoder
Scene Caption
Visual-Language
Multimodal Model
Caption Embedding
Image-Caption
Contrastive Loss
Image-Address
Contrastive Loss
Geographical
Distance
Spatial Distance
Matrix
D11D12 D1n
D21D22 D2n
Dn1Dn2 Dnn
Feature Similarity
Matrix
Image-Geography
Matching Loss
···
···
··· ··· ··· ···
···
···
···
···
··· ······ ···
···
···
···
V11
V1T1
c V1T2
c V1Tn
c
V2T1
c V2T2
c V2Tn
c
VnT1
c VnT2
c VnTn
c
···
···
··· ······ ···
···
V1T1
A V1T2
A V1Tn
A
···V1T1
A
V1
arg max
V1T2
A V1Tn
A
V2T1
A V2T2
A V2Tn
A
VnT1
A VnT2
A VnTn
A
V1
T1
c T2
c Tn
c
T1
A T2
A Tn
A
···T1
A T2
A Tn
A
V2
Vn
V12 V1n
V21V22 V2n
Vn1Vn2 Vnn
···
···
··· ··· ··· ···
···
located in South Side Flats, specifically
on a section of East Carson St. lying
between South 10th St. and 12th St.
located in South Side Flats, specifically
on a section of East Carson St. lying
between South 10th St. and 12th St.
a street view of a house with a tree in
front of it and a fence in front of it
located in the South Side Flats,
specifically on a section of South 13th
Street that near Muriel Street
a street view of a city with a bus
driving down the street and buildings
on both sides of the street located in
South Side Flats, specifically on a
section of East Carson St. that lies
between South 10th St. and 12th St.
FIGURE 6. An illustration of AddressCLIP framework [104] .
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
290 
across a wide range of visual perception tasks. Consequent-
ly, RSGPT [17]  and SkyEyeGPT [43]  adopt EVA as their 
vision encoder. An alternative to complementing CLIP is 
utilizing the strengths of diverse vision encoders. CLIP 
learns visual representations through language supervi -
sion, which limits the information retained about the 
image. Captions only approximate the main content of 
images, often failing to present complex pixel-level details. 
TABLE 4. SUMMARY OF INSTRUCTION-BASED VISION-LANGUAGE MODELS IN REMOTE SENSING.
MODEL VISION ENCODER CONNECTOR LLM 
TRAINING 
DATASET TRAINING STRATEGY PUBLIC 
RSGPT [17] EVA CLIP-G [116]  Q-Former 
[117 ] +Linear 
layer
Vicuna-
7B/13B [118] 
RSICap [17] •
  Load pret
rained weights of InstructBLIP 
[117 ].
•  Fine-tune
 Q-Former and linear layer.
✗
SkyEyeGPT [43]  EVA CLIP-G [116]  Linear layer LLaMA 
2-Chat [119] 
SkyEye-968k [43]  •
  Load pret
rained weights of [116] and [120].
•  Employ 
LoRA [121] for linear layer and LLM 
in two stages.
✗
EarthGPT [122]  DINOv2 ViT-L/14 
[123] 
+ CLIP ConvNeXt-L 
[27] 
Linear layer LLaMA 2-13B 
[119] 
LAION-400M 
[124] 
COCO Caption 
[125] 
MMRS-1M [122] 
•
  Load pret
rained weights of [27] , [119], and 
[123].
•  Train lin
ear layer and LLM on LAION-400M 
and COCO Caption.
•  Employ bi
as tuning [126] for LLM on 
MMRS-1M.
✗
EarthMarker [44]  DINOv2 ViT-L/14 
[123] 
+CLIP ConvNeXt 
[27] 
Projection 
layer
LLaMA 2-13B 
[119] 
COCO Caption 
[125] 
RSVP-3M [44] 
RefCOCO [127] 
RefCOCO+ [128] 
•
  Load pret
rained weights of [27] , [119], and 
[123].
•  Train pro
jection layer on COCO Caption 
and RSVP subset.
•  Fine-tu
ne attention layers of LLM on Ref-
COCO and RefCOCO+.
•  Employ 
LoRA [121] for LLM on RSVP-3M.
✗
Popeye [129] DINOv2 ViT-L/14 
[123] 
+CLIP ViT-L/14 [27]  
Projection 
layer
LLaMA-7B 
[130] 
COCO Caption 
[125] 
MMShip [129] 
•
  Load pret
rained weights of [27] , [123], and 
[130].
•  Train pro
jection layer and employ LoRA 
[121] for LLM on COCO Caption.
•  Fine-tu
ne the projection layer and employ 
LoRA and instruction adapters for LLM on 
MMShip.
✗
LHRS-Bot [24]
a CLIP ViT-L/14 [27]  Vision per -
ceiver
LLaMA 2-7B 
[119] 
LHRS-Align [24]  
LHRS-Instruct [24]  
Multitask dataset 
[24] 
LLaVA-Instruct-
158K [23] 
•
  Load pret rained weights of [27] , and [119].
•  Train vision
 perceiver on LHRS-Align.
•  Fine-tu
ne vision perceiver and employ 
LoRA [121] for LLM on LHRS-Instruct, 
multitask dataset.
•  Fine-tu
ne LLM on LHRS-Instruct, the 
multitask dataset, and the LLaVA-Instruct-
158K subset.
✓
VHM [42]
b CLIP ViT-L/14 [27]  MLP Vicuna-v1.5-
7B [118] 
VersaD [42] 
VersaD-Instruct 
[42] HnstD [42] 
VariousRS-Instruct 
[42] 
•
  Load pret rained weights of [27] , and [118].
•  Train vis
ion encoder, MLP, and LLM on 
VersaD.
•  Fine-tu
ne MLP and LLM on VersaD-In-
struct, HnstD, and VariousRS-Instruct.
✓
GeoChat [18]c CLIP ViT-L/14 [27]  MLP Vicuna-v1.5-
7B [118] 
Multimodal In -
struction Dataset 
[18] 
•
  Load pret rained weights of [93] , [118], and 
[131].
•  Employ 
LoRA [121] for LLM.
✓
RS-LLaVA [132]d CLIP ViT-L@336 [27]  MLP Vicuna-v1.5-
7B/13B [118] 
RS-Instructions 
[132] 
•
  Load pret rained weights of [27]  and [118].
•  Train MLP o
n a general image-language 
dataset.
•  Employ 
LoRA [121] for LLM.
✓
SkySenseGPT 
[25]
e
CLIP ViT-L/14 [27]  MLP Vicuna-v1.5 
[118] 
FIT-RS [25]  
Additional instruc -
tion dataset [25]  
•
  Load pret rained weights of [27] , [118], and 
[131].
•  Fine-tu
ne MLP and employ LoRA [121]  for 
LLM.
✓
IFShip [133] CLIP ViT-L/14 [27]  MLP Vicuna-13B 
[118] 
TITANIC-FGS [133]  •
  Load pret
rained weights of [27]  and [118].
•  Employ 
LoRA [121] for MLP and LLM.
✗
TEOChat [134]f CLIP ViT-L/14 [27]  MLP LLaMA 2 
[119] 
TEOChatlas [134]  •
  Load pret rained weights of [27]  and [135].
•  Employ 
LoRA [121] for LLM.
✗
ahttps://github.com/NJU-LHRS/LHRS-Bot; bhttps://github.com/opendatalab/VHM; chttps://github.com/mbzuai-oryx/geochat; dhttps://github.com/BigData-KSU/RS-LLaVA; ehttps://
github.com/Luo-Z13/SkySenseGPT?tab=readme-ov-file; fhttps://github.com/ermongroup/TEOChat?tab=readme-ov-file.
LoRA: Low-Rank Adaptation. Detailed information about training datasets can be found in Tables 12 and 13. “ Public” refers to the availability of both code and model weights.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
291 
Targeting this limitation, some works [44], [122], [129] pro-
pose combining vision encoders from CLIP and DINOv2 
[123]. DINOv2 learns visual representation from images 
alone via self-supervised learning, enabling it to capture 
both image-level and pixel-level information. Moreover, 
given the varied object sizes in remote sensing images, 
these works refine visual representations by incorporating 
multiscale information. In [122], a vision encoder with the 
convolutional neural network architecture is used to extract 
multiscale visual features. In [44] and [129], the input im-
age is downsampled to different resolutions and then re -
spectively fed into two vision encoders. The encoded visual 
features are transformed to the same dimension and con -
catenated channel-wise.
VISION-LANGUAGE CONNECTOR
The linear layer and MLP are widely used as vision-language 
connectors, serving as key components in most models 
[18], [25], [42], [43], [122], [132], [133], [134]. In contrast, 
RSGPT [17] and LHRS-Bot [24]  explore alternative con -
nector architectures. Following InstructBLIP [117] , RS -
GPT includes an instruction-aware query transformer 
(Q-Former) as an intermediate module between the vi -
sion encoder and LLM. The Q-Former is designed to ex -
tract task-relevant visual representations by interacting 
additional query embeddings with instruction and image 
embeddings. It achieves this through the attention mecha-
nism; first, self-attention is applied to implement interac -
tion between instruction and query embeddings, and then 
cross-attention is employed between the query and image 
embeddings. The resulting output from the Q-Former, af -
ter passing through a linear layer, is then input into the 
LLM along with the instruction embeddings to generate 
responses. LHRS-Bot proposes incorporating multilevel 
image embeddings to sufficiently capture the semantic 
content of images. To mitigate the computational burden 
and the risk of overwhelming language information with 
excessive visual information, it introduces a set of learn -
able queries for each level of visual representation. These 
queries are used to summarize the semantic information 
of each level through stacked cross-attention and MLP lay-
ers. As a result, a dedicated visual perceiver is developed, 
with experimental results demonstrating that, compared 
to single-level visual representations and a two-layer MLP, 
multilevel representations paired with the visual per -
ceiver improve the model’s performance in scene clas -
sification, VQA, and visual grounding (VG) tasks.
TRAINING STRATEGY
Training instruction-based vision-language models typical-
ly involves two stages: pretraining for modality alignment 
and SFT for following task-specific instructions.
ONLY SUPERVISED FINE-TUNING
Due to the lack of large-scale image-text datasets specifi -
cally designed for the remote sensing domain, most works 
target the SFT stage using carefully crafted instruction-fol -
lowing datasets [17, [18], [25], [43], [133], [134]. To preserve 
the general knowledge embedded in pretrained vision en -
coders, the vision encoder is typically kept frozen during 
training, with the connector or the LLM undergoing fine-
tuning. For instance, RSGPT [17] fine-tunes the connector, 
while GeoChat [18] and TEOChat [134] fine-tune the LLM. 
SkyEyeGPT [43], SkySenseGPT [25], and IFShip [133] fine-
tune both the connector and LLM. To avoid the expense of 
full-parameter tuning, Low-Rank Adaptation (LoRA) [121] 
is often adopted, which introduces low-rank learnable ma-
trices into the layers of the connector [43] , [133] or LLM 
[18], [25], [43], [133], [134].
PRETRAINING FOLLOWED BY SUPERVISED FINE-TUNING
A couple of recent works have investigated how to implement 
the pretraining stage to boost model performance. Based on 
the choice of training data, they can be categorized into two 
groups: those that combine available image-text pairs from 
multiple domains for pretraining and those that direct atten-
tion toward creating large-scale RS image-text datasets. For 
combining available data, EarthGPT [122] , Popeye [129], 
and RS-LLaVA [132]  utilize natural image-text datasets, 
while EarthMarker [44] integrates data from both computer 
vision and remote sensing domains. COCO Caption [125] 
is a commonly used pretraining dataset. Unlike RS-LLaVA, 
which limits pretraining to the connector, the other three 
models perform pretraining for the connector and the LLM.
To address the domain gap between remote sensing 
images and natural images, researchers have developed 
large-scale RS image-text datasets, such as LHRS-Align 
[24] and VersaD [42], both of which contain more than 1 
million training samples. Leveraging the LHRS-Align da -
taset, Muhtar et al. [24]  design a three-stage curriculum 
learning strategy for LHRS-Bot. First, the vision perceiver 
is pretrained on LHRS-Align. Next, the vision perceiver and 
LLM are fine-tuned on the LHRS-Instruct subset and mul -
titask dataset, equipping the model to handle multimodal 
tasks. Finally, the LLM undergoes additional fine-tuning 
on all instruction data from LHRS-Instruct, the multitask 
dataset, and the LLaVA-Instruct-158K subset to fully un -
lock LHRS-Bot’s capabilities. Note that these models still 
keep the vision encoder frozen. In contrast, Pang et al. [42] 
unfreeze the vision encoder, MLP, and LLM for pretrain -
ing on VersaD. They subsequently fine-tune the MLP and 
LLM on customized datasets, including VersaD-Instruct, 
HnstD, and VariousRS-Instruct. Their experimental results 
show that the model pretrained with RS data significantly 
outperforms the one only fine-tuned with RS data across 
multiple tasks, confirming the importance of incorporating 
extensive RS visual knowledge by pretraining.
MODEL CAPABILITY
Table 5 lists the capabilities of existing instruction-based 
vision-language models in remote sensing. Key observa -
tions include the following: 
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
292 
1) Most models are developed for general-purpose remote 
sensin
g data analysis [18] , [24], [25], [42], [43], [44], 
[122], [134], with only Popeye [129]  and IFShip [133]  
specifically tailored for remote sensing images of ships. 
These models primarily process optical images but have 
expanded to include synthetic aperture radar (SAR) 
[122], [129] and infrared (IR) [122] images. 
2)
 Most mod
els support conversational interaction with 
users and can perform a variety of tasks [18], [24], [25], 
[42], [43], [44], [122], [134], ranging from single-image 
analysis to temporal series analysis [134]. 
3) The granu
larity of analyzed information has progressed 
from the image level to the region level [25] , [44] and 
even to the point level [44]. 
In this section, we examine the models’ performance on 
common capabilities and offer an in-depth exploration of 
unique capabilities.
COMMON CAPABILITIES
As shown in Table 5, most models are capable of perform-
ing remote sensing tasks such as image captioning, scene 
classification, VQA, and VG. Accordingly, we present model 
performance on these tasks in Tables 6 –8. To ensure a fair 
comparison, we report only the models’ performance on 
public datasets. Although multiturn conversation (MTC) 
is also a common capability among existing models [18] , 
[24], [25], [42], [43], [44], [122], [129], [134], it is challeng-
ing to evaluate quantitatively. Figure 7 provides examples 
of conversations between various models and users.
1)
 Image ca
ptioning: Based on the input remote sensing im-
age and the language instruction, vision-language mod-
els generate a description of image content. Examples of 
instructions include “Describe this image in detail” [17], 
[43] or “Please provide a one-sentence caption for the 
provided remote sensing image in detail” [122]. From Ta-
ble 6, we can observe that SkyEyeGPT [43] outperforms 
RSGPT [17] on the RSICD [78], UCM-captions [3], and 
Sydney-captions [3] datasets in terms of BLEU [137], ME-
TEOR [138], and ROUGE_L [139], though it falls short of 
RSGPT in CIDEr [140] scores. On the UCM-captions da-
taset, RS-LLaVA achieves BLEU scores comparable to Sky-
EyeGPT while surpassing it in all other metrics. On the 
NWPU-Captions [141] dataset, EarthGPT [122] achieves 
state-of-the-art results, with a remarkable improvement 
in CIDEr (nearly 30%) over EarthMarker [44].
2)
 Scene clas
sification: Table 7  presents evaluations on 
10 remote sensing scene classification datasets under 
both fully supervised and zero-shot settings. The ab -
sorption of extensive remote sensing visual 
 knowledge 
i
mproves model accuracy and generalization in scene 
classification tasks. Specifically, in the supervised set -
ting, VHM [42] and EarthGPT [122] boost  classif
ication 
TABLE 5. CAPABILITY COMPARISONS OF INSTRUCTION-BASED VISION-LANGUAGE MODELS IN REMOTE SENSING.
MODEL 
SIT CAP. CLS 
VQA VG OD ORR MTC OTHERS Opt./SAR/IR Img./Reg./Pt. Img./Reg./Pt.
RSGPT [17] ✓/✗/✗ ✓/✗/✗ ✗/✗/✗ ✓ ✗ ✗ ✗ ✗ ✗
SkyEyeGPT [43]  ✓/✗/✗ ✓/✗/✗ ✓/✗/✗ ✓ ✓ ✓ ✗ ✓ Video captioning 
Referring expression generation
EarthGPT [122]  ✓/✓/✓ ✓ /✓/✗ ✓/✗/✗ ✓ ✓ ✓ ✗ ✓ ✗
EarthMarker [44]  ✓/✗/✗ ✓/✓/✓ ✓/✓/✓ ✗ ✗ ✓ ✓ ✓ ✗
Popeye [129]† ✓/✓/✗ ✓/✗/✗ ✗/✗/✗ ✗ ✗ ✓ ✗ ✓ ✗
LHRS-Bot [24]  ✓/✗/✗ ✓/✗/✗ ✓/✗/✗ ✓ ✓ ✗ ✓ ✓ Object counting 
Object attribute recognition 
Image property recognition
VHM [42] ✓/✗/✗ ✓/✗/✗ ✓/✗/✗ ✓ ✓ ✗ ✓ ✓ Object counting 
Geometric measurement 
Building vectorizing 
Image property recognition 
Multilabel classification 
Honest question answering
GeoChat [18] ✓/✗/✗ ✓/✓/✗ ✓/✗/✗ ✓ ✓ ✗ ✗ ✓ ✗
RS-LLaVA [132] ✓/✗/✗ ✓/✗/✗ ✗/✗/✗ ✓ ✗ ✗ ✗ ✗ ✗
SkySenseGPT [25]  ✓/✗/✗ ✓/✓/✗ ✓/✗/✗ ✓ ✓ ✓ ✓ ✓ Multilabel classification 
Image/region scene graph generation
IFShip [133]
† ✓/✗/✗ ✓/✗/✗ ✓/✗/✗ ✓ ✗ ✗ ✗ ✓ ✗
TEOChat [134] ✓/✗/✗ ✓/✓/✗ ✓/✗/✗ ✓ ✓ ✗ ✗ ✓ Change detection 
Temporal scene classification 
Temporal referring expression 
Spatial change referring expression 
Image/region change question answering
†indicates the model’s capability tailored for remote sensing images of ships. SIT: supported image type; Cap: captioning., CLS: classification; OD: object detection; ORR: object 
 relatio
nship reasoning; MTC: multiturn conversation. We present the Op.t/SAR/IR column to indicate whether the model supports input of optical (Opt.), SAR, or infrared (IR) images, 
and we present the Img./Reg./Pt. column to denote the granularity of the analyzed information, which includes image level (Img.), region level (Reg.), or point level (Pt.).
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
293 
performance to more than 93% on the NWPU-RE -
SISC45 [56] dataset, which includes 45 scene catego -
ries with image spatial resolutions ranging from 0.2 m 
to more than 30 m. For zero-shot scene classification, 
LHRS-Bot [24], VHM, and SkySenseGPT [25]  demon-
strate impressive generalization, achieving accuracy 
above 91% on the AID [70] dataset and more than 93% 
on the WHU-RS19 [142] dataset. 
SkySenseGPT notably achieves 
the highest accuracy, reaching 
92.25% and 97.02% on these da-
tasets, respectively. Despite these 
remarkable results, low-reso -
lution and fine-grained scene 
classification remain significant 
challenges. The EuroSAT [71]  
dataset, used for land use and 
land cover classification, has an 
image spatial resolution of 10 m, 
while fMoW [60]  contains more 
than 1 million images across 63 
scene categories. On these data -
sets, LHRS-Bot [24]  achieves an 
accuracy below 57%.
3)
 VQA: The RS
VQA-HR [4] and RS -
VQA-LR datasets are widely adopt-
ed to assess model performance 
in VQA tasks, as shown in Table 7.  
Under a supervised setting, RS -
GPT [17] establishes strong performance baselines on 
both datasets. Subsequent models, including EarthGPT 
[122], GeoChat [18], VHM [42], and SkySenseGPT [25], 
have progressively approached these benchmarks in 
both supervised and zero-shot settings. The latest model, 
SkySenseGPT, narrows the zero-shot performance gap 
to approximately 13%, coming closest to the baseline on 
TABLE 6. THE PERFORMANCE OF INSTRUCTION-BASED VISION-LANGUAGE MODELS 
ON IMAGE CAPTIONING.
MODEL BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE_L CIDEr
RSICD [78]
RSGPT [17] 70.32 54.23 44.04 36.83 30.1 53.34 102.94 
SkyEyeGPT [43]  86.71 76.66 67.31 59.99 35.35 62.63 83.65
UCM-captions  [3]
RSGPT [17] 86.12 79.14 72.31 65.74 42.21 78.34 333.23
RS-LLaVA [132] 90 84.88 80.3 76.03 49.21 85.78 355.61 
SkyEyeGPT [43]  90.71 85.69 81.56 78.41 46.24 79.49 236.75
Sydney-captions [3]
RSGPT [17] 82.26 75.28 68.57 62.23 41.37 74.77 273.08 
SkyEyeGPT [43]  91.85 85.64 80.88 77.4 46.62 77.74 181.06
NWPU-Captions  [141]
EarthMarker [44]  84.4 73.1 62.9 54.3 37.5 70 162.9
EarthGPT [122]  87.1 78.7 71.6 65.5 44.5 78.20 192.6 
BLEU, METEOR, ROUGE_L, and CIDEr are short for Bilingual Evaluation Understudy [137] , Metric for Evaluation of Translation 
with Explicit Ordering [138] ,  Recall
-Oriented Understudy for Gisting Evaluation [139] , and Consensus-based Image 
Description Evaluation [140]. Best results are highlighted in bold. 
TABLE 7. THE PERFORMANCE OF INSTRUCTION-BASED VISION-LANGUAGE MODELS ON SCENE CLASSIFICATION AND VQA.
SCENE CLASSIFICATION: TOP-1 ACCURACY
MODEL RESISC [56] CLRS [145] 
NaSC-TG2 
[146] UCM [80] AID [70] 
WHU-RS19 
[142] 
SIRI-WHU 
[79] 
EuroSAT [71] 
METER-
ML [147] 
fMoW 
[60] 
GeoChat [18] — — — 84.43† 72.03† — — — — —
EarthMarker [44]  — — — 86.52† 77.97† — — — — —
EarthGPT [122]  93.84 77.37† 74.72† — — — — — — —
LHRS-Bot [24]  83.94 — — — 91.26† 93.17† 62.66† 51.4† 69.81 56.56 
VHM [42] 94.54 — — — 91.7† 95.8† 70.88† — 72.74 —
SkySenseGPT [25]  — — — — 92.25† 97.02† 74.75† — — —
VQA
MODEL 
RSVQA-HR [4] TEST SET 1 RSVQA-HR [4] TEST SET 2 RSVQA-LR [4] 
PRESENCE COMPARISON AVG. ACC. PRESENCE COMPARISON AVG. ACC. PRESENCE COMPARISON
RURAL/
URBAN
AVG. 
ACC.
SkyEyeGPT [43]  84.95 85.63 85.29 83.5 80.28 81.89 88.93 88.63 75 84.19
RSGPT [17] 91.86 92.15 92 89.87 89.68 89.78 91.17 91.7 94 92.29
LHRS-Bot [24]  — — — — — — 88.51 90 89.07 89.19
RS-LLaVA [132] — — — — — — 92.27 91.37 95 88.1
GeoChat [18] — — — 58.45
† 83.19† 72.3† 91.09 90.33 94 90.7
EarthGPT [122]  — — — 62.77† 79.53† 72.06† — — — —
VHM [42] — — — 64† 83.5† 73.75† 90.11 89.89 88 89.33
SkySenseGPT [25] — — — 69.14† 84.14† 76.64† 91.07 92 95 92.69 
RESISC: NWPU-RESISC45 [56] ; Avg. Acc.: average accuracy. †indicates zero-shot performance. Best results are highlighted in bold. 
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
294 
test set 2 of the RSVQA-HR dataset and surpassing RS -
GPT on the RSVQA-LR dataset in the supervised setting.
4) VG: VG a
ims to locate specific objects within an image based 
on a natural language expression. Vision-language models 
accomplish this task by providing the coordinates of the tar-
get object. Table 8 presents the performance of several mod-
els on RSVG [143] and DIOR-RSVG [144] datasets, using an 
intersection over union (IoU) threshold of 0.5 adopted as 
the evaluation metric. LHRS-Bot [24] achieves the highest 
score on the RSVG test set, while SkyEyeGPT [43] leads on 
the DIOR-RSVG test set. Due to differences in metric calcula-
tions, VHM [42] achieves a score of 57.17% on DIOR-RSVG. 
TABLE 8. PERFORMANCE OF INSTRUCTION-BASED 
 VIS
ION-LANGUAGE MODELS ON VG.
MODEL RSVG [143] DIOR-RSVG [144] 
Val Test Test
SkyEyeGPT [43]  69.19 70.5 88.59
EarthGPT [122]  — — 76.65
LHRS-Bot [24]  — 73.45 88.1
VHM [42] — — 56.17
The evaluation metric adopts Acc@0.5, which means that the intersection over union 
(IoU) of the predicted box is at least 0.5 with the ground truth bounding box.
Describe thev ideo framei mage in detail.
The scene features al arge body of water, with 
severalb oats ﬁlled with people rowing in 
unison and forming lines. Someo f theb oats 
arec arryingﬂ ags, and thep articipantsc an be 
seen wearingu niforms. There is ap arking lot 
and severalb uildings near thes hore, possibly 
servinga sav iewinga rea fort he race.  In the 
top-left corner, there is a truck and ac ar 
parked next to each other, suggestin gt hat 
they might haveb een used to transport 
equipmento r spectators to the event.
[vqa] what is the event of this videof rame image?
boating
[caption] Provide a concise depiction of this 
video frame image.
people arer owingi nb oats on a river 
surrounded by buildings
UserUser
UserUser
UserUser
UserUser
SkyEyeGPT
SkyEyeGPT
SkyEyeGPT
SkyEyeGPT
SkyEyeGPT
SkyEyeGPT
[identify] This {<62><50><81><69>} is
two tennis courts
UserUser
UserUser
SkyEyeGPT
SkyEyeGPT
SkyEyeGPT
SkyEyeGPT
SkyEyeGPT
SkyEyeGPT
[vqa] Classify th ei mage within on eo f theg iven 
classes: sports ﬁeld, forest, river, parking lot,  
overpass, baseball diamond, freeway, agricultural, 
chaparral, beach, buildings, intersection, dens e 
residential, sparse residential, medium residential, 
airplane, tenniscourt, mobile home park ,g olf 
course,s torage tanks, runway, harbor .A nsweri t 
using the above category word or phrase.
two tennis courts with a baseball ﬁeld 
in the background
[vqa] How many tennis courts are there?
UserUser
2
[caption] Write a short description for 
the image.UserUser
sports ﬁeld
SkyEyeGPT
SkyEyeGPT
Describe this image in detail
New chat (First round)
SkyEyeGPT
SkyEyeGPT
UserUser
UserUser
This is ah igh-resolution aerial image 
showing ab aseball ﬁeld on the left 
sideo ft he image witht wo baseball 
ﬁelds, and two tennisc ourtso nt he 
right side of the image, there's a large 
grassy area surrounded by trees.
[refer] where can I locate the
baseball ﬁeld at the top?UserUser
SkyEyeGPT
SkyEyeGPT
[detection] the two brown 
baseball ﬁeldsUserUser
SkyEyeGPT
SkyEyeGPT
New chat (First round)
Describe th
i
s image in det
a
il
SkyEyeGPT
SkyEyeGPT
User
User
User
User
T
his
 is
 a
 high
-
resolution
 aerial
 image
showing
 a
 baseball
 ﬁeld
 on
 the
 left 
side
 of
 the
 image
 with
 two
 baseball 
ﬁelds,
 and
 two
 tennis
 courts
 on
 the 
right side of the image, there's a large
grassy area su
r
rounded b
y
 trees
.
[refer] where c
a
n I locate th
e
baseball ﬁel
d
 at the top?
User
User
SkyEyeGPT
SkyEyeGPT
[
detection
]
 the two
 b
rown
baseball
 ﬁelds
User
User
SkyEyeGPT
SkyEyeGPT
New chat 
(
First round)
[
identify
] 
This {<62><50><81><69>} is
two tenni
s
 courts
User
User
User
User
SkyEyeGPT
SkyEyeGPT
SkyEyeGPT
SkyEyeGPT
SkyEyeGPT
SkyEyeGPT
[
v
q
a]
 Cl
a
ssi
f
y
 t
h
e
 im
a
g
e
 wit
hi
n 
o
n
e
 of t
h
e
 g
i
v
e
n 
c
l
a
ss
e
s:
 s
por t
s
 ﬁ
e
l
d
,
 f
ore
 s
t
, 
r
iv
e
r,
 p
a
rk
i
n
g 
l
o
t
, 
overpass, baseball diamond, freeway, agricultural,
c
h
ap
a
r
r
a
l
,
 b
e
a
c
h
,
 b
uil
d
in
g
s
,
 in
t
er
s
ec
ti
o
n
,
 de
n
s
e 
residential, s
p
arse reside
n
tial, medium 
r
esidential,
ai
r
plan
 e
,
 t
e
nni
 sco
 u
r
t
,
 mo
bil
e
 h
om
e
 p
a
r
k,
 g
ol
f 
co
ur
s
e
,
 s
t
o
r
ag
e
 t
an
k
s
,
 run
way
,
 h
a
r
bo
r
.
 A
ns
w
e
r
 i
t
using the abo
v
e category wor
d
 or phrase.
two tennis co
u
rts with a bas
e
ball ﬁeld
in the backgrou
n
d
[vqa] How m
a
ny tennis cour
t
s are there?
User
User
2
[caption] Write 
a
 short des
c
ription for 
the image.
User
User
sports ﬁeld
SkyEyeGPT
SkyEyeGPT
New chat (Second round)
Describe the
 video frame
 image in detail.
The
 scene
 features
 a
 large
 body
 of
 water,
 with
several
 boats
 ﬁlled
 with
 people
 rowing
 in
unison
 and
 forming
 lines.
 Some
 of 
the
 boats
are
 carrying
 ﬂags,
 and
 the
 participants
 can
 be
seen
 wearing
 uniforms.
 There
 is
 a
 parking
 lot 
and
 several
 buildings
 near
 the
 shore,
 possibly 
serving
 as
 a
 viewing
 area
 for
 the
 race.
  
 In
 the
top
-
left
 corner,
 there
 is
 a
 truck
 and
 a
 car 
parked
 next
 to
 each
 other,
 suggesting
 that
they
 might
 have
 been
 used
 to
 transport 
equipment
 or spectators to 
t
he event.
[vqa] what
 is the event of thi
s
 video
 frame image
?
boating
[caption] Provide a
 concise depiction of
 this 
video f
r
ame im
a
ge.
people
 are
 rowing
 in
 boats
 on
 a
 river
surrou
n
ded by bui
l
dings
User
User
User
User
User
User
User
User
T
SkyEyeGPT
SkyEyeGPT
SkyEyeGPT
SkyEyeGPT
SkyEyeGPT
SkyEyeGPT
New chat 
(
Second round)
What can you see from this image?
From this image, I can see a beautiful and well-organized city
with many water bodies, including lakes. The city appears to be
a mix of residential and commercial areas, with several buildings
of different heights and architectural styles. Additionally, there is
a golf course, which suggests that the city provides leisure
activities for its residents.
How many water bodies can you see ?
Based on the image, I can see at least five water bodies.
[VG] The coordinates of the largest water bodies.
[0.02, 0.25, 0.24, 0.42]
[CLS] Residential or Commerical?
Residential
Is there any playground? If have, where is it?
Yes, there is a playground in the image. It
can be found on the right side of the road,.
[VG] Playground
[0.82, 0.50, 0.99, 0.83]
What can you infer from this image?
From this image, I can infer that the area depicted is a
densely populated residential neighborhood with many
houses and trees. The neat arrangement of the
houses and the presence of roads suggest a well-
planned and organized community. The image also
suggests that the area is relatively affluent, as there
are many white buildings and a dense concentration
of houses.
How many buildings?
There are many buildings in the dense residential area.
The exact number is not visible, but there appear to be
at least a dozen or more buildings in the area.
Describe the primary type of infrastructure visible in the center of the
image.The image prominently features a highway interchange with
multiple lanesHow many major roadways can you identify in the image?
There appear to be at least two major roadways intersecting. One is a
multi-lane highway running vertically, and the other is a horizontal
roadway crossing it.
Can you spot any residential areas, and how are they arranged in
relation to the main roads?
There is a residential area visible to the left (west) side of the vertical
highway. The homes are arranged in a typical suburban pattern, with
streets laid out in a block formation.
What type of facility is depicted
in the image?
The image appears to show an airport terminal with a large, open
area and multiple baggage claim areas.
Describe the layout of the terminal buildings.
The terminal buildings have a large, open layout with several rows
of gates and baggage claim areas. The layout suggests that the
airport is well-organized and efficiently designed to accommodate a
high volume of passengers and aircraft.
(a)
(b)
FIGURE 7. Examples of conversations between vision-language models and users. (a) SkyEyeGPT [43] . (b) LHRS-Bot [24].
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
295 
UNIQUE CAPABILITIES
Current research seeks to develop versatile vision-language 
models capable of handling various remote sensing image 
analysis tasks in a conversational manner. For instance, 
EarthMarker [44] supports not only language instructions 
but also visual prompts (e.g., boxes and points), enabling 
the model to perform fine-grained image understanding, 
such as region/point-level captioning and referring object 
classification. TEOChat [134]  excels in analyzing time-
series remote sensing images, detecting changes of interest 
(in the form of the bounding box), and answering ques -
tions related to changes. Coincidentally, LHRS-Bot [24] and 
VHM [42] both feature the ability to qualitatively recognize 
object attributes (e.g., color) and image properties (e.g., res-
olution and modality) while also quantitatively counting 
objects. Additionally, VHM offers insights into model hon-
esty, which is vital for applications such as national defense 
security. Beyond single-object analysis, relationship analy-
sis between objects is increasingly recognized as essential 
for understanding complex remote sensing scenes, drawing 
attention in recent models [24], [25], [42], [44]. This section 
delves into these unique capabilities, with a focus on their 
implementations as detailed.
1)
 Fine
-grained image understanding: Region-level image 
understanding is challenging but achievable. One can 
include the coordinates of the target region in language 
instructions to direct the model’s attention to a specific 
local region, as demonstrated in EarthGPT [122]  and 
GeoChat [18]. While using precise coordinates is effec -
tive, it lacks flexibility and is challenging to extend to 
the point level. In contrast, EarthMarker [44]  leverages 
visual prompting marks to guide the model to interpret 
specific regions or points. To ensure that the model un-
derstands the relationship between visual prompts and 
the whole image, the visual prompts share the same vi -
sual encoder and projection layer with the input image. 
Their embeddings are combined with instruction em -
beddings before being fed into the LLM. Visual prompts 
allow EarthMarker to perform multigranularity RS im -
age interpretation at the image, region, and point levels.
2)
 Time
-series image analysis: Change detection, the process 
of identifying changes on Earth’s surface from sequenc-
es of remote sensing images captured over the same 
area, plays an important role in many applications, such 
as urban planning [148]  and war damage assessment 
[149]. The work TEOChat [134] pioneers change detec -
tion in a conversational manner. Given a sequence of re-
mote sensing images, TEOChat employs a shared vision 
encoder to generate embeddings for each image. These 
embeddings are then projected to the input space of the 
LLM via a two-layer MLP. The LLM processes these pro-
jected image embeddings alongside instruction embed-
dings to produce change detection results in the form 
of bounding boxes. To train TEOChat, the authors of 
[134] develop TEOChatlas, the first instruction-follow -
ing dataset tailored for time-series image analysis tasks. 
Leveraging this dataset, TEOChat extends its capabili -
ties beyond change detection to include tasks such as 
change question answering, temporal scene classifica -
tion, temporal referring expression, and spatial change 
referring expression. Figure 8 provides specific examples 
illustrating each of these tasks.
3)
 Fro
m qualitative recognition to quantitative analysis: Existing 
models mostly concentrate on qualitative recognition 
tasks, excelling at describing the attributes or catego -
ries of images and objects in a nonnumerical manner. 
In contrast, LHRS-Bot [24] and VHM [42] broaden their 
capabilities to include quantitative image analysis, an -
swering “how many” questions. LHRS-Bot is equipped 
to count objects in an image and estimate image reso -
lution, using single-choice questions with two to four 
candidate answers. This question format requires a cer -
tain level of user expertise as the user needs to predefine 
several candidate answers, one of which must be correct. 
VHM goes a step further by not only handling these 
tasks but also measuring object size. Unlike LHRS-Bot, 
all quantitative tasks in VHM adopt open-ended ques -
tion formats, which may be more suitable for practical 
application. On the test set of the VariousRS-Instruct 
[42], VHM achieves mean absolute errors of 0.24 for 
mage resolution estimation, 6.75 for object counting, 
and 12.82 for geometric measurement. These results 
highlight the potential of vision-language models to 
advance quantitative analysis in remote sensing images. 
The successful implementation of such analysis relies 
heavily on the development of specialized instruction-
following datasets, which are discussed in detail in the 
“Instruction-Following Datasets” section.
4)
 End
owing models with honesty: Most instruction-following 
datasets designed for remote sensing tasks exclusively 
contain factual questions that query real objects within 
the images and are accompanied by affirmative answers. 
This makes vision-language models susceptible to ly-
ing, such as providing an affirmative answer regarding 
a category of a nonexistent object. A dishonest vision-
language model is particularly problematic in remote 
sensing applications, especially in critical areas like na -
tional defense security, as it may lead to serious negative 
consequences, including inaccurate intelligence gath -
ering. Given this, Pang et al. [42]  are the first to offer  
insights into the honesty of models. They create HnstD, 
an RS-specific honest dataset comprising questions with 
factual and deceptive categories. By utilizing it as an ad-
ditional instruction-following dataset, VHM is endowed 
with honesty. As shown in Figure 9, VHM refuses to an-
swer questions regarding the color or position of nonex-
istent objects.
5)
 Obj
ect relationship understanding: Remote sensing im -
ages often encompass a vast number of objects owing 
to their long-distance imaging. Comprehensively un -
derstanding the relationships between objects within 
an image is essential for interpreting complex remote 
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
296 
sensing scenes [150] , [151]. Consequently, the latest 
models, i.e., LHRS-Bot [24] , VHM [42] , EarthMarker 
[44], and SkySenseGPT [25] , extend their capabili -
ties to understand relationships between objects in 
an image. Specifically, LHRS-Bot and VHM target 
spatial relationships, using simple rules like “top” 
or “top right corner” to indicate object relationships. 
EarthMarker, on the other hand, explores functional 
relationships, such as the “overpass” and “toll sta -
tion” potentially contributing to the transportation 
infrastructure around the “stadium.” Meanwhile, 
SkySenseGPT describes semantic relationships, for 
example, the “crane” is over the “boat.” This promis -
ing understanding of relationships benefits from the 
design of specialized instruction-following datasets, 
as discussed in the “Instruction-Following Data
 sets
” 
section.
GENERATION-BASED VISION-LANGUAGE MODELING
Similar to research on contrastive-based VLM, generation-
based VLM follows two major research directions: the con-
struction of foundation models concerning the characteristics 
of remote sensing images and their application to promote 
various remote sensing data analysis tasks. This section first 
presents the development of generative foundation models and 
then discusses representative downstream applications.
GENERATIVE FOUNDATION MODELS
Building an effective generative foundation model is a for-
midable task because one needs to consider how to ensure 
the reliability and diversity of the generated images. Table 9  
provides a summary of existing generative foundation 
models in remote sensing.
ENHANCING RELIABILITY
Text, often in the form of descriptions of ground objects 
within images [15], [22], [47], [155], [158], [165] or semantic 
categories of images [21], [48], has been a common condi-
tion for conditional image generation. For instance, the text 
“a satellite view of San Francisco showing the bay, a street, 
and a bridge” is used to guide RS-SD [15], while the text “a 
fMoW satellite image of a car dealership in the United States 
of America” constrains DiffusionSat [21]. However, it is evi-
dent that these textual descriptions alone struggle to fully 
encapsulate the variety of objects and intricate relation -
ships present within a satellite image. The lack of sufficient 
constraint information poses a challenge to generating re -
liable images. To address this challenge, additional condi -
tions, such as metadata or images, are increasingly utilized 
to constrain the generation process.
1)
 Metada
ta: Metadata, such as latitude, longitude, ground 
sampling distance, cloud cover, and imaging time (year, 
month, and day), are adopted in both DiffusionSat [21] 
FIGURE 8. Examples showcasing the capabilities of TEOChat [134]  in time-series image analysis.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
297 
and CRS-Diff [22]. In contrast, MetaEarth [49] and Tex-
t2Earth [158] focus on spatial resolution, and GeoSynth 
[155] focuses on geographic location (latitude and longi-
tude). Compared to text conditions, metadata are more 
easily available as they are inherently embedded within 
remote sensing images. Furthermore, they allow gen -
erative foundation models to be trained on large-scale 
image datasets, benefiting from the diverse geographic 
distribution of these datasets. Consequently, the key 
problem lies in addressing how to inject metadata condi-
tions into diffusion models. Identical to the encoding of 
diffusion time steps, DiffusionSat and MetaEarth pro -
cess metadata values through sinusoidal encoding, fol -
lowed by MLPs. The resulting metadata embeddings are 
added with time step embeddings before being fed into 
the diffusion model. CRS-Diff maps metadata values to a 
fixed range and encodes them using different MLPs. The 
metadata embeddings are concatenated with text and 
content embeddings, providing global control informa-
tion. For latitude and longitude, an alternative approach 
is to utilize a pretrained location encoder. An example is 
[155] where the authors employ SatCLIP [64] to extract 
location embeddings and use CoordNet, which takes lo-
cation and time step embeddings as input, to integrate 
these conditions into the diffusion model. CoordNet 
consists of 13 multihead cross-attention blocks, each in-
cluding a zero-initialized feed-forward layer.
2)
 Imag
e: Metadata, such as geographic location, determines 
the visual appearance of objects in generated images to 
some extent. For example, the architectural styles of Chi-
nese and European buildings are notably distinct. How-
ever, metadata primarily impose constraints at a macro 
level, allowing considerable flexibility in the generated 
objects, which can result in uncontrollable object shapes. 
As a result, recent works have investigated the use of im-
age-form conditions to enable more precise control over 
the image generation process. Based on the information 
conveyed by the image, image conditions are roughly 
split into low-level visual conditions and high-level se -
mantic conditions. Low-level visual conditions pertain 
to the geometric information of images, such as edges, 
line segments, and sketches, which constrain the shape 
and structure of objects in generated images, as demon-
strated in works [22] and [48]. Off-the-shelf algorithms 
or models, including holistically nested edge detection 
(HED) [168], LETR [169], and the model proposed in 
[170], are employed to obtain these image conditions.
High-level semantic conditions, as the name implies, 
refer to the semantic information of images, providing con-
straints on object categories and their relationships in gen-
erated images. These conditions can be further categorized 
into two subgroups. The first group [21] , [47], [49], [158] 
conditions on associated remote sensing images, exempli -
fied by works such as MetaEarth [49] , which uses low-res-
olution images to prompt the diffusion model to produce 
high-resolution images, and GPG2A [47] , which synthe -
sizes aerial images guided by layout maps derived from 
corresponding ground images. The second group [22], [48], 
[155] leverages abstract representations of image content, as 
seen in works like CRS-Diff [22] and GeoSynth [155]. CRS-
Diff utilizes road maps, segmentation masks, and deep fea-
tures to constrain the layout and objects within generated 
FIGURE 9. Conversations between users and VHM [42] .
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
298 
images, while GeoSynth employs OSM images to achieve 
similar constraints on layout and object placement.
Image-form conditions are typically injected into diffu-
sion models using ControlNet [154]. ControlNet replicates 
the encoder blocks and the middle block of Stable Diffu -
sion’s U-Net and incorporates several zero convolutions. It 
processes both conditioning representations and noisy latent 
representations as inputs, with its outputs added to the de-
coder blocks and middle block of Stable Diffusion’s U-Net. 
In GeoSynth [155] and GPG2A [47], ControlNet is used for 
condition injection with different inputs—GeoSynth pro -
vides layout maps derived from ground images, while GP-
G2A inputs OSM images, text, and diffusion time steps. To 
enhance multicondition injection, DiffusionSat [21] extends 
ControlNet into a 3D version capable of accepting a sequence 
of satellite images. This 3D ControlNet retains the replicated 
encoder and middle blocks, with each followed by a tempo-
ral layer consisting of a 3D zero-convolution and a temporal 
pixel-wise transformer. Following Uni-ControlNet [171], the 
authors in [22] and 48] perform multiscale condition in -
jection and use attentional feature fusion [172] to combine 
conditioning and latent representations. Unlike these works 
relying on ControlNet, MetaEarth and Text2Earth directly 
concatenate latent and image condition representations. Me-
taEarth encodes low-resolution images via RRDBNet [157] 
with upsampling and convolution layers, while Text2Earth 
processes masked images using a VAE encoder [45].
IMPROVING DIVERSITY
Herein, diversity refers to two aspects; first, the generated 
objects exhibit a variety of semantic categories, with each 
category featuring varied visual characteristics, and second, 
the generated images capture a broad range of variations 
in imaging conditions (e.g., season and illumination) and 
imaging sensors (e.g., spatial resolution and viewpoint). 
By conditioning on metadata, the diversity of the gener -
ated data is improved, as shown in Figure 10. DiffusionSat 
[21] modifies geographic coordinates, imaging time, and 
TABLE 9. SUMMARY OF GENERATION-BASED VISION-LANGUAGE FOUNDATION MODELS IN REMOTE SENSING.
MODEL 
DIFFUSION 
MODEL 
CONDITION 
ENCODER 
CONDITION 
TRAINING 
DATASET 
GenImgType PUBLIC 
Text Metadata Image
RS-SD [15]a Stable Diffusion 
[28] 
CLIP Transformer [27]  ✓ ✗ ✗ 1% RS5M [15] Optical ✓
DiffusionSat [21]
b Stable Diffusion 
[28] 
CLIP Transformer [27]  
Sinusoidal 
Projection+MLP 3D 
ControlNet [21] 
✓ Latitude 
Longitude 
GSD 
Cloud cover 
Imaging time
Satellite 
 imag e fMoW [60] 
Satlas-small 
[152] 
SpaceNet [153] 
Optical ✓
CRS-Diff [22]
c Stable Diffusion 
[28] 
CLIP Transformer [27]  
MLP ControlNet [154]  
FFN
✓ Latitude 
Longitude 
GSD 
Cloud cover 
Imaging time
HED 
MLSD 
Depthmap 
Sketch 
Road map 
Seg. mask 
Deep feature
RSICD [78] 
fMoW [60] 
Million-AID [67]  
Optical ✓
GeoSynth [155]
d Stable Diffusion 
[28] 
CLIP Transformer [27]  
SatCLIP Location 
Encoder [64]  
CoordNet [155] 
ControlNet [154]  
✓ Latitude 
Longitude
OSM Image Satellite-OSM 
Dataset [155] 
Optical ✓
MetaEarth [49]  DDPM [156]  Sinusoidal 
Projection+MLP RRDB-
Net [157]+Upsampling
✗ Resolution Low-
 
Reso
lution 
Image
Multiresolution 
dataset [49]  
Optical ✗
Text2Earth [158] Stable Diffusion 
[28] 
CLIP Transformer [27]  
Projection Layer VAE 
Encoder [45] 
✓ Resolution Masked Image Git-10M [158] Optical ✗
HSIGene [48]
e Stable Diffusion 
[28] 
CLIP Transformer [27]  
ControlNet [154]  FFN
✓ ✗ HED 
MLSD 
Sketch 
Seg. mask 
Deep feature
Xiongan [159] 
Chikusei [160] 
DFC2013 [161] 
DFC2018 [162] 
Heihe [163] 
Hyperspectral ✓
GPG2A [47]
f Improved DDPM 
[164] 
CLIP Transformer [27]  
ControlNet [154]  
✓ ✗ Ground image VIGORv2 [47]  Optical ✓
RSDiff [165]  Imagen [166]  T5 [167] ✓ ✗ ✗ RSICD [78] Optical ✗
ahttps://huggingface.co/Zilun/GeoRSSD; bhttps://github.com/samar-khanna/DiffusionSat; chttps://github.com/Sonettoo/CRS-Diff; dhttps://github.com/mvrl/GeoSynth; ehttps://github.
com/LiPang/HSIGene; fhttps://github.com/AhmadArrabi/GPG2A.
GenImgType refers to the type of generated image, e.g., optical or hyperspectral images. GSD: ground sampling distance; HED: holistically nested edge detection; MLSD: multiscale line 
segment detection; Seg. Mask: Segmentation Mask. Public refers to the availability of both code and model weights.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
299 
ground sampling distance, resulting in varied stadiums, 
different scenes, and multiresolution images, respectively. 
In addition to considering the perspective of conditions, 
some works [49], [165] explore new frameworks for gener -
ating multiresolution images. In [165], cascaded diffusion 
models [166] are employed, where one model generates 
low-resolution images conditioned on text embeddings, 
and the other superresolves these low-resolution images. 
This framework achieves the gradual generation of high-
resolution satellite images purely from textual descriptions. 
In [49], a resolution-guided self-cascading framework is 
proposed to generate images at various resolutions instead 
of being limited to the two resolutions described in [165]. As 
illustrated in Figure 11, the generation process unfolds over 
multiple stages, with each stage conditioned on the low-res-
olution image output from the preceding stage and its cor-
responding spatial resolution. The resolution increases at a 
fixed ratio with each stage. Considering the resolution gap 
between adjacent stages, the image condition is injected 
through a sequential process involving an image encoder, 
followed by several upsampling and convolution layers. 
Building on this framework, the authors introduce sliding 
windows with overlaps and a noise sampling strategy to 
generate continuous unbounded scenes.
PERFORMANCE EVALUATION
With these efforts, existing generative foundation models 
have demonstrated the capability to generate optical re -
mote sensing images [15], [21], [22], [47], [49], [155], [165] 
or hyperspectral remote sensing images [48] . Their perfor-
mance is typically evaluated using metrics such as Fréchet 
Inception Distance (FID) [173]  and Inception Score (IS) 
[174]. For text-conditioned models, CLIP [27] is employed 
to measure the similarity between generated images and 
their corresponding textual descriptions, as detailed in  
Table 10. In addition to the direct evaluations, some works 
A Satellite
Image
Generic Caption,
Null Metadata
Fixed Caption and Metadata,
Varying Coordinates-France
to United States
A Satellite Image of a Stadium
48.98°N, 1.80°E 45.59°N, –122.33°E
Fixed Caption and
Metadata, Varying
Month-Summer to Winter
A Satellite Image of an 
Electric Substation in Finland
August January
Fixed Caption and Metadata,
Varying  Resolution-Low to
High Resolution
A Satellite Image of an 
Amusement Park in Australia
GSD: 1.4 mG SD: 0.5 m
FIGURE 10. Examples of remote sensing images generated by DiffusionSat [21] . GSD: ground-sampling distance.
Conditional Input
at Stage k – 1 Time Step tf mlp(;/theta.0t)
fmlp(;/theta.0s)
Time Step Embedding
Spatial
Resolution
s(k)
Frequency
Encoding f/omega.0 +
C
Spatial
Resolution
Sliding
Window Resolution Embedding
Self-Cascaded
Generation
Sliding Window Denoising
Model /element.0
/theta.0Image
Encoder Elr
Up-Sampling
Layers Fup
Conditional
Image x0
(k–1) ×T Generated
Image x0
(k)Gaussian Noise xT
(k)
Generation
Output at Stage k
Update Stage id From k – 1 to k
FIGURE 11. An illustration of MetaEarth architecture [49] .
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
300 
assess model performance by applying them or the gener -
ated images to specific downstream tasks and measuring 
their impact [21], [22], [47], [48], [49]. For example, Dif-
fusionSat [21] is applied to tasks such as superresolution, 
temporal generation, and in-painting, while CRS-Diff [22] 
uses its generated images to augment training datasets of 
the road detection model.
DOWNSTREAM APPLICATIONS
Generative foundation models serve as powerful and versa-
tile tools for advancing various remote sensing tasks. Their ex-
ceptional image generation capabilities have been utilized to 
tackle challenges such as limited image availability [175] and 
the high costs associated with annotation [176], [177]. Addi-
tionally, the conditionally controllable nature of these models 
makes them particularly suitable for remote sensing image en-
hancement tasks. Examples include tasks like superresolution 
[178], [179], where low-resolution images serve as conditions, 
and cloud removal [180], [181], where SAR images are used 
as conditions. Moreover, their advantages in learning diverse 
remote sensing image distribution equip them to handle the 
complexity of remote sensing scenes, improving the accuracy 
of remote sensing image interpretation tasks such as change 
detection [182], [183] and land cover classification [184]. These 
advancements predominantly center on the ability of genera-
tive foundation models to model image distribution, which 
has been comprehensively reviewed in [46]. In this discussion, 
however, we shift the focus to recently emerging applications 
that show how the joint distribution between images and texts 
in generative foundation models promotes advancements in 
remote sensing tasks. This includes tasks such as image or 
change captioning [50], [51], pansharpening [52], and zero-
shot target recognition [53], as illustrated in Table 11. This 
section delves into representative implementations in con-
junction with specific tasks.
CAPTIONING
Recent works [50], [51] propose generating textual descrip-
tions conditioned on visual features rather than generating 
images from textual descriptions, thus applying generative 
models for captioning remote sensing images. In [50], the 
authors focus on extracting both global and local features of 
remote sensing images to enhance the visual conditions for 
the generative model, addressing challenges such as intra -
class diversity, interclass similarity, and varying object sizes. 
They then improve the interaction between noisy represen-
tations and visual conditions in the decoder by sequentially 
interacting with global and local conditions, aiming for 
generated captions that better align with the image content. 
In [51], the difference features of bitemporal images condi-
tion the generative model to generate change captions from 
standard Gaussian noise. These difference features are inte-
grated into the generative model using cross-attention, fol-
lowed by stacking self-attention. Inspired by the success of 
generative models in change detection [188], the authors in 
[186] employ these models as a powerful feature extractor 
for multilevel multi-time step features of bitemporal images. 
A difference encoder, based on a time-channel-spatial atten-
tion mechanism, is then utilized to extract discriminative 
information from the features of bitemporal images, and a 
decoder generates change captions from this information.
PANSHARPENING
Pansharpening is the process of fusing high-resolution pan-
chromatic images with low-resolution multispectral images 
to produce high-resolution multispectral images. The idea 
of applying generative foundation models to this task in -
volves formulating it as an image generation problem con-
ditioned on panchromatic and multispectral images. To 
enhance the generalizability of the pansharpening model 
and enable it to uniformly handle multispectral images 
from different satellites, TMDiff [52]  incorporates textual 
descriptions, such as “Text Prompts 
of GaoFen-2 Satellite,” as identifiers 
to specify the satellites that capture 
multispectral images. These descrip -
tions, combined with panchromatic-
multispectral image pairs, jointly 
guide the generation process. Spe -
cifically, the textual descriptions 
are processed by a physics-informed 
HyperNet to get text embeddings, 
which are then used to modulate the 
denoising network. Meanwhile, the 
image pair is injected into the de -
coder of the denoising network via 
a condition encoder that shares the 
same architecture as the network’s 
encoder. To address the variability in 
spectral band configurations among 
different satellites, TMDiff, built on 
the U-Net architecture, organizes 
TABLE 10. PERFORMANCE OF GENERATIVE FOUNDATION MODELS.
MODEL CONDITION DATASET FID IS CLIP-SCORE 
DiffusionSat [21] Text+Metadata fMoW [60] 15.8 6.69 17.2
CRS-Diff [22]  Text 
HED 
MLSD 
Depthmap 
Sketch 
Seg. mask 
Road map 
Deep feature
RSICD [78] 50.72
30.18 
55.75 
54.4 
68.29 
70.05 
94.16 
44.93
18.39
—
—
—
—
—
—
—
20.33 
—
—
—
—
—
—
—
GeoSynth [155] Coordinate+OSM Image
Text+OSM Image
Satellite-OSM 
[155] 
11.9 
12.97
— 30.3
29.8
HSIGene [48]  MLSD
HED+MLSD
HED+MLSD+Sketch 
HED+MLSD+Sketch+Seg. Mask
AID [70] — — 79.27 
81.52 
81.4 
81.83
RSDiff [165] Text RSICD [78] 66.49 7.22 —
Text2Earth [158]  Text RSICD [78] 24.49 — —
CLIP-Score [27] measures the similarity between generated images and corresponding textual descriptions.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
301 
each block with stacked modulated ResBlock, a swish ac -
tivation layer, and frequency-aware downsampling (or 
upsampling). Compared to PanDiff [191] , which relies 
solely on image conditions, TMDiff demonstrates impres -
sive generalization capabilities.
ZERO-SHOT SYNTHETIC APERTURE  
RADAR TARGET RECOGNITION
Due to the high costs associated with SAR imaging and the lim-
ited azimuth range for capturing images, SAR datasets are often 
small, making it essential to explore SAR target recognition in 
zero-shot settings. From the perspective of SAR image simula-
tion, adopting generative foundation models for this task is a 
natural solution. Typically, Wang et al. [53] employ generative 
models to create 3D models of targets conditioned on semantic 
information, such as the category and structure of targets. Since 
most existing generative models are trained on RGB natural 
images or optical remote sensing images, the authors adopt a 
two-step process; they first use the generative model to gener-
ate optical images based on the target semantic information, 
and then they transform these optical images into 3D models 
using TripoSR [192] rather than directly generating 3D models 
or SAR images.
CLOUD REMOVAL
Passive remote sensing is susceptible to cloud interference. 
By conditioning on cloud-contaminated images and SAR 
images, generative models have been employed for cloud re-
moval of remote sensing images [180], [181]. Such solutions 
require geographically aligned multimodal image pairs, 
which are challenging to obtain in practical applications. To 
overcome this limitation, Czerkawski and Tachtatzis [189] 
propose a novel approach that uses historical edge infor -
mation and textual descriptions (e.g., a cloud-free satellite 
image) to constrain the generation process. Experimental 
results suggest that general-purpose generative models may 
not be directly applicable for cloud removal of remote sens-
ing images as they tend to generate undesirable artifacts in 
cloud-contaminated areas.
URBAN PREDICTION
Urban prediction aims to forecast future urban layouts 
based on current urban layouts and planned change maps, 
providing support for urban planning. The pioneering 
work, UP-Diff [190], incorporates ConvNeXt [193]  to en -
code current urban layouts and planned change maps into 
embeddings, which are subsequently injected into the de -
coder of diffusion models through cross-attention layers. 
Additionally, the text condition, derived from encoding “a 
remote sensing photo,” guides the model to generate out-
puts in a satellite image style.
DATASETS
Large-scale datasets are an essential prerequisite for vi -
sion-language research under the two-stage paradigm. 
Thus, a considerable amount of current research [15], [16], 
[17], [24], [25], [42], [194], [195], [196] focuses on dataset 
construction. In this section, we summarize these efforts, 
detailing the properties of datasets and their creation 
methods, with the aim of providing both convenience 
and inspiration for further research. Based on their usage, 
existing datasets can be broadly categorized into three 
groups: pretraining datasets , instruction-following datasets , 
and benchmark datasets.
TABLE 11. SUMMARY OF GENERATION-BASED VISION-LANGUAGE FOUNDATION  MOD ELS APPLIED TO REMOTE SENSING TASKS.
WORK TASK 
GENERATIVE FOUNDATION 
MODEL ADAPTATION 
VCC-DiffNet [50]  Image captioning D3PM [185] Extract local and global features to enhance visual conditions. 
Enhance interactions between noisy representations with visual 
conditions by sequentially interacting global and local conditions.
Diffusion-RSCC [51]  Change captioning Self-design Design cross-mode feature fusion module to inject the difference 
features of bitemporal images. 
Design stacking self-attention module to recreate change captions 
from Gaussian noise iteratively.
MADiffCC [186]  Change captioning SR3 [187], [188] Use a generative model to extract multilevel multi-time step 
features of bitemporal images.
Design an encoder using the time-channel-spatial attention to 
obtain discriminative information. 
Design a decoder guided by gated multihead cross-attention for 
generating change captions.
TMDiff [52]  Pansharpening SR3 [187] Use text and panchromatic-
 mul
tispectral image pairs together to 
condition the generation process, enhancing generalization.
Wang et al. [53] Zero-shot SAR  Tar
get 
Recognition
Stable Diffusion [28]  Generate optical images conditioned on target semantic 
information, aiding in the creation of 3D SAR target models.
Czerkawski and 
Tachtatzis [189]  
Cloud Removal Stable Diffusion [28]  Use text and historical edge information to constrain cloud-free 
image generation.
UP-Diff [190]  Urban Prediction Stable Diffusion [28]  Use text, current urban layouts, and
 pl
anned change maps as 
 con
ditions, guiding the model to generate future urban layouts.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
302 
PRETRAINING DATASETS
Pretraining datasets, which consist of remote sensing images 
and their corresponding texts, play a crucial role in infusing 
the model with a broad range of visual and language con -
cepts. This requires the images in pretraining datasets to be 
sufficiently diverse and rich. Currently, there are two alter-
native sources for collecting images; one is combining vari-
ous open source remote sensing image datasets [15] , [17], 
[20], [42], [83], [158], [195], and the other is utilizing public 
geographic databases [16], [24], [40], [158], [197]. Once the 
images are collected, corresponding textual descriptions 
can be generated through manual annotation [17], [40]. Al-
though manual annotation is easy to implement and highly 
accurate, the complexity and diversity of remote sensing im-
ages make it costly, which can significantly limit the dataset 
size. As a result, there is a growing shift toward automatic 
image captioning using rule-based methods [16], [20] or off-
the-shelf models [15], [24], [42], [158], [195], [197]. This sec-
tion begins by presenting the image collection strategies of 
existing pretraining datasets, followed by an introduction to 
their caption generation methods.
IMAGE COLLECTION
Object detection (OD) datasets in remote sensing typically 
feature diverse ground objects, such as DOTA [198], contain-
ing 1,793,658 instances across 18 categories, and DIOR [199], 
including 192,472 instances spanning 20 categories. These 
datasets have quickly garnered attention and have been in-
strumental in constructing pretraining datasets, leading to 
the development of RSICap [17] and DIOR-Captions [83]. 
Compared to relying on a single dataset, combining multiple 
datasets further enriches the diversity of images and objects 
while significantly increasing the dataset size. For instance, 
RET-3+SEG-4+DET-10 [20] integrates three datasets for im-
age retrieval, four for image segmentation, and 10 for OD. 
VersaD [42] further emphasizes multiresolution and mul -
tidomain coverage, drawing from datasets such as CVUSA 
[215] (0.08-m resolution), Million-AID [67]  (0.5–153-m 
resolution, covering diverse geographies), and LoveDA [212] 
(featuring urban and rural environments). RS5M [15] priori-
tizes large-scale image datasets, incorporating sources like 
BigEarthNet [68] (590,326 images), fMoW [60] (1,047,691 
images), and Million-AID (more than 1 million images). 
Moreover, RS5M integrates PUB11, a dataset of more than 3 
million remote sensing-related image-text pairs filtered from 
11 computer vision datasets, including LAION-400M [124] 
and CC3M [221]. PUB11 creation involved sequential steps 
such as invalid image checking, deduplication, filtering us-
ing a CLIP model, and employing a remote sensing image de-
tector. These pretraining datasets support the development 
of vision-language models beyond specific airborne or sat-
ellite sensors. An exception is ChatEarthNet [195], which is 
tailored for satellite image analysis and sources images from 
the Sentinel-2 collected in SatlasPretrain [152].
In addition to open source image datasets, public geo -
graphic databases contribute to the extensive collection 
of remote sensing images [16], [24], [40], [158], [197]. For 
example, LuoJiaHOG [40] derives its images from Google 
Maps, determining global sampling points through spa -
tial analysis and the evaluation of landscape indices. This 
method ensures the inclusion of images representing di -
verse topographies and varying economic conditions across 
countries and regions. Leveraging a customized OSM da -
tabase, LHRS-Align [24]  collects remote sensing images 
from Google Earth, ensuring that their centers are aligned 
with OSM features. These images then undergo a series of 
processing steps, including resizing to a uniform size, de -
duplication to remove images dominated by vast ocean 
areas or obscured by clouds, and pruning using a trained 
network. The final dataset encompasses images from 9,259 
cities across 129 countries. Similarly, Git-10M [158] consists 
mainly of remote sensing images from Google Earth, which 
are gathered from both randomly selected and manually 
curated regions worldwide. Manual screening is performed 
to discard redundant scenes, and an image enhancement 
model is applied to all collected images, thereby improv-
ing dataset quality. SkyScript [16]  and RSTeller [197]  are 
sourced from Google Earth Engine. Among these, the au -
thors of SkyScript carefully choose 10 image collections 
from Google Earth Engine, such as SWISSIMAGE 10-cm 
RGB imagery and Landsat 9 C2 T1 TOA Reflectance, form -
ing a multisource multiresolution image pool. In contrast, 
RSTeller relies solely on one image collection, the National 
Agriculture Imagery Program, which provides aerial imag -
es covering most of the continental United States and parts 
of Hawaii, with a ground sampling distance of 0.6 m.
CAPTION GENERATION
Texts associated with remote sensing images in pretraining 
datasets, also referred to as image captions, are typically 
human-understandable sentences that describe various as-
pects of the images. These descriptions often include infor-
mation about image properties, overall scenes, and specific 
local objects. In existing pretraining datasets, only RSICap 
[17] and DIOR-Captions [83] rely entirely on manual an -
notation to generate image captions. In [17] , five remote 
sensing experts carried out the annotation process follow -
ing predefined principles. The principles consist of three 
points: first, describing image attributes; second, describ -
ing object attributes; and third, providing a description of 
the overall scene before detailing specific objects. In [83] , 
the authors refer to bounding boxes from the original im -
age dataset to describe the most prominent or abundant 
objects. Although manual annotation guarantees high-
quality image-text pairs, it is a time-intensive and laborious 
process, particularly for large-scale datasets. To address this, 
later datasets have adopted automated methods, employing 
rules or models to generate captions. This approach signifi-
cantly reduces the cost of dataset creation while greatly ex-
panding its scale.
1)
 Rule
-based captioning: Motivated by the fact that open 
source image datasets are accompanied by image semantic 
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
303 
 information in the form of bounding boxes, segmentation 
masks, and class names, Liu et al. [20] propose the B2C 
and M2B methods to convert heterogeneous annotations 
into natural language captions. In detail, B2C, designed for 
bounding box annotations, generates five distinct captions 
for each image by considering the location (bounding box 
center) and the number of objects. Meanwhile, M2B, tai-
lored for segmentation mask annotations, converts seg-
mentation masks into bounding boxes by first extracting 
contours for each class and then sorting the contour points 
to define the bounding boxes. By applying M2B followed 
by B2C, it becomes convenient to create image-text datasets 
from image segmentation datasets. With a similar motiva-
tion, Wang et al. [16] leverage semantic information from 
OSM to caption remote sensing images from Google Earth 
Engine as the two can be linked through geo-coordinates. 
Due to its uncurated nature, semantic information from 
OSM is rich but often messy. Consequently, not all this 
information is suitable for describing images, especially if 
it is not visually discernible. To address this, the authors le-
verage CLIP embeddings of semantic information as input 
and apply a binary logistic regression model to filter out 
invisible information. After filtering, a description for each 
object is crafted by assembling OSM semantic data using 
connecting words such as “of,” “is,” and “and.” Each im-
age includes two captions: one describing the object used 
to determine the image boundary and another describing 
multiple objects within the image by assembling captions 
of individual objects based on their geospatial relation-
ships. To reduce noisy image-text pairs, they finally use a 
CLIP model to estimate the similarity between images and 
texts and perform image filtering.
2)
 Mod
el-based captioning: Numerous LLMs are available for 
generating remote sensing image captions, including 
the ChatGPT series [200] , Gemini [216], Vicuna [118], 
and Mixtural [220]. To effectively apply these models, 
the key research focus lies in prompt design as caption 
quality heavily depends on the prompt. The fundamen-
tal principle of prompt design is to align with the input 
requirements of the LLM being used. For multimodal 
LLMs such as ChatGPT-4V, Gemini-Vision, BLIP-2 [69], 
and MiniGPT-4 [41], which can process both text and 
image inputs, the prompt can simply instruct the model 
to generate a caption like “generate a description for 
the image’s visual content” [15]. However, to improve 
caption quality, prompts are optimized by adding con -
straints on the model’s response [40], [42], [195] or in-
corporating semantic information about the image [40], 
[158], [195]. For instance, VersaD [42]  defines prompts 
to ensure that the captions generated by Gemini-Vision 
encompass information about image properties, ob -
ject attributes, and scene context while preventing the 
model from describing uncertain objects. Furthermore, 
it imposes constraints on the format of the model’s re-
sponse. LuoJiaHOG [40] prompts MiniGPT-4 to follow 
principles such as describing object attributes, reducing 
vague words, using “next” instead of “up,” adding syn-
onyms, and more. Incorporating image semantic infor-
mation into the prompt helps guide the model’s focus 
toward the image content of interest while enriching the 
captions. For example, in ChatEarthNet [195] , the dis -
tribution and proportion of various land cover types in 
the image, derived from the European Space Agency’s 
WorldCover project, are embedded into the prompt to 
guide ChatGPT-4V in describing the land cover types 
of interest. In LuoJiaHOG, manually corrected OSM se-
mantics are provided to MiniGPT-4, allowing the model 
to describe objects of interest within the images.
For models like ChatGPT-3.5, Vicuna-v1.5, and Mix -
tral, which primarily handle text inputs, providing image 
semantic information in the prompt allows the models to 
simulate “seeing” the image and generate captions. LHRS-
Align [24] and RSTeller [197] both utilize OSM semantics 
to assist the model in understanding the image. Since the 
quality of the semantic information directly impacts the 
quality of the captions, a significant portion of the effort in 
caption generation for these two datasets is devoted to care-
fully processing the semantic data. For example, similar to 
SkyScript [16], LHRS-Align filters out invisible semantic 
information using rules, followed by manual inspection. 
Additionally, LHRS-Align removes duplicate semantics for 
each image and applies a threshold to ensure semantic bal-
ance across the entire dataset. Given that an image is often 
associated with abundant OSM semantic data, providing 
all of it to the LLM can be overwhelming, making it diffi -
cult for the model to generate accurate captions. Therefore, 
RSTeller focuses on describing the primary object in each 
image, defined by the largest size or longest length, and re-
fers to the OSM Wiki to interpret the OSM semantics, there-
by reducing the ambiguity of the semantic information. 
Based on the processed OSM semantic data, LHRS-Align 
and RSTeller use custom templates to integrate semantic 
information into the prompt. To enhance the model’s un-
derstanding of the captioning task, a few examples are pro-
vided for the model.
Based on the captions generated by models, some 
techniques are proposed to enrich the captions further. 
In RS5M [15], the metainformation of images, e.g., longi -
tude, latitude, and timestamp, is structured into readable 
sentences and combined with the generated captions. 
Additionally, RS5M includes rotation-invariant captions, 
which are those that exhibit the most stable similarity to 
the image features, regardless of their rotation, among the 
generated captions. In RSTeller, the LLM is guided to cre -
ate multiple revisions from the generated captions with 
different tones, resulting in each image being accompa -
nied by at least two captions and up to five in total.
DATASET PROPERTY
Table 12 provides an overview of existing pretraining da -
tasets for VLM in remote sensing, with several key points 
worth noting as follows:
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
304 
1)  Geographic coverage:  Th e datasets 
RS5M, VersaD, and ChatEarthNet 
benefit from large-scale remote 
sensing image datasets, while Sky-
Script, LHRS-Align, LuoJiaHOG, 
and Git-10M derive from public 
geographic databases. Each of these 
datasets provides relatively compre-
hensive global coverage, with Git-
10M being the largest (more than10 
million image-text pairs). In con -
trast, RSTeller’s coverage is primar -
ily limited to the continental United 
States and parts of Hawaii, poten -
tially introducing geographic bias in 
trained models.
2)
  Sce
ne diversity:  Large-scale datasets 
such as RS5M, LHRS-Align, and Sky-
Script have, to some extent, ensured 
diversity in remote sensing scenes. 
Taking this a step further, the creators 
of LuoJiaHOG employ geospatial 
analysis to collect images from global 
regions with varied topography and 
different development levels. Mean-
while, the Git-10M team manually 
selects specific areas to ensure com-
prehensive coverage of representative 
scenes, including urban areas, forests, 
mountains, and deserts.
3)
  Capt
ion quality: Manually annotated 
or rule-generated image captions 
typically achieve high accuracy. 
For instance, manual inspection of 
1,000 randomly sampled image-text 
pairs from SkyScript demonstrates 
96.1% precision. Conversely, mod -
el-generated captions inevitably 
contain errors despite considerable 
efforts to mitigate noise injection. 
For smaller-scale datasets like Chat-
EarthNet, manual correction can 
significantly improve caption quali-
ty. However, this approach becomes 
prohibitively expensive for million-
scale datasets such as VersaD and 
RS5M, making caption errors un -
avoidable. Surprisingly, the authors
   of Ver
saD experimentally verified 
that models pretrained on the noisy 
dataset VersaD (82.3%) outper -
formed those trained on the more 
accurate dataset SkyScript, sug -
gesting that rich-content and long 
captions may make models less sen-
sitive to noise.
TABLE 12. SUMMARY OF PRETRAINING DATASETS FOR VLM IN REMOTE SENSING.
DATASET NO. OF PAIRS IMAGE SOURCE IMAGE SIZE 
IMAGE 
RESOLUTION (M) 
CAPTION 
GENERATION 
NO. OF CAPTIONS 
PER IMAGE 
AVG. CAP. 
LENGTH 
PUBLIC 
RSICap [17]a 2,585 DOTA-v1.5 [198] 512 × 512 — Manual annotation 1 60 ✓
DIOR-Captions [83]  16,565 DIOR [199] 800 × 800 — Manual annotation 2 11 ✗
ChatEarthNet [195] b 173,488 SatlasPretrain [152]  256 × 256 10 ChatGPT-3.5 [200]  
ChatGPT-4V [200]  
1–2 155/90 ✓
RET-3+SEG-4 +DET-
10 [20]
c
828,725 AUAIR [201] 
DOTA [198] 
iSAID [202] 
Potsdam [203]  
RSOD [204]  
Vaihingen [205]  
CARPK [206]  
HRRSD [207]  
LEVIR [208]  
RSICD [78] 
Standford [209]  
VisDrone [210] 
DIOR [199] 
HRSC [211] 
LoveDA [212] 
RSITMD [82]  
UCM [80] 
224 × 224 
~1,920 × 1,080
— M2C+B2C 
 Gen
eration [20] 
5 — ✓
VersaD [42]d 1,390,405 CrowdAI [213] 
fMoW [60] 
CVACT [214] 
LoveDA [212] 
CVUSA [215] 
Million-AID [67]  
512 × 512 0.08–153 Gemini-Vision [216]  1 369 ✓
RS5M [15]e 5,062,377 BigEarthNet [68]  
Million-AID [67]  
fMoW [60] PUB11 [15] — — BLIP-2 (6.7B) [69] 1–5 49/87 ✓
Git-10M [158]f >10,000,000 DIOR [199] 
Million-AID [67]  
SSL4EO-S12 [217] 
GeoPile [218] 
RSICB [219] 
Google Earth 
SkyScript [16]  
— 0.5–128 ChatGPT-4o [200]  1 52 ✓
LuoJiaHOG [40]  94,856 Google Maps 1,280 × 1,280 — Manual Annotation 
MiniGPT-4 [41] 
1 124 ✗
LHRS-Align [24]
g 1,150,000 Google Earth 768 × 768 1 Vicuna-v1.5 (13B) 
[118] 
1 — ✓
RSTeller [197]
h 2,539,256 Google Earth Engine 448 × 448 0.6 Mixtral-7B [220]  2–5 54 ✓
SkyScript [16] i 2,600,000 Google Earth Engine — 0.1–30 Rule-based Assem -
bly [16] 
2 — ✓
ahttps://github.com/Lavender105/RSGPT?tab=readme-ov-file; bhttps://zenodo.org/records/11003436; chttps://huggingface.co/datasets/gzqy1026/RemoteCLIP; dhttps://github.com/opendatalab/VHM; ehttps://huggingface.co/datasets/Zilun/RS5M; 
fhttps://modelscope.cn/datasets/lcybuaa1111/Git-10M/; ghttps://github.com/NJU-LHRS/LHRS-Bot; hhttps://huggingface.co/datasets/SlytherinGe/RSTeller; ihttps://github.com/wangzhecheng/SkyScript.
The average caption length (Avg. Cap. Length) in ChatEarthNet is calculated for captions generated by ChatGPT-3.5 and ChatGPT-4V separately and in RS5M for PUB11 and other remote sensing datasets separately.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
305 
4) Distinctive characteristics:  C urrent pretraining datasets 
predominantly contain English text, with DIOR-Cap -
tions being the only exception that provides bilingual 
captions in both Chinese and English. This enables the 
exploration of cross-lingual vision-language alignment. 
Moreover, while most datasets focus on optical images, 
ChatEarthNet stands out by facilitating a deeper under-
standing of multispectral images by offering satellite im-
ages with nine specific bands, each paired with textual 
descriptions.
INSTRUCTION-FOLLOWING DATASETS
Instruction-following datasets are specifically designed for 
the SFT of instruction-based vision-language models, al -
lowing them to perform specific remote sensing tasks. For 
contrastive or generation-based vision-language models, 
the emphasis during task-specific applications lies more 
on designing network architectures than on developing 
new datasets, as discussed in the “ Effective Adaptation 
Methods” and the “ Downstream Applications ” sections. 
Therefore, in the fine-tuning phase of VLM, we exclusively 
summarize instruction-following datasets. This type of 
dataset is composed of images paired with conversations, 
structured as instructions (or questions) and answers. Its 
construction involves image collection and conversation 
generation, with images primarily sourced from open 
source remote sensing image datasets, as shown in Table 
13. The main challenge lies in generating conversations, 
which is addressed in this section, followed by an over -
view of notable instruction-following datasets.
CONVERSATION GENERATION
Since sourced image datasets come with task-specific anno-
tations, the most intuitive idea is to convert these annota -
tions into a dialogue format. Currently, three methods are 
commonly used for this conversion: template-based trans-
formation, large model assistance, and manual annotation. 
Next, we demonstrate each method with examples from 
specific tasks.
1)
 Tem
plate-based transformation: To create task-specific con-
versations, open source image datasets for the given task 
are typically used. Answers are derived directly from da-
taset annotations, while instructions are either manu -
ally defined or randomly selected from a pool generated 
by LLMs. This pool contains instructions tailored to 
specific tasks, conveying the same meanings with varied 
phrasing. For example, in MMRS-1M [122] , classifica -
tion datasets such as NWPU-RESISC45 [56]  and Euro -
SAT [71], along with OD datasets like DIOR [199]  and 
DOTA [198], are selected to create instruction-following 
data for classification and detection tasks. For scene clas-
sification, the authors design instructions using the tem-
plate “What is the category of this remote sensing image? 
Answer the question using a single word or phrase. Ref-
erence categories include category 1, …, and category 
n.” For OD, the template is “Detect all objects shown in 
the remote sensing image and describe using oriented 
bounding boxes.” In SkyEye-968k [43] , corresponding 
datasets are selected for image captioning and VG tasks, 
including UCM-captions [3], Sydney-captions [3], RSVG 
[143], and DIOR-RSVG [144]. Separate instruction pools 
are constructed for each task. The image captioning 
pool includes instructions such as “Briefly describe this 
image” or “Provide a concise depiction of this image.” 
Meanwhile, the VG pool contains instructions like “Give 
me the location of referring expression” or “Where is re-
ferring expression?”. Based on task-specific instruction-
following data, MTCs can be generated by mixing data 
from different tasks [43], [122].
2)
 Lar
ge model assistance:  Simple template-based trans -
formations are insufficient for generating instruction-  
foll
owing data for complex reasoning tasks, which 
equip vision-language models with higher-order cogni-
tive abilities, such as making decisions and identifying 
relationships. Additionally, MTCs created through mul-
titask mixing after template transformation often lack 
diversity. To address these issues, LLMs are utilized to 
generate instruction-following data. By providing a few 
manually defined in-context examples, the models learn 
to create high-quality instruction-answer pairs based on 
image captions and other related information. For in -
stance, Kuckreja et al. [18] prompt Vicuna [118] to gener-
ate 30,000 detailed image descriptions, 65,000 MTCs, 
and 10,000 examples of complex reasoning based on 
short descriptions supplied. Similarly, Muhtar et al. [24] 
use Vicuna [118] to produce instruction-following data 
based on image captions from RSITMD [82]  and NW -
PU-Captions [141]. To ensure that the generated conver-
sations focus on the visual content of the images, their 
prompts are designed to generate questions that inquire 
about object types, actions, locations, relative positions 
between objects, and more. However, considering the 
limitations of RSITMD and NWPU-Captions, where 
captions are short and lack detailed content, the authors 
supplemented their data with additional image-caption 
pairs from LHRS-Align [24] . GPT-4 is prompted with 
image captions, object bounding boxes, and object attri-
butes to generate detailed image descriptions, complex 
reasoning, and MTCs that incorporate questions about 
object locations and counts.
3)
 Man
ual annotation: In constructing instruction-following 
datasets, manual annotation primarily serves to provide ac-
curate information that supports template-based or large 
model-assisted conversation generation methods rather 
than directly creating conversations. For example, in FIT-
RS [25], an instruction-following dataset for understand-
ing semantic relationships between objects, Luo et al. [25] 
manually annotate very high-resolution remote sensing 
images with detailed scene graph labels, laying the ground-
work for generating conversations focused on relationship 
comprehension tasks. In TITANIC-FGS [133], the authors 
manually summarize the common and private features of 
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
306 
fine-grained objects, which are used to populate predefined 
templates for creating object descriptions. These descrip-
tions are subsequently used to prompt GPT-4, enabling 
the generation of MTCs that simulate human-like logical  
decision-making.
The three methods can be combined to facilitate the con-
struction of instruction-following datasets [25], [44], with the 
goal of creating rich and diverse samples across a variety of tasks. 
To help vision-language models better distinguish between 
tasks, a common practice is to incorporate task-specific identi-
fiers into the instructions, as demonstrated in [18], [24], [42], 
and [43]. One example is [43] where the authors introduce task 
identifiers such as “[caption],” “[vqa],” and “[refer]” to specify 
captioning, VQA, and VG tasks, respectively.
IMPRESSIVE DATASETS
Table 13  provides an overview of existing instruction-  
following datasets. MMShip [129] and TITANIC-FGS [133] 
are specifically tailored for ship image analysis, whereas 
the others are designed for general-purpose remote sens -
ing data analysis. Most datasets include instruction data for 
tasks such as captioning, VQA, VG, and MTCs [18] , [24], 
[25], [43], [122], [133], [134], [228], [228]. Among these, 
MMRS-1M [122] stands out for featuring instruction data 
that involve not only optical remote sensing images but 
also SAR and IR images. To achieve this, three SAR im -
age datasets, i.e., AIR-SARShip-2.0 [237], SSDD [224], and 
HRISD [223], along with six IR image datasets (HIT-UAV 
[245], Sea-shipping [242], IR-security [240], Aerial-mancar 
[238], Double-light-vehicle [241], and Oceanic ship [246]) 
are incorporated into the creation of instruction-following 
data. Beyond supporting common tasks, some datasets 
incorporate innovative instruction data that endow vi -
sion-language models with impressive capabilities. These 
include fine-grained image understanding, time-series im -
age analysis, quantitative analysis, honest question answer-
ing, and object relationship comprehension. The remainder 
of this section offers a detailed discussion of these datasets.
1)
 RSVP-
3M: RSVP-3M  [44] is the first visual prompting 
instruction dataset for remote sensing, with samples 
consisting of images, visual prompts, and conversations. 
The visual prompts take the form of masks that match 
the size of the corresponding image. These masks in -
clude bounding boxes or points, highlighting regions or 
points of interest specified by the user within the image. 
For a specific task, visual prompts in the instructions 
are described as “each marked point” or “each marked 
region,” directing the model to focus on the indicated 
areas and perform the corresponding analysis. For ex -
ample, in point-level classification, the instruction can 
be “Please identify the category of each marked point 
in the image,” with the model’s response formatted as 
“<Mark 1>: Label 1\n <Mark 2>: Label 2\n.” Similarly, 
for region-level classification, instructions like “Please 
identify the category of each marked region in the im -
age” yield “<Region 1>: Label 1\n<Region 2>: Label 2\n.” 
To preserve the model’s image-level capabilities, image-
level visual prompts are represented as bounding boxes 
with dimensions [0, 0, width, height]. Based on these 
definitions, RSVP-3M source images from 28 existing 
datasets spanning tasks such as image classification, im-
age caption, OD, and instance segmentation. Ground 
truth bounding boxes or masks from these datasets are 
used as visual prompts. Furthermore, GPT-4V is em -
ployed to automatically generate instruction data, re -
sulting in more than 3 million samples. This large-scale 
dataset empowers EarthMarker [44] with a multigranu-
larity understanding of remote sensing images across 
image, region, and point levels for tasks like classifica -
tion, captioning, and relationship analysis.
2)
 TEOC
hatlas: TEOChatlas [134] seeks to unlock the po -
tential of vision-language models for time-series im -
age analysis. It provides instruction data for a variety of 
temporal tasks, including temporal scene classification, 
change detection (represented by bounding boxes), tem-
poral referring expression, spatial change referring ex -
pression, and image/region change question answering. 
These tasks span two real-world applications: disaster 
response and urban development monitoring. Figure 8  
illustrates examples of each task. To ensure diverse im -
age sequence lengths and image sources, TEOChatlas 
integrates datasets such as xBD [256], S2Looking [255], 
QFabric [254], and fMoW [60], which cover bitemporal, 
pentatemporal, and multitemporal sequences. These 
datasets are sourced from six different sensors, namely 
WordView-2/3, Sentinel-2, GaoFen, SuperView, and Bei -
Jing-2. Building on this diverse data foundation, conver-
sation generation is assisted by GPT-4o [200], resulting 
in 245,210 samples tailored for temporal tasks. In ad -
dition, TEOChatlas incorporates 308,861 samples from 
the instruction-following dataset of GeoChat [18], which 
focuses on single-image analysis. To help the model 
differentiate between single-image tasks and temporal 
tasks, task-specific instructions are supplemented with 
prompts that explicitly specify the input consists of a 
sequence of images and, optionally, indicate the resolu-
tion and sensor name of the input images.
3)
 LHR
S-Instruct and VariousRS-Instruct: LHRS-Instruct [24] 
and VariousRS-Instruct [42]  initiate the exploration of 
applying vision-language models for quantitative image 
analysis. In LHRS-Instruct, GPT-4 is prompted with im-
age captions and detailed information about the types 
and coordinates of objects within the image. This allows 
GPT-4 to generate conversations centered on the num -
ber of objects, thereby equipping LHRS-Bot [24]  with 
the capability to perform object counting. VariousRS-
Instruct, on the other hand, extends its functionality 
beyond object counting by incorporating instruction 
data designed for geometric measurement tasks, aimed 
at estimating the length and width of objects within im-
ages. These instruction data are derived from the DOTA 
[198] and FAIR1M [225]  datasets through a process of 
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
307 
TABLE 13. SUMMARY OF INSTRUCTION-FOLLOWING DATASETS FOR FINE-TUNING VISION-LANGUAGE MODELS IN REMOTE 
SENSING.
DATASET TASK DATA SOURCE 
NO. OF SAMPLE 
TRAIN/TEST PUBLIC 
MMShip [129] OD DOSR[222] DOTA ship 
subset [198] 
HRSID [223]  SSDD [224]  81,000/— ✗
HnstD [42]a Honest question 
answering
DOTA v2 [198] FAIR1M [225] 45,000/1,642 ✓
VersaD-In -
struct [42]
b
Complex reasoning 
MTC
DIOR [199] DOTA v2 
[198] 
FAIR1M [225] 30,000/— ✓
RS-Instruc -
tions [132] 
Image captioning 
VQA
RSIVQA [226]  RSVQA-LR [4]  UAV [227]  UCM-captions 
[3] 
5,506/1,552 ✓
RS-GPT4V 
[228] 
VG 
Complex reasoning 
Image/region cap -
tioning 
VQA
DIOR-RSVG 
[144] RSIVQA 
[226] 
FloodNet 
[229] RSVQA-
HR [4] 
NWPU-Captions 
[141] RSVQA-
LR [4] 
RSICD [78] 
Sydney-cap -
tions [3] 
RSITMD [82]  
UCM-captions 
[3] 
991,206/258,419 ✗
SkyEye-968k 
[43]
c
VG 
Image/video cap -
tioning 
VQA 
MTC
CapERA [230]  
NWPU-
Captions [141] 
RSVQA-HR [4]  
UCM-captions 
[3] 
DIOR-RSVG 
[144] RSICD 
[78] RSVQA-
LR [4] UCM-
Conversa [43] 
DIOR-Conversa 
[43] RSITMD 
[82] RSVG [143] 
DOTA-
Conversa [43] 
RSIVQA [226]  
Sydney-cap -
tions [3] 
ERA-VQA [43]
RSPG [43] 
Sydney-Con -
versa [43]
968,000/— ✓
Multitask 
dataset [24]
d
VG 
Scene classification 
Image captioning
VQA
DIOR-RSVG 
[144] 
RSITMD [82]
fMoW [60] 
RSVG [143]
METER-ML [147]  
RSVQA-HR [4]
NWPU-
RESISC45 [56]  
RSVQA-LR [4]
RSICD [78] 
UCM-captions 
[3]
42,322/— ✓
TITANIC-FGS 
[133] 
Fine-grained ship 
classification 
Ship image captioning 
Ship Image VQA 
MTC
Google Baidu 16,876/2,053 ✗
RSVP-3M [44]  Scene/region/point 
classification 
Image/region/point 
captioning 
ORR 
MTC
DIOR-RSVG 
[144] 
Hi-UCD [231] 
LEVIR [208]  
Optimal-31 
[73] 
SOTA [232] 
VisDrone [210] 
DOSR [222] 
HRRSD [207] 
MAR20 [233]  
Potsdam 
[203] 
SOTA [232] 
WHU [234] 
DOTA v2 [198] 
HRSC2016 [211] 
NWPU-Captions 
[141] 
RSD46-WHU 
[204] 
UAVid [235] 
WHU-RS19 [142] 
FAIR1M [225] 
iSAID [202] 
NWPU-RE -
SISC45 [56]  
RSITMD [82]  
UCAS-AOD 
[236] 
FAST [232] 
Kuckreja et al. 
[18] 
NWPUVHR10 
[237] 
RSOD [204]  
Vaihingen 
[205] 
3,648,884/— ✗
Kuckreja et 
al. [18]
e
VG 
Complex reasoning 
Scene classification 
Image/region cap -
tioning 
VQA 
MTC
DIOR [199] 
RSVQA [4] 
DOTA [198] FAIR1M [225] FloodNet 
[229] 
NWPU-RE -
SISC45 [56]  
306,000/12,000 ✓
LHRS-Instruct 
[24]
f
Object counting 
Image captioning 
Complex reasoning 
Object attribute 
recognition 
Object relationship 
analysis 
MTC
LHRS-Align 
[24] 
NWPU-Cap -
tions [141] 
RSITMD [82]  39,800/— ✓
MMRS-1M 
[122]
g
VG 
OD 
Scene classification 
Image/region cap -
tioning 
VQA 
MTC
Aerial-mancar 
[238] 
DOTA [198] 
FGSCR-42 
[239] 
IR-security 
[240] 
R
SICD [78] 
RSVQA-LR [4]  
UCM [80] 
AIR-SARShip- 
2.0 [237] 
Double-light-
vehicle [241] 
FloodNet [229] 
NWPU-Cap-
tions [141] 
RSITMD [82] 
Sea-shipping 
[242] 
UCM-captions 
[3] 
CRSVQA [243]  
DSCR [244] 
HIT-UAV [245] 
NWPU-RE -
SISC45 [56]  
RSIVQA [226]  
SSDD [224] 
WHU-RS19 [142] 
DIOR [199] 
EuroSAT [71] 
HRRSD [207]  
NWPUVHR10 
[237] RSOD 
[204] Sydney-
captions [3]  
VisDrone [210] 
DIOR-RSVG 
[144] 
FAIR1M [225] 
HRSID [223]  
Oceanic ship 
[246] 
RSSCN7 [75]  
UCAS-AOD 
[236] 
1,005,842/— ✓
(Continued )
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
308 
template-based conversion. Ground truth for object 
sizes is determined using image resolution and the sizes 
of objects’ bounding boxes. Beyond quantitative tasks, 
VariousRS-Instruct also introduces new qualitative tasks 
such as building vectorizing and multilabel classifica -
tion. These advancements contribute to the evolution 
of a versatile vision-language model for comprehensive 
remote sensing image analysis.
4)
 Hnst
D: HnstD [42]  aims to enhance the honesty of 
vision-language models. It incorporates four rec -
ognition tasks: identifying the relative positions 
between objects and their presence, color, and abso -
lute positions. Except for the presence task, all oth -
ers include both factual and deceptive instructions. 
Specifically, deceptive instructions regarding object 
color arise from either the absence of objects or their 
presence in panchromatic images, while those for 
relative and absolute positions stem from the ab -
sence of objects. This combination of factual and 
deceptive instructions helps prevent models from 
producing affirmative answers to unreasonable user 
queries. To construct HnstD, template-based trans -
formations are applied to images sourced from the 
DOTA [198] and FAIR1M [225]  datasets. The model’s 
response formats vary by task: “yes” or “no” for the 
presence task, free-form text for the color task, and 
option selection from candidates for relative and 
absolute position tasks. Figure 12  presents samples 
from the HnstD dataset.
5)
 FIT-R
S: FIT-RS [25]  is designed to advance vision-
language models’ ability to comprehend fine-grained 
semantic relationships, introducing tasks such as ob -
ject relationship reasoning (ORR) and scene graph 
generation at both region and image levels. ORR is 
further divided into relation detection and relation 
reasoning. Relation detection requires the model to 
TABLE 13. SUMMARY OF INSTRUCTION-FOLLOWING DATASETS FOR FINE-TUNING VISION-LANGUAGE MODELS IN REMOTE 
SENSING.
DATASET TASK DATA SOURCE 
NO. OF SAMPLE 
TRAIN/TEST PUBLIC 
FIT-RS [25] h VG 
OD 
Image/region cap -
tioning 
VQA 
Multilabel classifica -
tion 
ORR 
Image/region scene 
graph generation 
MTC
extended 
STAR [247] 
1,440,681/360,170 ✓
VariousRS-
Instruct [42]
i
VG 
Object counting 
Scene classification 
Building vectorizing 
Geometric measure -
ment 
Image property 
recognition 
VQA 
Multilabel classifica -
tion
BANDON 
[248] 
FAIR1M [225] 
MSAR [249] 
RSVQA-LR [4] 
CrowdAI [213] 
FBP [250] 
MtS-WH [251] 
UCM-captions 
[3] 
DeepGlobe 
[252] 
fMoW [60] 
NWPU-RE -
SISC45 [56]  
DIOR-RSVG 
[144] 
GID [253] 
Potsdam [203]  
DOTA v2 [198] 
METER-ML 
[147] 
RSITMD [82]  
76,445/13,020 ✓
TEOChatlas 
[134]
j
VG 
Change detection 
Scene classification 
Image/region cap -
tioning 
VQA 
Temporal scene clas-
sification 
Temporal referring 
expression 
Spatial change refer-
ring expression 
Image/region change 
question answering
fMoW [60] Kuckreja et al. 
[18] 
QFabric [254]  S2Looking 
[255] 
xBD [256] 554,071/— ✓
ahttps://github.com/opendatalab/VHM; bhttps://github.com/opendatalab/VHM; chttps://huggingface.co/datasets/ZhanYang-nwpu/SkyEye-968k; dhttps://github.com/NJU-LHRS/LHRS-
Bot?tab=readme-ov-file; ehttps://github.com/mbzuai-oryx/geochat; fhttps://github.com/NJU-LHRS/LHRS-Bot?tab=readme-ov-file; ghttps://github.com/wivizhang/EarthGPT; hhttps://
huggingface.co/datasets/ll-13/FIT-RS; ihttps://github.com/opendatalab/VHM; jhttps://huggingface.co/datasets/jirvin16/TEOChatlas.
UAV: unmanned aerial vehicle.
(Continued).
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
309 
predict relationships between targets based on the 
provided categories and locations of subjects and ob -
jects, while relation reasoning demands the model 
to additionally infer the categories of the subject or 
object along with their relationships. Region-level 
scene graph generation involves describing an object’s 
relative size, position, and visibility, followed by gen -
erating all subject-relation-object triples. Image-level 
scene graph generation extends this by creating scene 
graphs for entire images, appending isolated objects 
(with no relationships) to the answer in an OD for -
mat. These tasks challenge vision-language models 
to accurately interpret relationships between targets 
and distinguish between subjects and objects within 
those relationships. To construct FIT-RS, the scene 
graph generation dataset STAR [247] , containing more 
than 400,000 triplets across 48 object categories and 
58 semantic relationship categories, serves as the data 
source. Template-based transformations are applied 
to generate task-specific instruction data. Similar to 
most instruction-following datasets, FIT-RS also sup -
ports tasks such as captioning and VQA. In total, it 
comprises 1,800,851 samples.
BENCHMARK DATASETS
Benchmark datasets are essential for evaluating and fairly 
comparing the performance of different models. To fos -
ter research in VLM for remote sensing, efforts have been 
made to develop benchmark datasets that integrate re -
mote sensing images with text. Consequently, datasets like 
UCM-captions [3], RSICD [78], and RSVQA [4] have been 
established and are widely utilized to assess models’ abili-
ties in visual perception, text understanding, and image-
text alignment [15], [20], [24], [42]. However, the limited 
scale and task-specific design of these existing datasets 
have proven inadequate for a comprehensive evaluation of 
modern vision-language models. These advanced models, 
benefiting from the two-stage training paradigm and large-
scale training datasets, excel at handling diverse remote 
sensing image analysis tasks in a conversational manner. 
In response to this, researchers have begun crafting large-
scale benchmark datasets that encompass diverse tasks and 
are carefully designed to align with the input–output re -
quirements of models, aiming to challenge vision-language 
models and push the boundaries of their capabilities. This 
section highlights these new research advancements [194], 
[196], [257], [258], [259], categorizing benchmark datasets 
(a)
(c)
(d)
(b)
FIGURE 12. Samples in the HnstD dataset [42] . (a) Presence. (b) Color. (c) Absolute position. (d) Relative position.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
310 
into two types: instruction-specific datasets and general-
purpose datasets, based on the organization of their sam -
ples. Note that remote sensing image datasets, such as AID 
[70] and NWPU-RESISC45 [56], which have been adapted 
for evaluating the performance of vision-language models 
through appropriate transformations, are comprehensively 
reviewed in [67] and [260].
INSTRUCTION-SPECIFIC DATASETS
This type of dataset is specifically designed for instruc -
tion-based vision-language models, with samples typi -
cally presented as image-instruction-answer pairs. Its 
creation follows a similar process to instruction-follow -
ing datasets, involving image collection and conversation 
generation. As summarized in Table 14, most benchmark 
datasets source their images from open source image da -
tasets and leverage methods such as template transfor -
mation, assistance from LLMs, and human annotation 
to produce high-quality conversations. Detailed meth -
ods for conversation generation are provided in the “ In-
struction-Following Datasets ” section, while this section 
focuses on dataset properties, including task formula -
tions, the number of samples, and question format. From 
Table  14, three key observations can be made. First, to 
align with the development trend of multifunctional ca -
pabilities in vision-language models, benchmark datasets 
now include an increasing variety of tasks, extending be -
yond common ones like image captioning and VQA. They 
now encompass more complex reasoning tasks as well 
as expansions from single-image analysis to temporal or 
cross-view image analysis. Second, the scale of these da -
tasets is increasing, often comprising tens of thousands of 
samples. However, as these samples are spread across an 
ever-growing variety of tasks, only a few hundred samples 
are typically available for each specific task. Third, ques -
tion formats in the datasets can be categorized into three 
types: single choice, multiple choice, and open ended. For 
single-choice and multiple-choice questions, each ques -
tion is accompanied by a set of candidate answers, facili -
tating the objective assessment of model performance. In 
contrast, open-ended questions allow models to gener -
ate answers freely, closely reflecting real-world scenarios 
where users may not know the answer in advance. Next, 
we present a detailed discussion of these datasets.
1)
 RSI
Eval and VRSBench: RSIEval [17] and VRSBench [194] 
both focus on common tasks. RSIEval, which relies en -
tirely on human annotation, is limited in scale, with 
only 100 samples for image captioning and 936 sam -
ples for VQA. Such a small dataset may be insufficient 
for evaluating a model’s practicality and robustness. In 
contrast, VRSBench leverages a semiautomatic creation 
pipeline, substantially increasing the dataset size. It con-
tains 29,614, 52,472, and 123,221 samples for image 
captioning, VG, and VQA, respectively, with LLaVA-1.5 
[131], GeoChat [18], and GPT-4V achieving the best per-
formance in each task.
2)
 FIT -RSRC and LHRS-Bench:  FIT-RSRC [25]  and LHRS-
Bench [24] challenge a model’s capability to understand 
relationships between objects in images. Specifically, 
FIT-RSRC examines the comprehension of semantic re -
lationships among objects, using terms like “run along” 
and “around” to describe these relationships. It features 
four types of questions: querying the relationship be -
tween two objects, the existence of a specific relationship, 
the subject of a relationship, and the object of a relation-
ship. Furthermore, each type of question contains unan-
swerable variants to assess the model’s robustness and 
veracity. LHRS-Bot, on the other hand, targets spatial 
relationships, describing them using terms like “top,” 
“middle,” and “bottom.” It also includes tasks such as ob-
ject counting, object attribute recognition, image proper-
ty recognition, and visual reasoning. However, with only 
690 samples in total and some tasks containing merely 
a few dozen samples, its scale is highly inadequate. On 
these datasets, SkySenseGPT [25]  and LHRS-Bot [24]  
demonstrate the best performance, respectively.
3)
 CORE
val: COREval [258]  expands the evaluation of 
model capabilities from perception to reasoning. In 
terms of perception, it encompasses image-level com -
prehension, single-instance identification, and cross-
instance discernment. For reasoning, it emphasizes 
inferring attributes of scenes or instances, such as the 
CO
2 emissions or population density of a scene, the 
height of a building, or the imaging season of an im -
age. These two evaluations are divided into six subdi -
mensions and 22 specific tasks, with a total of 6,263 
samples. Recognizing the significant regional intraclass 
variations in remote sensing images, COREval incorpo-
rates images sourced from multiple satellites (Landsat-8, 
SDGSAT-1, and Sentinel-1/2) and geographic databases 
(Google Earth, Natural Earth, and SWISSIMAGE), of -
fering geographic coverage of 50 cities spanning six 
continents. On this dataset, 13 open source vision-
language models from both general and remote sensing 
domains are evaluated. The experimental results reveal 
that while existing models perform well in image-level 
comprehension, they struggle with fine-grained in -
stance perception and complex reasoning tasks.
4)
 VLEO
-Bench and GEOBench-VLM:  VLEO-Bench [261]  
and GEOBench-VLM [259], from the perspective of re -
mote sensing applications (e.g., urban monitoring and 
disaster management), integrate remote sensing image 
analysis tasks such as temporal analysis of changes, 
counting objects, and understanding relationships be -
tween objects. In VLEO-Bench, three types of tasks are 
included: scene understanding, which tests a model’s 
ability to combine high-level image semantics with 
knowledge expressed in language; object localization 
and counting, which evaluate fine-grained perception; 
and change detection, which assesses a model’s 
capability to identify differences between multiple 
images. Experimental results on VLEO-Bench show that 
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
311 
although state-of-the-art models like GPT-4V achieve 
strong performance in scene understanding, their poor 
spatial reasoning limits their effectiveness in object  
localization, object counting, and change detection 
TABLE 14. SUMMARY OF BENCHMARK DATASETS FOR VISION-LANGUAGE MODELS IN REMOTE SENSING.
DATASET DATA SOURCE TASK 
NO. OF 
SAMPLE 
QUESTION 
FORMAT 
PUBLIC 
FIT-RSRC [25] a — Relation comprehension — Single choice ✓
RSIEval [17]b DOTA-v1.5 [198] Image captioning 
VQA
100
936
Open ended 
Open ended
✓
VRSBench 
[194]
c
DIOR [199] DOTA v2 [198]  Image captioning 
VG 
VQA
29,614 
52,472 
123,221
Open ended ✓
LHRS-Bench 
[24]†
d
Google Earth Object counting 
Visual reasoning 
Object attribute recognition 
Image property recognition 
ORR
690 Single choice ✓
COREval [258]
† Google Earth 
SDGSAT-1 
SWISSIMAGE
Landsat-8 
Sentinel-1
Natural Earth 
Sentinel-2
Image-level comprehension 
Single choice 
Cross-instance discernment 
Attribute reasoning 
Assessment reasoning 
Common sense reasoning
3,257 
1,244 
562 
300 
400 
500
Single choice 
Single choice 
Open ended 
Single choice 
Single choice 
Single choice
✗
VLEO-Bench 
[261]
e
Aerial Landmark 
[261] 
COWC [262] 
NeonTreeEvalua -
tion [263] 
xBD [256] 
Animal Detection 
[264] 
DIOR-RSVG [144]  
PatternNet [74]  
BigEarthNet [68]  
fMoW-WILDS [265]  
RSICD [78] 
Location recognition 
Image captioning 
LULC classification 
Multilabel LULC classification 
VG 
Object counting 
Change detection
602 
1,009 
3,000 
1,000 
— 
2,239 
—
Single choice 
Open ended 
Single choice 
Multiple choice 
Open ended 
Open ended 
Open ended
✓
GEOBench-
VLM [259]
†f
AiRound [266]  
Deforestation 
[267] 
FAIR1M [225] 
fMoW [60] 
GeoNRW [268]  
NASA Marine 
Debris [269]  
PatternNet [74]  
So2Sat [270] 
COWC [262] 
DIOR [199] 
FGSCR-42 [239]  
Forest Damage 
[271] 
GVLM [272] 
NWPU-RESICS45 
[56] 
QuakeSet [273]  
xBD [256] 
DeepGlobe [252]  
DOTA [198] 
FireRisk [274]  
FPCD [275] 
MtSCCD [276] 
PASTIS [277]  
RarePlanes [278]  
Scene understanding 
Object classification 
Object localization and 
detection 
Event detection 
Caption generation 
Semantic segmentation 
Temporal understanding 
Nonoptical imagery
10,000 Single choice 
Single choice 
Single choice 
Single choice 
Open ended 
Open ended 
Single choice
✓
UrBench [196]
† Cityscapes [279]  
IM2GPS [280]  
Google Earth 
Google Street 
View
MTSD [281] 
VIGOR [282] 
Geo-localization 
Scene reasoning 
Scene understanding 
Object understanding
4,562 
1,872 
2,606 
2,968
Single choice 
Single choice 
Single choice 
Open ended 
Single choice
✗
GeoText-1652 
[283]
g
University-1652 
[284] 
Image-text retrieval 1,652 N/A ✓
DIOR-RSVG 
[144]
h
DIOR [199] VG 38,320 N/A ✓
RemoteCount 
[20] 
DOTA [198] Object counting 947 N/A ✗
SATIN [257]
i AID [70] 
BCS Scenes [285]  
EuroSAT [71] 
MLRSNet [72]  
NWPU-RESISC45 
[56] 
Post Hurricane 
[286] 
RSI-CB256 [77]  
SAT-6 [287] 
UCM [80] 
AWTP [288]  
Canadian Crop -
land [289] 
GID [253] 
MultiScene [290]  
Optimal-31 [73]  
R SC11 [76] 
RSSCN7 [75]  
SIRI-WHU [79]  
USTC-SmokeRS 
[291] 
BC Scenes [292]  
CLRS [145] 
Million-AID [67]  
NaSC-TG2 [146] 
PatternNet [74]  
RSD46-WHU [204]  
SAT-4 [287] 
SISI [293] 
WHU-RS19 [142] 
Land cover classification 
Land use classification 
Hierarchical land use clas-
sification 
Complex scene classification 
Rare scene classification 
False color scene classification
201,000 
260,205 
34,000 
135,261 
105,840 
39,326
N/A ✓
ahttps://huggingface.co/datasets/ll-13/FIT-RS/tree/main/FIT-RSRC; bhttps://github.com/Lavender105/RSGPT; chttps://huggingface.co/datasets/xiang709/VRSBench; dhttps://huggingface.
co/datasets/PumpkinCat/LHRS_Data/tree/main/LHRS-Bench; ehttps://huggingface.co/collections/mit-ei/vleo-benchmark-datasets-65b789b0466555489cce0d70; fhttps://huggingface.co/
datasets/aialliance/GEOBench-VLM; ghttps://multimodalgeo.github.io/GeoText/; hhttps://github.com/zhanyang-nwpu/rsvg-pytorch; ihttps://huggingface.co/datasets/jonathan-roberts1/SATIN.
†indicates that the dataset’s task follows a hierarchical taxonomy. In this table, only the broad dimensions of task design are displayed rather than specific tasks. N/A: not applicable.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
312 
tasks. GEOBench-VLM, on the other hand, includes 
more than 10,000 questions across 31 tasks, categorized 
into eight broad categories. Compared to VLEO-Bench, 
GEOBench-VLM offers a more diverse range of tasks, in-
cluding unique ones like referring expression segmen -
tation and nonoptical imagery analysis. Additionally, 
unlike VLEO-Bench, which confines change detection 
to counting damaged buildings and estimating damage 
severity, GEOBench-VLM’s temporal understanding 
covers a wider range of tasks. These include detect -
ing the presence of changes, reasoning about change 
causes, assessing disaster impacts, and classifying crop 
types based on long-term time-series images. Detailed 
experiments with 10 vision-language models show that 
none excels across all tasks in GEOBench-VLM. Signifi -
cant effort is still required to develop remote sensing-
specific models capable of addressing challenging tasks 
like referring expression segmentation.
5)
 UrBe
nch: UrBench [196] concentrates on exploring the 
potential applications of vision-language models in ur -
ban scenarios. It comprises 11,600 questions across 14 
tasks, spanning four dimensions: geo-localization, scene 
reasoning, scene understanding, and object understand-
ing. Unlike the previously mentioned datasets that in -
clude only single-view questions in satellite or aerial 
images, UrBench also incorporates cross-view questions 
in which each question pairs images of the same scenario 
captured from satellite and street views. These questions 
involve tasks such as image retrieval, orientation identifi-
cation, camera localization, road understanding, object 
attribute recognition, and object matching. Meanwhile, 
they encompass region-level questions, which examine a 
model’s ability to understand urban scenarios at a region 
level, and role-level questions, which evaluate its poten-
tial to assist humans in daily life. Experiments with 21 
general-purpose models on UrBench reveal that current 
models still lag significantly behind human experts in 
urban environments. They struggle to understand multi-
view image relations, and their performance varies incon-
sistently across different views.
GENERAL-PURPOSE DATASETS
General-purpose datasets are designed to evaluate various 
types of vision-language models. These datasets typically 
target specific remote sensing multimodal tasks, with sam-
ples consisting of images and task ground truth presented in 
text form. As summarized in [2] , commonly used general-
purpose datasets for vision-language model evaluation have 
been well documented. This section, therefore, focuses on 
the latest research advancements from the past two years, 
as outlined in Table 14. GeoText-1652 [283] is a language-
guided geo-localization benchmark built on the Universi -
ty-1652 image dataset [284]. Each image includes detailed 
image-level descriptions as well as region-level brief descrip-
tions with corresponding bounding boxes. DIOR-RSVG 
[144] is designed for VG toward remote sensing. Through an 
automatic expression generation method, it contains 38,320 
image-expression-box triplets. The large scale of this data -
set makes it a popular choice for evaluating vision-language 
models. RemoteCount [20] evaluates object-counting abili-
ties. Each image is paired with a human-annotated caption, 
such as “a photo of nine tennis courts.” As a result, this da-
taset is small in scale, containing only 947 samples. To meet 
the input requirements of contrastive learning-based foun-
dation models, the original caption is augmented with nine 
additional captions by replacing the number in the caption 
with all numbers from one to 10. 
SATIN [257] sources images from 27 remote sensing da-
tasets and is designed to evaluate the classification capa -
bilities of models for satellite images. This dataset covers six 
classification tasks: land cover, land use, hierarchical land 
use, complex scenes, rare scenes, and false color scenes. 
Among these, the hierarchical land use task tests the abil -
ity to classify land use across varying levels of granularity, 
while the complex scene task leverages the large view fields 
of remote sensing images to assess the capability of iden -
tifying multiple land use types within a single image. Ex -
periments with 40 vision-language models on SATIN in a 
zero-shot setting show that this dataset poses a significant 
challenge, with even models trained on billions of natural 
images achieving an accuracy of just over 50%.
CONCLUSION AND FUTURE DIRECTIONS
From the perspectives of models and datasets, we have cov-
ered the advancements in VLM for remote sensing, know -
ing how remote sensing images and natural language can 
be effectively bridged, which remote sensing tasks existing 
vision-language models can address, and which datasets 
are suitable for developing and testing vision-language 
models. Naturally, this raises two important questions.
1)
 Are e
xisting vision-language models adequate for practi-
cal applications? 
2) If not, w
hich directions are worth pursuing to advance 
this field further? 
The answer to the first question is, unsurprisingly, no. VLM re-
mains a highly challenging task and is far from meeting practical 
needs. In this section, we aim to share insights on future research 
directions from two perspectives: models and datasets.
EFFECTIVE REPRESENTATION AND  
ALIGNMENT FOR CROSS-MODAL DATA
A growing trend seeks to advance vision-language mod -
els to accommodate a wide range of remote sensing im -
ages, including optical, SAR, and IR, thereby enabling 
the acquisition of more comprehensive information 
about Earth’s surface [122] , [129]. However, in applica -
tions such as disaster risk assessment, these models may 
need to integrate additional information sources beyond 
remote sensing images, such as geospatial vector data and 
social media, to perform complex reasoning [294] , [295]. 
Geospatial vector data present complex data structures 
in the form of points, polylines, polygons, and networks. 
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
313 
Meanwhile, social media encompasses texts in various 
languages (e.g., Chinese, English, and French), diverse 
types of images (e.g., photographs and emojis), videos, 
and more. This complexity and diversity pose challenges 
for models in comprehending the embedded information 
and associating it with remote sensing data. Consequent -
ly, there is a pressing need for effective representation and 
alignment across a broader scope of cross-modal data.
REQUIREMENTS ARBITRARILY  
DESCRIBED IN NATURAL LANGUAGE
Existing instructions for prompting vision-language models 
are usually definite, such as using task identifiers to specify 
particular remote sensing tasks [18], [43] or providing candi-
date answers [24]. However, in more realistic and practical sce-
narios, requirements described in natural language tend to be 
vague and complex, involving the sequential execution of mul-
tiple tasks. For example, given an instruction like “Please assess 
the water quality in the indicated area of the image,” the model 
must be capable of decomposing water quality assessment into 
two subtasks—water detection and quantitative retrieval—and 
then accomplish each task in sequence. This creates the need to 
advance further models’ language understanding capability to 
adapt to users’ arbitrary or flexible demands.
ENHANCING THE RELIABILITY OF MODEL  
ANSWERS VIA EXPERT EXPLANATIONS
Existing vision-language models take language instructions 
and visual representations as input, producing image analysis 
results in the form of natural language. However, these models 
typically do not provide expert explanations for their answers, 
leading users to doubt the reliability of the outputs, especially 
in tasks requiring complex reasoning rather than simple recog-
nition. For instance, in precision agriculture, where the model 
may be used to guide pesticide spraying schedules, the lack 
of explanations regarding crop diseases and pest conditions 
makes it hard to convince farmers to follow the recommenda-
tions. Consequently, expert explanations accompanying im-
age analysis results are necessary as they not only enhance the 
reliability and interpretability of the model’s answers but also 
offer insight into the decision-making process, thereby foster-
ing user trust. A recent initiative [133] has begun exploring the 
feasibility of developing the vision-language model capable 
of performing fine-grained ship classification while provid-
ing reasoning behind its classification. This is accomplished 
by integrating domain knowledge into the construction of the 
instruction-following dataset.
CONTINUALLY ADAPTING VISION- 
LANGUAGE MODELS
It is well known that new remote sensing images are collect-
ed daily from around the world, and human demands may 
change accordingly. This necessitates continually adapting 
vision-language models to these dynamic changes rather 
than relying solely on one-time pretraining and fine-
tuning. Combining newly collected data with previously 
gathered data for continual learning is a straightforward 
and effective approach. However, the growing volume of 
data poses significant challenges in terms of computation-
al and storage costs. Training the model exclusively on new 
data risks catastrophic forgetting [296], [297]. Therefore, it 
is crucial for the research community to explore effective 
learning strategies that enable vision-language models to 
learn new knowledge from new data while maintaining 
old knowledge.
MORE DIVERSE AND RICH REMOTE  
SENSING MULTIMODAL DATASET
The original CLIP model was trained on 400 million 
image-text pairs, whereas the largest remote sensing pre -
training dataset (Git-10M [158] ) contains only 10 mil -
lion pairs, predominantly limited to optical images. This 
data scarcity restricts models’ capacity to capture a broad 
range of visual concepts, a challenge that is particularly 
pressing in remote sensing. Variations in imaging condi -
tions, sensor parameters, and geographic locations result 
in highly varied visual characteristics of objects in re -
mote sensing images [67] . Therefore, substantial efforts 
are required to create image-text datasets that encompass 
diverse ground objects and a rich variety of remote sens -
ing image modalities.
CHALLENGING AND APPLICATION- 
SPECIFIC BENCHMARKS
Most benchmark datasets [17], [194] for model evaluation 
are limited to a narrow range of remote sensing tasks (e.g., 
VQA and image captioning), whereas contemporary vision-
language models [25], [42], [134] in remote sensing dem -
onstrate versatile capabilities across diverse image analysis 
tasks. Furthermore, while some benchmarks include multi-
ple tasks [258], [259], each task typically has a limited num-
ber of samples. Such datasets are insufficient for thoroughly 
testing and comparing the performance of different vision-
language models. Beyond general-purpose benchmarks, ex-
ploring application-specific benchmarks is also a promising 
research direction as some efforts have focused on develop-
ing versatile vision-language models tailored to specific real-
world applications [129], [133].
ACKNOWLEDGMENT
This work was supported by the National Natural Sci -
ence Foundation of China under Grants 62325111 and 
U22B2011. Gui-Song Xia is the corresponding author.
AUTHOR INFORMATION
Xingxing Weng (xingxingw@whu.edu.cn) received her B.S. 
and M.S. degrees from the School of Remote Sensing and 
Information Engineering, Wuhan University, Wuhan, Chi-
na, in 2017 and 2020, respectively. She is currently pursu -
ing a Ph.D. degree in computer science and technology with 
the School of Computer Science, Wuhan University, Wu -
han 430072, China. Her research interests include remote 
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
314 
sensing image interpretation, incremental learning, and 
computer vision.
Chao Pang  (pangchao@whu.edu.cn) received his B.S. 
degree from the China University of Mining and Tech -
nology, Xuzhou, China, in 2017, his M.S. degree from the 
School of Geodesy and Geomatics, Wuhan University, Wu-
han, China, in 2020, and his Ph.D. degree from the School 
of Computer Science, Wuhan University, in 2024. He is cur-
rently a postdoctoral fellow with the School of Artificial In-
telligence, Wuhan University, Wuhan 430072, China, and 
he is also with the School of Computer Science, Wuhan 
University, Wuhan 430072, China. His research interests 
include time-series remote sensing image analysis and large 
multimodal models.
Gui-Song Xia  (guisong.xia@whu.edu.cn) received his 
B.S. degree in electronic engineering and M.S. degree in sig-
nal processing from Wuhan University, Wuhan, China, in 
2005 and 2007, respectively, and his Ph.D. degree in image 
processing and computer vision from the CNRS Informa -
tion Processing and Communications Laboratory, Télécom 
ParisTech, Paris, France, in 2011. He is currently a professor 
with the School of Artificial Intelligence, Wuhan Univer -
sity, Wuhan 430072, China, and he is also with the School 
of Computer Science, Wuhan University, Wuhan 430072, 
China. His research interests include artificial intelligence, 
computer vision, photogrammetry, remote sensing, and ro-
botics. He is a Senior Member of IEEE.
REFERENCES
[1] L. Bashmal, Y. Baz i, F. Melgani, M. M. Al Rahhal, and M. A . 
Al Zuair, “Language integration in remote sensing: Tasks, 
datasets, and future directions ,” IEEE Geosci. Remote Sens. 
Mag., vol. 11 , no. 4, pp. 63 –93, Dec. 2023 , doi: 10.1109/
MGRS.2023.3316438.
[2] X. Li, C. Wen, Y. Hu, Z. Yua
n, and X. X. Zhu, “Vision-language 
models in remote sensing: Current progress and future trends,” 
IEEE Geosci. Remote Sens. Mag. , vol. 12, no. 2, pp. 32–66, Jun. 
2024, doi: 10.1109/MGRS.2024.3383473.
[3] B. Qu, X. Li, D. Tao, an
d X. Lu, “Deep semantic understand -
ing of high resolution remote sensing image,” in Proc. Int. Conf. 
Comput. Inf. Telecom. Syst. (CITS), 2016, pp. 1–5, doi: 10.1109/
CITS.2016.7546397.
[4] S. Lobr
y, D. Marcos, J. Murray, and D . Tuia, “RSVQA: Visual 
question answering for remote sensing data ,” IEEE Trans. Geos-
ci. Remote Sens., vol . 58, no. 12, pp. 8555–8566, Dec. 2020, doi: 
10.1109/TGRS.2020.2988782.
[5] T. Abdu
llah, Y. Bazi, M. M. Al Rahhal , M. L. Mekhalfi , L. Ran-
garajan, and M . Zua ir, “TextRS: Deep bidirectional triplet 
network for matching text to remote sensing images ,” Remote 
Sens., vol. 12, no. 3, p. 405, 2020, doi: 10.3390/rs12030405.
[6] M. B. Be
jiga, F. Melgani, and A. Vascotto, “Retro-remote sens -
ing: Generating images from ancient texts ,” IE EE J. Sel. Topics 
Appl. Earth Observ. Remote Sens. , vol. 12 , no. 3 , pp. 950 –960, 
Mar. 2019, doi: 10.1109/JSTARS.2019.2895693.
[7] Z. Zha
ng, W. Zhang, W. Diao, M. Yan, X. Gao, and X . Sun, 
“VAA: Visual aligning attention model for remote sensing im -
age captioning,” IE EE Access, vol. 7, pp. 137,355–137,364, 2019, 
doi: 10.1109/ACCESS.2019.2942154.
[8] Y. Li, S. Fang, L. J
iao, R. Liu, and R. Shang, “A multi-level atten-
tion model for remote sensing image captions ,” Re mote Sens., 
vol. 12, no. 6, p. 939, 2020, doi: 10.3390/rs12060939.
[9] R. Zha
o, Z. Shi, and Z . Zou, “High-resolution remote sensing 
image captioning based on structured attention ,” IEEE Trans. 
Geosci. Remote Sens. , vol. 60 , pp. 1– 14, 2022, doi: 10.1109/
TGRS.2021.3070383.
[10] U. Zia, M. M . R
iaz, and A . Ghafoor , “Transforming remote 
sensing images to textual descriptions ,” Int. J. Appl. Earth Ob -
serv. Geoinf., vol . 108, Apr. 2022, Art. no. 102741, doi: 10.1016/j.
jag.2022.102741.
[11
] K. Simo
nyan and A. Zisserman , “Very deep convolutional 
networks for large-scale image recognition ,” 2014, arX-
iv:1409.1556.
[12
] K. He, X. Zha
ng, S. Ren, and J . Sun, “Deep residual learning 
for image recognition,” in Proc. IEEE/CVF Conf. Comput. Vis. 
Pattern Recognit. (CVPR) , 2016, pp. 770 –778, doi: 10.1109/
CVPR.2016.90.
[13] S. Hoch
reiter and J. Schmidhuber, “Long short-term memory,” 
Neural Comput., vol. 9, no. 8, pp. 1735–1780, 1997, doi: 10.1162/
neco.1997.9.8.1735.
[14] A. Vasw
ani et al., “Attention is all you need,” in Proc. 31st Int. 
Conf. Neural Inf. Process. Syst., 2017, vol. 30, pp. 6000–6010.
[15] Z. Zha
ng, T. Zhao, Y. Guo, and J. Yin, “RS5M and geoRSCLIP: A 
large scale vision-language dataset and a large vision-language 
model for remote sensing,” IEEE Trans. Geosci. Remote Sens., vol. 
62, pp. 1–23, 2024, doi: 10.1109/TGRS.2024.3449154.
[16]
 Z. Wang, R. P
rabha, T. Huang, J. Wu, and R . Rajagopal, “Sky-
Script: A large and semantically diverse vision-language data -
set for remote sensing, ” in Proc. AAAI Conf. Artif. Intell. , 2024, 
vol. 38, pp. 5805–5813.
[17] Y. Hu, J. Yuan, C. We
n, X. Lu, Y. Liu, and X. Li, “RSGPT: A re-
mote sensing vision language model and benchmark ,” ISPR S 
J. Photogramm. Remote Sens., vol. 224, pp. 272–286, Jun. 2025, 
doi: 10.1016/j.isprsjprs.2025.03.028.
[18] K. Kuc
kreja, M. S. Danish, M. Naseer, A. Das, S. Khan, and F. 
S. Khan, “GeoChat: Grounded large vision-language model 
for remote sensing,” in Proc. IEEE/CVF Conf. Comput. Vis. Pat -
tern Recognit. (CVPR), 2024, p p. 27,831–27,840, doi: 10.1109/
CVPR52733.2024.02629.
[19] R. Had
sell, S. Chopra, and Y. LeCun, “Dimensionality reduc -
tion by learning an invariant mapping, ” i n Proc. IEEE/CVF 
Conf. Comput. Vis. Pattern Recognit. (CVPR) , 2006, vol. 2, pp. 
1735–1742, doi: 10.1109/CVPR.2006.100.
[20] F. Liu et a
l., “RemoteCLIP: A vision language foundation mod-
el for remote sensing,” IE EE Trans. Geosci. Remote Sens., vol. 62, 
pp. 1–16, 2024, doi: 10.1109/TGRS.2024.3390838.
[21] S. Khan
na et al., “DiffusionSat: A generative foundation model 
for satellite imagery,” in Proc. Int. Conf. Learn. Representations 
(ICLR), 2023, pp. 1–15. 
[22]
 D. Tang, X. C
ao, X. Hou, Z. Jiang, J. Liu, and D. Meng, “CRS-
Diff: Controllable remote sensing image generation with diffu-
sion model,” I EEE Trans. Geosci. Remote Sens., vol. 62, pp. 1–14, 
2024, doi: 10.1109/TGRS.2024.3453414.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
315 
[23] H. Liu, C. Li, Q. Wu, an d Y. J. Lee, “Visual instruction tuning ,” 
in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS) , 2023, vol. 36, 
pp. 34,892–34,916.
[24] D. Muht
ar, Z. Li, F. Gu, X. Zhang, and P. Xiao, “LHRS-Bot: Em -
powering remote sensing with VGI-enhanced large multimod -
al language model,” i n Proc. Eur. Conf. Comput. Vis. (ECCV) , 
2024, pp. 440–457.
[25] J. Luo et 
al., “SkySenseGPT: A fine-grained instruction tuning 
dataset and model for remote sensing vision-language under -
standing,” 20 24, arXiv:2406.10100.
[26] Y. Zhou e
t al., “Towards vision-language geo-foundation mod-
el: A survey,” 20 24, arXiv:2406.09385.
[27] A. Rad
ford et al., “Learning transferable visual models from 
natural language supervision,” in Proc. 38th Int. Conf. Mach. 
Learn. (ICML), 2021, pp. 8748–8763.
[28] R. Romb
ach, A. Blattmann, D. Lorenz, P. Esser, and B . Om-
mer, “High-resolution image synthesis with latent diffu -
sion m
odels,” i n Proc. IEEE/CVF Conf. Comput. Vis. Pattern 
Recognit. (CVPR) , 2022, pp. 10,684 –10,695, doi: 10.1109/
CVPR52688.2022.01042.
[29] A. va
n den Oord , Y. Li, and O . Vinyals , “ Representation 
learning with contrastive predictive coding ,” 2018, arX-
iv:1807.03748.
[30] Y. Li, Y. Pan, T. Yao, a
nd T. Mei, “Comprehending and ordering 
semantics for image captioning,” in Proc. IEEE/CVF Conf. Com-
put. Vis. Pattern Recognit. (CVPR), 2022, p p. 17,990–17,999, doi: 
10.1109/CVPR52688.2022.01746.
[31] H. Wang, Y. Li, H. Ya
o, and X . Li, “CLIPN for zero-shot OOD 
detection: Teaching CLIP to say no,” in Proc. IEEE/CVF Int. 
Conf. Comput. Vis. (ICCV), 2023, pp. 1802–1812, doi: 10.1109/
ICCV51070.2023.00173.
[32] Z. Zhou, Y. L
ei, B. Zhang, L. Liu, and Y. Liu, “ZegCLIP: Towards 
adapting clip for zero-shot semantic segmentation,” in Proc. 
IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 2023, pp. 
11,175–11,185, doi: 10.1109/CVPR52729.2023.01075.
[33]
 U. Mall, C
. P. Phoo, M. K. Liu, C. Vondrick, B. Hariharan , and 
K. Bala, “Remote sensing vision-language foundation models 
without annotations via ground remote alignment,” in Proc. 
Int. Conf. Learn. Representations (ICLR), 2024, pp. 1–13.
[34]
 Z. Son
g et al., “Set-CLIP: Exploring aligned semantic from 
low-alignment multimodal data through a distribution view ,” 
2024, arXiv:2406.05766.
[35] X. Li, C. Wen, Y. Hu, an
d N. Zhou, “RS-CLIP: Zero shot remote 
sensing scene classification via contrastive vision-language su -
pervision,” In t. J. Appl. Earth Observ. Geoinf, vol. 124, Nov. 2023, 
Art. no. 103497, doi: 10.1016/j.jag.2023.103497.
[36] S. Wang e
t al., “Multi-view feature fusion and visual prompt for 
remote sensing image captioning ,” IEEE Trans. Geosci. Remote 
Sens., vol. 62, pp. 1–17, 2024, doi: 10.1109/TGRS.2024.3426359.
[37] M. M. Al R
ahhal, Y. Bazi, H. Elgibreen, and M. Zuair, “Vision-
language models for zero-shot classification of remote sensing 
ima
ges,” Appl. Sci., vol. 13, no. 22, p. 12462, 2023.
[38] W. Xu, J. Wang, Z. We
i, M. Peng, and Y. Wu, “Deep semantic-
visual alignment for zero-shot remote sensing image scene 
classification ,” ISPRS J. Photogramm. Remote Sens., vol. 198, pp. 
140–152, Apr. 2023, doi: 10.1016/j.isprsjprs.2023.02.012.
[39]
 A. Zavr
as, D. Michail, B. Demir, and I . Papoutsis, “Mind the 
modality gap: Towards a remote sensing vision-language mod-
el via cross-modal alignment ,” 20 24, arXiv:2402.09816.
[40] Y. Zha
o, M. Zhang, B. Yang, Z. Zhang, J. Kang, and J . Gong, 
“LuoJiaHOG: A hierarchy oriented geo-aware image caption 
dataset for remote sensing image–text retrieval ,” ISPRS J. Pho-
togramm. Remote Sens. , vol . 222, pp. 130 –151, Apr. 2025, doi: 
10.1016/j.isprsjprs.2025.02.009.
[41] D. Zhu, J. Che
n, X. Shen, X. Li, and M. Elhoseiny, “MiniGPT-4: 
Enhancing vision-language understanding with advanced 
large language models,” in Proc. Int. Conf. Learn. Representations 
(ICLR), 2024.
[42]
 C. Pang e
t al., “VHM: Versatile and honest vision language 
model for remote sensing image analysis,” in Proc. AAAI Conf. 
Artif. Intell., 2025, vol. 39, pp. 6381–6388.
[43]
 Y. Zhan, Z. 
Xiong, and Y. Yuan, “SkyEyeGPT: Unifying remote 
sensing vision-language tasks via instruction tuning with large 
language model ,” ISPRS J. Photogramm. Remote Sens., vol. 221, 
pp. 64–77, Mar. 2025, doi: 10.1016/j.isprsjprs.2025.01.020.
[44]
 W. Zha
ng, M. Cai, T. Zhang, Y. Zhuang, J. Li, and X . Mao, 
“EarthMarker: A visual prompting multimodal large language 
model for remote sensing,” IEEE Trans. Geosci. Remote Sens., vol. 
63, pp. 1–19, 2024, doi: 10.1109/TGRS.2024.3523505.
[45] D. P . K
ingma and M . Welling, “Auto-encoding variational 
bayes,” 2013, arXiv:1312.6114.
[46] Y. Liu, J. Yue, S. X
ia, P. Ghamisi, W. Xie, and L . Fang, “Diffu-
sion models meet remote sensing: Principles, methods, and 
pers
pectives,” IEEE Trans. Geosci. Remote Sens., vol. 62, pp. 1–22, 
2024, doi: 10.1109/TGRS.2024.3464685.
[47] A. Arr
abi, X. Zhang, W. Sultan, C. Chen, and S. Wshah, “Cross-
view meets diffusion: Aerial image synthesis with geom -
etry and text guidance,” i n Proc. IEEE/CVF Winter Conf. Appl. 
Comput. Vis. (WACV) , 2025, pp. 5356 –5366, doi: 10.1109/
WACV61041.2025.00523.
[48] L. Pang e
t al., “HSIGene: A foundation model for hyperspectral 
image generation,” 2024, arXiv:2409.12470.
[49] Z. Yu, C. Liu, L. Liu, Z. Sh
i, and Z . Zou, “ MetaEarth: A 
generative foundation model for global-scale remote sens -
ing image generation ,” IE EE Trans. Pattern Anal. Mach. In -
tell., vol. 47 , n o . 3 , pp. 1764 –1781, 2025, doi: 10.1109/
TPAMI.2024.3507010 .
[50] Q. Chen
g, Y. Xu, and Z . Hua ng, “VCC-DiffNet: Visual con -
ditional control diffusion network for remote sensing image 
cap
tioning,” Remote Sens., vol. 16, no. 16, p. 2961, 2024, doi: 
10.3390/rs16162961.
[51] X. Yu, Y. Li, J. Ma, C. Li, and 
H. Wu, “Diffusion-RSCC: Diffu -
sion probabilistic model for change captioning in remote sens-
ing images,” IE EE Trans. Geosci. Remote Sens., vol. 63, pp. 1–13, 
2025, doi: 10.1109/TGRS.2025.3554360.
[52] Y. Xing, L. Qu, S. Z
hang, J. Feng, X. Zhang, and Y. Zhang, “Em-
power generalizability for pansharpening through text-modu -
lated diffusion model,” IE EE Trans. Geosci. Remote Sens., vol. 62, 
pp. 1–12, 2024, doi: 10.1109/TGRS.2024.3434431.
[53] J. Wang e
t al., “Leveraging visual language model and genera -
tive diffusion model for zero-shot sar target recognition,” Re mote 
Sens., vol. 16, no. 16, p. 2927, 2024, doi: 10.3390/rs16162927.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
316 
[54] A. Dosovitskiy et a l., “An image is worth 16x16 words: Trans -
formers for image recognition at scale,” i n Proc. Int. Conf. Learn. 
Representations (ICLR), 2021, pp. 1–12.
[55] S. Mo, M. Kim, K. Le
e, and J. Shin, “S-CLIP: Semi-supervised 
vision-language learning using few specialist captions ,” in 
Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), 2023, vol. 36, pp. 
61,187–61,212.
[56] G. Chen
g, J. Han, and X . Lu, “Remote sensing image scene 
classification: Benchmark and state of the art ,” Proc. IEEE , 
vol. 105, no. 10 , pp. 1865 –1883, Oct. 2017 , doi: 10.1109/
JPROC.2017.2675998.
[57] T. Chen, S. K
ornblith, M. Norouzi, and G . Hinton, “A simple 
framework for contrastive learning of visual representations ,” 
in Proc. Int. Conf. Mach. Learn. (ICML), 2020, pp. 1597–1607.
[58] G. Mai, N. La
o, Y. He, J. Song, and S . Ermon, “Csp: Self-  
supervised contrastive spatial pre-training for geospatial-visual 
repr
esentations,” in Proc. Int. Conf. Mach. Learn. (ICML), 2023, 
pp. 23,498–23,515.
[59] G. Mai, K. Ja
nowicz, B. Yan, R. Zhu, L. Cai, and N. Lao, “Multi-
scale representation learning for spatial feature distributions 
using grid cells,” in Proc. Int. Conf. Learn. Representations (ICLR), 
2020, pp. 1–12. 
[60]
 G. Chr
istie, N. Fendley, J. Wilson, and R . Mukherjee, “Func-
tional map of the world,” in Proc. IEEE/CVF Conf. Comput. Vis. 
Pattern Recognit. (CVPR) , 2018, pp. 6172–6180, doi: 10.1109/
CVPR.2018.00646.
[61] V. V. C
epeda, G. K . Nayak, and M . Shah, “GeoCLIP: Clip-  
inspired alignment between locations and images for effective 
worl
dwide geo-localization,” in Proc. Adv. Neural Inf. Process. 
Syst. (NeurIPS), 2023, vol. 36, pp. 8690–8701.
[62] M. Tanc
ik et al., “Fourier features let networks learn high fre -
quency functions in low dimensional domains,” i n Proc. Adv. 
Neural Inf. Process. Syst. (NeurIPS), 2020, vol. 33, pp. 7537–7547.
[63] M. Lar
son, M. Soleymani, G. Gravier, B. Ionescu, and G. J . 
Jones, “The benchmarking initiative for multimedia evalua -
tion: MediaEval 2016,” IE EE Multimedia Mag., vol. 24, no. 1, pp. 
93–96, Jan./Mar. 2017, doi: 10.1109/MMUL.2017.9.
[64] K. Klem
mer, E. Rolf, C. Robinson, L. Mackey, and M. Rußwurm, 
“SatCLIP: Global, general-purpose location embeddings with 
satellite imagery ,” in Proc. AAAI Conf. Artif. Intell. , vol. 39, pp. 
4347–4355, 2025.
[65] M. Rußw
urm, K. Klemmer, E. Rolf, R. Zbinden, and D . Tuia, 
“Geographic location encoding with spherical harmonics and 
sinusoidal representation networks,” in Proc. Int. Conf. Learn. Rep-
resentations (ICLR), 202 4, pp. 1–12.
[66] J. Pan, M. Ma, Q. Ma, C. Bai, a
nd S. Chen, “PIR: Remote sens-
ing image-text retrieval with prior instruction representation 
lear
ning,” 2024, arXiv:2405.10160.
[67] Y. Lon
g et al., “On creating benchmark dataset for aerial image 
in
terpretation: Reviews, guidances, and million-aid ,” IEEE J. 
Sel. Topics Appl. Earth Observ. Remote Sens. , vol. 14, pp. 4205–
4230, 2021, doi: 10.1109/JSTARS.2021.3070368.
[68] G. Sum
bul, M. Charfuelan, B. Demir, and V. Markl, “Bigearthnet: 
A large-scale benchmark archive for remote sensing image under-
standing,” in Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS), 
20
19, pp. 5901–5904, doi: 10.1109/IGARSS.2019.8900532.
[69] J. Li, D. Li, S. Sav
arese, and S . Hoi, “BLIP-2: Bootstrapping 
language-image pre-training with frozen image encoders and 
large language models,” in Proc. Int. Conf. Mach. Learn. (ICML), 
2023, pp. 19,730–19,742.
[70]
 G.-
S. Xia et al., “AID: A benchmark data set for performance 
evaluation of aerial scene classification,” IEEE Trans. Geosci. Re-
mote Sens., vol . 55, no. 7, pp. 3965–3981, Jul. 2017, doi: 10.1109/
TGRS.2017.2685945.
[71] P. Helb
er, B. Bischke, A. Dengel, and D . Borth , “EuroSAT: A 
novel dataset and deep learning benchmark for land use and 
land cover classification ,” IEEE J. Sel. Topics Appl. Earth Ob -
serv. Remote Sens, vol
 . 12, no. 7, pp. 2217–2226, Jul. 2019, doi: 
10.1109/JSTARS.2019.2918242.
[72] X. Qi et 
al., “MLRSNet: A multi-label high spatial resolution re-
mote sensing dataset for semantic scene understanding,” ISPR S 
J. Photogramm. Remote Sens., vol. 169, pp. 337–350, Nov. 2020, 
doi: 10.1016/j.isprsjprs.2020.09.020.
[73] Q. Wang, S. L
iu, J. Chanussot, and X. Li, “Scene classification 
with recurrent attention of VHR remote sensing images ,” IEEE 
Trans. Geosci. Remote Sens., vol. 57, no. 2, pp. 1155–1167, 2018, 
doi: 10.1109/TGRS.2018.2864987.
[74] W. Zhou, S. Ne
wsam , C. Li, and Z . Shao, “ PatternNet: A 
benchmark dataset for performance evaluation of remote 
sensing image retrieval ,” ISPRS J. Photogramm. Remote 
Sens., vol. 145 , pp. 197 –209, Nov. 2018 , doi: 10.1016/j.is -
prsjprs.2018.01.004 .
[75]
 Q. Zou, L. Ni, T. Zh
ang, and Q. Wang, “Deep learning based 
feature selection for remote sensing scene classification ,” IEEE 
Geosci. Remote Sens. Lett. , vol. 12, no. 11, pp. 2321–2325, Nov. 
2015, doi: 10.1109/LGRS.2015.2475299.
[76] L. Zha
o, P. Tang, and L. Huo, “Feature significance-based mul -
tibag-of-visual-words model for remote sensing image scene 
cla
ssification,” J. Appl. Remote Sens. , vol. 10, no. 3 , 2016, Art. 
no. 035004, doi: 10.1117/1.JRS.10.035004.
[77] H. Li et al
., “RSI-CB: A large-scale remote sensing image clas -
sification benchmark using crowdsourced data ,” Se nsors, vol. 
20, no. 6, p. 1594, 2020, doi: 10.3390/s20061594.
[78] X. Lu, B. Wang, X. Z
heng, and X . Li, “Exploring models and 
data for remote sensing image caption generation,” IEEE Trans. 
Geosci. Remote Sens. , vol. 56, no. 4, pp. 2183–2195, Apr. 2018, 
doi: 10.1109/TGRS.2017.2776321.
[79] B. Zha
o, Y. Zhong, G.-S. Xia, and L. Zhang, “Dirichlet-derived 
multiple topic scene classification model for high spatial reso -
lution remote sensing imagery ,” IE EE Trans. Geosci. Remote 
Sens., vol. 54 , no. 4 , pp. 2108–2123, Apr. 2016, doi: 10.1109/
TGR
S.2015.2496185.
[80] Y. Yang a
nd S. Newsam, “Bag-of-visual-words and spatial ex -
tensions for land-use classification,” in Proc. ACM SIGSPATIAL 
Int. Conf. Adv. Geogr. Inf. Syst. (SIGSPATIAL), 2010, pp. 270–279.
[81] G.-
S. Xia, W. Yang, J. Delon, Y. Gousseau, H. Sun, and H. Maître, 
“Structural high-resolution satellite image indexing,” in Proc. IS-
PRS TC VII Symp.-100 Years ISPRS, 201 0, vol. 38, pp. 298–303.
[82] Z. Yuan e
t al., “Exploring a fine-grained multiscale method 
for cross-modal remote sensing image retrieval ,” IEEE Trans. 
Geosci. Remote Sens. , vol. 60 , pp. 1– 19, 2022, doi: 10.1109/
TGRS.2021.3078451.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
317 
[83] T. Wei, W. Yua n, J. Luo, W. Zhang, and L. Lu, “VLCA: Vision-lan-
guage aligning model with cross-modal attention for bilingual 
rem
ote sensing image captioning,” J. Syst. Eng. Electron., vol. 34, 
no. 1, pp. 9–18, Feb. 2023, doi: 10.23919/JSEE.2023.000035.
[84] M.-
H. Guo, Z.-N. Liu, T.-J. Mu, and S.-M . Hu, “Beyond self-
attention: External attention using two linear layers for visual 
tasks,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 5, pp. 
5436–5447, 2022, doi: 10.1109/TPAMI.2022.3211006.
[85]
 C. Yang, Z. Li, a
nd L . Zhang, “Bootstrapping interactive im -
age–text alignment for remote sensing image captioning ,” 
IE
EE Trans. Geosci. Remote Sens. , vol. 62 , pp. 1–12, 2024, doi: 
10.1109/TGRS.2024.3359316.
[86] C. Yang, Z. Li, a
nd L. Zhang, “MGIMM: Multi-granularity in -
struction multimodal model for attribute-guided remote sens -
ing image detailed description,” IE EE Trans. Geosci. Remote Sens., 
vol. 62, pp. 1–13, 2024, doi: 10.1109/TGRS.2024.3497976.
[87] R. Ricc
i, Y. Bazi, and F. Melgani, “Machine-to-machine visual dia-
loguing with ChatGPT for enriched textual image description,” 
Re
mote Sens., vol. 16, no. 3, p. 441, 2024, doi: 10.3390/rs16030441.
[88] C. Liu, R. Zh
ao, J. Chen, Z. Qi, Z. Zou, and Z . Shi, “A decou-
pling paradigm with prompt learning for remote sensing image 
cha
nge captioning ,” IEEE Trans. Geosci. Remote Sens. , vol. 61 , 
pp. 1–18, 2023, doi: 10.1109/TGRS.2023.3321752.
[89] L. Mi, X. Dai, J. Ca
stillo-Navarro , and D . Tuia, “Knowledge-
aware text–image retrieval for remote sensing images ,” IEEE 
Trans. Geosci. Remote Sens., vol. 62, pp. 1–13, 2024, doi: 10.1109/
TGRS.2024.3486977.
[90] J. Li, D. Li, C. Xiong, a
nd S. Hoi, “Blip: Bootstrapping language-
image pre-training for unified vision-language understanding 
and generation,” in Proc. Int. Conf. Mach. Learn. (ICML), 2022, 
pp. 12,888–12,900.
[91]
 B. Pso
mas et al., “Composed image retrieval for remote sens -
ing,” in P roc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS) , 
2
024, pp. 8526–8534.
[92] L. Lan, F. Wan
g, X. Zheng, Z. Wang, and X . Liu, “Efficient 
prompt tuning of large vision-language model for fine-grained 
ship classification,” IEEE Trans. Geosci. Remote Sens., vol. 63, pp. 
1–10, 2025, doi: 10.1109/TGRS.2024.3509721.
[93]
 Y. Tay, M. C. Ph
an, L. A. Tuan, and S. C. Hui, “Learning to rank 
question answer pairs with holographic dual LSTM architec -
ture,” in Proc. Int. ACM SIGIR Conf. Res. Dev. Inf. Retr. (SIGIR), 
20
17, pp. 695–704.
[94] J. Zha
ng et al., “Text2Seg: Zero-shot remote sensing image se -
mantic segmentation via text-guided visual foundation mod -
els,” in Proc. ACM SIGSPATIAL Int. Workshop AI Geographic 
Kn
owl. Discov., 2024, pp. 63–66.
[95] Y. Lin, K. Suz
uki, and S. Sogo, “Practical techniques for vision-
language segmentation model in remote sensing,” Int. Arch. Photo-
gramm. Remote Sens. Spatial Inf. Sci., vol. XLVIII-2-2024, pp. 203–210, 
Jun. 2024, doi: 10.5194/isprs-archives-XLVIII-2-2024-203-2024.
[96] T. Lüdde
cke and A. Ecker, “Image segmentation using text and 
image prompts,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern 
Recognit. (CVPR), 2022, pp. 7086–7096.
[97] K. Li, R. Liu, X. Ca
o, D. Meng, and Z . Wang, “SegEarth-OV: 
Towards-free open-vocabulary segmentation for remote sens -
ing images,” 20 24, arXiv:2410.01768.
[98] X. Tan, G. Che
n, T. Wang , J. Wang , and X . Zhang , “ Seg -
ment change model (SCM) for unsupervised change 
det
ection in VHR remote sensing images: A case study 
of buildings,” in Proc. IEEE Int. Geosci. Remote Sens. 
Symp. (IGARSS) , 2024, pp. 8577 –8580 , doi: 10.1109/
IGARSS53475.2024.10642429 .
[99]
 S. Don
g, L. Wang, B. Du, and X. Meng, “ChangeCLIP: Re -
mote sensing change detection with multimodal vision-
lan
guage representation learning ,” ISPRS J. Photogramm. 
Remote Sens. , vol. 208 , pp. 53 –69, Feb. 2024 , doi: 10.1016/  
j.isprsjprs.2024.01.004 .
[10
0] K. Li, X. Cao, a
nd D . Meng, “A new learning paradigm for 
foundation model-based remote-sensing change detection ,” 
IEEE Trans. Geosci. Remote Sens. , vol. 62 , pp. 1–12, 2024, doi: 
10.1109/TGRS.2024.3365825.
[101] Y. Bazi
, M. M. A. Rahhal, M. L. Mekhalfi, M. A. A. Zuair, and F. Mel-
gani, “Bi-modal transformer-based approach for visual question 
ans
wering in remote sensing imagery,” IEEE Trans. Geosci. Remote 
Sens., vol. 60, pp. 1–11, 2022, doi: 10.1109/TGRS.2022.3192460.
[102] M. Czerk
awski, R. Atkinson, and C. Tachtatzis, “Detecting cloud 
presence in satellite images using the RGB-based clip vision-lan-
guage model,” i n Proc. IEEE Int. Geosci. Remote Sens. Symp., 2023, 
pp. 5170–5173, doi: 10.1109/IGARSS52108.2023.10282016.
[103] H. Tang, W. Z
hao, G. Hu, Y. Xiao, Y. Li, and H . Wang, “Text-
guided diverse image synthesis for long-tailed remote sensing 
object classification ,” IEEE Trans. Geosci. Remote Sens., vol. 62, 
pp. 1–13, 2024, doi: 10.1109/TGRS.2024.3422095.
[104]
 S. Xu, C. Zha
ng, L. Fan, G. Meng, S. Xiang , and J. Ye, “Ad-
dressCLIP: Empowering vision-language models for city-wide 
ima
ge address localization,” in Proc. Eur. Conf. Comput. Vis. 
(ECCV), 2024, pp. 76–92.
[105] A. Rad
ford et al., “Language models are unsupervised multi -
task learners ,” Op enAI Blog, vol. 1, no. 8, p. 9, 2019.
[106] J. Dev
lin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-train-
ing of deep bidirectional transformers for language understand-
ing,” in Proc. Conf. North Amer. Chapter Assoc. Comput. Linguist.: 
Hum
. Lang. Technol. (NAACL-HLT), 2019, vol. 1, pp. 4171–4186.
[107] S. Zha
ng et al., “OPT: Open pre-trained transformer language 
models,” 2022, arXiv:2205.01068.
[108] T. Brow
n et al., “Language models are few-shot learners ,” in 
Proc. Adv. Neural Inf. Process. Syst., 2020, vol. 33, pp. 1877–1901.
[1
09] P. Gao e
t al., “CLIP-adapter: Better vision-language models 
with feature adapters ,” Int. J. Comput. Vis., vol. 132, no. 2, pp. 
581–595, 2024, doi: 10.1007/s11263-023-01891-x.
[110] Y. Li, D. Kon
g, Y. Zhang , Y. Tan, and L. Chen, “Robust deep 
alignment network with remote sensing knowledge graph 
for zero-shot and generalized zero-shot remote sensing 
image scene classification ,” ISPRS J. Photogramm. Remote 
Sens., vol. 179 , pp. 145 –158, Sep. 2021 , doi: 10.1016/j.is -
prsjprs.2021.08.001.
[111]
 R. Spee
r, J. Chin, and C . Havasi, “ConceptNet 5.5: An open 
multilingual graph of general knowledge,” in Proc. AAAI Conf. 
Artif. Intell., 2017, vol. 31, pp. 4444–4451.
[112]
 A. Kiri
llov et al., “ Segment anything ,” in Proc. IEEE/CVF Int. 
Conf. Comput. Vis. (ICCV), 2023, pp. 3992–4003, doi: 10.1109/
ICCV51070.2023.00371.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
318 
[113] H. Chen, Z. Qi, a nd Z. Shi, “Remote sensing image change de-
tection with transformers,” IE EE Trans. Geosci. Remote Sens., vol. 
60, pp. 1–14, 2022, doi: 10.1109/TGRS.2021.3095166.
[114] W. G. C . Ba
ndara and V. M . Patel, “A transformer-based Sia -
mese network for change detection, ” i n Proc. IEEE Int. Geosci. 
Remote Sens. Symp. (IGARSS), 2022, pp. 207–210, doi: 10.1109/
IGARSS46834.2022.9883686.
[115] K. Zhou, J. Ya
ng, C. C. Loy, and Z. Liu, “Learning to prompt for 
vision-language models,” Int. J. Comput. Vis., vol. 130, no. 9, pp. 
2337–2348, 2022, doi: 10.1007/s11263-022-01653-1.
[116] Y. Fang e
t al., “EVA: Exploring the limits of masked visual rep -
resentation learning at scale, ” in Proc. IEEE/CVF Conf. Comput. 
Vis. Pattern Recognit. (CVPR) , 2023, pp. 19,358 –19,369, doi: 
10.1109/CVPR52729.2023.01855.
[117] W. Dai e
t al., “InstructBLIP: Towards general-purpose vision-
language models with instruction tuning,” in Proc. Adv. Neural 
Inf. Process. Syst. (NeurIPS), 2023, pp. 49,250–49,267.
[118] W.-L. C
hiang et al., “Vicuna: An open-source Chatbot im -
pressing GPT-4 with 90%* Chatgpt quality ,” LM SYS Org , 
Mar. 2023 . [Online]. Available: https://lmsys.org/blog/2023  
-03-30-vicuna/
[11
9] H. Touv
ron et al., “Llama 2: Open foundation and fine-tuned 
chat models,” 2023, arXiv:2307.09288.
[120] J. Chen e
t al., “MiniGPT-v2: Large language model as a uni -
fied interface for vision-language multi-task learning ,” 202 3, 
arXiv:2310.09478.
[121] E. J. Hu e
t al., “LoRA: Low-rank adaptation of large language 
models,” in Proc. Int. Conf. Learn. Representations (ICLR), 2022, 
pp. 1–13. 
[122] W. Zha
ng, M. Cai, T. Zhang, Y. Zhuang, and X . Mao, “Earth -
GPT: A universal multimodal large language model for mul -
tisensor image comprehension in remote sensing domain ,” 
IE
EE Trans. Geosci. Remote Sens. , vol. 62 , pp. 1–20, 2024, doi: 
10.1109/TGRS.2024.3409624.
[123] M. Oqu
ab et al., “DINOv2: Learning robust visual features 
without supervision ,” Trans. Mach. Learn. Res. , pp. 1– 31, 
2024. 
[124] C. Sch
uhmann et al., “LAION-400m: Open dataset of CLIP-
filtered 400 million image-text pairs,” in Proc. Adv. Neural Inf. 
Process. Syst. (NeurIPS), 2021, pp. 1–5.
[125]
 X. Chen e
t al., “Microsoft COCO captions: Data collection and 
evaluation server,” 2015, arXiv:1504.00325.
[126] P. Gao e
t al., “Llama-adapter v2: Parameter-efficient visual in -
struction model,” 202 3, arXiv:2304.15010.
[127] S. Kaz
emzadeh, V. Ordonez, M. Matten, and T. Berg, “ReferIt-
Game: Referring to objects in photographs of natural scenes ,” 
in P
roc. Conf. Empirical Methods Nat. Lang. Process. (EMNLP) , 
2014, pp. 787–798.
[128] L. Yu, P. Poirs
on, S. Yang, A. C. Berg, and T. L . Berg, “Model-
ing context in referring expressions,” in Proc. Eur. Conf. Comput. 
Vis. (ECCV), 2016, pp. 69–85.
[129] W. Zha
ng, M. Cai, T. Zhang, G. Lei, Y. Zhuang, and X . Mao, 
“Popeye: A unified visual-language model for multi-source 
ship detection from remote sensing imagery ,” IEEE J. Sel. Top-
ics Appl. Earth Observ. Remote Sens., vol . 17, pp. 20,050–20,063, 
2024, doi: 10.1109/JSTARS.2024.3488034.
[130] H. Touvron et al., “LLaMA: Open and efficient foundation lan-
guage models,” 202 3, arXiv:2302.13971.
[131] H. Liu, C. Li, Y. Li, an
d Y. J. Lee, “Improved baselines with visu-
al instruction tuning,” in Proc. IEEE/CVF Conf. Comput. Vis. Pat-
tern Recognit. (CVPR), 2024, p p. 26,286–26,296, doi: 10.1109/
CVPR52733.2024.02484.
[132] Y. Bazi
, L. Bashmal , M. M . Al Rahhal , R. Ricci, and F. Mel-
gani, “R S-LLaVA: A large vision-language model for joint 
ca
ptioning and question answering in remote sensing imag -
ery,” Remote Sens., v ol. 16, no. 9, pp. 1477, 2024, doi: 10.3390/
rs16091477.
[133] M. Guo, M. Wu, Y. She
n, H. Li, and C . Tao, “IfShip: A large 
vision-language model for interpretable fine-grained ship  
classification via domain knowledge-enhanced instruction 
tun
ing,” 2024, arXiv:2408.06631.
[134] J. A. Ir
vin et al., “TEOChat: A large vision-language assistant 
for temporal earth observation data,” in Proc. Int. Conf. Learn. 
Representations (ICLR), 2025, pp. 1–16.
[135]
 B. Lin et a
l., “Video-llava: Learning united visual representa -
tion by alignment before projection,” i n Proc. Conf. Empirical 
Methods Nat. Lang. Process. (EMNLP), 2024, pp. 5971–5984.
[136] J. Ainsl
ie, J. Lee-Thorp, M. De Jong, Y. Zemlyanskiy, F. Lebrón, and 
S. Sanghai, “GQA: Training generalized multi-query transformer 
models from multi-head checkpoints,” in Proc. Conf. Empirical 
Methods Nat. Lang. Process. (EMNLP), 2023, pp. 4895–4901.
[137]
 K. Pap
ineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: A meth-
od for automatic evaluation of machine translation, ” in Proc. 
Annu. Meet. Assoc. Comput. Linguist. (ACL), 2002, pp. 311–318.
[138] S. Bane
rjee and A . Lavie, “METEOR: An automatic metric for 
MT evaluation with improved correlation with human judg -
ments,” in Proc. ACL Workshop Intrinsic Extrinsic Eval. Measures 
Ma
ch. Transl. Summarization, 2005, pp. 65–72.
[139] C.-Y. L
in, “ROUGE: A package for automatic evaluation of summa-
ries,” in Proc. Text Summarization Branches Out, 200 4, pp. 74–81.
[140] R. Vedan
tam, C. L. Zitnick, and D. Parikh, “CIDEr: Consensus-
based image description evaluation,” in Proc. IEEE/CVF Conf. 
Comput. Vis. Pattern Recognit. (CVPR) , 2015, pp. 4566 –4575, 
doi: 10.1109/CVPR.2015.7299087.
[141] Q. Chen
g, H. Huang, Y. Xu, Y. Zhou, H. Li, and Z. Wang, “NW-
PU-captions dataset and MLCA-net for remote sensing image 
cap
tioning,” IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–19, 
2022, doi: 10.1109/TGRS.2022.3201474.
[142] D. Dai an
d W. Yang, “Satellite image classification via two-
layer sparse coding with biased image representation ,” IEEE 
Geosci. Remote Sens. Lett. , vol. 8, no. 1, pp. 173–176, Jan. 2011, 
doi: 10.1109/LGRS.2010.2055033.
[143] Y. Sun, S. Fe
ng, X. Li, Y. Ye, J. Kang, and X . Huang, “Visual 
grounding in remote sensing images,” in Proc. ACM Int. Conf. 
Multimedia, 2022, pp. 404–412.
[144] Y. Zhan, Z. Xiong, a nd Y. Yuan, “RSVG: Exploring data and 
models for visual grounding on remote sensing data ,” IEEE 
Trans. Geosci. Remote Sens., vol. 61, pp. 1–13, 2023, doi: 10.1109/
TGRS.2023.3250471.
[145] H. Li et al
., “CLRS: Continual learning benchmark for remote 
sensing image scene classification ,” Sensors, vol. 20 , no. 4 , p. 
1226, 2020, doi: 10.3390/s20041226.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
319 
[146] Z. Zhou et a l., “NaSC-TG2: Natural scene classification with 
tiangong-2 remotely sensed imagery ,” IEEE J. Sel. Topics Appl. 
Earth Observ. Remote Sens. , vol. 14, pp. 3228–3242, 2021, doi: 
10.1109/JSTARS.2021.3063096.
[147] B. Zhu et a
l., “METER-ML: A multi-sensor earth observation 
benchmark for automated methane source mapping ,” 2022, 
arXiv:2207.11166.
[148] X. Huan
g, Y. Cao, and J . Li, “An automatic change detection 
method for monitoring newly constructed building areas us -
ing time-series multi-view high-resolution optical satellite im -
ages,” Remote Sens. Environ., v ol. 244, Jul. 2020, Art. no. 111802, 
doi: 10.1016/j.rse.2020.111802.
[149] S. Holai
l et al., “Time-series satellite remote sensing reveals grad-
ually increasing war damage in the gaza strip,” Na tl. Sci. Rev., vol. 
11, no. 9, 2024, Art. no. nwae304, doi: 10.1093/nsr/nwae304.
[150] Z. Lin, F. Zhu, Y. K
ong, Q. Wang, and J. Wang, “SRSG and S2SG: 
A model and a dataset for scene graph generation of remote sens-
ing images from segmentation results,” IE EE Trans. Geosci. Remote 
Sens., vol. 60, pp. 1–11, 2022, doi: 10.1109/TGRS.2022.3185678.
[151] Y. Yang, X. Ta
ng, Y.-M. Cheung, X. Zhang, and L. Jiao, “SAGN: 
Semantic-aware graph network for remote sensing scene clas -
sification,” IE EE Trans. Image Process. , vol. 32 , pp. 1011–1025, 
2023, doi: 10.1109/TIP.2023.3238310.
[152] F. Bast
ani, P. Wolters, R. Gupta, J. Ferdinando, and A . Kemb-
havi, “SatlasPretrain: A large-scale dataset for remote sensing 
im
age understanding,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. 
(ICCV), 2023, pp. 16,772–16,782.
[153] A. Van E
tten, D. Lindenbaum, and T. M . Bacastow, “SpaceNet: 
A remote sensing dataset and challenge series ,” 2018, arX-
iv:1807.01232.
[15
4] L. Zha
ng, A. Rao, and M. Agrawala, “Adding conditional con -
trol to text-to-image diffusion models,” i n Proc. IEEE/CVF Int. 
Conf. Comput. Vis. (ICCV), 2023, pp. 3813–3824, doi: 10.1109/
ICCV51070.2023.00355.
[155] S. Sast
ry, S. Khanal, A. Dhakal, and N. Jacobs, “GeoSynth: Con-
textually-aware high-resolution satellite image synthesis,” i n Proc. 
IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) Workshops, 
2024, pp. 460–470, doi: 10.1109/CVPRW63382.2024.00051.
[156] J. Ho, A. Jain, 
and P. Abbeel, “Denoising diffusion probabilistic 
models,” in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), 2020, 
vol. 33, pp. 6840–6851.
[157] X. Wang e
t al., “Esrgan: Enhanced super-resolution generative 
adversarial networks,” in Proc. Eur. Conf. Comput. Vis. (ECCV), 
2018, pp. 63–79.
[158] C. Liu, K. C
hen, R. Zhao, Z. Zou, and Z. Shi, “Text2Earth: Unlock-
ing text-driven remote sensing image generation with a global-scale 
dat
aset and a foundation model,” IEEE Geosci. Remote Sens. Mag., 
early access, May 6, 2025, doi: 10.1109/MGRS.2025.3560455.
[159] C. Yi et al
., “Aerial hyperspectral remote sensing classification 
dataset of Xiongan new area (Matiwan village),” J. Remote Sens., 
vol. 24, no. 11, pp. 1299–1306, 2020.
[160] N. Yok
oya and A . Iwasaki, “Airborne hyperspectral data over 
Chikusei ,” Univ. of Tokyo, Tokyo, Japan, Tech. Rep. SAL-2016-
05-27, May 2016.
[16
1] “Ho
uston hyperspectral dataset,” I. G. D. F. Contest, 2013. [Online].  
Available: https://hyperspectral.ee.uh.edu/?page_id=459 
[16
2] “Ho
uston hyperspectral dataset,” I. G. D. F. Contest, 2018. [On-
line]. Available: https://hyperspectral.ee.uh.edu/?page_id=1075 
[16
3] X. Li et al
., “A multiscale dataset for understanding complex 
eco-hydrological processes in a heterogeneous oasis system ,” 
Sci. Data, vol. 4, Jun. 2017, Art. no. 170083 , doi: 10.1038/sda-
ta.2017.83.
[16
4] A. Q. Ni
chol and P. Dhariwal, “Improved denoising diffusion 
probabilistic models,” in Proc. Int. Conf. Mach. Learn. (ICML) , 
2021, pp. 8162–8171.
[165] A. Seba
q and M. ElHelw, “RsDiff: Remote sensing image gen -
eration from text using diffusion model,” Ne ural Comput. Appl., 
vol. 36, no. 36, pp. 23,103–23,111, 2024, doi: 10.1007/s00521-
024-10363-3.
[166] C. Sah
aria et al., “Photorealistic text-to-image diffusion mod -
els with deep language understanding,” i n Proc. Adv. Neural Inf. 
Process. Syst. (NeurIPS), 2022, vol. 35, pp. 36,479–36,494. 
[167] C. Raf
fel et al., “Exploring the limits of transfer learning with 
a unified text-to-text transformer ,” J. Mach. Learn. Res., vol. 21, 
no. 1, pp. 1–67, 2020. 
[168] S. Xie a
nd Z. Tu, “Holistically-nested edge detection,” in Proc. 
IEEE/CVF Int. Conf. Comput. Vis. (ICCV), 2015, pp. 1395–1403, 
doi: 10.1109/ICCV.2015.164.
[169] Y. Xu, W. Xu, D. Cheun
g, and Z . Tu, “Line segment detection 
using transformers without edges,” in Proc. IEEE/CVF Conf. 
Comput. Vis. Pattern Recognit. (CVPR) , 2021, pp. 4255 –4264, 
doi: 10.1109/CVPR46437.2021.00424.
[170]
 E. Simo
-Serra , S. Iizuka , K. Sasaki, and H . Ishikawa, “Learn-
ing to simplify: Fully convolutional networks for rough sketch 
clea
nup,” ACM Trans. Graph, vol. 35, no. 4, pp. 1–11, 2016, doi: 
10.1145/2897824.2925972.
[171] S. Zha
o et al., “Uni-ControlNet: All-in-one control to text-to-
image diffusion models,” in Proc. Adv. Neural Inf. Process. Syst. 
(NeurIPS), 2023, vol. 36, pp. 11,127–11,150.
[172]
 Y. Dai, F. Gie
seke, S. Oehmcke, Y. Wu, and K . Barnard, “At-
tentional feature fusion, ” i n Proc. IEEE/CVF Winter Conf. Appl. 
Comput. Vis. (WACV) , 2021, pp. 3559 –3568, doi: 10.1109/
WACV48630.2021.00360.
[173] M. Heus
el, H. Ramsauer, T. Unterthiner, B. Nessler, and S. 
Hochreiter, “GANs trained by a two time-scale update rule 
converge to a local Nash equilibrium,” in Proc. Adv. Neural Inf. 
Process. Syst. (NeurIPS), 2017, vol. 30, pp. 6629–6640.
[174]
 T. Salim
ans, I. Goodfellow, W. Zaremba, V. Cheung, A. Rad-
ford, and X. Che n, “Improved techniques for training GANs ,” 
in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), 2016, vol. 29, pp. 
2234–2242.
[175] L. Liu, B. Che
n, H. Chen, Z. Zou, and Z . Shi, “Diverse hyper -
spectral remote sensing image synthesis with diffusion mod -
els,” IEEE Trans. Geosci. Remote Sens. , v ol. 61, pp. 1–16, 2023, 
doi: 10.1109/TGRS.2023.3335975.
[176] C. Zha
o, Y. Ogawa, S. Che n, Z. Yang, and Y. Sekimoto, “Label 
freedom: Stable diffusion for remote sensing image seman -
tic segmentation data generation,” i n Proc. IEEE Int. Conf. Big 
Data (BigData) , 2023, pp. 1022 –1030, doi: 10.1109/BigDa -
ta59044.2023.10386381.
[17
7] Z. Yuan e
t al., “Efficient and controllable remote sensing fake 
sample generation based on diffusion model ,” IEEE Trans. 
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
320 
Geosci. Remote Sens. , vol. 61 , pp. 1– 12, 2023, doi: 10.1109/
TGRS.2023.3268331.
[178] Y. Xia
o, Q. Yuan, K. Jiang, J. He, X. Jin, and L. Zhang, “EDiffSR: 
An efficient diffusion probabilistic model for remote sensing 
image super-resolution,” IEEE Trans. Geosci. Remote Sens. , vol. 
62, pp. 1–14, 2024, doi: 10.1109/TGRS.2023.3341437.
[179] F. Meng e
t al., “A conditional diffusion model with fast sam -
pling strategy for remote sensing image super-resolution,” IEEE  
Trans. Geosci. Remote Sens., vol. 62, pp. 1–16, 2024, doi: 10.1109/
TGRS.2024.3458009.
[180] X. Zha
o and K . Jia, “Cloud removal in remote sensing using 
sequential-based diffusion models ,” Remote Sens., vol. 15, no. 
11, p. 2861, 2023, doi: 10.3390/rs15112861.
[181] R. Jing, F. D
uan, F. Lu, M. Zhang, and W. Zhao, “Denoising dif-
fusion probabilistic feature-based network for cloud removal 
in s e
ntinel-2 imagery ,” Remote Sens. , vol. 15 , no. 9 , p. 2217, 
2023, doi: 10.3390/rs15092217.
[182] J. Jia, G. Le
e, Z. Wang, L. Zhi, and Y. He, “Siamese meets dif -
fusion network: SMDNet for enhanced change detection in 
hig
h-resolution RS imagery ,” IEEE J. Sel. Topics Appl. Earth Ob-
serv. Remote Sens., vol . 17, pp. 8189–8202, 2024, doi: 10.1109/
JSTARS.2024.3384545.
[183] Y. Wen, Z. Zh
ang, Q. Cao, and G. Niu, “TransC-GD-CD: Trans-
former-based conditional generative diffusion change detection 
mode
l,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 17, 
pp. 7144–7158, 2024, doi: 10.1109/JSTARS.2024.3373201.
[184] N. Chen, J. Yu
e, L. Fang, and S. Xia, “SpectralDiff: A generative 
framework for hyperspectral image classification with diffu -
sion models,” IE EE Trans. Geosci. Remote Sens., vol. 61, pp. 1–16, 
2023, doi: 10.1109/TGRS.2023.3310023.
[185] J. Aust
in, D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg, 
“Structured denoising diffusion models in discrete state-spac -
es,” in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), 20 21, vol. 
34, pp. 17,981–17,993.
[186] Y. Yang, T. L
iu, Y. Pu, L. Liu, Q. Zhao, and Q . Wan, “Remote 
sensing image change captioning using multi-attentive net -
work with diffusion model ,” Re mote Sens., vol. 16 , no. 21, p. 
4083, 2024, doi: 10.3390/rs16214083.
[187] C. Sah
aria, J. Ho, W. Chan, T. Salimans, D. J. Fleet, and M. No-
rouzi, “Image super-resolution via iterative refinement ,” I EEE 
Trans. Pattern Anal. Mach. Intell., vol. 45, no. 4, pp. 4713–4726, 
Apr. 2023, doi: 10.1109/TPAMI.2022.3204461.
[188] W. G. C . Ba
ndara, N. G . Nair, and V. M . Patel, “DDPM-CD: 
Denoising diffusion probabilistic models as feature extractors 
for change detection,” 2022, arXiv:2206.11892.
[189]
 M. Czerk
awski and C . Tachtatzis, “ E xploring the capabil -
ity of text-to-image diffusion models with structural edge 
gui
dance for multispectral satellite image inpainting ,” IEEE 
Geosci. Remote Sens. Lett. , vol. 21, pp. 1–5, 2024, doi: 10.1109/
LGRS.2024.3370212.
[190] Z. Wang, Z. H
ao, Y. Zhang, Y. Feng, and Y. Guo, “Up-Diff: La-
tent diffusion model for remote sensing urban prediction ,” 
IE
EE Geosci. Remote Sens. Lett. , vol. 22 , pp. 1–5 , 2025, doi: 
10.1109/LGRS.2024.3520133.
[191] Q. Meng, W. Sh
i, S. Li, and L . Zhang, “PanDiff: A novel pan -
sharpening method based on denoising diffusion probabilistic 
mode
l,” IEEE Trans. Geosci. Remote Sens., vol. 61, pp. 1–17, 2023, 
doi: 10.1109/TGRS.2023.3279864.
[192] D. Toc
hilkin et al., “TripoSR: Fast 3D object reconstruction 
from a single image,” 2024, arXiv:2403.02151.
[193] Z. Liu, H. Ma
o, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. 
Xie, “A ConvNet for the 2020s,” in Proc. IEEE/CVF Conf. Com -
put. Vis. Pattern Recognit. (CVPR), 2022, p p. 11,966–11,976, doi: 
10.1109/CVPR52688.2022.01167.
[194] X. Li, J. Ding, a
nd M. Elhoseiny, “VRSBench: A versatile vision-
language benchmark dataset for remote sensing image under -
standing,” in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS) Data-
se
ts Benchmarks Track, 202 4, pp. 1–13. 
[195] Z. Yuan, Z. 
Xiong, L. Mou, and X. X . Zhu, “ChatEarthNet: A 
global-scale image-text dataset empowering vision-language 
geo-foundation models,” Earth Syst. Sci. Data, vol. 17, no. 3, pp. 
1245–1263, 2025, doi: 10.5194/essd-17-1245-2025.
[196]
 B. Zhou e
t al., “UrBench: A comprehensive benchmark for evalu-
ating large multimodal models in multi-view urban scenarios,” 
in P
roc. AAAI Conf. Artif. Intell., 2025, vol. 39, pp. 10,707–10,715.
[197] J. Ge, Y. Zhen
g, K. Guo, and J. Liang, “RSTeller: Scaling up vi -
sual language modeling in remote sensing with rich linguistic 
sem
antics from openly available data and large language mod-
els,” 2024, a rXiv:2408.14744.
[198] J. Ding e
t al., “Object detection in aerial images: A large-scale 
benchmark and challenges ,” IEEE Trans. Pattern Anal. Mach. In-
tell., vol. 44, n o. 11, pp. 7778–7796, Nov. 2022, doi: 10.1109/
TPAMI.2021.3117983.
[199] K. Li, G. Wan, G. Che
ng, L. Meng, and J . Han, “Object detec-
tion in optical remote sensing images: A survey and a new 
benc
hmark,” ISPRS J. Photogramm. Remote Sens. , vol. 159, pp. 
296–307, Jan. 2020, doi: 10.1016/j.isprsjprs.2019.11.023.
[200] “Cha
tGPT,” OpenAI. Accessed: Oct. 30, 2024. [Online]. Avail -
able: https://openai.com/chatgpt/overview/ 
[2
01] S. Vuja
sinovic ´, S. Becker, T. Breuer, S. Bullinger, N. Scherer-
Negenborn, and M. Arens, “Integration of the 3D environment 
for UAV onboard visual object tracking ,” Appl. Sci., vol. 10, no. 
21, p. 7622, 2020, doi: 10.3390/app10217622.
[202] S. Waqa
s Zamir et al., “iSAID: A large-scale dataset for instance 
segmentation in aerial images,” in Proc. IEEE/CVF Conf. Com -
put. Vis. Pattern Recognit. (CVPR) Workshops, 201 9, pp. 28–37.
[203] “2D s
emantic labeling contest - Potsdam,” Int. Soc. for Pho -
togrammetry and Remote Sens. (ISPRS), Hannover, Germany , 
201
2. [Online]. Available: https://www.isprs.org/resources/
datasets/benchmarks/UrbanSemLab/2d-sem-label-potsdam.
aspx
[2
04] Y. Lon
g, Y. Gong, Z. Xiao, and Q. Liu, “Accurate object localiza-
tion in remote sensing images based on convolutional neural 
net
works,” IEEE Trans. Geosci. Remote Sens. , vol. 55, no. 5, pp. 
2486–2498, May 2017, doi: 10.1109/TGRS.2016.2645610.
[205] “2D s
emantic labeling - Vaihingen data,” Int. Soc. for Photo -
grammetry and Remote Sens. (ISPRS), Hannover, Germany , 
201
2. [Online]. Available: https://www.isprs.org/resources/
datasets/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.
aspx
[206]
 M.-
R. Hsieh, Y.-L. Lin, and W. H . Hsu, “Drone-based object 
counting by spatially regularized regional proposal network ,” 
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
321 
in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV) , 2017, pp. 
4165–4173, doi: 10.1109/ICCV.2017.446.
[207] Y. Zha
ng, Y. Yuan, Y. Feng, and X . Lu, “Hierarchical and ro -
bust convolutional neural network for very high-resolution 
rem
ote sensing object detection ,” IEEE Trans. Geosci. Remote 
Sens., vol. 57, no. 8 , pp. 5535–5548, Aug. 2019, doi: 10.1109/
TGRS.2019.2900302.
[208] H. Chen a
nd Z. Shi, “A spatial-temporal attention-based meth-
od and a new dataset for remote sensing image change detec -
tion,” Remote Sens., v ol. 12, no. 10, p. 1662, 2020, doi: 10.3390/
rs12101662.
[209] A. Robi
cquet, A. Sadeghian, A. Alahi, and S. Savarese, “Learn-
ing social etiquette: Human trajectory understanding in 
crow
ded scenes,” in Proc. Eur. Conf. Comput. Vis. (ECCV), 2016, 
pp. 549–565.
[210] P. Zhu et 
al., “Detection and tracking meet drones challenge ,” 
IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 11, pp. 7380–
7399, Nov. 2022, doi: 10.1109/TPAMI.2021.3119563.
[211] Z. Liu, L. Yua
n, L. Weng, and Y. Yang, “A high resolution opti-
cal satellite image dataset for ship recognition and some new 
base
lines,” in Proc. Int. Conf. Pattern Recognit. Appl. Methods 
(ICPRAM), 2017, vol. 1, pp. 324–331.
[212] J. Wang, Z. Z
heng, A. Ma, X. Lu, and Y. Zhong, “LoveDA: A re-
mote sensing land-cover dataset for domain adaptive semantic 
seg
mentation,” in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS) 
Datasets Benchmarks Track, 2021, pp. 1–12. 
[213] “Cr
owdAI mapping challenge: CrowdAI dataset.” AIcrowd. Ac-
cessed: Oct. 30, 2024. [Online]. Available: https://www.crow -
dai.org/challenges/mapping-challenge/dataset_files
[214] L. Liu a
nd H. Li, “Lending orientation to neural networks for 
cross-view geo-localization,” in Proc. IEEE/CVF Conf. Com -
put. Vis. Pattern Recognit. (CVPR) , 201 9, pp. 5624 –5633, doi: 
10.1109/CVPR.2019.00577.
[215] S. Work
man, R. Souvenir, and N . Jacobs, “Wide-area image 
geolocalization with aerial reference imagery,” in Proc. IEEE/
CVF Int. Conf. Comput. Vis. (ICCV) , 2015, pp. 3961–3969, doi: 
10.1109/ICCV.2015.451.
[216]
 R. Ani
l et al., “Gemini: A family of highly capable multimodal 
models,” 2023, arXiv:2312.11805.
[217] Y. Wang, N
. A. A. Braham, Z. Xiong, C. Liu, C. M. Albrecht, and 
X. X. Zhu, “SSL4EO-S12: A large-scale multimodal, multitem -
poral dataset for self-supervised learning in earth observation 
[Sof
tware and Data Sets],” IEEE Geosci. Remote Sens. Mag., vol. 11, 
no. 3, pp. 98–106, 2023, doi: 10.1109/MGRS.2023.3281651.
[218] M. Men
dieta, B. Han, X. Shi, Y. Zhu, and C . Chen, “Towards 
geospatial foundation models via continual pretraining ,” 
in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV) , 2023, pp. 
16,760–16,770, doi: 10.1109/ICCV51070.2023.01541.
[219] H. Li et al
., “RSI-CB: A large scale remote sensing image 
classification benchmark via crowdsource data ,” 2017, arX-
iv:1705.10450.
[220
] A. Q. Ji
ang et al., “Mixtral of experts ,” 2024, arXiv:2401.04088.
[221] P. Shar
ma, N. Ding, S. Goodman, and R. Soricut, “Conceptual 
captions: A cleaned, hypernymed, image alt-text dataset for 
automatic image captioning,” in Proc. 56th Annu. Meet. Assoc. 
Comput. Linguist. (ACL), 2018, pp. 2556–2565.
[222
] Y. Han, X. Ya
ng, T. Pu, and Z. Peng, “Fine-grained recognition 
for oriented ship against complex scenes in optical remote 
sensing images ,” IEEE Trans. Geosci. Remote Sens. , vol. 60 , pp. 
1–18, 2021, doi: 10.1109/TGRS.2021.3123666.
[223]
 S. Wei, X. Ze
ng, Q. Qu, M. Wang, H. Su, and J. Shi, “HRSID: A 
high-resolution SAR images dataset for ship detection and in -
stance segmentation,” IE EE Access, vol. 8, pp. 120,234–120,254, 
2020, doi: 10.1109/ACCESS.2020.3005861.
[224] T. Zha
ng et al., “SAR ship detection dataset (SSDD): Official 
release and comprehensive data analysis,” Remote Sens., vol. 13, 
no. 18, p. 3690, 2021, doi: 10.3390/rs13183690.
[225] X. Sun et a
l., “FAIR1M: A benchmark dataset for fine-grained 
object recognition in high-resolution remote sensing imagery,” 
ISPRS J. Photogramm. Remote Sens., vol. 184, pp. 116–130, Feb. 
2022, doi: 10.1016/j.isprsjprs.2021.12.004.
[226] X. Zhen
g, B. Wang, X. Du, and X. Lu, “Mutual attention incep-
tion network for remote sensing visual question answering ,” 
IE
EE Trans. Geosci. Remote Sens. , vol. 60 , pp. 1–14, 2021, doi: 
10.1109/TGRS.2021.3079918.
[227] G. Hoxh
a and F . Melgani , “A novel SVM-based decoder for 
remote sensing image captioning ,” IEEE Trans. Geosci. Remote 
Sens., vol. 60, pp. 1–14, 2022, doi: 10.1109/TGRS.2021.3105004.
[228] L. Xu et al
., “RS-GPT4V: A unified multimodal instruction-
following dataset for remote sensing image understanding ,” 
2024, arXiv:2406.12479.
[229] M. Rah
nemoonfar, T. Chowdhury, A. Sarkar, D. Varshney, M. 
Yari, and R. R. Murphy, “FloodNet: A high resolution aerial imag-
ery dataset for post flood scene understanding,” IE EE Access, vol. 
9, pp. 89,644–89,654, 2021, doi: 10.1109/ACCESS.2021.3090981.
[230] L. Bash
mal, Y. Bazi, M. M. Al Rahhal, M. Zuair, and F. Melgani, 
“CapERA: Captioning events in aerial videos ,” Remote Sens. , 
vol. 15, no. 8, p. 2139, 2023, doi: 10.3390/rs15082139.
[231] S. Tian, A. Ma, Z. Z
heng, and Y. Zhong, “Hi-UCD: A large-scale 
dataset for urban semantic change detection in remote sensing 
imagery,” 2020, arXiv:2011.03247.
[232]
 D. Wang e
t al., “SAMRS: Scaling-up remote sensing segmenta -
tion dataset with segment anything model,” i n Proc. Adv. Neu-
ral Inf. Process. Syst. (NeurIPS) Datasets Benchmarks Track, 2023, 
v
ol. 36, pp. 8815–8827.
[233] Y. Wenq
i et al., “MAR20: A benchmark for military aircraft rec-
ognition in remote sensing images ,” Na tl. Remote Sens. Bull. , 
vol. 27, no. 12, pp. 2688–2696, 2024.
[234] S. Ji, S. Wei, an
d M . Lu, “Fully convolutional networks for 
multisource building extraction from an open aerial and sat -
ellite imagery data set ,” IE EE Trans. Geosci. Remote Sens. , vol. 
57, no. 1 , pp. 574 –586, Jan. 2019 , doi: 10.1109/TGRS.2018.  
2858817.
[23
5] Y. Lyu, G. V
osselman, G.-S. Xia, A. Yilmaz, and M. Y . Yang, 
“UA
Vid: A semantic segmentation dataset for UAV imagery ,” 
ISPRS J. Photogramm. Remote Sens. , vol. 165, pp. 108–119, Jul. 
2020, doi: 10.1016/j.isprsjprs.2020.05.009.
[236] H. Zhu, X. Che
n, W. Dai, K. Fu, Q. Ye, and J . Jiao, “Orien -
tation robust object detection in aerial images using deep 
conv
olutional neural network,” in Proc. IEEE Int. Conf. Im -
age Process. (ICIP) , 201 5, pp. 3735 –3739, doi: 10.1109/
ICIP.2015.7351502.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE    SEPTEMBER 2025
322 
[237] K. Li, G. Chen g, S. Bu, and X . You, “Rotation-insensitive and 
context-augmented object detection in remote sensing imag -
es,” IEEE Trans. Geosci. Remote Sens. , v ol. 56, no. 4, pp. 2337–
2348, Apr. 2018, doi: 10.1109/TGRS.2017.2778300.
[238] “Ae
rial-Mancar,” InfiRay, Yantai, China, 2021. [Online]. Avail-
able: http://openai.raytrontek.com/apply/Aerial_mancar.html/
[2
39] Y. Di, Z. Jia
ng, and H. Zhang, “A public dataset for fine-grained 
ship classification in optical remote sensing images ,” Remote 
Sens., vol. 13, no. 4, p. 747, 2021, doi: 10.3390/rs13040747.
[240] “In
frared-security ,” InfiRay, Yantai, China, 2021 . [Online]. 
Available: http://openai.raytrontek.com/apply/Infrared_secu -
rity.html/ 
[24
1] “Do
uble-light-vehicle,” InfiRay, Yantai, China, 2021. [Online]. 
Available: http://openai.raytrontek.com/apply/Double_light_
vehicle.html/
[242] “Se
a-shipping,” InfiRay, Yantai, China, 2021 . [Online]. Avail-
able: http://openai.raytrontek.com/apply/Double_light_  
vehicle.html/
[24
3] M. Zha
ng, F. Chen, and B . Li, “ Multistep question-driven 
visual question answering for remote sensing ,” IEEE Trans. 
Geosci. Remote Sens. , vol. 61 , pp. 1– 12, 2023, doi: 10.1109/
TGRS.2023.3312479.
[244] Y. Di, Z. Jia
ng, H. Zhang, and G . Meng, “A public dataset for 
ship classification in remote sensing images ,” Proc. SPIE, vol. 
11155, pp. 515–521, Oct. 2019.
[245] J. Suo, T. Wan
g, X. Zhang, H. Chen, W. Zhou, and W. Shi, “HIT-
UAV: A high-altitude infrared thermal dataset for unmanned 
aerial vehicle-based object detection ,” Sci. Data, vol. 10, no. 1, 
p. 227, 2023, doi: 10.1038/s41597-023-02066-6.
[246]
 “Oc
eanic-Ship,” Center for Optics Research and Engineering of 
Shandong University, Qingdao, Shandong, China, 2020. [Online]. 
Available: http://www.gxzx.sdu.edu.cn/info/1133/2174.htm/
[247] Y. Li et al
., “STAR: A first-ever dataset and a large-scale bench -
mark for scene graph generation in large-size satellite imag -
ery,” IEEE Trans. Pattern Anal. Mach. Intell. , v ol. 47, no. 3 , pp. 
1832–1849, Mar. 2025, doi: 10.1109/TPAMI.2024.3508072.
[248] C. Pang, J. Wu, J. D
ing, C. Song, and G.-S . Xia, “Detecting 
building changes with off-nadir aerial images ,” Sci. China Inf. 
Sci., vol. 66, no. 4, 2023, Art. no. 140306, doi: 10.1007/s11432-
022-3691-4.
[249] R. Xia et a
l., “Crtranssar: A visual transformer based on contex-
tual joint representation learning for sar ship detection,” Re mote 
Sens., vol. 14, no. 6, p. 1488, 2022, doi: 10.3390/rs14061488.
[250] X.-Y. To
ng, G.-S. Xia, and X. X . Zhu, “Enabling country-scale 
land cover mapping with meter-resolution satellite imagery ,” 
ISPRS J. Photogramm. Remote Sens., vol. 196, pp. 178–196, 2023.
[251] C. Wu, L. Zha
ng, and L . Zhang, “A scene change detection 
framework for multi-temporal very high resolution remote 
sensing images ,” Signal Process. , vol. 124 , pp. 184 –197, Jul. 
2016, doi: 10.1016/j.sigpro.2015.09.020.
[252]
 I. Demi
r et al., “DeepGlobe 2018: A challenge to parse the earth 
through satellite images,” in Proc. IEEE/CVF Conf. Comput. Vis. 
Pattern Recognit. (CVPR) Workshops, 2018, pp. 172–17209, doi: 
10.1109/CVPRW.2018.00031.
[253]
 X.-Y. To
ng et al., “Land-cover classification with high-resolu -
tion remote sensing images using transferable deep models ,” 
Re
mote Sens. Environ., vol. 237, Feb. 2020, Art. no. 111322, doi: 
10.1016/j.rse.2019.111322.
[254] S. Verm
a, A. Panigrahi, a n d S . Gupta, “QFabric: Multi-task 
change detection dataset,” in Proc. IEEE/CVF Conf. Comput. Vis. 
Pattern Recognit. (CVPR) Workshops, 2021, pp. 1052–1061, doi: 
10.1109/CVPRW53098.2021.00116.
[255]
 L. Shen e
t al., “S2Looking: A satellite side-looking dataset for 
building change detection ,” Remote Sens. , vol. 13 , no. 24 , p. 
5094, 2021, doi: 10.3390/rs13245094.
[256] R. Gupt
a et al., “Creating xbd: A dataset for assessing building 
damage from satellite imagery,” in Proc. IEEE/CVF Conf. Com -
put. Vis. Pattern Recognit. (CVPR) Workshops, 201 9, pp. 10–17.
[257] J. Robe
rts, K. Han, and S. Albanie, “SATIN: A multi-task meta-
dataset for classifying satellite imagery using vision-language 
mode
ls,” 2023, arXiv:2304.11619.
[258] X. An, J. Sun, Z. Gui, 
and W. He, “COREval: A comprehen -
sive and objective benchmark for evaluating the remote sens -
ing capabilities of large vision-language models ,” 20 24, arX-
iv:2411.18145.
[25
9] M. S. Da
nish et al., “GEOBench-VLM: Benchmarking vision-
language models for geospatial tasks ,” 2024, arXiv:2411.19325.
[260] Z. Xiong
, F. Zhang, Y. Wang, Y. Shi, and X. X. Zhu, “EarthNets: 
Empowering artificial intelligence for earth observation ,” 
IEEE Geosci. Remote Sens. Mag., early access, Oct. 23, 2024, doi: 
10.1109/MGRS.2024.3466998.
[261] C. Zha
ng and S . Wang, “Good at captioning bad at counting: 
Benchmarking GPT-4V on earth observation data,” in Proc. 
IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) Workshops, 
2024, pp. 7839–7849, doi: 10.1109/CVPRW63382.2024.00780.
[262]
 T. N. Mu
ndhenk, G. Konjevod, W. A. Sakla, and K. Boakye, “A 
large contextual dataset for classification, detection and count-
ing of cars with deep learning,” i n Proc. Eur. Conf. Comput. Vis. 
(ECCV), 2016, pp. 785–800.
[263] B. G. 
Weinstein et al., “A benchmark dataset for canopy crown 
detection and delineation in co-registered airborne RGB, Li -
DAR and hyperspectral imagery from the national ecological 
obs
ervation network ,” PLoS Comput. Biol., vol. 17, no. 7, 2021, 
Art. no. e1009180.
[264] J. A. Ei
kelboom et al., “Improving the precision and accuracy 
of animal population estimates with aerial image object detec-
tion,” Methods Ecol. Evol., v ol. 10, no. 11, pp. 1875–1887, 2019.
[265] P. W. Koh e
t al., “Wilds: A benchmark of in-the-wild distribu -
tion shifts,” in Proc. Int. Conf. Mach. Learn. (ICML) , 20 21, pp. 
5637–5664.
[266] G. Mach
ado et al., “AiRound and CV-BrCT: Novel multiview 
datasets for scene classification ,” IEEE J. Sel. Topics Appl. Earth 
Observ. Remote Sens., vol. 14, pp. 488–503, 2020, doi: 10.1109/
JSTARS.2020.3033424.
[267] “De
forestation-satellite-imagery dataset ,” CSE499 Deforestation 
Satellite, 2024. [Online]. Available: https://universe.roboflow.
com/cse499deforestationsatellite/deforestation-satellite-imag -
ery-335n4
[268
] G. Baie
r, A. Deschemps, M. Schmitt, and N. Yokoya, “Synthe-
sizing optical and SAR imagery from land cover maps and aux-
iliary raster data ,” IE EE Trans. Geosci. Remote Sens., vol. 60, pp. 
1–12, 2022, doi: 10.1109/TGRS.2021.3068532.
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.
SEPTEMBER 2025    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE                                                        
323 
[269] A. Shah, L. T homas, and M . Maskey, “Marine debris dataset 
for object detection in planetscope imagery,” NASA, Washing -
ton, DC, USA, 2021. [On line]. Available: https://cmr.earthdata.
nasa.gov/search/concepts/C2781412735-MLHUB.html
[270] X. X. Zhu e
t al., “So2Sat LCZ42: A benchmark data set for the 
classification of global local climate zones [Software and Data 
Sets],” IEEE Geosci. Remote Sens. Mag., vol. 8, no. 3, pp. 76–89, 
Sep. 2020, doi: 10.1109/MGRS.2020.2964708.
[271] “For
est damages – Larch casebearer,” The Swedish Forest Agen-
cy, Jönköping, Sweden, 2021 . [On line]. Available: https://lila.
science/datasets/forest-damages-larch-casebearer/
[272] X. Zha
ng, W. Yu, M.-O. Pun, and W. Shi, “Cross-domain land-
slide mapping from large-scale remote sensing images using 
prot
otype-guided domain-aware progressive representation 
learning,” ISPRS J. Photogramm. Remote Sens., vol. 197, pp. 1–17, 
Mar. 2023, doi: 10.1016/j.isprsjprs.2023.01.018.
[273] D. R. Ca
mbrin and P . Garza, “Quakeset: A dataset and low-
resource models to monitor earthquakes through sentinel-1 ,” 
in Proc. Int. ISCRAM Conf., 2024, pp. 1–12.
[274] S. Shen, S. S
eneviratne, X. Wanyan, and M . Kirley, “FireRisk: 
A remote sensing dataset for fire risk assessment with bench -
marks using supervised and self-supervised learning,” i n Proc. 
Int. Conf. Digit. Image Comput.: Tech. Appl. (DICTA) , 2023, pp. 
189–196, doi: 10.1109/DICTA60407.2023.00034.
[275] C. Tund
ia, R. Kumar, O. Damani, and G . Sivakumar, “FPCD: 
An open aerial VHR dataset for farm pond change detection ,” 
2023, arXiv:2302.14554.
[276] J. Liu, W. Zho
u, H. Guan, and W. Zhao, “Similarity learning for 
land use scene-level change detection ,” IEEE J. Sel. Topics Appl. 
Earth Observ. Remote Sens. , vol. 17, pp. 6501–6513, 2024, doi: 
10.1109/JSTARS.2024.3373401.
[277] V. S. F. G
arnot and L. Landrieu, “Panoptic segmentation of sat-
ellite image time series with convolutional temporal attention 
net
works,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV) , 
2021, pp. 4852–4861.
[278] J. Sher
meyer, T. Hossler, A. Van Etten, D. Hogan, R. Lewis, and 
D. Kim, “RarePlanes: Synthetic data takes flight,” in Proc. IEEE 
Winter Conf. Appl. Comput. Vis. (WACV) , 2020, pp. 207 –217, 
doi: 10.1109/WACV48630.2021.00025.
[279] M. Cord
ts et al., “The cityscapes dataset for semantic urban 
scene understanding ,” in Proc. IEEE/CVF Conf. Comput. Vis. 
Pattern Recognit. (CVPR) , 2016, pp. 3213–3223, doi: 10.1109/
CVPR.2016.350.
[280] J. Hay
s and A. A . Efros, “IM2GPS: Estimating geographic in -
formation from a single image,” i n Proc. IEEE/CVF Conf. Com-
put. Vis. Pattern Recognit. (CVPR), 2008, p p. 1–8, doi: 10.1109/
CVPR.2008.4587784.
[281] C. Ert
ler, J. Mislej, T. Ollmann, L. Porzi, G. Neuhold, and Y. 
Kuang, “The mapillary traffic sign dataset for detection and 
classification on a global scale,” in Proc. Eur. Conf. Comput. Vis. 
(ECCV), 2020, pp. 68–84.
[282]
 S. Zhu, T. Yan
g, and C. Chen, “VIGOR: Cross-view image geo-lo-
calization beyond one-to-one retrieval,” in Proc. IEEE/CVF Conf. 
Comput. Vis. Pattern Recognit. (CVPR), 2021, pp. 3640–3649.
[283] M. Chu, Z. Zhe
ng, W. Ji, T. Wang, and T.-S. Chua, “Towards natural 
language-guided drones: Geotext-1652  ben
chmark with spatial  
relation matching ,” in Proc. Eur. Conf. Comput. Vis. (ECCV) , 
2025, pp. 213–231, doi: 10.1109/CVPR46437.2021.00364.
[2
84] Z. Zhen
g, Y. Wei, and Y. Yang, “University-1652: A multi-view 
multi-source benchmark for drone-based geo-localization ,” in 
Proc. ACM Int. Conf. Multimedia, 2020, pp. 1395–1403.
[285] K. Nogue
ira, J. A. Dos Santos, T. Fornazari, T. S. F. Silva, and L. 
P. Morellato, and R. D. S. Torres, “Towards vegetation species 
discrimination by using data-driven descriptors,” in Proc. IAPR 
Workshop Pattern Recognit. Remote Sens. (PRRS), 2016, pp. 1–6, 
doi: 10.1109/PRRS.2016.7867024.
[286]
 Q. D. C
ao and Y. Choe, “Building damage annotation on post-
hurricane satellite imagery based on convolutional neural net -
works,” 2018, a rXiv:1807.01688.
[287] S. Basu, S. G
anguly, S. Mukhopadhyay, R. DiBiano, M. Karki, 
and R. Nemani, “DeepSat: A learning framework for satellite 
imagery,” in Proc. ACM SIGSPATIAL Int. Conf. Adv. Geogr. Inf. 
Syst. (SIGSPATIAL), 2015, pp. 1–10.
[288] “Ai
rbus wind turbine patches.” Kaggle. Accessed: Oct. 30, 
2024. [Online]. Available: https://www.kaggle.com/datasets/
airbusgeo/airbus-wind-turbines-patches  
[289]
 A. A. B . Ja
cques, A. B. Diallo, and E . Lord, “Towards the cre -
ation of a canadian land-use dataset for agricultural land clas -
sification,” in Proc. Can. Symp. Remote Sens.: Understanding Our 
Wo
rld: Remote Sens. Sustain. Future, 2021, vol. 4, p. 6.
[290] Y. Hua, L. Mou, P. Ji
n, and X. X. Zhu, “MultiScene: A large-scale 
dataset and benchmark for multiscene recognition in single 
aerial images ,” IEEE Trans. Geosci. Remote Sens. , vol. 60 , pp. 
1–13, 2021, doi: 10.1109/TGRS.2021.3110314.
[291]
 R. Ba, C. Chen, J. Yu
an, W. Song, and S. Lo, “SmokeNet: Satel-
lite smoke scene detection using convolutional neural network 
wit
h spatial and channel-wise attention ,” Remote Sens., vol. 11, 
no. 14, p. 1702, 2019, doi: 10.3390/rs11141702.
[292] O. A . 
Penatti, K. Nogueira, and J. A . Dos Santos , “Do deep 
features generalize from everyday objects to remote sensing 
and aerial scenes domains?” in Proc. IEEE/CVF Conf. Comput. 
Vis. Pattern Recognit. (CVPR) Workshops , 2015, pp. 44 –51, doi: 
10.1109/CVPRW.2015.7301382.
[293]
 R. Ham
mell. “Ships in satellite imagery.” Kaggle. Accessed: 
Oct. 30, 2024. [Online]. Available: https://www.kaggle.com/
datasets/rhammell/ships-in-satellite-imagery  
[294]
 J. F. R
osser, D. G. Leibovici, and M. J. Jackson, “Rapid flood in-
undation mapping using social media, remote sensing and topo-
graphic data,” Nat. Hazards, v ol. 87, pp. 103–120, Jan. 2017.
[295] J. Li et al
., “Social Media: New perspectives to improve remote 
sensing for emergency response ,” Proc. IEEE, vol. 105, no. 10, 
pp. 1900–1912, Oct. 2017, doi: 10.1109/JPROC.2017.2684460.
[296] Y. Zhai e
t al., “Investigating the catastrophic forgetting in mul-
timodal large language model fine-tuning,” in Proc. Conf. Parsi-
mony Learn., 2024, v ol. 234, pp. 202–227.
[297] D. Zhu et a
l., “Model tailor: Mitigating catastrophic forget -
ting in multi-modal large language models,” i n Proc. Int. Conf. 
Mach. Learn. (ICML), 2024, pp. 62,581–62,598. GRS
Authorized licensed use limited to: University of Illinois. Downloaded on January 31,2026 at 22:33:48 UTC from IEEE Xplore.  Restrictions apply.